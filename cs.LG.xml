<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#21151;&#33021;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#20808;&#39564;&#24341;&#23548;&#19979;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;GP-UNIT&#65289;&#12290;&#36890;&#36807;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#31867;&#21035;&#26465;&#20214; GAN &#20013;&#25552;&#28860;&#29983;&#25104;&#20808;&#39564;&#65292;GP-UNIT &#33021;&#22815;&#22312;&#25509;&#36817;&#30340;&#39046;&#22495;&#21644;&#36828;&#31163;&#30340;&#39046;&#22495;&#20043;&#38388;&#25191;&#34892;&#26377;&#25928;&#30340;&#32763;&#35793;&#12290;&#23545;&#20110;&#25509;&#36817;&#30340;&#39046;&#22495;&#65292;GP-UNIT &#21487;&#20197;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2306.04636</link><description>&lt;p&gt;
GP-UNIT&#65306;&#29983;&#25104;&#20808;&#39564;&#29992;&#20110;&#22810;&#21151;&#33021;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation. (arXiv:2306.04636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#21151;&#33021;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#20808;&#39564;&#24341;&#23548;&#19979;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;GP-UNIT&#65289;&#12290;&#36890;&#36807;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#31867;&#21035;&#26465;&#20214; GAN &#20013;&#25552;&#28860;&#29983;&#25104;&#20808;&#39564;&#65292;GP-UNIT &#33021;&#22815;&#22312;&#25509;&#36817;&#30340;&#39046;&#22495;&#21644;&#36828;&#31163;&#30340;&#39046;&#22495;&#20043;&#38388;&#25191;&#34892;&#26377;&#25928;&#30340;&#32763;&#35793;&#12290;&#23545;&#20110;&#25509;&#36817;&#30340;&#39046;&#22495;&#65292;GP-UNIT &#21487;&#20197;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#35265;&#35777;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#65292;&#23427;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#25104;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20004;&#20010;&#35270;&#35273;&#39046;&#22495;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#21508;&#31181;&#39046;&#22495;&#20043;&#38388;&#30340;&#24378;&#22823;&#26144;&#23556;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#35270;&#35273;&#24046;&#24322;&#24040;&#22823;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#21151;&#33021;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#20808;&#39564;&#24341;&#23548;&#19979;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65288;GP-UNIT&#65289;&#65292;&#23427;&#25552;&#39640;&#20102;&#29616;&#26377;&#36716;&#25442;&#27169;&#22411;&#30340;&#36136;&#37327;&#12289;&#36866;&#29992;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290; GP-UNIT &#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;&#39044;&#20808;&#35757;&#32451;&#30340;&#31867;&#21035;&#26465;&#20214; GAN &#20013;&#25552;&#28860;&#29983;&#25104;&#20808;&#39564;&#65292;&#20197;&#26500;&#24314;&#31895;&#30053;&#32423;&#21035;&#30340;&#36328;&#22495;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#23558;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#24212;&#29992;&#20110;&#23545;&#25239;&#24615;&#36716;&#25442;&#65292;&#20197;&#25366;&#25496;&#32454;&#33410;&#32423;&#21035;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#22810;&#32423;&#20869;&#23481;&#23545;&#24212;&#20851;&#31995;&#65292;GP-UNIT &#33021;&#22815;&#22312;&#25509;&#36817;&#30340;&#39046;&#22495;&#21644;&#36828;&#31163;&#30340;&#39046;&#22495;&#20043;&#38388;&#25191;&#34892;&#26377;&#25928;&#30340;&#32763;&#35793;&#12290;&#23545;&#20110;&#25509;&#36817;&#30340;&#39046;&#22495;&#65292;GP-UNIT &#21487;&#20197;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have witnessed many successful unsupervised image-to-image translation models that learn correspondences between two visual domains without paired data. However, it is still a great challenge to build robust mappings between various domains especially for those with drastic visual discrepancies. In this paper, we introduce a novel versatile framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), that improves the quality, applicability and controllability of the existing translation models. The key idea of GP-UNIT is to distill the generative prior from pre-trained class-conditional GANs to build coarse-level cross-domain correspondences, and to apply the learned prior to adversarial translations to excavate fine-level correspondences. With the learned multi-level content correspondences, GP-UNIT is able to perform valid translations between both close domains and distant domains. For close domains, GP-UNIT can be condition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;</title><link>http://arxiv.org/abs/2306.04633</link><description>&lt;p&gt;
&#23545;&#27604;&#25552;&#21319;&#65306;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#23454;&#29616;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#29289;&#20307;&#25968;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#65292;&#36890;&#36807;&#38480;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;(Messy Rooms)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#19977;&#32500;&#25968;&#25454;&#38598;&#65292;&#19977;&#32500;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20108;&#32500;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#26377;&#25928;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#22330;&#34920;&#31034;&#23558;2D&#20998;&#27573;&#21521;&#19978;&#25552;&#21319;&#21040;3D&#65292;&#24182;&#23558;&#23427;&#20204;&#36890;&#36807;&#24930;-&#24555;&#23545;&#27604;&#34701;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#40723;&#21169;&#36328;&#24103;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29289;&#20307;&#30340;&#22330;&#26223;&#30340;&#24930;-&#24555;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#29289;&#20307;&#25968;&#37327;&#25110;&#36328;&#24103;&#29289;&#20307;&#36319;&#36394;&#36827;&#34892;&#35774;&#32622;&#19978;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#24930;-&#24555;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Messy Rooms&#30340;&#26032;&#30340;&#21322;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22330;&#26223;&#20013;&#26368;&#22810;&#26377;500&#20010;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ScanNet&#12289;Hypersim&#21644;Replica&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#28210;&#26579;&#22120;&#30340;&#23454;&#26102;&#36924;&#30495;&#24230;&#25552;&#39640;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#23884;&#20837;&#24335;&#21644;&#31227;&#21160;GPU&#31561;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#35774;&#22791;&#65292;&#24182;&#33021;&#22815;&#22312;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#30446;&#26631;&#22270;&#20687;&#38598;&#22806;&#35266;&#30340;&#24544;&#23454;&#20877;&#29616;&#65292;&#25552;&#20379;&#19982;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04629</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#28210;&#26579;&#22120;&#30340;&#23454;&#26102;&#36924;&#30495;&#24230;&#25552;&#39640;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Shaders for Real-Time Realism Enhancement. (arXiv:2306.04629v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#28210;&#26579;&#22120;&#30340;&#23454;&#26102;&#36924;&#30495;&#24230;&#25552;&#39640;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#23884;&#20837;&#24335;&#21644;&#31227;&#21160;GPU&#31561;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#35774;&#22791;&#65292;&#24182;&#33021;&#22815;&#22312;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#30446;&#26631;&#22270;&#20687;&#38598;&#22806;&#35266;&#30340;&#24544;&#23454;&#20877;&#29616;&#65292;&#25552;&#20379;&#19982;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20110;&#29616;&#23454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#26102;&#21644;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#22330;&#26223;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20195;&#20215;&#24456;&#39640;&#12290;&#36825;&#20123;&#26041;&#27861;&#21482;&#26377;&#22312;&#38271;&#26102;&#38388;&#36816;&#34892;&#21644;&#39640;&#24102;&#23485;&#12289;&#39640;&#20869;&#23384;&#21644;&#39640;&#21151;&#29575;&#35201;&#27714;&#19979;&#25165;&#33021;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65306;&#19968;&#31181;&#39640;&#24615;&#33021;&#12289;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#28210;&#26579;&#22120;&#30340;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23884;&#20837;&#24335;&#21644;&#31227;&#21160;GPU&#12290;&#25152;&#25552;&#20986;&#30340;&#21487;&#23398;&#20064;&#28210;&#26579;&#31649;&#36947;&#21253;&#25324;&#21487;&#24494;&#20998;&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#23545;&#25239;&#30446;&#26631;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#30446;&#26631;&#22270;&#20687;&#38598;&#22806;&#35266;&#30340;&#24544;&#23454;&#20877;&#29616;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#12290;&#35813;&#28210;&#26579;&#31649;&#36947;&#38024;&#23545;&#30446;&#26631;&#35774;&#22791;&#36827;&#34892;&#20102;&#39640;&#24230;&#20248;&#21270;&#65292;&#25552;&#20379;&#26102;&#38388;&#31283;&#23450;&#12289;&#27604;&#23454;&#38469;&#26102;&#38388;&#26356;&#24555;&#30340;&#32467;&#26524;&#65292;&#19982;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#36136;&#37327;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of realism enhancement methods, particularly in real-time and resource-constrained settings, has been frustrated by the expense of existing methods. These achieve high quality results only at the cost of long runtimes and high bandwidth, memory, and power requirements. We present an efficient alternative: a high-performance, generative shader-based approach that adapts machine learning techniques to real-time applications, even in resource-constrained settings such as embedded and mobile GPUs. The proposed learnable shader pipeline comprises differentiable functions that can be trained in an end-to-end manner using an adversarial objective, allowing for faithful reproduction of the appearance of a target image set without manual tuning. The shader pipeline is optimized for highly efficient execution on the target device, providing temporally stable, faster-than-real time results with quality competitive with many neural network-based methods.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#38477;&#32500;&#25216;&#26415;SLCE&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#31867;&#21035;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#20854;&#31867;&#21035;&#20013;&#24515;&#28857;&#26469;&#24037;&#20316;&#12290;&#35813;&#21464;&#25442;&#21487;&#20197;&#22312;&#20302;&#32500;&#24230;&#31354;&#38388;&#20013;&#37325;&#26500;&#26679;&#26412;&#20197;&#26368;&#23567;&#21270;&#20013;&#24515;&#28857;&#37325;&#26500;&#25439;&#22833;&#12290;SLCE&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04622</link><description>&lt;p&gt;
&#21478;&#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#31639;&#27861;&#65306;&#30417;&#30563;&#32447;&#24615;&#20013;&#24515;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yet Another Algorithm for Supervised Principal Component Analysis: Supervised Linear Centroid-Encoder. (arXiv:2306.04622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04622
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#38477;&#32500;&#25216;&#26415;SLCE&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#31867;&#21035;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#20854;&#31867;&#21035;&#20013;&#24515;&#28857;&#26469;&#24037;&#20316;&#12290;&#35813;&#21464;&#25442;&#21487;&#20197;&#22312;&#20302;&#32500;&#24230;&#31354;&#38388;&#20013;&#37325;&#26500;&#26679;&#26412;&#20197;&#26368;&#23567;&#21270;&#20013;&#24515;&#28857;&#37325;&#26500;&#25439;&#22833;&#12290;SLCE&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#38477;&#32500;&#25216;&#26415;&#65292;&#31216;&#20026;&#30417;&#30563;&#32447;&#24615;&#20013;&#24515;&#32534;&#30721;&#65288;SLCE&#65289;&#65292;&#23427;&#26159;&#38750;&#32447;&#24615;&#20013;&#24515;&#32534;&#30721;&#65288;CE&#65289;\citep{ghosh2022supervised}&#30340;&#32447;&#24615;&#23545;&#24212;&#29289;&#12290;SLCE&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#31867;&#21035;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#20854;&#31867;&#21035;&#20013;&#24515;&#28857;&#26469;&#24037;&#20316;&#12290;&#35813;&#21464;&#25442;&#26159;&#19968;&#20010;&#25237;&#24433;&#65292;&#23427;&#37325;&#26500;&#19968;&#20010;&#28857;&#65292;&#20351;&#20854;&#19982;&#30456;&#24212;&#31867;&#21035;&#20013;&#24515;&#28857;&#30340;&#36317;&#31163;&#65288;&#21363;&#20013;&#24515;&#28857;&#37325;&#26500;&#25439;&#22833;&#65289;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#31216;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#35299;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#35299;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20123;&#20851;&#38190;&#25968;&#23398;&#24615;&#36136;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#21644;&#23637;&#31034;&#12290;%&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#19979;&#38477;&#26041;&#27861;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#30340;&#36845;&#20195;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#20540;&#19982;&#20013;&#24515;&#28857;&#37325;&#26500;&#25439;&#22833;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#19982;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#37325;&#26500;&#26679;&#26412;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30456;&#27604;&#65292;&#35813;&#21464;&#25442;&#21487;&#20197;&#22312;&#20302;&#32500;&#24230;&#31354;&#38388;&#20013;&#37325;&#26500;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) \citep{ghosh2022supervised}. SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. %We also provide an iterative solution approach based solving the optimization problem using a descent method. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04621</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23545;&#40784;&#12289;&#25552;&#28860;&#21644;&#25193;&#20805;&#25152;&#26377;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#65292;&#38656;&#38754;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#24050;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#36793;&#32536;&#20998;&#24067;&#30340;&#21306;&#21035;&#65292;&#21069;&#32773;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#19988;&#21487;&#33021;&#19982;&#21518;&#32773;&#19981;&#21516;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20351;&#20266;&#26631;&#31614;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#20559;&#20506;&#65292;&#22914;&#24050;&#26631;&#35760;&#25968;&#25454;&#25110;&#24179;&#34913;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#30830;&#20445;&#25512;&#29702;&#26102;&#30340;&#24179;&#34913;&#26410;&#26631;&#35760;&#20998;&#24067;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#36880;&#28176;&#23558;&#20998;&#31867;&#22120;&#20174;&#21160;&#24577;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#21040;&#24179;&#34913;&#20998;&#24067;&#65307;&#21033;&#29992;&#34987;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#33293;&#24323;&#30340;&#20302;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30340;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65307;&#20197;&#21450;&#19968;&#31181;&#23558;&#26631;&#35760;&#37096;&#20998;&#30340;&#36755;&#20837;&#25968;&#25454;&#25193;&#23637;&#21040;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#23646;&#24615;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#19968;&#31181;&#26356;&#22909;&#30340;&#65292;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#65292;&#20197;&#27492;&#35299;&#20915;&#29616;&#26377;&#26631;&#37327;&#21270;&#26041;&#26696;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04620</link><description>&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;GFlowNets&#29992;&#20110;&#21487;&#25511;&#22810;&#30446;&#26631;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular Design. (arXiv:2306.04620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#23646;&#24615;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#19968;&#31181;&#26356;&#22909;&#30340;&#65292;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#65292;&#20197;&#27492;&#35299;&#20915;&#29616;&#26377;&#26631;&#37327;&#21270;&#26041;&#26696;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20110;&#21407;&#23376;&#20998;&#23376;&#35774;&#35745;&#30340;&#20851;&#27880;&#36880;&#28176;&#21152;&#21095;&#12290;&#22312;&#35774;&#35745;&#33647;&#29289;&#24212;&#29992;&#30340;&#26032;&#21270;&#21512;&#29289;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#20248;&#21270;&#20998;&#23376;&#30340;&#22810;&#20010;&#23646;&#24615;&#65292;&#22914;&#19982;&#30446;&#26631;&#30340;&#32467;&#21512;&#33021;&#12289;&#21487;&#21512;&#25104;&#24615;&#12289;&#27602;&#24615;&#12289;EC50&#31561;&#31561;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#37319;&#29992;&#26631;&#37327;&#21270;&#26041;&#26696;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#20197;&#20559;&#22909;&#20026;&#26465;&#20214;&#30340;&#21333;&#20010;&#30446;&#26631;&#65292;&#20294;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#20855;&#26377;&#20985;&#24418;Pareto&#21069;&#27839;&#30340;&#38382;&#39064;&#19978;&#65292;&#36825;&#31181;&#20943;&#23569;&#26041;&#26696;&#21487;&#33021;&#20250;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#20542;&#21521;&#20110;&#28369;&#21521;&#30446;&#26631;&#31354;&#38388;&#30340;&#26497;&#31471;&#28857;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21478;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20844;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#25511;&#30340;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#20415;&#27839;&#25972;&#20010;Pareto&#21069;&#27839;&#22343;&#21248;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, in-silico molecular design has received much attention from the machine learning community. When designing a new compound for pharmaceutical applications, there are usually multiple properties of such molecules that need to be optimised: binding energy to the target, synthesizability, toxicity, EC50, and so on. While previous approaches have employed a scalarization scheme to turn the multi-objective problem into a preference-conditioned single objective, it has been established that this kind of reduction may produce solutions that tend to slide towards the extreme points of the objective space when presented with a problem that exhibits a concave Pareto front. In this work we experiment with an alternative formulation of goal-conditioned molecular generation to obtain a more controllable conditional model that can uniformly explore solutions along the entire Pareto front.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04618</link><description>&lt;p&gt;
&#37325;&#28201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;: &#22522;&#20934;&#65292;&#20998;&#26512;&#21644;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;(OOD)&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#24448;&#30740;&#31350;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#35774;&#32622;&#26222;&#36941;&#32570;&#20047;&#36275;&#22815;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26500;&#24314;&#26041;&#26696;&#65292;&#30830;&#20445;&#20102;&#26126;&#30830;&#30340;&#21306;&#20998;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BOSS&#65292;&#19968;&#20010;&#28085;&#30422;5&#20010;&#20219;&#21153;&#21644;20&#20010;&#25968;&#25454;&#38598;&#30340;&#29992;&#20110;&#35780;&#20272;OOT&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;&#22522;&#20110;BOSS&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39321;&#33609;&#24494;&#35843;&#30340;ID&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#20856;&#22411;&#31867;&#22411;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#39044;&#27979;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;ID&#25968;&#25454;&#38598;&#19978;&#30340;&#36827;&#23637;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;BOSS&#19978;&#35780;&#20272;&#20102;5&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;OOD&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#19982;ID&#24615;&#33021;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#20102;&#29305;&#21035;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#65288;&#28508;&#22312;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21463;&#21040;&#31995;&#32479;&#35823;&#24046;&#24433;&#21709;&#30340;&#25968;&#25454;&#20013;&#36824;&#21407;&#20986;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04600</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#25968;&#25454;&#35823;&#24046;&#20462;&#27491;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Uncovering solutions from data corrupted by systematic errors: A physics-constrained convolutional neural network approach. (arXiv:2306.04600v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21463;&#21040;&#31995;&#32479;&#35823;&#24046;&#24433;&#21709;&#30340;&#25968;&#25454;&#20013;&#36824;&#21407;&#20986;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#29616;&#35937;&#21644;&#24037;&#31243;&#31995;&#32479;&#30340;&#20449;&#24687;&#36890;&#24120;&#21253;&#21547;&#22312;&#25968;&#25454;&#20013;&#12290; &#28982;&#32780;&#65292;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#27169;&#22411;&#21644;&#23454;&#39564;&#20013;&#30340;&#31995;&#32479;&#24615;&#35823;&#24046;&#25152;&#25439;&#22351;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#21363;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21024;&#38500;&#31995;&#32479;&#35823;&#24046;&#26469;&#25581;&#31034;&#28508;&#22312;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#35299;&#12290; PC-CNN&#32467;&#21512;&#20102;&#31995;&#32479;&#25511;&#21046;&#26041;&#31243;&#21644;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#30340;&#22522;&#30784;&#29616;&#35937;&#65292;&#20363;&#22914;&#32447;&#24615;&#23545;&#27969;&#65292;Burgers&#26041;&#31243;&#21644;&#20108;&#32500;&#28237;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information on natural phenomena and engineering systems is typically contained in data. Data can be corrupted by systematic errors in models and experiments. In this paper, we propose a tool to uncover the spatiotemporal solution of the underlying physical system by removing the systematic errors from data. The tool is the physics-constrained convolutional neural network (PC-CNN), which combines information from both the systems governing equations and data. We focus on fundamental phenomena that are modelled by partial differential equations, such as linear convection, Burgers equation, and two-dimensional turbulence. First, we formulate the problem, describe the physics-constrained convolutional neural network, and parameterise the systematic error. Second, we uncover the solutions from data corrupted by large multimodal systematic errors. Third, we perform a parametric study for different systematic errors. We show that the method is robust. Fourth, we analyse the physical properti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#26377;&#25928;&#38477;&#20302;&#23545;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#21435;&#20559;&#32622;&#65306;&#23569;&#26679;&#26412;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#38477;&#20302;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#26377;&#25928;&#38477;&#20302;&#23545;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#27969;&#30340;&#21435;&#20559;&#32622;&#25216;&#26415;&#22823;&#22810;&#38598;&#20013;&#22312;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#21482;&#35201;&#23548;&#20837;10&#20010;&#21435;&#20559;&#32622;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21487;&#26174;&#33879;&#20943;&#23569;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#22240;&#27492;&#26159;&#39640;&#24230;&#21487;&#34892;&#21644;&#23454;&#29992;&#30340;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04595</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization Across Observation Shifts in Reinforcement Learning. (arXiv:2306.04595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35266;&#27979;&#36716;&#31227;&#27867;&#21270;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#23545;&#20110;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;&#24212;&#29992;&#21644;&#36328;&#29615;&#22659;&#36716;&#31227;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#22522;&#20110;&#20223;&#30495;&#22120;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#21464;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#20215;&#24230;&#37327;&#30340;&#26032;&#22411;&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#35266;&#27979;&#36716;&#31227;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning policies which are robust to changes in the environment are critical for real world deployment of Reinforcement Learning agents. They are also necessary for achieving good generalization across environment shifts. We focus on bisimulation metrics, which provide a powerful means for abstracting task relevant components of the observation and learning a succinct representation space for training the agent using reinforcement learning. In this work, we extend the bisimulation framework to also account for context dependent observation shifts. Specifically, we focus on the simulator based learning setting and use alternate observations to learn a representation space which is invariant to observation shifts using a novel bisimulation based objective. This allows us to deploy the agent to varying observation settings during test time and generalize to unseen scenarios. We further provide novel theoretical bounds for simulator fidelity and performance transfer guarantees for using a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#36807;&#31243;&#20013;&#20302;&#25509;&#36817;&#24230;&#25968;&#25454;&#21644;&#39640;&#25509;&#36817;&#24230;&#25968;&#25454;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#26657;&#20934;&#23545;&#20110;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#12290;&#24050;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#25968;&#25454;&#65288;&#21363;&#20998;&#24067;&#30340;&#31232;&#30095;&#21306;&#22495;&#65289;&#20013;&#20542;&#21521;&#20110;&#26356;&#33258;&#20449;&#65292;&#32780;&#22312;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#20013;&#34920;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#35823;&#26657;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30740;&#31350;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#65306;1&#65289;&#25509;&#36817;&#24230;&#20559;&#24046;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#20043;&#38388;&#65307;2&#65289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#27604;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25509;&#36817;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65307;3&#65289;&#21363;&#20351;&#37319;&#29992;&#27969;&#34892;&#30340;&#26657;&#20934;&#31639;&#27861;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;&#25509;&#36817;&#24230;&#20559;&#24046;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#65307;4&#65289;&#27169;&#22411;&#22312;&#20302;&#25509;&#36817;&#24615;&#26679;&#26412;&#19978;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#27604;&#39640;&#25509;&#36817;&#24615;&#26679;&#26412;&#26356;&#20005;&#37325;&#12290;&#22312;&#36825;&#20123;&#23454;&#35777;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProCal&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.04581</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#25945;&#24072;&#25110;&#19987;&#23478;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#19978;&#25193;&#23637;&#30340;&#31574;&#30053;&#25110;&#36873;&#39033;&#36827;&#34892;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#36712;&#36857;&#20998;&#27495;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#21644;&#20002;&#24323;&#24050;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#36712;&#36857;&#37096;&#20998;&#65292;&#24182;&#21487;&#33021;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#22914;&#26524;&#29992;&#20110;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36873;&#39033;&#30340;&#31639;&#27861;&#26469;&#20998;&#21106;&#36712;&#36857;&#65292;&#24182;&#21482;&#20174;&#24050;&#30830;&#23450;&#20026;&#21487;&#25509;&#21463;&#30340;&#36712;&#36857;&#37096;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;&#20462;&#22797;&#37096;&#20998;&#36712;&#36857;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
&lt;/p&gt;</description></item><item><title>&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#25972;&#21512;&#20026;&#20892;&#19994;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#20892;&#19994;&#29983;&#20135;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04566</link><description>&lt;p&gt;
&#20892;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20135;&#37327;&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#19968;&#27425;&#37325;&#35201;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Recent applications of machine learning, remote sensing, and iot approaches in yield prediction: a critical review. (arXiv:2306.04566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04566
&lt;/p&gt;
&lt;p&gt;
&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#25972;&#21512;&#20026;&#20892;&#19994;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#25552;&#39640;&#20102;&#20892;&#19994;&#29983;&#20135;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#20892;&#19994;&#20013;&#30340;&#25972;&#21512;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#27934;&#23519;&#21644;&#39044;&#27979;&#65292;&#27491;&#22312;&#25913;&#21464;&#34892;&#19994;&#12290;&#36825;&#31181;&#32467;&#21512;&#23548;&#33268;&#20102;&#25913;&#21892;&#20135;&#37327;&#39044;&#27979;&#21644;&#27700;&#31649;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25552;&#39640;&#25928;&#29575;&#12289;&#33719;&#24471;&#26356;&#22909;&#30340;&#20135;&#37327;&#21644;&#26356;&#21487;&#25345;&#32493;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;&#21487;&#20197;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#20892;&#19994;&#31227;&#21160;&#25110;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#20892;&#27665;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21644;&#24037;&#20855;&#65292;&#20197;&#25913;&#21892;&#20316;&#29289;&#31649;&#29702;&#21644;&#25552;&#39640;&#25928;&#29575;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#12289;&#36965;&#24863;&#21644;&#29289;&#32852;&#32593;&#26041;&#27861;&#22312;&#20135;&#37327;&#39044;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#24212;&#29992;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of remote sensing and machine learning in agriculture is transforming the industry by providing insights and predictions through data analysis. This combination leads to improved yield prediction and water management, resulting in increased efficiency, better yields, and more sustainable agricultural practices. Achieving the United Nations' Sustainable Development Goals, especially "zero hunger," requires the investigation of crop yield and precipitation gaps, which can be accomplished through, the usage of artificial intelligence (AI), machine learning (ML), remote sensing (RS), and the internet of things (IoT). By integrating these technologies, a robust agricultural mobile or web application can be developed, providing farmers and decision-makers with valuable information and tools for improving crop management and increasing efficiency. Several studies have investigated these new technologies and their potential for diverse tasks such as crop monitoring, yield predi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.04563</link><description>&lt;p&gt;
ChatGPT&#24456;&#26377;&#36259;&#65292;&#20294;&#24182;&#19981;&#22909;&#31505;&#65281;&#24189;&#40664;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#34987;&#35299;&#20915;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#33021;&#22815;&#25429;&#25417;&#38544;&#21547;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;OpenAI&#30340;ChatGPT&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#22522;&#20110;GPT3&#30340;&#27169;&#22411;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#27700;&#24179;&#65292;&#29978;&#33267;&#33021;&#22815;&#35762;&#31505;&#35805;&#12290;&#20294;&#26159;&#65292;ChatGPT&#30495;&#30340;&#24456;&#26377;&#36259;&#21527;&#65311;&#20316;&#32773;&#38024;&#23545;&#27169;&#22411;&#30340;&#24189;&#40664;&#24863;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#26816;&#27979;&#31561;&#29615;&#33410;&#65292;&#35797;&#22270;&#20102;&#35299;ChatGPT&#29702;&#35299;&#24182;&#20877;&#29616;&#20154;&#31867;&#24189;&#40664;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#27169;&#22411;&#26412;&#36523;&#19981;&#21487;&#35775;&#38382;&#65292;&#20316;&#32773;&#37319;&#29992;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#39564;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#24182;&#19981;&#26159;&#30001;&#35813;&#27169;&#22411;&#26032;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#37325;&#22797;&#20102;&#23569;&#37327;&#30340;&#20960;&#32452;&#31505;&#35805;&#12290;&#34429;&#28982;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#22320;&#35299;&#37322;&#26377;&#25928;&#30340;&#31505;&#35805;&#65292;&#20294;&#20063;&#20250;&#25552;&#20986;&#34394;&#26500;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;StudentEval&#65292;&#30001;&#21021;&#23398;&#32773;&#32534;&#20889;&#22810;&#20010;&#25552;&#31034;&#26469;&#27979;&#35797;Code LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#22320;&#21306;&#20998;&#27169;&#22411;&#24615;&#33021;&#65307;&#20998;&#26512;&#25552;&#31034;&#21457;&#29616;&#23398;&#29983;&#25552;&#31034;&#25216;&#24039;&#24046;&#24322;&#26174;&#33879;&#65292;&#38750;&#30830;&#23450;&#24615;LLM&#25277;&#26679;&#21487;&#33021;&#20250;&#35823;&#23548;&#23398;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.04556</link><description>&lt;p&gt;
&#23398;&#29983;&#32534;&#20889;&#30340;&#38382;&#39064;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;: StudentEval
&lt;/p&gt;
&lt;p&gt;
StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code. (arXiv:2306.04556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;StudentEval&#65292;&#30001;&#21021;&#23398;&#32773;&#32534;&#20889;&#22810;&#20010;&#25552;&#31034;&#26469;&#27979;&#35797;Code LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#22320;&#21306;&#20998;&#27169;&#22411;&#24615;&#33021;&#65307;&#20998;&#26512;&#25552;&#31034;&#21457;&#29616;&#23398;&#29983;&#25552;&#31034;&#25216;&#24039;&#24046;&#24322;&#26174;&#33879;&#65292;&#38750;&#30830;&#23450;&#24615;LLM&#25277;&#26679;&#21487;&#33021;&#20250;&#35823;&#23548;&#23398;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#34987;&#24555;&#36895;&#37096;&#32626;&#65292;&#24182;&#19988;&#26377;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#19987;&#19994;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#30446;&#21069;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#26159;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#26681;&#25454;&#19987;&#23478;&#25552;&#31034;&#29983;&#25104;&#27491;&#30830;&#30340;&#31243;&#24207;&#26469;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#27599;&#20010;&#38382;&#39064;&#30340;&#22810;&#20010;&#25552;&#31034;&#65292;&#30001;&#29305;&#23450;&#30340;&#38750;&#19987;&#19994;&#25552;&#31034;&#32773;&#65288;&#21363;&#21021;&#23398;&#32773;&#31243;&#24207;&#21592;&#65289;&#32534;&#20889;&#12290;StudentEval&#21253;&#21547;&#20102;&#30001;80&#21517;&#20165;&#23436;&#25104;&#20102;&#19968;&#23398;&#26399;Python&#32534;&#31243;&#30340;&#23398;&#29983;&#32534;&#20889;&#30340;48&#20010;&#38382;&#39064;&#30340;1749&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23398;&#29983;&#22312;&#19982;Code LLM&#20114;&#21160;&#24037;&#20316;&#26102;&#32534;&#20889;&#20102;&#36825;&#20123;&#25552;&#31034;&#65292;&#35266;&#23519;&#21040;&#25104;&#21151;&#29575;&#38750;&#24120;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#20351;&#29992;StudentEval&#35780;&#20272;&#20102;5&#31181;Code LLM&#65292;&#24182;&#21457;&#29616;StudentEval&#27604;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#22320;&#21306;&#20998;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#65292;&#24182;&#21457;&#29616;&#23398;&#29983;&#25552;&#31034;&#25216;&#24039;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#38750;&#30830;&#23450;&#24615;LLM&#25277;&#26679;&#21487;&#33021;&#20250;&#35753;&#23398;&#29983;&#35823;&#20197;&#20026;&#20182;&#20204;&#30340;&#25552;&#31034;&#27604;&#23454;&#38469;&#25928;&#26524;&#26356;&#22909;&#65288;&#25110;&#26356;&#24046;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04551</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#35757;&#32451;&#32467;&#21512;&#39046;&#22495;&#20869;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26159;&#22686;&#24378;&#20020;&#24202;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#21644;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#35786;&#26029;&#25512;&#29702;&#22522;&#20934;&#65288;DR.BENCH&#65289;&#20316;&#20026;&#20840;&#38754;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#30001;&#20845;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#20195;&#34920;&#20020;&#24202;&#25512;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880; DR.BENCH &#30340;&#38382;&#39064;&#24635;&#32467;&#20219;&#21153;&#65288;Gao &#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20248;&#20110;&#20854;&#19968;&#33324;&#39046;&#22495;&#30340;&#23545;&#24212;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292; ROUGE-L &#24471;&#20998;&#20026; 28.55&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#22312;&#20248;&#21270;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;SARSA&#31639;&#27861;&#22312;&#38543;&#26426;&#26102;&#38480;MDPs&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#34892;&#20026;&#31574;&#30053;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#26435;&#37325;&#21521;&#37327;&#30456;&#20851;&#65292;Lipschitz&#24120;&#25968;&#36275;&#22815;&#23567;&#26102;&#65292;&#31639;&#27861;&#20197;&#27010;&#29575;&#19968;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.04548</link><description>&lt;p&gt;
&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;SARSA&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;: &#38543;&#26426;&#26102;&#38480;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Convergence of SARSA with linear function approximation: The random horizon case. (arXiv:2306.04548v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;SARSA&#31639;&#27861;&#22312;&#38543;&#26426;&#26102;&#38480;MDPs&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;&#34892;&#20026;&#31574;&#30053;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#26435;&#37325;&#21521;&#37327;&#30456;&#20851;&#65292;Lipschitz&#24120;&#25968;&#36275;&#22815;&#23567;&#26102;&#65292;&#31639;&#27861;&#20197;&#27010;&#29575;&#19968;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;(MDPs)&#20013;&#65292;SARSA&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#24050;&#34987;&#35777;&#26126;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#22312;&#38543;&#26426;&#26102;&#38480;MDPs&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#36825;&#20043;&#21069;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#31867;&#20284;&#20110;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MDPs&#30340;&#26089;&#26399;&#32467;&#26524;&#65292;&#22914;&#26524;&#34892;&#20026;&#31574;&#30053;&#20851;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#26435;&#37325;&#21521;&#37327;&#26159;$\varepsilon$-soft&#19988;&#19982;Lipschitz&#24120;&#25968;&#30456;&#20851;&#65292;&#24182;&#19988;Lipschitz&#24120;&#25968;&#36275;&#22815;&#23567;&#65292;&#21017;&#35813;&#31639;&#27861;&#23558;&#22312;&#32771;&#34385;&#38543;&#26426;&#26102;&#38480;MDP&#26102;&#20197;&#27010;&#29575;&#19968;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reinforcement learning algorithm SARSA combined with linear function approximation has been shown to converge for infinite horizon discounted Markov decision problems (MDPs). In this paper, we investigate the convergence of the algorithm for random horizon MDPs, which has not previously been shown. We show, similar to earlier results for infinite horizon discounted MDPs, that if the behaviour policy is $\varepsilon$-soft and Lipschitz continuous with respect to the weight vector of the linear function approximation, with small enough Lipschitz constant, then the algorithm will converge with probability one when considering a random horizon MDP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04539</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#20445;&#35777;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#29702;&#35299;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26412;&#36136;&#65306;&#22312;&#20174;&#20004;&#20010;&#37117;&#27809;&#26377;&#30340;&#27169;&#24577;&#23398;&#20064;&#26102;&#20986;&#29616;&#20102;&#26032;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#36825;&#19968;&#20132;&#20114;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#21482;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26080;&#26631;&#31614;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65292;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#65289;&#12290;&#21033;&#29992;&#31934;&#30830;&#30340;&#20449;&#24687;&#35770;&#20132;&#20114;&#23450;&#20041;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25512;&#23548;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#37327;&#21270;&#36825;&#31181;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#37327;&#21644;&#21333;&#29420;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#21040;&#36817;&#20284;&#31639;&#27861;&#26469;&#25512;&#23548;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
&lt;/p&gt;</description></item><item><title>Git-Theta&#26159;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;Git&#25193;&#23637;&#65292;&#21487;&#25903;&#25345;&#39640;&#25928;&#30340;&#36890;&#20449;&#26356;&#26032;&#12289;&#33258;&#21160;&#27169;&#22411;&#21512;&#24182;&#20197;&#21450;&#26377;&#20851;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#24046;&#24322;&#30340;&#26377;&#24847;&#20041;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2306.04529</link><description>&lt;p&gt;
Git-Theta: &#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;Git&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models. (arXiv:2306.04529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04529
&lt;/p&gt;
&lt;p&gt;
Git-Theta&#26159;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;Git&#25193;&#23637;&#65292;&#21487;&#25903;&#25345;&#39640;&#25928;&#30340;&#36890;&#20449;&#26356;&#26032;&#12289;&#33258;&#21160;&#27169;&#22411;&#21512;&#24182;&#20197;&#21450;&#26377;&#20851;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#24046;&#24322;&#30340;&#26377;&#24847;&#20041;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#30001;&#20013;&#22830;&#22242;&#38431;&#22521;&#35757;&#65292;&#24456;&#23569;&#36827;&#34892;&#26356;&#26032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24320;&#25918;&#28304;&#20195;&#30721;&#36719;&#20214;&#24320;&#21457;&#36890;&#36807;&#20998;&#24067;&#24335;&#21327;&#20316;&#20351;&#29992;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;&#36827;&#34892;&#20849;&#20139;&#24037;&#20214;&#30340;&#36845;&#20195;&#24320;&#21457;&#12290;&#20026;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#21644;&#19981;&#26029;&#25913;&#36827;&#65292;&#26412;&#25991;&#20171;&#32461; Git-Theta&#65292;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;&#25193;&#23637;&#12290;Git-Theta&#26159;Git&#30340;&#25193;&#23637;&#65292;&#21487;&#31934;&#32454;&#36319;&#36394;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#25913;&#20197;&#21450;&#20195;&#30721;&#21644;&#20854;&#20182;&#24037;&#20214;&#12290;&#19982;&#29616;&#26377;&#30340;&#29256;&#26412;&#25511;&#21046;&#31995;&#32479;&#19981;&#21516;&#65292;Git-Theta&#21033;&#29992;&#26816;&#26597;&#28857;&#30340;&#32467;&#26500;&#25903;&#25345;&#39640;&#25928;&#30340;&#36890;&#20449;&#26356;&#26032;&#12289;&#33258;&#21160;&#27169;&#22411;&#21512;&#24182;&#20197;&#21450;&#26377;&#20851;&#20004;&#20010;&#27169;&#22411;&#29256;&#26412;&#20043;&#38388;&#24046;&#24322;&#30340;&#26377;&#24847;&#20041;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;Git-Theta&#36824;&#21253;&#25324;&#19968;&#25554;&#20214;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#28155;&#21152;&#23545;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, most machine learning models are trained by centralized teams and are rarely updated. In contrast, open-source software development involves the iterative development of a shared artifact through distributed collaboration using a version control system. In the interest of enabling collaborative and continual improvement of machine learning models, we introduce Git-Theta, a version control system for machine learning models. Git-Theta is an extension to Git, the most widely used version control software, that allows fine-grained tracking of changes to model parameters alongside code and other artifacts. Unlike existing version control systems that treat a model checkpoint as a blob of data, Git-Theta leverages the structure of checkpoints to support communication-efficient updates, automatic model merges, and meaningful reporting about the difference between two versions of a model. In addition, Git-Theta includes a plug-in system that enables users to easily add support for 
&lt;/p&gt;</description></item><item><title>ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04527</link><description>&lt;p&gt;
ContriMix&#65306;&#26174;&#24494;&#38236;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#26080;&#30417;&#30563;&#20869;&#23481;&#23646;&#24615;&#20998;&#31163;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04527
&lt;/p&gt;
&lt;p&gt;
ContriMix&#26159;&#19968;&#31181;&#26080;&#38656;&#26631;&#35782;&#21644;&#25163;&#24037;&#35843;&#20248;&#30340;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#31561;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ContriMix&#65292;&#23427;&#37319;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#24335;&#20998;&#31163;&#20986;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#29983;&#29289;&#23398;&#20869;&#23481;&#21644;&#25216;&#26415;&#21464;&#24322;&#65292;&#24182;&#23398;&#20064;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25163;&#24037; fine-tuning &#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#32452;&#32455;&#23398;&#21644;&#33639;&#20809;&#25104;&#20687;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102; ContriMix &#30340;&#26377;&#25928;&#24615;&#65292;&#21462;&#24471;&#20102;&#22522;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#20248;&#21270;&#20102;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#22120;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04520</link><description>&lt;p&gt;
&#21033;&#29992;&#33609;&#22270;&#25216;&#26415;&#20272;&#35745;Koopman&#31639;&#23376;&#24182;&#21487;&#38752;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Estimating Koopman operators with sketching to provably learn large scale dynamical systems. (arXiv:2306.04520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#25216;&#26415;&#20248;&#21270;&#20102;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#22120;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#29702;&#35770;&#20801;&#35768;&#20351;&#29992;&#38750;&#21442;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#65288;&#33609;&#22270;&#25216;&#26415;&#65289;&#25552;&#39640;&#22522;&#20110;&#26680;&#30340;Koopman&#31639;&#23376;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#65292;&#32473;&#20986;&#20102;&#32479;&#35745;&#23398;&#20064;&#36895;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#31934;&#30830;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;SLGrad&#65292;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.04519</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks. (arXiv:2306.04519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;SLGrad&#65292;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#21487;&#20197;&#36890;&#36807;&#19982;&#30456;&#20851;&#20219;&#21153;&#20849;&#20139;&#34920;&#31034;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#20063;&#21487;&#33021;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#23475;&#24178;&#25200;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#21152;&#26435;&#20316;&#20026;&#35299;&#20915;&#24178;&#25200;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#23558;&#20219;&#21153;&#35270;&#20026;&#21407;&#23376;&#24615;&#65292;&#32570;&#20047;&#23558;&#26377;&#23475;&#21644;&#26377;&#29992;&#20449;&#21495;&#26126;&#30830;&#20998;&#31163;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLGrad&#65292;&#19968;&#31181;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;&#19979;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26679;&#26412;&#32423;&#21152;&#26435;&#31639;&#27861;&#12290;&#36890;&#36807;&#26679;&#26412;&#29305;&#23450;&#30340;&#20219;&#21153;&#26435;&#37325;&#65292;SLGrad&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#22609;&#36896;&#20219;&#21153;&#20998;&#24067;&#65292;&#28040;&#38500;&#26377;&#23475;&#30340;&#36741;&#21161;&#20449;&#21495;&#24182;&#22686;&#24378;&#26377;&#29992;&#30340;&#20219;&#21153;&#20449;&#21495;&#12290;&#22312;(&#21322;)&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#30340;&#30417;&#30563;&#22810;&#20219;&#21153;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) can improve the generalization performance of neural networks by sharing representations with related tasks. Nonetheless, MTL can also degrade performance through harmful interference between tasks. Recent work has pursued task-specific loss weighting as a solution for this interference. However, existing algorithms treat tasks as atomic, lacking the ability to explicitly separate harmful and helpful signals beyond the task level. To this end, we propose SLGrad, a sample-level weighting algorithm for multi-task learning with auxiliary tasks. Through sample-specific task weights, SLGrad reshapes the task distributions during training to eliminate harmful auxiliary signals and augment useful task signals. Substantial generalization performance gains are observed on (semi-) synthetic datasets and common supervised multi-task problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#26377;&#38480;&#30340;&#27979;&#37327;&#20301;&#32622;&#31616;&#27905;&#22320;&#37325;&#24314;&#39118;&#21387;&#21147;&#22330;&#65292;&#26174;&#33879;&#20943;&#23569;&#20256;&#24863;&#22120;&#25968;&#37327;&#24182;&#25552;&#20379;&#20102;&#31283;&#23450;&#21644;&#26368;&#20339;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04518</link><description>&lt;p&gt;
&#36890;&#36807;&#21387;&#32553;&#24863;&#30693;&#30830;&#23450;&#24314;&#31569;&#21608;&#22260;&#39118;&#21387;&#21147;&#22330;&#30340;&#26368;&#20339;&#20256;&#24863;&#22120;&#24067;&#32622;
&lt;/p&gt;
&lt;p&gt;
Optimal sensor placement for reconstructing wind pressure field around buildings using compressed sensing. (arXiv:2306.04518v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#26377;&#38480;&#30340;&#27979;&#37327;&#20301;&#32622;&#31616;&#27905;&#22320;&#37325;&#24314;&#39118;&#21387;&#21147;&#22330;&#65292;&#26174;&#33879;&#20943;&#23569;&#20256;&#24863;&#22120;&#25968;&#37327;&#24182;&#25552;&#20379;&#20102;&#31283;&#23450;&#21644;&#26368;&#20339;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#12289;&#22797;&#26434;&#12289;&#31354;&#38388;&#25193;&#23637;&#30340;&#32467;&#26500;&#29289;&#65292;&#22914;&#20309;&#30830;&#23450;&#26368;&#20339;&#20256;&#24863;&#22120;&#37096;&#32626;&#20301;&#32622;&#20197;&#30830;&#20445;&#20934;&#30830;&#25429;&#25417;&#34920;&#38754;&#21387;&#21147;&#22330;&#65292;&#26159;&#20445;&#35777;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25968;&#23383;&#23402;&#29983;&#30340;&#21457;&#23637;&#65289;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#36873;&#25321;&#31639;&#27861;&#65292;&#26088;&#22312;&#25552;&#20379;&#26368;&#22810;&#20449;&#24687;&#20869;&#23481;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#27979;&#37327;&#20301;&#32622;&#31616;&#27905;&#22320;&#37325;&#24314;&#39118;&#21387;&#30340;&#31354;&#27668;&#21160;&#21147;&#29305;&#24615;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20256;&#24863;&#22120;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deciding how to optimally deploy sensors in a large, complex, and spatially extended structure is critical to ensure that the surface pressure field is accurately captured for subsequent analysis and design. In some cases, reconstruction of missing data is required in downstream tasks such as the development of digital twins. This paper presents a data-driven sparse sensor selection algorithm, aiming to provide the most information contents for reconstructing aerodynamic characteristics of wind pressures over tall building structures parsimoniously. The algorithm first fits a set of basis functions to the training data, then applies a computationally efficient QR algorithm that ranks existing pressure sensors in order of importance based on the state reconstruction to this tailored basis. The findings of this study show that the proposed algorithm successfully reconstructs the aerodynamic characteristics of tall buildings from sparse measurement locations, generating stable and optimal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#29992;&#21608;&#26399;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#21516;&#26102;&#20316;&#29992;&#20110;&#26230;&#20307;&#26230;&#26684;&#21644;&#21407;&#23376;&#20301;&#32622;&#65292;&#36890;&#36807;&#38477;&#20302;&#24635;&#33021;&#37327;&#20197;&#36798;&#21040;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#26469;&#23398;&#20064;&#20219;&#24847;&#26230;&#20307;&#26230;&#26684;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.04510</link><description>&lt;p&gt;
&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#30340;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Model for Crystalline Material Generation. (arXiv:2306.04510v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#29992;&#21608;&#26399;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#21516;&#26102;&#20316;&#29992;&#20110;&#26230;&#20307;&#26230;&#26684;&#21644;&#21407;&#23376;&#20301;&#32622;&#65292;&#36890;&#36807;&#38477;&#20302;&#24635;&#33021;&#37327;&#20197;&#36798;&#21040;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#26469;&#23398;&#20064;&#20219;&#24847;&#26230;&#20307;&#26230;&#26684;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#26032;&#22411;&#21019;&#26032;&#26230;&#20307;&#26448;&#26009;&#26159;&#25105;&#20204;&#31038;&#20250;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#26230;&#20307;&#26448;&#26009;&#30340;&#38382;&#39064;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#65292;&#25105;&#20204;&#33021;&#21542;&#24320;&#21457;&#21516;&#26102;&#32771;&#34385;&#26230;&#20307;&#32467;&#26500;&#30340;&#21608;&#26399;&#24615;&#21644;&#31561;&#20215;&#20960;&#20309;&#30340;&#29983;&#25104;&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#23427;&#20204;&#21516;&#26102;&#20316;&#29992;&#20110;&#26230;&#20307;&#26230;&#26684;&#21644;&#21407;&#23376;&#20301;&#32622;&#65292;&#20351;&#29992;&#21608;&#26399;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#38477;&#20302;&#24635;&#33021;&#37327;&#20197;&#36798;&#21040;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#26469;&#23398;&#20064;&#20219;&#24847;&#26230;&#20307;&#26230;&#26684;&#21464;&#24418;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/aklipf/GemsNet&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the greatest challenges facing our society is the discovery of new innovative crystal materials with specific properties. Recently, the problem of generating crystal materials has received increasing attention, however, it remains unclear to what extent, or in what way, we can develop generative models that consider both the periodicity and equivalence geometric of crystal structures. To alleviate this issue, we propose two unified models that act at the same time on crystal lattice and atomic positions using periodic equivariant architectures. Our models are capable to learn any arbitrary crystal lattice deformation by lowering the total energy to reach thermodynamic stability. Code and data are available at https://github.com/aklipf/GemsNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21033;&#29992;AFC&#22312;&#39640;&#20445;&#30495;&#24230;&#21644;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#19979;&#65292;&#36873;&#25321;&#26080;&#20449;&#24687;&#35777;&#20070;&#30340;&#20219;&#21153;&#26159; NP-hard &#30340;&#12290;</title><link>http://arxiv.org/abs/2306.04505</link><description>&lt;p&gt;
&#20266;&#35777;&#20070;&#36873;&#25321;&#30340;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hardness of Deceptive Certificate Selection. (arXiv:2306.04505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21033;&#29992;AFC&#22312;&#39640;&#20445;&#30495;&#24230;&#21644;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#19979;&#65292;&#36873;&#25321;&#26080;&#20449;&#24687;&#35777;&#20070;&#30340;&#20219;&#21153;&#26159; NP-hard &#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#30340;&#20998;&#31867;&#22120;&#22312;&#23454;&#29616;AI&#29702;&#35770;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#35777;&#26126;&#32773;&#20174;&#25968;&#25454;&#28857;&#20013;&#36873;&#25321;&#35777;&#20070;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#39564;&#35777;&#32773;&#65292;&#21518;&#32773;&#20915;&#23450;&#20998;&#31867;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35777;&#20070;&#21487;&#20197;&#26159;&#20449;&#24687;&#24615;&#30340;&#29305;&#24449;&#12290;&#23545;&#20110;&#39640;&#24230;&#20445;&#30495;&#19982;&#23436;&#22791;&#24615;&#30340;&#35774;&#23450;&#65292;&#20132;&#25442;&#30340;&#35777;&#20070;&#24517;&#39035;&#19982;&#25968;&#25454;&#28857;&#30340;&#30495;&#23454;&#20998;&#31867;&#26377;&#39640;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#35777;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#30028;&#38480;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#38590;&#20197;&#20272;&#35745;&#39640;&#32500;&#25968;&#25454;&#30340;&#23646;&#24615;&#12290;W\"aldchen&#31561;&#20154;&#29468;&#27979;&#65292;&#21033;&#29992;AFC&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32771;&#34385;&#24694;&#24847;&#30340;&#35777;&#26126;&#32773;-&#39564;&#35777;&#32773;&#20108;&#20803;&#32452;&#65292;&#26088;&#22312;&#21033;&#29992;AFC&#26469;&#23454;&#29616;&#39640;&#30340;&#23436;&#22791;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27809;&#26377;&#20449;&#24687;&#30340;&#35777;&#20070;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20219;&#21153;&#26159;$\mathsf{NP}$&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in W\"aldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here.  We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is $\mathsf{NP}$-hard an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#20844;&#24179;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#21644;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#32467;&#26524;&#36951;&#25022;&#38454;&#25968;&#20174;$O(\log T \log\log T)$&#21040;&#20102;$O\left(N^3 \log N \log T \right)$&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04498</link><description>&lt;p&gt;
&#20844;&#24179;&#22810;&#26234;&#33021;&#20307;&#36172;&#21338;&#26426;&#30340;&#26368;&#20248;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Fair Multi-Agent Bandits. (arXiv:2306.04498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#20844;&#24179;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#21644;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#32467;&#26524;&#36951;&#25022;&#38454;&#25968;&#20174;$O(\log T \log\log T)$&#21040;&#20102;$O\left(N^3 \log N \log T \right)$&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#19981;&#30456;&#20114;&#36890;&#20449;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21482;&#26377;&#22312;&#21516;&#26102;&#35775;&#38382;&#21516;&#19968;&#20010;&#33218;&#26102;&#25165;&#25552;&#20379;&#30896;&#25758;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#20026;$O\left(N^3 \log N \log T \right)$&#65288;&#20551;&#35774;&#22870;&#21169;&#26377;&#30028;&#65292;&#20294;&#26410;&#30693;&#19978;&#30028;&#65289;&#12290;&#36825;&#22823;&#22823;&#25913;&#36827;&#20102;&#20043;&#21069;&#32467;&#26524;&#65292;&#20854;&#36951;&#25022;&#38454;&#25968;&#20026;$O(\log T \log\log T)$&#65292;&#24182;&#19988;&#23545;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#25351;&#25968;&#20381;&#36182;&#24615;&#12290;&#32467;&#26524;&#26159;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#25293;&#21334;&#31639;&#27861;&#26469;&#23398;&#20064;&#26679;&#26412;&#26368;&#20248;&#21305;&#37197;&#65292;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#38454;&#27573;&#65292;&#20854;&#38271;&#24230;&#26469;&#33258;&#20110;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#30340;&#36951;&#25022;&#20998;&#26512;&#23454;&#29616;&#30340;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#20102;&#36951;&#25022;&#23545;$\log T$&#30340;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assuming bounded rewards, with unknown bound). This significantly improves previous results which had regret of order $O(\log T \log\log T)$ and exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching, a new type of exploitation phase whose length is derived from the observed samples, and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on $\log T$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#22270;&#20013;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;GraphOps&#26497;&#38480;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20851;&#20110;&#19981;&#21516;&#22823;&#23567;&#22270;&#24418;&#19978;&#30340;GNN&#20043;&#38388;&#36317;&#31163;&#30340;&#23450;&#37327;&#30028;&#38480;&#21644;&#32467;&#26500;&#23646;&#24615;&#30340;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2306.04495</link><description>&lt;p&gt;
&#36890;&#36807;GraphOps&#25506;&#31350;GNN&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38480;&#21046;&#12289;&#36924;&#36817;&#21644;&#22823;&#23567;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits, approximation and size transferability for GNNs on sparse graphs via graphops. (arXiv:2306.04495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#31232;&#30095;&#22270;&#20013;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;GraphOps&#26497;&#38480;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20851;&#20110;&#19981;&#21516;&#22823;&#23567;&#22270;&#24418;&#19978;&#30340;GNN&#20043;&#38388;&#36317;&#31163;&#30340;&#23450;&#37327;&#30028;&#38480;&#21644;&#32467;&#26500;&#23646;&#24615;&#30340;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#25512;&#24191;&#21040;&#19982;&#20854;&#35757;&#32451;&#22270;&#19981;&#21516;&#30340;&#22270;&#24418;&#20013;&#12290;&#20026;&#20102;&#21253;&#25324;&#32463;&#24120;&#36935;&#21040;&#30340;&#31232;&#30095;&#22270;&#65292;&#22914;&#26377;&#30028;&#24230;&#25110;&#24130;&#24459;&#22270;&#65292;&#25105;&#20204;&#37319;&#21462;&#20174;&#22270;&#24418;&#23548;&#20986;&#36816;&#31639;&#31526;&#30340;&#35270;&#35282;&#65292;&#22914;&#32452;&#25104;GNN&#30340;&#32858;&#21512;&#25805;&#20316;&#12290;&#36825;&#23548;&#33268;&#20102;&#26368;&#36817;&#20171;&#32461;&#30340;GraphOps&#26497;&#38480;&#27010;&#24565;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36816;&#31639;&#31526;&#35270;&#35282;&#22914;&#20309;&#20801;&#35768;&#25105;&#20204;&#24320;&#21457;&#20851;&#20110;&#26377;&#38480;GNN&#19982;&#20854;&#22312;&#26080;&#38480;&#22270;&#19978;&#30340;&#26497;&#38480;&#20043;&#38388;&#36317;&#31163;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#20197;&#21450;&#20855;&#26377;&#20849;&#20139;&#32467;&#26500;&#23646;&#24615;&#30340;&#19981;&#21516;&#22823;&#23567;&#22270;&#24418;&#19978;&#30340;GNN&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22312;&#39564;&#35777;&#21508;&#31181;&#22270;&#24207;&#21015;&#30340;&#35268;&#21017;&#24615;&#20551;&#35774;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can graph neural networks generalize to graphs that are different from the graphs they were trained on, e.g., in size? In this work, we study this question from a theoretical perspective. While recent work established such transferability and approximation results via graph limits, e.g., via graphons, these only apply non-trivially to dense graphs. To include frequently encountered sparse graphs such as bounded-degree or power law graphs, we take a perspective of taking limits of operators derived from graphs, such as the aggregation operation that makes up GNNs. This leads to the recently introduced limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the operator perspective allows us to develop quantitative bounds on the distance between a finite GNN and its limit on an infinite graph, as well as the distance between the GNN on graphs of different sizes that share structural properties, under a regularity assumption verified for various graph sequences. Our res
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04489</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fair Column Subset Selection. (arXiv:2306.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04489
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20013;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#36873;&#21015;&#23376;&#38598;&#24517;&#39035;&#30456;&#23545;&#20110;&#23427;&#20204;&#21508;&#33258;&#30340;&#26368;&#20339;&#31209;-k&#36924;&#36817;&#25552;&#20379;&#33391;&#22909;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20844;&#24179;&#35774;&#32622;&#24341;&#20837;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#20026;&#20102;&#25193;&#23637;&#24050;&#30693;&#32467;&#26524;&#65292;&#20154;&#20204;&#19981;&#33021;&#20570;&#24471;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#21407;&#22987;&#26041;&#27861;&#30340;&#20004;&#20493;&#21015;&#26356;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#30340;&#24050;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#37319;&#26679;&#36866;&#24403;&#22823;&#23567;&#30340;&#23376;&#38598;&#23601;&#21464;&#24471;NP&#38590;&#12290;&#32780;&#25214;&#21040;&#20004;&#20493;&#20110;&#25152;&#38656;&#22823;&#23567;&#30340;&#23376;&#38598;&#21017;&#38750;&#24120;&#31616;&#21333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#26412;&#19978;1.5&#20493;&#30340;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04488</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#21512;&#22810;&#26679;&#21270;&#22870;&#21169;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#23545;&#40784;&#30340;&#22870;&#21169;&#27748;
&lt;/p&gt;
&lt;p&gt;
Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#37327;&#26080;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26377;&#26631;&#27880;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#32593;&#32476;&#19982;&#39044;&#26399;&#30340;&#20351;&#29992;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#22870;&#21169;&#30340;&#32570;&#38519;&#21487;&#33021;&#20250;&#22952;&#30861;&#35757;&#32451;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#29616;&#23454;&#20219;&#21153;&#21644;&#20154;&#31867;&#24847;&#35265;&#30340;&#22810;&#26679;&#24615;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#37319;&#29992;&#22810;&#31574;&#30053;&#26041;&#27861;&#26469;&#25317;&#25265;&#22810;&#26679;&#21270;&#22870;&#21169;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#20808;&#39564;&#22870;&#21169;&#65292;&#32780;&#26159;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; rewarded soup&#65292;&#39318;&#20808;&#29420;&#31435;&#22320;&#19987;&#38376;&#21270;&#22810;&#20010;&#32593;&#32476;(&#27599;&#20010;&#20195;&#29702;&#22870;&#21169;&#19968;&#20010;)&#65292;&#28982;&#21518;&#22312;&#23427;&#20204;&#30340;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#25104;&#21151;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#22810;&#26679;&#21270;&#22870;&#21169;&#26469;&#33258;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#26102;&#65292;&#26435;&#37325;&#20173;&#28982;&#20445;&#25345;&#32447;&#24615;&#36830;&#25509;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#29992;&#20110;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20445;&#35777;&#22312;&#25968;&#25454;&#36873;&#25321;&#26102;NN&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04454</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Training-Free Neural Active Learning with Initialization-Robustness Guarantees. (arXiv:2306.04454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#29992;&#20110;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20445;&#35777;&#22312;&#25968;&#25454;&#36873;&#25321;&#26102;NN&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#40065;&#26834;&#24615;&#20063;&#26159;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#37325;&#35201;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26399;&#26395;&#26041;&#24046;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;EV-GP&#65289;&#26631;&#20934;&#26469;&#36827;&#34892;&#31070;&#32463;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#26631;&#20934;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#36873;&#25321;&#25968;&#25454;&#28857;&#21487;&#20197;&#23548;&#33268;&#35757;&#32451;&#30340; NN &#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340; EV-GP &#26631;&#20934;&#26159;&#26080;&#38656;&#35757;&#32451;&#30340;&#65292;&#21363;&#22312;&#25968;&#25454;&#36873;&#25321;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#23545; NN &#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#65292;&#36825;&#20351;&#20854;&#22312;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340; EV-GP &#26631;&#20934;&#19982;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#21644;&#27010;&#25324;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20004;&#20010;&#26399;&#26395;&#26041;&#38754;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks (NNs) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with Gaussian processes (EV-GP) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained NNs with both (a) good predictive performances and (b) initialization robustness. Importantly, our EV-GP criterion is training-free, i.e., it does not require any training of the NN during data selection, which makes it computationally efficient. We empirically demonstrate that our EV-GP criterion is highly correlated with both initialization robustness and generalization performance, and show that it consistently outperforms baseline methods in terms of both desiderata, especiall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#27169;&#24577;&#38388;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04445</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28508;&#22312;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Latent Diffusion. (arXiv:2306.04445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#27169;&#24577;&#38388;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26080;&#22788;&#19981;&#22312;&#65292;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36830;&#36143;&#24615; - &#36136;&#37327;&#25240;&#34935;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#29983;&#25104;&#36136;&#37327;&#30340;&#27169;&#22411;&#32570;&#20047;&#27169;&#24577;&#38388;&#30340;&#29983;&#25104;&#36830;&#36143;&#24615;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#35752;&#35770;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35828;&#26126;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#30830;&#23450;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#36830;&#25509;&#21040;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;&#25513;&#34109;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#23454;&#29616;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#26465;&#20214;&#24471;&#20998;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#37117;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal data-sets are ubiquitous in modern applications, and multi-modal Variational Autoencoders are a popular family of models that aim to learn a joint representation of the different modalities. However, existing approaches suffer from a coherence-quality tradeoff, where models with good generation quality lack generative coherence across modalities, and vice versa. We discuss the limitations underlying the unsatisfactory performance of existing methods, to motivate the need for a different approach. We propose a novel method that uses a set of independently trained, uni-modal, deterministic autoencoders. Individual latent variables are concatenated into a common latent space, which is fed to a masked diffusion model to enable generative modeling. We also introduce a new multi-time training method to learn the conditional score network for multi-modal diffusion. Our methodology substantially outperforms competitors in both generation quality and coherence, as shown through an e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProjUnit&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#20302;&#32500;&#31354;&#38388;&#23454;&#29616;&#26368;&#20248;&#35299;&#65292;&#19988;&#20855;&#26377;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.04444</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#24555;&#36895;&#33719;&#24471;&#26368;&#20248;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast Optimal Locally Private Mean Estimation via Random Projections. (arXiv:2306.04444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProjUnit&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#20302;&#32500;&#31354;&#38388;&#23454;&#29616;&#26368;&#20248;&#35299;&#65292;&#19988;&#20855;&#26377;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#39640;&#32500;&#21521;&#37327;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#35823;&#24046;&#65292;&#35201;&#20040;&#20855;&#26377;&#39640;&#36890;&#20449;&#21644;/&#25110;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;ProjUnit&#65292;&#29992;&#20110;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#30340;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#36890;&#20449;&#22797;&#26434;&#24230;&#20302;&#19988;&#35823;&#24046;&#19982;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#26368;&#22823;&#20026;1 + o(1)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65306;&#27599;&#20010;&#38543;&#26426;&#21270;&#22120;&#23558;&#20854;&#36755;&#20837;&#25237;&#24433;&#21040;&#19968;&#20010;&#38543;&#26426;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#28982;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#36816;&#34892;&#19968;&#20010;&#26368;&#20248;&#31639;&#27861;&#65292;&#20363;&#22914;PrivUnitG&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36866;&#24403;&#22320;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#38543;&#26426;&#25237;&#24433;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#30340;&#24615;&#36136;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#35823;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23454;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ProjUnit&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04440</link><description>&lt;p&gt;
&#20197;&#21452;&#31574;&#30053;&#20026;&#33258;&#27169;&#22411;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20195;&#29702;&#36890;&#36807;&#25506;&#32034;&#21487;&#33021;&#30340;&#26410;&#26469;&#29366;&#24577;&#26469;&#36873;&#25321;&#20505;&#36873;&#21160;&#20316;&#12290;&#24403;&#23384;&#22312;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#20026;&#20102;&#27169;&#25311;&#26410;&#26469;&#29366;&#24577;&#65292;&#24517;&#39035;&#20351;&#29992;&#33258;&#24049;&#30340;&#20915;&#31574;&#31574;&#30053;&#26469;&#38480;&#21046;&#25152;&#38656;&#25506;&#32034;&#30340;&#21160;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#23558;&#29992;&#20110;&#27169;&#25311;&#33258;&#24049;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;&#20195;&#29702;&#30340;&#33258;&#25105;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#35268;&#21010;&#34892;&#21160;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#19982;&#33258;&#25105;&#27169;&#22411;&#19968;&#36215;&#38544;&#21547;&#22320;&#20351;&#29992;&#65292;&#20294;&#22914;&#20309;&#35774;&#35745;&#33258;&#25105;&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#21463;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#22312;&#36825;&#26679;&#30340;&#21452;&#31574;&#30053;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#20998;&#21035;&#29992;&#20110;&#26080;&#27169;&#22411;&#21160;&#20316;&#21644;&#35745;&#21010;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29983;&#24577;&#30456;&#20851;&#30340;&#21442;&#25968;&#29615;&#22659;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04431</link><description>&lt;p&gt;
&#24544;&#23454;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35299;&#20915;&#23398;&#29983;&#32622;&#20449;&#24230;&#21644;&#36719;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#35777;&#21644;&#35748;&#35777;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#20351;&#20854;&#33021;&#22815;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#20294;&#36807;&#21435;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#22312;&#36719;&#32622;&#20449;&#24230;&#26041;&#38754;&#30340;&#30456;&#23545;&#26657;&#20934;&#38382;&#39064;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#23545;&#20013;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;i&#65289;&#25945;&#24072;&#21644;&#23398;&#29983;&#26159;&#21542;&#22312;&#25509;&#36817;&#27491;&#30830;&#20998;&#31867;&#30340;&#25968;&#25454;&#26679;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#65288;ii&#65289;&#22312;&#25968;&#25454;&#26679;&#26412;&#21608;&#22260;&#65292;&#32463;&#36807;&#33976;&#39311;&#30340;&#23398;&#29983;&#26159;&#21542;&#20687;&#25945;&#24072;&#19968;&#26679;&#33258;&#20449;&#12290;&#36825;&#20123;&#37117;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#32771;&#34385;&#20174;&#40065;&#26834;&#25945;&#24072;&#20013;&#35757;&#32451;&#36739;&#23567;&#23398;&#29983;&#32593;&#32476;&#30340;&#37096;&#32626;&#26102;&#38750;&#24120;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24544;&#23454;&#30340;&#27169;&#20223;&#26694;&#26550;&#26469;&#35752;&#35770;&#32622;&#20449;&#24230;&#30340;&#30456;&#23545;&#26657;&#20934;&#65292;&#24182;&#25552;&#20379;&#23454;&#35777;&#21644;&#35748;&#35777;&#26041;&#27861;&#26469;&#35780;&#20272;&#23398;&#29983;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;PCGRL&#26694;&#26550;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24179;&#34913;&#22522;&#20110;&#22270;&#22359;&#30340;&#31454;&#20105;&#24615;&#21452;&#20154;&#28216;&#25103;&#32423;&#21035;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#26032;&#22411;&#22522;&#20110;&#20132;&#25442;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#30340;&#20132;&#25442;&#34892;&#20026;&#26469;&#21028;&#26029;&#21738;&#20123;&#22270;&#22359;&#31867;&#22411;&#24433;&#21709;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.04429</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#31454;&#20105;&#21452;&#20154;&#28216;&#25103;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Balancing of competitive two-player Game Levels with Reinforcement Learning. (arXiv:2306.04429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;PCGRL&#26694;&#26550;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24179;&#34913;&#22522;&#20110;&#22270;&#22359;&#30340;&#31454;&#20105;&#24615;&#21452;&#20154;&#28216;&#25103;&#32423;&#21035;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#26032;&#22411;&#22522;&#20110;&#20132;&#25442;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#30340;&#20132;&#25442;&#34892;&#20026;&#26469;&#21028;&#26029;&#21738;&#20123;&#22270;&#22359;&#31867;&#22411;&#24433;&#21709;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31454;&#20105;&#24615;&#30340;&#21452;&#20154;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#27700;&#24179;&#30340;&#24179;&#34913;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#21644;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#23545;&#31216;&#30340;&#28216;&#25103;&#27700;&#24179;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#36817;&#25512;&#20986;&#30340;PCGRL&#26694;&#26550;&#20013;&#33258;&#21160;&#24179;&#34913;&#22522;&#20110;&#22270;&#22359;&#30340;&#32423;&#21035;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65306;(1)&#32423;&#21035;&#29983;&#25104;&#22120;&#65292;(2)&#24179;&#34913;&#20195;&#29702;&#21644;(3)&#22870;&#21169;&#24314;&#27169;&#27169;&#25311;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#21453;&#22797;&#29609;&#28216;&#25103;&#27700;&#24179;&#65292;&#24179;&#34913;&#20195;&#29702;&#20250;&#26681;&#25454;&#25152;&#26377;&#29609;&#23478;&#30340;&#30456;&#21516;&#32988;&#29575;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#32780;&#21463;&#21040;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20132;&#25442;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#23545;&#21487;&#29609;&#24615;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25945;&#20250;&#20195;&#29702;&#26356;&#22909;&#12289;&#26356;&#24555;&#22320;&#25913;&#21464;&#24179;&#34913;&#32423;&#21035;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#30340;PCGRL&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#30340;&#20132;&#25442;&#34892;&#20026;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#21738;&#20123;&#22270;&#22359;&#31867;&#22411;&#24433;&#21709;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The balancing process for game levels in a competitive two-player context involves a lot of manual work and testing, particularly in non-symmetrical game levels. In this paper, we propose an architecture for automated balancing of tile-based levels within the recently introduced PCGRL framework (procedural content generation via reinforcement learning). Our architecture is divided into three parts: (1) a level generator, (2) a balancing agent and, (3) a reward modeling simulation. By playing the level in a simulation repeatedly, the balancing agent is rewarded for modifying it towards the same win rates for all players. To this end, we introduce a novel family of swap-based representations to increase robustness towards playability. We show that this approach is capable to teach an agent how to alter a level for balancing better and faster than plain PCGRL. In addition, by analyzing the agent's swapping behavior, we can draw conclusions about which tile types influence the balancing mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.04425</link><description>&lt;p&gt;
&#21033;&#29992;&#31283;&#23450;&#24179;&#34913;&#28857;&#25512;&#36827;&#39640;&#24615;&#33021;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards High-Performance Exploratory Data Analysis (EDA) Via Stable Equilibrium Point. (arXiv:2306.04425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65288;EDA&#65289;&#26159;&#25968;&#25454;&#31185;&#23398;&#39033;&#30446;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24179;&#34913;&#28857;&#65288;SEP&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;EDA&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;SEP&#20316;&#20026;&#20195;&#34920;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20026;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#38750;&#24120;&#29420;&#29305;&#30340;&#23646;&#24615;&#26159;&#65292;SEP&#23558;&#30452;&#25509;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#23646;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploratory data analysis (EDA) is a vital procedure for data science projects. In this work, we introduce a stable equilibrium point (SEP) - based framework for improving the efficiency and solution quality of EDA. By exploiting the SEPs to be the representative points, our approach aims to generate high-quality clustering and data visualization for large-scale data sets. A very unique property of the proposed method is that the SEPs will directly encode the clustering properties of data sets. Compared with prior state-of-the-art clustering and data visualization methods, the proposed methods allow substantially improving computing efficiency and solution quality for large-scale data analysis tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04423</link><description>&lt;p&gt;
&#35745;&#31639;&#26368;&#20248;&#26641;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Computing Optimal Tree Ensembles. (arXiv:2306.04423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#21644;&#20915;&#31574;&#26641;&#38598;&#21512;&#26159;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#23637;&#20801;&#35768;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#65288;&#22914;&#22823;&#23567;&#25110;&#28145;&#24230;&#65289;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#19981;&#30693;&#36947;&#26377;&#20851;&#26641;&#38598;&#21512;&#30340;&#27492;&#31867;&#30740;&#31350;&#65292;&#24182;&#26088;&#22312;&#20026;&#35813;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#20027;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#19979;&#38480;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33021;&#22815;&#36716;&#31227;&#21644;&#22823;&#22823;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#65292;&#33719;&#24471;&#19968;&#20010; $(6\delta D S)^S \cdot poly$-time &#31639;&#27861;&#65292;&#20854;&#20013; $S$ &#26159;&#26641;&#38598;&#21512;&#20013;&#21106;&#25968;&#65292;$D$ &#26159;&#26368;&#22823;&#22495;&#22823;&#23567;&#65292;$\delta$ &#26159;&#20004;&#20010;&#31034;&#20363;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#29305;&#24449;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35777;&#26126;&#26641;&#25216;&#26415;&#65292;&#36825;&#20284;&#20046;&#23545;&#23454;&#36341;&#20063;&#24456;&#26377;&#21069;&#36884;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21160;&#24577;&#35268;&#21010;&#23545;&#20110;&#20915;&#31574;&#26641;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#32780;&#23545;&#20110;&#26641;&#38598;&#21512;&#20063;&#21487;&#33021;&#26159;&#21487;&#34892;&#30340;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010; $\ell^n \cdot poly$-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forests and, more generally, (decision\nobreakdash-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#33258;&#23545;&#25239;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#21382;&#21490;&#31574;&#30053;&#32435;&#20837;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20195;&#29702;&#30340;&#25928;&#29575;&#25552;&#21319;&#21644;&#24378;&#22823;&#30340;&#36712;&#36857;&#26597;&#25214;&#12290;</title><link>http://arxiv.org/abs/2306.04403</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#33258;&#23545;&#25239;&#31639;&#27861;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Policy-Based Self-Competition for Planning Problems. (arXiv:2306.04403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#33258;&#23545;&#25239;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#21382;&#21490;&#31574;&#30053;&#32435;&#20837;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20195;&#29702;&#30340;&#25928;&#29575;&#25552;&#21319;&#21644;&#24378;&#22823;&#30340;&#36712;&#36857;&#26597;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AlphaZero&#31867;&#22411;&#30340;&#31639;&#27861;&#22312;&#21333;&#20154;&#20219;&#21153;&#19978;&#22914;&#26524;&#30001;&#20110;&#24341;&#23548;&#26641;&#25628;&#32034;&#30340;&#20215;&#20540;&#32593;&#32476;&#26080;&#27861;&#36275;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#19968;&#27425;&#36816;&#31639;&#30340;&#32467;&#26524;&#65292;&#23601;&#26377;&#21487;&#33021;&#20572;&#27490;&#25552;&#39640;&#12290;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#20043;&#19968;&#26159;&#36890;&#36807;&#33258;&#23545;&#25239;&#26469;&#36716;&#21270;&#21333;&#20154;&#20219;&#21153;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20174;&#20195;&#29702;&#30340;&#21382;&#21490;&#34920;&#29616;&#35745;&#31639;&#26631;&#37327;&#22522;&#32447;&#65292;&#24182;&#23558;&#19968;&#20010;&#24207;&#21015;&#30340;&#22870;&#21169;&#37325;&#26500;&#20026;&#20108;&#36827;&#21046;&#36755;&#20986;&#65292;&#25351;&#31034;&#26159;&#21542;&#36229;&#36807;&#20102;&#22522;&#32447;&#12290;&#20294;&#26159;&#65292;&#27492;&#22522;&#32447;&#23545;&#20195;&#29702;&#25928;&#29575;&#25552;&#21319;&#30340;&#25112;&#30053;&#20449;&#24687;&#26377;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23545;&#25239;&#30340;&#24605;&#24819;&#65292;&#30452;&#25509;&#23558;&#21382;&#21490;&#31574;&#30053;&#32435;&#20837;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#26631;&#37327;&#34920;&#29616;&#32435;&#20837;&#12290;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;Gumbel AlphaZero&#65288;GAZ&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAZ'Play-to-Plan'&#65288;GAZ PTP&#65289;&#31639;&#27861;&#65292;&#20195;&#29702;&#36890;&#36807;&#35268;&#21010;&#21453;&#23545;&#33258;&#24049;&#30340;&#21487;&#33021;&#31574;&#30053;&#26469;&#23398;&#20064;&#25214;&#21040;&#24378;&#22823;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent's historical performances and to reshape an episode's reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ 'Play-to-Plan' (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#36896;&#25104;&#30340;&#20559;&#35265;&#65292;&#24182;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04400</link><description>&lt;p&gt;
&#19968;&#31181;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Fair Classifier Embracing Triplet Collapse. (arXiv:2306.04400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#36896;&#25104;&#30340;&#20559;&#35265;&#65292;&#24182;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#22312;&#20351;&#29992;&#38543;&#26426;&#19977;&#20803;&#32452;&#36873;&#25321;&#26102;&#65292;&#22312;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36793;&#38469;&#22823;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#20219;&#24847;&#20004;&#28857;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#26102;&#37319;&#29992;&#19977;&#20803;&#32452;&#25240;&#21472;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the behaviour of the triplet loss and show that it can be exploited to limit the biases created and perpetuated by machine learning models. Our fair classifier uses the collapse of the triplet loss when its margin is greater than the maximum distance between two points in the latent space, in the case of stochastic triplet selection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;&#30340;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.04396</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25913;&#36827;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance. (arXiv:2306.04396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#25193;&#25955;&#22270;&#20687;&#32763;&#35793;&#30340;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#65292;&#36890;&#24120;&#23384;&#22312;&#30528;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#30041;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#26799;&#24230;&#24341;&#23548;&#26469;&#25351;&#23548;&#25193;&#25955;&#37319;&#26679;&#30340;&#21453;&#21521;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#23548;&#33268;&#20102;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#22270;&#20687;&#25805;&#20316;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#22270;&#20687;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04384</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65306;&#32763;&#35793;&#36824;&#26159;&#36328;&#35821;&#35328;&#36801;&#31227;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#38750;&#33521;&#35821;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31561;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#65288;CLT&#65289;&#26159;&#19968;&#31181;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#35821;&#35328;&#19978;&#25552;&#20379;&#39640;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#20854;&#20182;&#21033;&#29992;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25191;&#34892;NER&#65292;&#32780;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#32763;&#35793;&#35757;&#32451;&#38598;&#25110;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#19982;&#36825;&#20004;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#25191;&#34892;&#20020;&#24202;NER&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#35821;&#35328;&#30340;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MedNERF&#65292;&#36825;&#26159;&#20174;&#27861;&#22269;&#33647;&#29289;&#22788;&#26041;&#20013;&#25552;&#21462;&#30340;&#21307;&#23398;NER&#27979;&#35797;&#38598;&#65292;&#24182;&#20351;&#29992;&#19982;&#33521;&#35821;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#25351;&#21335;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24503;&#35821;&#21307;&#23398;&#35821;&#26009;&#24211;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#20004;&#31181;&#30446;&#26631;&#35821;&#35328;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JWINS&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#20849;&#20139;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#34917;&#20607;&#30001;&#31232;&#30095;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#25130;&#26029;&#26469;&#20943;&#23569;&#36890;&#20449;&#29992;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;&#26356;&#23569;&#30340;&#23383;&#33410;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04377</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#20013;&#29992;&#26356;&#23569;&#30340;&#20195;&#20215;&#33719;&#24471;&#26356;&#22810;&#12290;&#65288;arXiv:2306.04377v1 [cs.DC]&#65289;
&lt;/p&gt;
&lt;p&gt;
Get More for Less in Decentralized Learning Systems. (arXiv:2306.04377v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JWINS&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#20849;&#20139;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#34917;&#20607;&#30001;&#31232;&#30095;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36890;&#20449;&#25130;&#26029;&#26469;&#20943;&#23569;&#36890;&#20449;&#29992;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;&#26356;&#23569;&#30340;&#23383;&#33410;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#22240;&#20026;&#33021;&#22815;&#36991;&#20813;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#32780;&#20165;&#36890;&#36807;&#20132;&#27969;&#27169;&#22411;&#21442;&#25968;&#20445;&#25252;&#20102;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#22823;&#35268;&#27169;&#23545;&#20998;&#24067;&#24335;&#35757;&#32451;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#8212;&#8212;&#27599;&#20010;&#33410;&#28857;&#38656;&#35201;&#20132;&#25442;&#25968;&#21315;&#19975;&#20010;&#25968;&#25454;&#65292;&#23481;&#26131;&#23548;&#33268;&#32593;&#32476;&#36229;&#36127;&#33655;&#12290;&#26412;&#25991;&#25552;&#20986;JWINS&#65306;&#19968;&#31181;&#36890;&#20449;&#25928;&#29575;&#39640;&#19988;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#31232;&#30095;&#21270;&#26469;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#12290;JWINS&#20351;&#29992;&#23567;&#27874;&#21464;&#25442;&#26469;&#38480;&#21046;&#22240;&#31232;&#30095;&#21270;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#36890;&#20449;&#20999;&#26029;&#26469;&#38477;&#20302;&#36890;&#20449;&#29992;&#37327;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;96&#20010;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33410;&#28857;&#30340;&#23454;&#29616;&#25928;&#26524;&#65292;&#35777;&#26126;JWINS&#21487;&#20197;&#22312;&#21457;&#36865;64&#65285;&#26356;&#23569;&#30340;&#23383;&#33410;&#26102;&#23454;&#29616;&#19982;&#23436;&#20840;&#20849;&#20139;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#30456;&#20284;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36890;&#20449;&#39044;&#31639;&#19979;&#65292;JWINS&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning (DL) systems have been gaining popularity because they avoid raw data sharing by communicating only model parameters, hence preserving data confidentiality. However, the large size of deep neural networks poses a significant challenge for decentralized training, since each node needs to exchange gigabytes of data, overloading the network. In this paper, we address this challenge with JWINS, a communication-efficient and fully decentralized learning system that shares only a subset of parameters through sparsification. JWINS uses wavelet transform to limit the information loss due to sparsification and a randomized communication cut-off that reduces communication usage without damaging the performance of trained models. We demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can achieve similar accuracies to full-sharing DL while sending up to 64% fewer bytes. Additionally, on low communication budgets, JWINS outperforms the state-of-the-art com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04376</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#30340;&#26631;&#31614;&#20559;&#31227;&#37327;&#37327;&#21270;&#21450;&#20854;&#40065;&#26834;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DFM&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26631;&#31614;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#19978;&#38480;&#21644;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#23398;&#20064;&#22788;&#29702;&#22312;&#26631;&#31614;&#20559;&#31227;&#19979;&#20272;&#35745;&#30446;&#26631;&#26631;&#31614;&#20998;&#24067;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20998;&#24067;&#29305;&#24449;&#21305;&#37197;&#65288;DFM&#65289;&#65292;&#23558;&#20808;&#21069;&#25991;&#29486;&#20013;&#24341;&#20837;&#30340;&#21508;&#31181;&#20272;&#35745;&#22120;&#24674;&#22797;&#20026;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;DFM&#31243;&#24207;&#30340;&#19968;&#33324;&#24615;&#33021;&#30028;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25512;&#23548;&#30340;&#30028;&#38480;&#30340;&#33509;&#24178;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#20998;&#26512;&#25193;&#23637;&#21040;&#30740;&#31350;DFM&#31243;&#24207;&#22312;&#26410;&#31934;&#30830;&#20551;&#35774;&#26631;&#31614;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#21463;&#21040;&#26410;&#30693;&#20998;&#24067;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35814;&#32454;&#30340;&#25968;&#23383;&#30740;&#31350;&#30830;&#35748;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21407;&#29702;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;DFM&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#30028;&#38480;&#26174;&#33879;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.04375</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning via Wasserstein-Based High Probability Generalisation Bounds. (arXiv:2306.04375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#30028;&#38480;&#26174;&#33879;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#20013;&#65292;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#25110;&#27867;&#21270;&#24046;&#36317;&#19978;&#38480;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#23588;&#20854;&#26159;PAC-Bayesian&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;PAC-Bayesian&#26694;&#26550;&#30340;&#23616;&#38480;&#26159;&#22823;&#22810;&#25968;&#30028;&#38480;&#28041;&#21450;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#39033;&#65288;&#25110;&#20854;&#21464;&#21270;&#65289;&#65292;&#36825;&#21487;&#33021;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#34892;&#20026;&#24182;&#26080;&#27861;&#25429;&#25417;&#23398;&#20064;&#38382;&#39064;&#30340;&#24213;&#23618;&#20960;&#20309;&#32467;&#26500;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#20225;&#22270;&#29992;Wasserstein&#36317;&#31163;&#26367;&#25442;PAC-Bayesian&#30028;&#38480;&#20013;&#30340;KL&#25955;&#24230;&#12290;&#21363;&#20351;&#36825;&#20123;&#30028;&#38480;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#20445;&#25345;&#26399;&#26395;&#65292;&#35201;&#20040;&#23545;&#26377;&#30028;&#25439;&#22833;&#26377;&#25928;&#65292;&#35201;&#20040;&#38590;&#20197;&#22312;SRM&#26694;&#26550;&#20013;&#26368;&#23567;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30028;&#38480;&#20197;&#26174;&#33879;&#24615;&#22320;&#25193;&#23637;&#20102;PAC-Bayesian&#30028;&#38480;&#30340;&#33539;&#22260;&#65292;&#24182;&#22312;&#20960;&#31181;&#32463;&#20856;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26032;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimising upper bounds on the population risk or the generalisation gap has been widely used in structural risk minimisation (SRM) - this is in particular at the core of PAC-Bayesian learning. Despite its successes and unfailing surge of interest in recent years, a limitation of the PAC-Bayesian framework is that most bounds involve a Kullback-Leibler (KL) divergence term (or its variations), which might exhibit erratic behavior and fail to capture the underlying geometric structure of the learning problem - hence restricting its use in practical applications. As a remedy, recent studies have attempted to replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein distance. Even though these bounds alleviated the aforementioned issues to a certain extent, they either hold in expectation, are for bounded losses, or are nontrivial to minimize in an SRM framework. In this work, we contribute to this line of research and prove novel Wasserstein distance-based PAC-Bayesian ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;LASR&#65289;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21644;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#20108;&#32773;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#35821;&#31181;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.04374</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#35821;&#31181;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Label Aware Speech Representation Learning For Language Identification. (arXiv:2306.04374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;LASR&#65289;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21644;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#20108;&#32773;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#35821;&#31181;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#38750;&#35821;&#20041;&#20219;&#21153;&#65292;&#20363;&#22914;&#35821;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#25506;&#32034;&#20351;&#29992;&#20998;&#31867;&#22120;&#27169;&#22411;&#30340;&#30417;&#30563;&#23884;&#20837;&#25552;&#21462;&#26041;&#27861;&#65292;&#35201;&#20040;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19982;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#31216;&#20026;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#65288;LASR&#65289;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#35821;&#35328;&#26631;&#31614;&#19982;&#33258;&#25105;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#12290;&#35821;&#38899;&#34920;&#31034;&#36827;&#19968;&#27493;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#35821;&#35328;&#35782;&#21035;&#23454;&#39564;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598; - FLEURS&#21644;Dhwani&#19978;&#36827;&#34892;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;LASR&#26694;&#26550;&#22312;&#35821;&#35328;&#35782;&#21035;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;LASR&#26041;&#27861;&#23545;&#20110;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20102; PtSe$_2$ &#19981;&#21516;&#23610;&#24230;&#30340;&#32435;&#31859;&#32467;&#26500;&#65292;&#21457;&#29616;&#22312; 10nm &#20197;&#19979;&#30340;&#36739;&#23567;&#23610;&#23544;&#20013;&#65292;&#20854;&#30005;&#20256;&#36755;&#20027;&#35201;&#23616;&#38480;&#20110;&#36793;&#32536;&#65292;&#32780;&#38750;&#33180;&#26412;&#36523;&#65292;&#36825;&#25110;&#35768;&#20250;&#23545; PtSe$_2$&#30340;&#24212;&#29992;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04365</link><description>&lt;p&gt;
PtSe$_2$ &#32435;&#31859;&#32467;&#26500;&#20013;&#30340;&#36793;&#32536;&#30005;&#23548;
&lt;/p&gt;
&lt;p&gt;
Edge conductivity in PtSe$_2$ nanostructures. (arXiv:2306.04365v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04365
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20102; PtSe$_2$ &#19981;&#21516;&#23610;&#24230;&#30340;&#32435;&#31859;&#32467;&#26500;&#65292;&#21457;&#29616;&#22312; 10nm &#20197;&#19979;&#30340;&#36739;&#23567;&#23610;&#23544;&#20013;&#65292;&#20854;&#30005;&#20256;&#36755;&#20027;&#35201;&#23616;&#38480;&#20110;&#36793;&#32536;&#65292;&#32780;&#38750;&#33180;&#26412;&#36523;&#65292;&#36825;&#25110;&#35768;&#20250;&#23545; PtSe$_2$&#30340;&#24212;&#29992;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PtSe$_2$&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20108;&#32500;&#26448;&#26009;&#65292;&#29992;&#20110;&#32435;&#31859;&#26426;&#30005;&#24863;&#24212;&#21644;&#32418;&#22806;&#20809;&#25506;&#27979;&#12290;&#20854;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#28857;&#20043;&#19968;&#26159;&#22312;&#20302;&#20110;500&#176;C&#30340;&#28201;&#24230;&#19979;&#36827;&#34892;&#30340;&#31616;&#26131;&#21512;&#25104;&#65292;&#19982;&#30446;&#21069;&#30340;&#21518;&#31471;&#21322;&#23548;&#20307;&#21152;&#24037;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#29983;&#25104;&#20102;&#22810;&#26230;&#34180;&#33180;&#65292;&#20855;&#26377;&#23610;&#23544;&#20026;5&#21040;100&#32435;&#31859;&#30340;&#32435;&#31859;&#29255;&#29366;&#39046;&#22495;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#23610;&#23544;&#33539;&#22260;&#20869;&#30340;&#27178;&#21521;&#37327;&#23376;&#32422;&#26463;&#25928;&#24212;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;DFT&#31934;&#24230;&#33719;&#21462;&#21407;&#23376;&#38388;&#21183;&#33021;&#65292;&#24182;&#29992;&#23427;&#26469;&#27169;&#25311;PtSe$_2$&#30340;&#24102;&#29366;&#12289;&#34920;&#38754;&#12289;&#32435;&#31859;&#29255;&#21644;&#32435;&#31859;&#26495;&#65292;&#27178;&#21521;&#23485;&#24230;&#22312;5&#21040;15&#32435;&#31859;&#20043;&#38388;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21738;&#20123;&#36793;&#32536;&#32456;&#27490;&#26159;&#26368;&#31283;&#23450;&#30340;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#23567;&#20110;10&#32435;&#31859;&#30340;&#27178;&#21521;&#23610;&#23544;&#65292;&#30005;&#23548;&#29575;&#23616;&#38480;&#20110;&#36793;&#32536;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;PtSe$_2$&#34180;&#33180;&#20013;&#65292;&#20256;&#36755;&#36890;&#36947;&#21487;&#33021;&#30001;&#36793;&#32536;&#32593;&#32476;&#20027;&#23548;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#33180;&#26412;&#36523;&#36827;&#34892;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
PtSe$_2$ is a promising 2D material for nanoelectromechanical sensing and photodetection in the infrared regime. One of its most compelling features is the facile synthesis at temperatures below 500 {\deg}C, which is compatible with current back-end-of-line semiconductor processing. However, this process generates polycrystalline thin films with nanoflake-like domains of 5 to 100 nm size. To investigate the lateral quantum confinement effect in this size regime, we train a deep neural network to obtain an interatomic potential at DFT accuracy and use that to model ribbons, surfaces, nanoflakes, and nanoplatelets of PtSe$_2$ with lateral widths between 5 to 15 nm. We determine which edge terminations are the most stable and find evidence that the electrical conductivity is localized on the edges for lateral sizes below 10 nm. This suggests that the transport channels in thin films of PtSe$_2$ might be dominated by networks of edges, instead of transport through the layers themselves.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#24212;&#29992;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21487;&#34892;&#24615;&#21644;&#24212;&#29992;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#24212;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04343</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#27668;&#20505;&#21464;&#21270;&#23545;&#25239;&#65306;&#24212;&#29992;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation Against Climate Change: Applications and Benchmarks. (arXiv:2306.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#24212;&#29992;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21487;&#34892;&#24615;&#21644;&#24212;&#29992;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#24212;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#26377;&#21147;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#19988;&#27809;&#26377;&#28176;&#36827;&#20449;&#24687;&#30340;&#35774;&#32622;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#27169;&#25311;&#22120;&#27169;&#22411;&#19981;&#21487;&#29992;&#25110;&#38590;&#20197;&#20174;&#20013;&#25277;&#21462;&#26679;&#26412;&#12290;&#34429;&#28982;&#22312;&#27668;&#20505;&#30456;&#20851;&#24212;&#29992;&#20013;&#24050;&#32463;&#26377;&#20102;&#20960;&#20010;&#21487;&#34892;&#24615;&#28436;&#31034;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26696;&#20363;&#65292;&#20294;&#36824;&#27809;&#26377;&#32479;&#19968;&#30340;&#24212;&#29992;&#21644;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#36825;&#26679;&#30340;&#32508;&#36848;&#65292;&#20197;&#40723;&#21169;&#22312;&#37325;&#35201;&#21644;&#36866;&#23452;&#30340;&#24212;&#29992;&#39046;&#22495;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#65306;&#26448;&#26009;&#21457;&#29616;&#12289;&#39118;&#30005;&#22330;&#24067;&#23616;&#12289;&#26368;&#20248;&#21487;&#20877;&#29983;&#33021;&#28304;&#25511;&#21046;&#21644;&#29615;&#22659;&#30417;&#27979;&#12290;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26131;&#20110;&#20351;&#29992;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20195;&#34920;&#30528;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#24314;&#35758;&#26410;&#26469;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation is a powerful method for optimising black-box functions, popular in settings where the true function is expensive to evaluate and no gradient information is available. Bayesian optimisation can improve responses to many optimisation problems within climate change for which simulator models are unavailable or expensive to sample from. While there have been several feasibility demonstrations of Bayesian optimisation in climate-related applications, there has been no unifying review of applications and benchmarks. We provide such a review here, to encourage the use of Bayesian optimisation in important and well-suited application domains. We identify four main application domains: material discovery, wind farm layout, optimal renewable control and environmental monitoring. For each domain we identify a public benchmark or data set that is easy to use and evaluate systems against, while being representative of real-world problems. Due to the lack of a suitable benchma
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;CycleGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.04339</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from Dynamic Contrast-Enhanced MRI. (arXiv:2306.04339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;CycleGAN&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687; (DCE-MRI) &#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#34880;&#31649;&#36890;&#36879;&#24615;&#21644;&#32452;&#32455;&#28748;&#27880;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#28041;&#21450;&#25311;&#21512;&#31034;&#36394;&#21058;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#30001;&#20110;&#22122;&#22768;&#21160;&#33033;&#36755;&#20837;&#20989;&#25968; (AIF) &#30340;&#27979;&#37327;&#24120;&#24120;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#20302;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26377;&#26631;&#31614;&#30340; DCE-MRI &#21644;&#24050;&#26631;&#27880;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#22270;&#12290;&#36825;&#20381;&#36182;&#20110;&#26631;&#31614;&#25968;&#25454;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#38480;&#21046;&#65292;&#21487;&#33021;&#20250;&#24341;&#20837;&#26631;&#31614;&#22122;&#22768;&#65292;&#20351;&#24471;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#32463;&#24120;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#37197;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340; CycleGAN &#26041;&#27861;&#20272;&#35745;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#21644; AIF&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; CycleGAN &#26694;&#26550;&#22522;&#20110;&#33258;&#23545;&#25239;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#33647;&#20195;&#21160;&#21147;&#23398;&#21442;&#25968;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
DCE-MRI provides information about vascular permeability and tissue perfusion through the acquisition of pharmacokinetic parameters. However, traditional methods for estimating these pharmacokinetic parameters involve fitting tracer kinetic models, which often suffer from computational complexity and low accuracy due to noisy arterial input function (AIF) measurements. Although some deep learning approaches have been proposed to tackle these challenges, most existing methods rely on supervised learning that requires paired input DCE-MRI and labeled pharmacokinetic parameter maps. This dependency on labeled data introduces significant time and resource constraints, as well as potential noise in the labels, making supervised learning methods often impractical. To address these limitations, here we present a novel unpaired deep learning method for estimating both pharmacokinetic parameters and the AIF using a physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;&#23448;&#26041;&#32479;&#35745;&#23398;&#20013;&#65292;&#25968;&#25454;&#28304;&#21464;&#26356;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12289;&#36131;&#20219;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#19968;&#20221;&#28165;&#21333;&#21015;&#20986;&#39640;&#39057;&#30340;&#21464;&#26356;&#36215;&#22240;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2306.04338</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;&#32479;&#35745;&#23398;&#25968;&#25454;&#26469;&#28304;&#30340;&#21464;&#26356;
&lt;/p&gt;
&lt;p&gt;
Changing Data Sources in the Age of Machine Learning for Official Statistics. (arXiv:2306.04338v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#26102;&#20195;&#23448;&#26041;&#32479;&#35745;&#23398;&#20013;&#65292;&#25968;&#25454;&#28304;&#21464;&#26356;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12289;&#36131;&#20219;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#19968;&#20221;&#28165;&#21333;&#21015;&#20986;&#39640;&#39057;&#30340;&#21464;&#26356;&#36215;&#22240;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#22312;&#23448;&#26041;&#32479;&#35745;&#25968;&#25454;&#30340;&#29983;&#20135;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#22823;&#37327;&#25968;&#25454;&#30340;&#33258;&#21160;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#38543;&#30528;&#36825;&#26679;&#30340;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#23427;&#20351;&#24471;&#25253;&#21578;&#21464;&#24471;&#26356;&#21450;&#26102;&#12289;&#26356;&#26377;&#28145;&#24230;&#21644;&#26356;&#20855;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31185;&#23398;&#39537;&#21160;&#30340;&#32479;&#35745;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#23436;&#25972;&#24615;&#21462;&#20915;&#20110;&#25968;&#25454;&#28304;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25968;&#25454;&#28304;&#30340;&#21464;&#26356;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#23427;&#20204;&#20250;&#24341;&#21457;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32479;&#35745;&#23398;&#20013;&#24517;&#39035;&#24471;&#21040;&#22949;&#21892;&#22788;&#29702;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22312;&#23448;&#26041;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19982;&#25968;&#25454;&#28304;&#21464;&#26356;&#30456;&#20851;&#30340;&#20027;&#35201;&#39118;&#38505;&#12289;&#36131;&#20219;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#21333;&#65292;&#21015;&#20986;&#20102;&#25968;&#25454;&#28304;&#21464;&#26356;&#26368;&#24120;&#35265;&#30340;&#36215;&#22240;&#21644;&#21407;&#22240;&#65292;&#19981;&#20165;&#26159;&#22312;&#25216;&#26415;&#23618;&#38754;&#65292;&#32780;&#19988;&#28041;&#21450;&#25152;&#26377;&#26435;&#12289;&#20262;&#29702;&#21644;&#27861;&#35268;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, 
&lt;/p&gt;</description></item><item><title>CaptAinGlove&#26159;&#19968;&#27454;&#22522;&#20110;&#32442;&#32455;&#21697;&#30340;&#25163;&#22871;&#65292;&#29992;&#20110;&#35782;&#21035;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#25163;&#21183;&#65292;&#20855;&#26377;&#20302;&#21151;&#32791;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04319</link><description>&lt;p&gt;
CaptAinGlove&#65306;&#22522;&#20110;&#30005;&#23481;&#21644;&#24815;&#24615;&#34701;&#21512;&#30340;&#25163;&#22871;&#65292;&#29992;&#20110;&#23454;&#26102;&#36793;&#32536;&#25163;&#21183;&#35782;&#21035;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time on Edge Hand Gesture Recognition for Drone Control. (arXiv:2306.04319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04319
&lt;/p&gt;
&lt;p&gt;
CaptAinGlove&#26159;&#19968;&#27454;&#22522;&#20110;&#32442;&#32455;&#21697;&#30340;&#25163;&#22871;&#65292;&#29992;&#20110;&#35782;&#21035;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#25163;&#21183;&#65292;&#20855;&#26377;&#20302;&#21151;&#32791;&#12289;&#38544;&#31169;&#20445;&#25252;&#21644;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; CaptAinGlove&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32442;&#32455;&#21697;&#12289;&#20302;&#21151;&#29575;&#65288;1.15&#29926;&#65289;&#12289;&#27880;&#37325;&#38544;&#31169;&#12289;&#23454;&#26102;&#36793;&#32536;&#65288;RTE&#65289;&#25163;&#22871;&#35299;&#20915;&#26041;&#26696;&#65292;&#25317;&#26377;&#24456;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65288;2MB&#65289;&#65292;&#26088;&#22312;&#35782;&#21035;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#25163;&#21183;&#12290;&#25105;&#20204;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#22810;&#27169;&#24577;&#34701;&#21512;&#20943;&#23569;&#21151;&#32791;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;9&#20010;&#31867;&#21035;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;&#65292;&#21253;&#25324;&#20843;&#20010;&#25163;&#21183;&#21629;&#20196;&#21644;&#31354;&#38386;&#29366;&#24577;&#65292;&#20854;F1&#20998;&#25968;&#20026;80%&#12290;&#23545;&#20110;RTE&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;67&#65285;&#30340;F1&#20998;&#25968;&#65288;&#19968;&#20010;&#29992;&#25143;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CaptAinGlove, a textile-based, low-power (1.15Watts), privacy-conscious, real-time on-the-edge (RTE) glove-based solution with a tiny memory footprint (2MB), designed to recognize hand gestures used for drone control. We employ lightweight convolutional neural networks as the backbone models and a hierarchical multimodal fusion to reduce power consumption and improve accuracy. The system yields an F1-score of 80% for the offline evaluation of nine classes; eight hand gesture commands and null activity. For the RTE, we obtained an F1-score of 67% (one user).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04299</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#36807;&#31243;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timing Process Interventions with Causal Inference and Reinforcement Learning. (arXiv:2306.04299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29702;&#35299;&#21644;&#39044;&#27979;&#36807;&#31243;&#21040;&#20854;&#20248;&#21270;&#65292;&#36825;&#22330;&#36716;&#21464;&#20026;&#20225;&#19994;&#21644;&#20854;&#20182;&#32452;&#32455;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21033;&#30410;&#12290;&#31934;&#30830;&#35745;&#26102;&#30340;&#36807;&#31243;&#24178;&#39044;&#26159;&#26377;&#25928;&#20248;&#21270;&#30340;&#22522;&#30707;&#12290;&#22788;&#26041;&#24335;&#36807;&#31243;&#30417;&#25511;(PresPM)&#26159;&#36807;&#31243;&#25366;&#25496;&#30340;&#23376;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#36807;&#31243;&#20248;&#21270;&#12290;&#26032;&#20852;&#30340;PresPM&#25991;&#29486;&#30830;&#23450;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22240;&#26524;&#25512;&#26029;(CI)&#21644;&#24378;&#21270;&#23398;&#20064;(RL)&#65292;&#20294;&#27809;&#26377;&#36827;&#34892;&#23450;&#37327;&#27604;&#36739;&#12290;&#22823;&#22810;&#25968;&#23454;&#39564;&#26159;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#30340;&#65292;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#38459;&#27490;&#20102;&#22312;&#32447;RL&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23545;&#23450;&#26102;&#36807;&#31243;&#24178;&#39044;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#24471;&#30495;&#27491;&#30340;&#22312;&#32447;RL&#21644;&#19982;CI&#30340;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#20801;&#35768;&#23545;&#32467;&#26524;&#36827;&#34892;&#20934;&#30830;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;RL&#30340;&#31574;&#30053;&#32988;&#36807;CI&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21516;&#26102;&#26356;&#21152;&#31283;&#20581;&#12290;&#20107;&#23454;&#19978;&#65292;RL&#31574;&#30053;&#30340;&#32467;&#26524;&#27604;CI&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04293</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#20250;&#35805;&#20381;&#36182;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;&#30340;&#30701;&#35821;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning. (arXiv:2306.04293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;(ODConvQA)&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19968;&#20010;&#36861;&#28335;-&#38405;&#35835;&#22120;&#27169;&#22411;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#20854;&#22312;ODConvQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.04288</link><description>&lt;p&gt;
&#22522;&#20110;&#20572;&#36710;&#22330;&#21344;&#29992;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#36710;&#22330;&#24341;&#23548;&#31995;&#32479;&#20316;&#20026;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#30340;&#19968;&#37096;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#36235;&#21183;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#20801;&#35768;&#39550;&#39542;&#21592;&#22312;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#25628;&#32034;&#21487;&#29992;&#20572;&#36710;&#20301;&#30340;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#25668;&#20687;&#22836;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#22312;&#29305;&#23450;&#30340;&#35270;&#35273;&#26465;&#20214;&#19979;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24403;&#30340;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#36136;&#37327;&#19982;&#26368;&#36817;&#20986;&#29616;&#30340;&#35270;&#35273;Transformer&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22522;&#20110;EfficientNet&#26550;&#26500;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31649;&#36947;&#12290;&#36827;&#34892;&#30340;&#35745;&#31639;&#23454;&#39564;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#26377;&#20102;&#25552;&#39640;&#65292;&#35813;&#27169;&#22411;&#22312;5&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#23548;&#33322;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21521;&#21307;&#29983;&#25552;&#20379;&#21487;&#25805;&#20316;&#21644;&#26131;&#29702;&#35299;&#30340;&#25351;&#23548;&#65292;&#20197;&#25351;&#23548;&#26410;&#26816;&#26597;&#30340;&#32467;&#32928;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#26597;&#36136;&#37327;&#21644;&#21457;&#29616;&#24322;&#24120;&#30149;&#21464;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04269</link><description>&lt;p&gt;
ColNav: &#32467;&#32928;&#38236;&#19979;&#30340;&#23454;&#26102;&#32467;&#32928;&#23548;&#33322;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ColNav: Real-Time Colon Navigation for Colonoscopy. (arXiv:2306.04269v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#23548;&#33322;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21521;&#21307;&#29983;&#25552;&#20379;&#21487;&#25805;&#20316;&#21644;&#26131;&#29702;&#35299;&#30340;&#25351;&#23548;&#65292;&#20197;&#25351;&#23548;&#26410;&#26816;&#26597;&#30340;&#32467;&#32928;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#26597;&#36136;&#37327;&#21644;&#21457;&#29616;&#24322;&#24120;&#30149;&#21464;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#32928;&#38236;&#26816;&#26597;&#26159;&#22823;&#32928;&#30284;&#31579;&#26597;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#26816;&#26597;&#36136;&#37327;&#21487;&#33021;&#21463;&#21040;&#25805;&#20316;&#32773;&#25216;&#26415;&#12289;&#35757;&#32451;&#12289;&#21028;&#26029;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#32928;&#38236;&#23548;&#33322;&#31995;&#32479;&#65292;&#37319;&#29992;&#23454;&#26102;&#26041;&#27861;&#23637;&#31034;&#32467;&#32928;&#30340;&#23637;&#24320;&#35270;&#22270;&#21644;&#25351;&#31034;&#26410;&#26816;&#26597;&#21306;&#22495;&#30340;&#23616;&#37096;&#25351;&#31034;&#22120;&#65292;&#21521;&#21307;&#29983;&#25552;&#20379;&#23454;&#26102;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Colorectal cancer screening through colonoscopy continues to be the dominant global standard, as it allows identifying pre-cancerous or adenomatous lesions and provides the ability to remove them during the procedure itself. Nevertheless, failure by the endoscopist to identify such lesions increases the likelihood of lesion progression to subsequent colorectal cancer. Ultimately, colonoscopy remains operator-dependent, and the wide range of quality in colonoscopy examinations among endoscopists is influenced by variations in their technique, training, and diligence. This paper presents a novel real-time navigation guidance system for Optical Colonoscopy (OC). Our proposed system employs a real-time approach that displays both an unfolded representation of the colon and a local indicator directing to un-inspected areas. These visualizations are presented to the physician during the procedure, providing actionable and comprehensible guidance to un-surveyed areas in real-time, while seaml
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04262</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#25506;&#32034;&#19981;&#30830;&#23450;&#21306;&#22495;&#21644;&#21033;&#29992;&#26377;&#25215;&#35834;&#21306;&#22495;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#23567;&#22411;&#35780;&#20272;&#39044;&#31639;&#30340;&#40657;&#31665;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#31867;&#12290;BO&#31649;&#36947;&#26412;&#36523;&#39640;&#24230;&#21487;&#37197;&#32622;&#65292;&#28041;&#21450;&#21021;&#22987;&#35774;&#35745;&#12289;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#21151;&#33021;&#65288;AF&#65289;&#30340;&#35768;&#22810;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#22914;&#20309;&#20026;&#25163;&#22836;&#38382;&#39064;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#20214;&#30340;&#29702;&#35299;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;AF&#30340;&#23450;&#20041;&#65292;&#20854;&#20027;&#35201;&#30446;&#30340;&#26159;&#24179;&#34913;&#23545;&#39640;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#23545;&#22909;&#35299;&#20915;&#26041;&#26696;&#26377;&#39640;&#25215;&#35834;&#30340;&#21306;&#22495;&#20043;&#38388;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#26399;&#26395;&#25913;&#36827;&#26041;&#27861;&#65288;SAWEI&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#35753;&#25506;&#32034; - &#21033;&#29992;&#26435;&#34913;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#22522;&#20110;BO&#30340;&#25910;&#25947;&#20934;&#21017;&#12290;&#22312;COCO&#22522;&#20934;&#24179;&#21488;&#30340;&#26080;&#22122;&#22768;&#40657;&#31665;BBOB&#20989;&#25968;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#32447;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#20219;&#20309;&#26102;&#38388;&#24615;&#33021;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;SP&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23384;&#22312;&#20449;&#24687;&#25277;&#26679;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36870;&#24378;&#24230;&#21152;&#26435;&#26469;&#23398;&#20064;&#27835;&#30103;&#25928;&#26524;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TESAR-CDE&#12290;</title><link>http://arxiv.org/abs/2306.04255</link><description>&lt;p&gt;
&#24403;&#23398;&#20064;&#38543;&#26102;&#38388;&#39044;&#27979;&#27835;&#30103;&#25928;&#26524;&#26102;&#32771;&#34385;&#20449;&#24687;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accounting For Informative Sampling When Learning to Forecast Treatment Outcomes Over Time. (arXiv:2306.04255v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23384;&#22312;&#20449;&#24687;&#25277;&#26679;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36870;&#24378;&#24230;&#21152;&#26435;&#26469;&#23398;&#20064;&#27835;&#30103;&#25928;&#26524;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TESAR-CDE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20934;&#30830;&#39044;&#27979;&#27835;&#30103;&#25928;&#26524;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#26368;&#32456;&#21487;&#20197;&#20351;&#26356;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37319;&#29992;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#34987;&#22823;&#37327;&#24573;&#35270;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#35266;&#27979;&#25968;&#25454;&#20013;&#23384;&#22312;&#20449;&#24687;&#25277;&#26679;&#12290;&#24403;&#23454;&#20363;&#22312;&#26102;&#38388;&#19978;&#19981;&#35268;&#21017;&#35266;&#27979;&#26102;&#65292;&#25277;&#26679;&#26102;&#38388;&#36890;&#24120;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340; - &#21462;&#20915;&#20110;&#23454;&#20363;&#30340;&#29305;&#24449;&#12289;&#36807;&#21435;&#30340;&#32467;&#26524;&#21644;&#26045;&#29992;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20449;&#24687;&#25277;&#26679;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21327;&#21464;&#25442;&#31227;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#19981;&#36866;&#24403;&#22320;&#32771;&#34385;&#23427;&#20250;&#38480;&#21046;&#27835;&#30103;&#25928;&#26524;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#20449;&#24687;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;TESAR-CDE&#65292;&#26469;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) holds great potential for accurately forecasting treatment outcomes over time, which could ultimately enable the adoption of more individualized treatment strategies in many practical applications. However, a significant challenge that has been largely overlooked by the ML literature on this topic is the presence of informative sampling in observational data. When instances are observed irregularly over time, sampling times are typically not random, but rather informative -- depending on the instance's characteristics, past outcomes, and administered treatments. In this work, we formalize informative sampling as a covariate shift problem and show that it can prohibit accurate estimation of treatment outcomes if not properly accounted for. To overcome this challenge, we present a general framework for learning treatment outcomes in the presence of informative sampling using inverse intensity-weighting, and propose a novel method, TESAR-CDE, that instantiates this f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#21270;&#21521;&#37327;&#22330;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24178;&#20928;&#36755;&#20837;&#21644;&#24322;&#24120;&#36755;&#20837;&#30340;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.04252</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sample Detection Through Neural Network Transport Dynamics. (arXiv:2306.04252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#21270;&#21521;&#37327;&#22330;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24178;&#20928;&#36755;&#20837;&#21644;&#24322;&#24120;&#36755;&#20837;&#30340;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#31163;&#25955;&#21160;&#24577;&#31995;&#32479;&#30340;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#36890;&#36807;&#23618;&#36981;&#24490;&#30340;&#31163;&#25955;&#21521;&#37327;&#22330;&#65292;&#35813;&#26816;&#27979;&#22120;&#21487;&#20197;&#23558;&#24178;&#20928;&#36755;&#20837;&#20174;&#24322;&#24120;&#36755;&#20837;&#20013;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35813;&#21521;&#37327;&#22330;&#36827;&#34892;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#32593;&#32476;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#25903;&#25345;&#19978;&#26356;&#21152;&#35268;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24178;&#20928;&#36755;&#20837;&#30340;&#28608;&#27963;&#26356;&#23481;&#26131;&#19982;&#24322;&#24120;&#36755;&#20837;&#30340;&#28608;&#27963;&#21306;&#20998;&#24320;&#26469;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26816;&#27979;&#22120;&#19982;&#20854;&#20182;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#32593;&#32476;&#21160;&#24577;&#21487;&#20197;&#25552;&#39640;&#23558;&#20869;&#37096;&#23884;&#20837;&#29992;&#20316;&#36755;&#20837;&#30340;&#23545;&#25239;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a detector of adversarial samples that is based on the view of neural networks as discrete dynamic systems. The detector tells clean inputs from abnormal ones by comparing the discrete vector fields they follow through the layers. We also show that regularizing this vector field during training makes the network more regular on the data distribution's support, thus making the activations of clean inputs more distinguishable from those of abnormal ones. Experimentally, we compare our detector favorably to other detectors on seen and unseen attacks, and show that the regularization of the network's dynamics improves the performance of adversarial detectors that use the internal embeddings as inputs, while also improving test accuracy.
&lt;/p&gt;</description></item><item><title>SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04251</link><description>&lt;p&gt;
&#38543;&#26426;&#22349;&#32553;&#65306;&#22914;&#20309;&#21033;&#29992;&#26799;&#24230;&#22122;&#22768;&#20351;SGD&#21160;&#24577;&#36235;&#21521;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04251
&lt;/p&gt;
&lt;p&gt;
SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#20010;&#24378;&#28872;&#38544;&#24335;&#20559;&#22909;&#65292;&#23427;&#23558;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#39537;&#21160;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#29420;&#31435;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#20559;&#22909;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#19981;&#21464;&#38598;&#65292;&#25110;&#32773;&#35828;&#26159;SGD&#26410;&#20462;&#25913;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31867;&#19981;&#21464;&#38598;&#65292;&#23427;&#20204;&#23545;&#24212;&#20110;&#29616;&#20195;&#26550;&#26500;&#20013;&#24120;&#35265;&#30340;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SGD&#22312;&#36825;&#20123;&#31616;&#21333;&#19981;&#21464;&#38598;&#26041;&#38754;&#20855;&#26377;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25439;&#22833;&#26223;&#35266;&#22312;&#19981;&#21464;&#38598;&#21608;&#22260;&#30340;&#26354;&#29575;&#21644;&#38543;&#26426;&#26799;&#24230;&#24341;&#20837;&#30340;&#22122;&#22768;&#20043;&#38388;&#30340;&#31454;&#20105;&#24314;&#31435;&#20102;&#19968;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#24378;&#21560;&#24341;&#21147;&#65292;&#23548;&#33268;&#19982;&#38797;&#28857;&#25110;&#35757;&#32451;&#25439;&#22833;&#30340;&#23616;&#37096;&#26497;&#22823;&#20540;&#30456;&#20851;&#30340;&#21560;&#24341;&#19981;&#21464;&#38598;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
&lt;/p&gt;</description></item><item><title>MobileNMT&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#32763;&#35793;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#21407;&#21017;&#21644;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;NMT&#27169;&#22411;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#19988;&#20854;&#36895;&#24230;&#25552;&#21319;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20294;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;</title><link>http://arxiv.org/abs/2306.04235</link><description>&lt;p&gt;
MobileNMT&#65306;&#22312;15MB&#21644;30ms&#20869;&#23454;&#29616;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
MobileNMT: Enabling Translation in 15MB and 30ms. (arXiv:2306.04235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04235
&lt;/p&gt;
&lt;p&gt;
MobileNMT&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#32763;&#35793;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#21407;&#21017;&#21644;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;NMT&#27169;&#22411;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#19988;&#20854;&#36895;&#24230;&#25552;&#21319;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20294;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#12289;&#20302;&#24310;&#36831;&#21644;&#31163;&#32447;&#22330;&#26223;&#19979;&#65292;&#23558;NMT&#27169;&#22411;&#37096;&#32626;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;NMT&#27169;&#22411;&#30340;&#23481;&#37327;&#36739;&#22823;&#65292;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#23384;&#20648;&#12289;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MobileNMT&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;15MB&#21644;30ms&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#21387;&#32553;&#30340;&#21407;&#21017;&#65292;&#24182;&#19982;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26379;&#21451;INT8&#21644;&#35299;&#30721;&#30340;&#24341;&#25806;&#12290;&#36890;&#36807;&#27169;&#22411;&#21644;&#24341;&#25806;&#30340;&#21327;&#21516;&#35774;&#35745;&#65292;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21152;&#36895;&#20102;47.0&#20493;&#65292;&#33410;&#30465;&#20102;99.5%&#30340;&#20869;&#23384;&#65292;&#20165;&#25439;&#22833;&#20102;11.6%&#30340;BLEU&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/zjersey/Lightseq-ARM &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. The code is publicly available at https://github.com/zjersey/Lightseq-ARM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#36870;&#38382;&#39064;&#30340;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#65292;&#36890;&#36807;&#38181;&#24418;&#25130;&#26029;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;Kohonen&#33258;&#32452;&#32455;&#26144;&#23556;&#21487;&#20197;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#26356;&#22909;&#22320;&#35299;&#37322;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04228</link><description>&lt;p&gt;
&#25968;&#25454;&#25366;&#25496;&#22312;&#36870;&#38382;&#39064;&#20013;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;: &#20197;&#28155;&#21152;&#21046;&#36896;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Data Mining for Faster, Interpretable Solutions to Inverse Problems: A Case Study Using Additive Manufacturing. (arXiv:2306.04228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#36870;&#38382;&#39064;&#30340;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#65292;&#36890;&#36807;&#38181;&#24418;&#25130;&#26029;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#39044;&#27979;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;Kohonen&#33258;&#32452;&#32455;&#26144;&#23556;&#21487;&#20197;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#26356;&#22909;&#22320;&#35299;&#37322;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#20135;&#29983;&#26399;&#26395;&#36755;&#20986;&#20540;&#30340;&#36755;&#20837;&#20540;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#29702;&#35299;&#35299;&#20915;&#26041;&#26696;&#20063;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#38024;&#23545;&#28155;&#21152;&#21046;&#36896;&#20013;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20351;&#36870;&#38382;&#39064;&#30340;&#35299;&#20915;&#26356;&#21152;&#23481;&#26131;&#65292;&#24182;&#21033;&#29992;&#20854;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#25551;&#36848;&#20102;&#36890;&#36807;&#23545;&#38181;&#24418;&#25130;&#26029;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#22914;&#20309;&#26174;&#33879;&#21152;&#36895;&#20195;&#29702;&#32780;&#19981;&#22833;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Kohonen &#33258;&#32452;&#32455;&#26144;&#23556;&#21487;&#20197;&#29992;&#20110;&#22312;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#35266;&#22320;&#35299;&#37322;&#36870;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38024;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#24182;&#38750;&#25152;&#26377;&#36755;&#20837;&#32500;&#24230;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21152;&#26435;&#36317;&#31163;&#24471;&#21040;&#26356;&#22909;&#32452;&#32455;&#26144;&#23556;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving inverse problems, where we find the input values that result in desired values of outputs, can be challenging. The solution process is often computationally expensive and it can be difficult to interpret the solution in high-dimensional input spaces. In this paper, we use a problem from additive manufacturing to address these two issues with the intent of making it easier to solve inverse problems and exploit their results. First, focusing on Gaussian process surrogates that are used to solve inverse problems, we describe how a simple modification to the idea of tapering can substantially speed up the surrogate without losing accuracy in prediction. Second, we demonstrate that Kohonen self-organizing maps can be used to visualize and interpret the solution to the inverse problem in the high-dimensional input space. For our data set, as not all input dimensions are equally important, we show that using weighted distances results in a better organized map that makes the relations
&lt;/p&gt;</description></item><item><title>SAM&#31639;&#27861;&#20013;&#65292;&#20165;&#25200;&#21160;&#35268;&#33539;&#21270;&#23618;&#21487;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20013;&#37117;&#36866;&#29992;&#65292;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#19981;&#34892;&#12290;&#36825;&#21457;&#29616;&#23545;SAM&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04226</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#23618;&#26159;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Normalization Layers Are All That Sharpness-Aware Minimization Needs. (arXiv:2306.04226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04226
&lt;/p&gt;
&lt;p&gt;
SAM&#31639;&#27861;&#20013;&#65292;&#20165;&#25200;&#21160;&#35268;&#33539;&#21270;&#23618;&#21487;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20013;&#37117;&#36866;&#29992;&#65292;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#19981;&#34892;&#12290;&#36825;&#21457;&#29616;&#23545;SAM&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26088;&#22312;&#20943;&#23569;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#65292;&#24182;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;SAM&#30340;&#23545;&#25239;&#27493;&#39588;&#20013;&#21482;&#25200;&#21160;&#20223;&#23556;&#35268;&#33539;&#21270;&#21442;&#25968;&#65288;&#20165;&#21344;&#24635;&#21442;&#25968;&#30340;0.1%&#20197;&#19979;&#65289;&#20248;&#20110;&#25200;&#21160;&#25152;&#26377;&#21442;&#25968;&#12290;&#36825;&#19968;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;SAM&#21464;&#20307;&#21644;ResNet&#65288;&#25209;&#37327;&#24402;&#19968;&#21270;&#65289;&#20197;&#21450;Vision Transformer&#65288;&#23618;&#24402;&#19968;&#21270;&#65289;&#26550;&#26500;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26367;&#20195;&#30340;&#31232;&#30095;&#25200;&#21160;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#22914;&#27492;&#26497;&#31471;&#30340;&#31232;&#30095;&#27700;&#24179;&#19979;&#26080;&#27861;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#26126;&#36825;&#31181;&#34892;&#20026;&#26159;&#35268;&#33539;&#21270;&#23618;&#29305;&#26377;&#30340;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#21457;&#29616;&#37325;&#26032;&#35777;&#23454;&#20102;SAM&#22312;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20943;&#23569;&#38160;&#24230;&#26159;&#21542;&#21807;&#19968;&#23548;&#33268;&#24615;&#33021;&#25552;&#39640;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312; https://github.com/mueller-mp/SAM-ON &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima and has been shown to enhance generalization performance in various settings. In this work we show that perturbing only the affine normalization parameters (comprising less than 0.1% of the total parameters) in the adversarial step of SAM outperforms perturbing all of the parameters. This finding generalizes to different SAM variants and both ResNet (Batch Normalization) and Vision Transformer (Layer Normalization) architectures. We consider alternative sparse perturbation approaches and find that these do not achieve similar performance enhancement at such extreme sparsity levels, showing that this behaviour is unique to the normalization layers. Although our findings reaffirm the effectiveness of SAM in improving generalization performance, they cast doubt on whether this is solely caused by reduced sharpness. The code for our experiments is publicly available at https://github.com/mueller-mp/SAM-ON.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.04225</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#36873;&#25321;&#30340;&#39640;&#25928;&#35270;&#35273;Transformer&#26041;&#27861;&#65292;&#22823;&#24133;&#24230;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#26159;&#35270;&#35273;Transformer&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#21147;&#26367;&#20195;&#32773;&#65292;&#36890;&#36807;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#32780;&#23853;&#38706;&#22836;&#35282;&#12290; &#28982;&#32780;&#65292;&#35270;&#35273;Transformer&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#38271;&#35270;&#39057;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;Transformer&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#22522;&#20110;&#36873;&#25321;&#21644;&#22788;&#29702;&#23569;&#37327;&#26368;&#20855;&#20449;&#24687;&#30340;&#34917;&#19969;&#65292;&#32780;&#24573;&#30053;&#20854;&#20182;&#22320;&#26041;&#30340;&#34917;&#19969;&#12290;&#25105;&#20204;&#21033;&#29992;&#36731;&#37327;&#32423;&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#26469;&#25351;&#23548;&#34917;&#19969;&#36873;&#25321;&#36807;&#31243;&#65292;&#30830;&#20445;&#25152;&#36873;&#34917;&#19969;&#21253;&#21547;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;2D&#23039;&#24577;&#20272;&#35745;&#22522;&#20934;&#65288;&#21363;COCO&#12289;MPII&#21644;OCHuman&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#34429;&#28982;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Convolutional Neural Networks (CNNs) have been widely successful in 2D human pose estimation, Vision Transformers (ViTs) have emerged as a promising alternative to CNNs, boosting state-of-the-art performance. However, the quadratic computational complexity of ViTs has limited their applicability for processing high-resolution images and long videos. To address this challenge, we propose a simple method for reducing ViT's computational complexity based on selecting and processing a small number of most informative patches while disregarding others. We leverage a lightweight pose estimation network to guide the patch selection process, ensuring that the selected patches contain the most important information. Our experimental results on three widely used 2D pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the effectiveness of our proposed methods in significantly improving speed and reducing computational complexity with a slight drop in performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#36820;&#24037;&#27493;&#39588;&#65292;&#20026;&#38646;&#20214;&#36820;&#24037;&#21046;&#23450;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.04223</link><description>&lt;p&gt;
&#23398;&#20064;&#21046;&#23450;&#26368;&#20339;&#36820;&#24037;&#31574;&#30053;&#30340;&#22240;&#26524;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causally Learning an Optimal Rework Policy. (arXiv:2306.04223v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#36820;&#24037;&#27493;&#39588;&#65292;&#20026;&#38646;&#20214;&#36820;&#24037;&#21046;&#23450;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#36820;&#24037;&#26159;&#19968;&#31181;&#26088;&#22312;&#28040;&#38500;&#38169;&#35823;&#25110;&#32416;&#27491;&#19981;&#31526;&#21512;&#25152;&#38656;&#36136;&#37327;&#26631;&#20934;&#30340;&#20135;&#21697;&#30340;&#21487;&#36873;&#29983;&#20135;&#27493;&#39588;&#12290;&#37325;&#26032;&#21152;&#24037;&#29983;&#20135;&#25209;&#27425;&#28041;&#21450;&#37325;&#22797;&#20197;&#21069;&#30340;&#29983;&#20135;&#38454;&#27573;&#65292;&#24182;&#36827;&#34892;&#35843;&#25972;&#20197;&#30830;&#20445;&#26368;&#32456;&#20135;&#21697;&#31526;&#21512;&#25152;&#38656;&#35268;&#26684;&#12290;&#34429;&#28982;&#25552;&#20379;&#20102;&#25913;&#21892;&#20135;&#37327;&#20174;&#32780;&#22686;&#21152;&#29983;&#20135;&#25209;&#27425;&#25910;&#20837;&#30340;&#26426;&#20250;&#65292;&#20294;&#36820;&#24037;&#27493;&#39588;&#20063;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#21152;&#24037;&#24050;&#28385;&#36275;&#30446;&#26631;&#35268;&#26684;&#30340;&#38646;&#20214;&#21487;&#33021;&#20250;&#25439;&#22351;&#23427;&#20204;&#24182;&#38477;&#20302;&#20135;&#37327;&#12290;&#26412;&#25991;&#24212;&#29992;&#21452;&#37325;/&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#26469;&#20272;&#35745;&#20809;&#30005;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#39068;&#33394;&#36716;&#25442;&#36807;&#31243;&#20013;&#19968;&#27425;&#36820;&#24037;&#27493;&#39588;&#23545;&#26368;&#32456;&#20135;&#21697;&#20135;&#37327;&#30340;&#26465;&#20214;&#22788;&#29702;&#25928;&#24212;&#12290; &#25105;&#20204;&#21033;&#29992;DoubleML&#23454;&#29616;&#21046;&#23450;&#38646;&#20214;&#36820;&#24037;&#31574;&#30053;&#24182;&#20174;&#32463;&#39564;&#19978;&#20272;&#35745;&#23427;&#20204;&#30340;&#20215;&#20540;&#12290;&#20174;&#25105;&#20204;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#20013;
&lt;/p&gt;
&lt;p&gt;
In manufacturing, rework refers to an optional step of a production process which aims to eliminate errors or remedy products that do not meet the desired quality standards. Reworking a production lot involves repeating a previous production stage with adjustments to ensure that the final product meets the required specifications. While offering the chance to improve the yield and thus increase the revenue of a production lot, a rework step also incurs additional costs. Additionally, the rework of parts that already meet the target specifications may damage them and decrease the yield. In this paper, we apply double/debiased machine learning (DML) to estimate the conditional treatment effect of a rework step during the color conversion process in opto-electronic semiconductor manufacturing on the final product yield. We utilize the implementation DoubleML to develop policies for the rework of components and estimate their value empirically. From our causal machine learning analysis we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DualHGNN&#65292;&#21033;&#29992;&#21452;&#37325;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#21644;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04214</link><description>&lt;p&gt;
DualHGNN: &#22522;&#20110;&#22810;&#35270;&#22270;&#23398;&#20064;&#21644;&#23494;&#24230;&#24863;&#30693;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#21452;&#37325;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DualHGNN: A Dual Hypergraph Neural Network for Semi-Supervised Node Classification based on Multi-View Learning and Density Awareness. (arXiv:2306.04214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DualHGNN&#65292;&#21033;&#29992;&#21452;&#37325;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#21644;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#24050;&#34987;&#35777;&#26126;&#26159;&#35768;&#22810;&#20855;&#26377;&#39640;&#30740;&#31350;&#20215;&#20540;&#21644;&#24847;&#20041;&#30340;&#24212;&#29992;&#20013;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#21407;&#22987;&#30340;&#20869;&#22312;&#25110;&#20154;&#20026;&#24314;&#31435;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25968;&#25454;&#20043;&#38388;&#30340;&#8220;&#30495;&#23454;&#8221;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22823;&#22810;&#21033;&#29992;&#26174;&#24335;&#22270;&#32467;&#26500;&#65292;&#32780;&#19968;&#20123;&#38544;&#24335;&#20449;&#24687;&#65292;&#20363;&#22914;&#23494;&#24230;&#20449;&#24687;&#65292;&#20063;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DualHGNN&#65292;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#36830;&#25509;&#27169;&#22411;&#65292;&#23558;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#21644;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#21516;&#26102;&#38598;&#25104;&#22312;&#32479;&#19968;&#30340;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based semi-supervised node classification has been shown to become a state-of-the-art approach in many applications with high research value and significance. Most existing methods are only based on the original intrinsic or artificially established graph structure which may not accurately reflect the "true" correlation among data and are not optimal for semi-supervised node classification in the downstream graph neural networks. Besides, while existing graph-based methods mostly utilize the explicit graph structure, some implicit information, for example, the density information, can also provide latent information that can be further exploited. To address these limitations, this paper proposes the Dual Hypergraph Neural Network (DualHGNN), a new dual connection model integrating both hypergraph structure learning and hypergraph representation learning simultaneously in a unified architecture. The DualHGNN first leverages a multi-view hypergraph learning network to explore the o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairMigration&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#26469;&#22266;&#23450;&#26063;&#32676;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2306.04212</link><description>&lt;p&gt;
&#36801;&#31227;&#26063;&#32676;&#20197;&#23454;&#29616;&#20844;&#24179;GNN
&lt;/p&gt;
&lt;p&gt;
Migrate Demographic Group For Fair GNNs. (arXiv:2306.04212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FairMigration&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#26469;&#22266;&#23450;&#26063;&#32676;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;&#35774;&#35745;GNN&#26102;&#24120;&#24120;&#24573;&#30053;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26377;&#20559;&#20449;&#24687;&#24456;&#23481;&#26131;&#24433;&#21709;&#26222;&#36890;&#30340;GNN&#65292;&#23548;&#33268;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#65288;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#31181;&#26063;&#21644;&#24180;&#40836;&#21010;&#20998;&#65289;&#30340;&#20559;&#35265;&#32467;&#26524;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#21162;&#21147;&#26469;&#35299;&#20915;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#25216;&#26415;&#36890;&#24120;&#36890;&#36807;&#21407;&#22987;&#25935;&#24863;&#23646;&#24615;&#23558;&#26063;&#32676;&#36827;&#34892;&#21010;&#20998;&#65292;&#24182;&#20551;&#23450;&#23427;&#20204;&#26159;&#22266;&#23450;&#30340;&#12290;&#19982;&#21407;&#22987;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#26377;&#20559;&#20449;&#24687;&#23558;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#65292;&#26080;&#35770;&#23454;&#26045;&#20844;&#24179;&#25216;&#26415;&#19982;&#21542;&#12290;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;GNN&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;FairMigration&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#36801;&#31227;&#26063;&#32676;&#65292;&#32780;&#19981;&#26159;&#29992;&#21407;&#22987;&#30340;&#25935;&#24863;&#23646;&#24615;&#22266;&#23450;&#23427;&#20204;&#12290;FairMigration&#30001;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which can dynamically migrate the demographic groups instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04203</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#22686;&#24378;&#20851;&#31995;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction. (arXiv:2306.04203v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#35299;&#20915;&#20219;&#21153;&#26041;&#38754;&#30340;&#26174;&#30528;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#20808;&#35757;&#32451;&#22312;&#28023;&#37327;&#35821;&#26009;&#24211;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#35821;&#26009;&#24211;&#25552;&#20379;&#30340;&#30693;&#35782;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;&#65292;&#22312;&#19981;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26524;&#20196;&#20154;&#20852;&#22859;&#19988;&#24456;&#26377;&#24847;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction task is a crucial and challenging aspect of Natural Language Processing. Several methods have surfaced as of late, exhibiting notable performance in addressing the task; however, most of these approaches rely on vast amounts of data from large-scale knowledge graphs or language models pretrained on voluminous corpora. In this paper, we hone in on the effective utilization of solely the knowledge supplied by a corpus to create a high-performing model. Our objective is to showcase that by leveraging the hierarchical structure and relational distribution of entities within a corpus without introducing external knowledge, a relation extraction model can achieve significantly enhanced performance. We therefore proposed a relation extraction approach based on the incorporation of pretrained knowledge graph embeddings at the corpus scale into the sentence-level contextual representation. We conducted a series of experiments which revealed promising and very interesting res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#26469;&#20860;&#39038;&#21464;&#20998;&#25512;&#26029;&#21644;&#26399;&#26395;&#20256;&#25773;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04201</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36817;&#20284;&#25512;&#26029;&#20013;&#25913;&#21892;&#36229;&#21442;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Hyperparameter Learning under Approximate Inference in Gaussian Process Models. (arXiv:2306.04201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#26469;&#20860;&#39038;&#21464;&#20998;&#25512;&#26029;&#21644;&#26399;&#26395;&#20256;&#25773;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#38750;&#20849;&#36717;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#20013;&#65292;&#36817;&#20284;&#25512;&#26029;&#19982;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#23398;&#20064;&#32416;&#32544;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102; GP &#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#23398;&#20064;&#65292;&#24182;&#20851;&#27880;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#19982;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#34429;&#28982; VI &#23545;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#30340;&#19979;&#30028;&#26159;&#25512;&#26029;&#36817;&#20284;&#21518;&#39564;&#30340;&#21512;&#36866;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20687;&#26399;&#26395;&#20256;&#25773;&#65288;EP&#65289;&#20013;&#30452;&#25509;&#36924;&#36817;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#26159;&#26356;&#36866;&#21512;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#35757;&#32451;&#36807;&#31243;&#65292;&#23558;&#26368;&#20339;&#25928;&#26524;&#32467;&#21512;&#21040;&#19968;&#36215;&#65306;&#21033;&#29992;&#20849;&#36717;&#35745;&#31639; VI &#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#20110; EP &#30340;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#36229;&#21442;&#25968;&#23398;&#20064;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102; VI&#12289;EP&#12289;Laplace &#36817;&#20284;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate inference in Gaussian process (GP) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in GP models and focus on the interplay between variational inference (VI) and the learning target. While VI's lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in Expectation Propagation (EP) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation VI for inference and uses an EP-like marginal likelihood approximation for hyperparameter learning. We compare VI, EP, Laplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#24182;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04190</link><description>&lt;p&gt;
&#22522;&#20110;ASR&#30340;&#38405;&#35835;&#25945;&#23398;&#36741;&#23548;&#31995;&#32479;&#65306;&#23545;&#23567;&#23398;&#29983;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to First Graders. (arXiv:2306.04190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04190
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#24182;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#24212;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20110;&#38405;&#35835;&#32451;&#20064;&#20013;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#22312;&#19968;&#39033;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#27454;&#22522;&#20110;ASR&#30340;&#33655;&#20848;&#35821;&#38405;&#35835;&#36741;&#23548;&#36719;&#20214;&#65292;&#26088;&#22312;&#24110;&#21161;&#27491;&#22312;&#23398;&#20064;&#38405;&#35835;&#30340;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ASR&#22312;&#38405;&#35835;&#30340;&#21021;&#32423;&#38454;&#27573;&#26377;&#28508;&#21147;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#38405;&#35835;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#35821;&#26009;&#24211;&#65288;JASMIN&#65289;&#20013;&#30340;&#20799;&#31461;&#35821;&#38899;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Cohen's Kappa&#12289;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F-measures&#31561;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21333;&#35789;&#20026;&#21333;&#20301;&#20998;&#26512;ASR&#31995;&#32479;&#30340;&#27491;&#30830;/&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#24037;&#21028;&#26029;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#26032;&#24320;&#21457;&#30340;ASR&#31995;&#32479;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#21644;&#27491;&#30830;&#30340;&#25298;&#32477;&#38169;&#35823;&#35821;&#38899;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interest in employing automatic speech recognition (ASR) in applications for reading practice has been growing in recent years. In a previous study, we presented an ASR-based Dutch reading tutor application that was developed to provide instantaneous feedback to first-graders learning to read. We saw that ASR has potential at this stage of the reading process, as the results suggested that pupils made progress in reading accuracy and fluency by using the software. In the current study, we used children's speech from an existing corpus (JASMIN) to develop two new ASR systems, and compared the results to those of the previous study. We analyze correct/incorrect classification of the ASR systems using human transcripts at word level, by means of evaluation measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC), precision, recall and F-measures. We observe improvements for the newly developed ASR systems regarding the agreement with human-based judgment and correct reje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65307;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65307;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04186</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#38899;&#39057;&#24072;&#29983;Transformer&#29992;&#20110;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65307;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65307;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#23398;&#20064;&#38899;&#39057;&#34920;&#31034;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#38899;&#39057;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#21253;&#25324;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20004;&#31181;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65292;&#20998;&#21035;&#29992;&#20110;&#23398;&#20064;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#34920;&#31034;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. The ultimate goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. Clip-level tasks classify the scene or sound of an entire audio clip, e.g. audio tagging, instrument recognition, etc. While frame-level tasks detect event-level timestamps from an audio clip, e.g. sound event detection, speaker diarization, etc. Prior studies primarily evaluate on clip-level downstream tasks. Frame-level tasks are important for fine-grained acoustic scene/event understanding, and are generally more challenging than clip-level tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame, responsible for learning clip-level and frame-level representations, respectively. ATST stands for Aud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;</title><link>http://arxiv.org/abs/2306.04181</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#32771;&#23448;&#8221;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23427;&#26159;&#27979;&#35797;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;&#20840;&#38754;&#27979;&#35797;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30475;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65288;LMAE&#65289;&#65292;&#20854;&#20013;LM&#20316;&#20026;&#30693;&#35782;&#28170;&#21338;&#30340;&#32771;&#23448;&#65292;&#26681;&#25454;&#20854;&#30693;&#35782;&#21046;&#23450;&#38382;&#39064;&#24182;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#22240;&#20026;&#21487;&#20197;&#37319;&#29992;&#21508;&#31181;LM&#20316;&#20026;&#32771;&#23448;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#26029;&#26356;&#26032;&#38382;&#39064;&#65292;&#32473;&#20104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35302;&#21457;&#20027;&#39064;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#21644;&#20844;&#27491;&#22320;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31574;&#30053;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25351;&#31034;LM&#32771;&#23448;&#22312;&#35768;&#22810;&#39046;&#22495;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04178</link><description>&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport Model Distributional Robustness. (arXiv:2306.04178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#25239;&#24615;&#20363;&#23376;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#26356;&#23567;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#27169;&#22411;&#31354;&#38388;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#30340;Wasserstein&#29699;&#20013;&#30340;&#27169;&#22411;&#20998;&#24067;&#65292;&#35813;&#27169;&#22411;&#20998;&#24067;&#26368;&#22823;&#21270;&#20102;&#25439;&#22833;&#12290;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#29702;&#35770;&#65292;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#26368;&#20339;&#30340;&#40065;&#26834;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#29305;&#23450;&#24418;&#24335;&#30340;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#65288;&#22914;&#21333;&#20010;&#27169;&#22411;&#19978;&#30340;Dirac delta&#20998;&#24067;&#65292;&#22810;&#20010;&#27169;&#22411;&#19978;&#30340;&#22343;&#21248;&#20998;&#24067;&#21644;&#19968;&#33324;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#30340;&#27010;&#24565;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#26041;&#24335;&#20027;&#35201;&#26159;&#35757;&#32451;&#20915;&#31574;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2306.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Learning for Stochastic Optimization: A Bayesian Perspective. (arXiv:2306.04174v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#26041;&#24335;&#20027;&#35201;&#26159;&#35757;&#32451;&#20915;&#31574;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#19978;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#35270;&#35282;&#30340;&#38543;&#26426;&#20248;&#21270;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26631;&#20934;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#21518;&#39564;&#36125;&#21494;&#26031;&#34892;&#21160;&#26144;&#23556;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20026;&#35299;&#20915;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#21512;&#25104;&#30340;newsvendor&#38382;&#39064;&#21644;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#32463;&#27982;&#20998;&#37197;&#38382;&#39064;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#35757;&#32451;&#26041;&#26696;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#20197;&#21450;&#20915;&#31574;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes. We also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04169</link><description>&lt;p&gt;
&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#21450;&#20854;&#22312;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#26159;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#23558;&#36816;&#34892;&#26102;&#38388;&#20174; n^2k^2 &#20248;&#21270;&#21040;&#20102; n^2k&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two matrices $U, V \in \mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized, where $\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#22122;&#22768;&#26631;&#31614;&#20026;&#24369;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#35889;&#32858;&#31867;&#26694;&#26550;&#29992;&#20110;&#36716;&#25442;&#24369;&#30417;&#30563;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.04160</link><description>&lt;p&gt;
&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#24369;&#30417;&#30563;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking Weak Supervision in Helping Contrastive Learning. (arXiv:2306.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#21644;&#22122;&#22768;&#26631;&#31614;&#20026;&#24369;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#35889;&#32858;&#31867;&#26694;&#26550;&#29992;&#20110;&#36716;&#25442;&#24369;&#30417;&#30563;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26082;&#21487;&#20197;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20063;&#21487;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#20063;&#34987;&#24341;&#20837;&#12290;&#23613;&#31649;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#21322;&#30417;&#30563;&#26631;&#31614;&#21487;&#20197;&#25913;&#21892;&#23545;&#27604;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26159;&#21542;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#25163;&#21160;&#21435;&#22122;&#21518;&#20877;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#24369;&#30417;&#30563;&#19979;&#25506;&#35752;&#20102;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#21644;&#22122;&#22768;&#26631;&#31614;&#20043;&#38388;&#30340;&#26426;&#26800;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#30452;&#35266;&#30340;&#32852;&#21512;&#35757;&#32451;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30340;&#33539;&#24335;&#12290;&#36890;&#36807;&#23558;&#24369;&#30417;&#30563;&#20449;&#24687;&#36716;&#25442;&#20026;&#22522;&#20110;&#21518;&#39564;&#27010;&#29575;&#30340;&#35889;&#32858;&#31867;&#26694;&#26550;&#19979;&#30340;&#30456;&#20284;&#24615;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite the empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. Therefore, to explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. Specifically, we investigate the most intuitive paradigm of jointly training supervised and unsupervised contrastive losses. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23384;&#20648;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#38899;&#20048;&#20449;&#24687;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.04148</link><description>&lt;p&gt;
SANGEET: &#19968;&#31181;&#22522;&#20110;XML&#30340;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#30740;&#31350;&#24320;&#25918;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet. (arXiv:2306.04148v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23384;&#20648;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#38899;&#20048;&#20449;&#24687;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#20016;&#23500;&#38899;&#20048;&#25968;&#25454;&#38598;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#19987;&#27880;&#20110;&#23384;&#20648;&#35821;&#38899;&#25110;&#20048;&#22120;&#24405;&#38899;&#25968;&#25454;&#65292;&#24182;&#24573;&#30053;&#20102;&#20854;&#35270;&#35273;&#34920;&#29616;&#21644;&#26816;&#32034;&#38656;&#27714;&#12290;&#26412;&#25991;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#21517;&#20026;SANGEET&#30340;&#22522;&#20110;XML&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#23384;&#20648;&#33879;&#21517;&#38899;&#20048;&#23398;&#23478;Pt. Vishnu Narayan Bhatkhande&#21019;&#20316;&#30340;&#21271;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20316;&#21697;&#30340;&#20840;&#38754;&#20449;&#24687;&#12290;SANGEET&#20197;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#20445;&#23384;&#20102;&#20219;&#20309;&#32473;&#23450;&#20316;&#21697;&#30340;&#25152;&#38656;&#20449;&#24687;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#12289;&#32467;&#26500;&#12289;&#31526;&#35760;&#12289;&#33410;&#22863;&#21644;&#26059;&#24459;&#20449;&#24687;&#65292;&#20197;&#20415;&#20110;&#38899;&#20048;&#20449;&#24687;&#30340;&#26131;&#20110;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#20026;&#38899;&#20048;&#20449;&#24687;&#30740;&#31350;&#20219;&#21153;&#25552;&#20379;&#30495;&#23454;&#22522;&#30784;&#20449;&#24687;&#65292;&#22240;&#27492;&#25903;&#25345;&#20174;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#36827;&#34892;&#22810;&#39033;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#28436;&#31034;&#20854;&#22312;&#26059;&#24459;&#39044;&#27979;&#20219;&#21153;&#21644;&#27468;&#26354;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is very important to access a rich music dataset that is useful in a wide variety of applications. Currently, available datasets are mostly focused on storing vocal or instrumental recording data and ignoring the requirement of its visual representation and retrieval. This paper attempts to build an XML-based public dataset, called SANGEET, that stores comprehensive information of Hindustani Sangeet (North Indian Classical Music) compositions written by famous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the required information of any given composition including metadata, structural, notational, rhythmic, and melodic information in a standardized way for easy and efficient storage and extraction of musical information. The dataset is intended to provide the ground truth information for music information research tasks, thereby supporting several data-driven analysis from a machine learning perspective. We present the usefulness of the dataset by demonstrating i
&lt;/p&gt;</description></item><item><title>UCTB&#26159;&#19968;&#20010;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;&#65292;&#23427;&#22312;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26041;&#38754;&#25972;&#21512;&#20102;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#35813;&#39046;&#22495;&#22797;&#26434;&#24230;&#39640;&#12289;&#30693;&#35782;&#22810;&#26679;&#12289;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04144</link><description>&lt;p&gt;
UCTB&#65306;&#38754;&#21521;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#30340;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
UCTB: An Urban Computing Tool Box for Spatiotemporal Crowd Flow Prediction. (arXiv:2306.04144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04144
&lt;/p&gt;
&lt;p&gt;
UCTB&#26159;&#19968;&#20010;&#22478;&#24066;&#35745;&#31639;&#24037;&#20855;&#31665;&#65292;&#23427;&#22312;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26041;&#38754;&#25972;&#21512;&#20102;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#35299;&#20915;&#35813;&#39046;&#22495;&#22797;&#26434;&#24230;&#39640;&#12289;&#30693;&#35782;&#22810;&#26679;&#12289;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#26159;&#26234;&#24935;&#22478;&#24066;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#30446;&#21069;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#20154;&#32676;&#27969;&#19982;&#22810;&#20010;&#39046;&#22495;&#30693;&#35782;&#22240;&#32032;&#30456;&#20851;&#65292;&#20294;&#30001;&#20110;&#24212;&#29992;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#65292;&#38590;&#20197;&#21512;&#29702;&#20840;&#38754;&#22320;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#65307;&#20854;&#27425;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#30456;&#20851;&#25216;&#26415;&#30340;&#23454;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#22797;&#21046;&#20808;&#36827;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#32791;&#26102;&#21644;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;UCTB&#30340;&#26102;&#31354;&#20154;&#32676;&#27969;&#39044;&#27979;&#24037;&#20855;&#31665;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#22810;&#20010;&#26102;&#31354;&#39046;&#22495;&#30693;&#35782;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#30456;&#20851;&#20195;&#30721;&#21644;&#25903;&#25345;&#25991;&#26723;&#24050;&#22312; https://github.com/uctb/UCTB &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal crowd flow prediction is one of the key technologies in smart cities. Currently, there are two major pain points that plague related research and practitioners. Firstly, crowd flow is related to multiple domain knowledge factors; however, due to the diversity of application scenarios, it is difficult for subsequent work to make reasonable and comprehensive use of domain knowledge. Secondly, with the development of deep learning technology, the implementation of relevant techniques has become increasingly complex; reproducing advanced models has become a time-consuming and increasingly cumbersome task. To address these issues, we design and implement a spatiotemporal crowd flow prediction toolbox called UCTB (Urban Computing Tool Box), which integrates multiple spatiotemporal domain knowledge and state-of-the-art models simultaneously. The relevant code and supporting documents have been open-sourced at https://github.com/uctb/UCTB.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.04139</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;generative diffusion models&#65289;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#23613;&#31649;&#20854;&#26080;&#22788;&#19981;&#22312;&#19988;&#24212;&#29992;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#25968;&#25454;&#24418;&#24335;&#30456;&#27604;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#24314;&#27169;&#30340;&#25991;&#29486;&#21450;&#20854;&#32508;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#39046;&#22495;&#20013;&#26368;&#36817;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29702;&#35770;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#38543;&#21518;&#21448;&#35814;&#32454;&#25551;&#36848;&#20102;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#29992;&#20219;&#21153;&#21644;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#20013;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22823;&#37096;&#20998;&#24320;&#21019;&#24615;&#24037;&#20316;&#30340;&#25216;&#26415;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30418;&#29366;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25903;&#25345;&#21547;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2306.04133</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#35770;&#23884;&#20837;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Compositional Queries with Set-Theoretic Embeddings. (arXiv:2306.04133v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30418;&#29366;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22238;&#31572;&#32452;&#21512;&#26597;&#35810;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25903;&#25345;&#21547;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#20998;&#38754;&#23548;&#33322;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#35768;&#22810;&#37325;&#35201;&#20219;&#21153;&#20013;&#65292;&#32039;&#20945;&#32780;&#31283;&#20581;&#22320;&#34920;&#31034;&#39033;&#30446;-&#23646;&#24615;&#20851;&#31995;&#30340;&#38656;&#27714;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#39033;&#20219;&#21153;&#30340;&#19968;&#31181;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#30340;&#39640;&#28857;&#31215;&#34920;&#31034;&#39033;&#30446;&#20855;&#26377;&#23646;&#24615;&#65292;&#36825;&#31181;&#34920;&#31034;&#19981;&#20165;&#26159;&#23494;&#38598;&#30340;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#20462;&#27491;&#26377;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36890;&#36807;&#21333;&#20010;&#23646;&#24615;&#65288;&#20363;&#22914;&#8220;&#21916;&#21095;&#30005;&#24433;&#8221;&#65289;&#26816;&#32034;&#39033;&#30446;&#30340;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21521;&#37327;&#23884;&#20837;&#24182;&#19981;&#22826;&#20934;&#30830;&#25903;&#25345;&#32452;&#21512;&#26597;&#35810;&#65288;&#20363;&#22914;&#26082;&#26159;&#21916;&#21095;&#30005;&#24433;&#21448;&#26159;&#33521;&#22269;&#30340;&#65292;&#20294;&#19981;&#26159;&#28010;&#28459;&#30005;&#24433;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38598;&#21512;&#35770;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#29992;&#30418;&#29366;&#23884;&#20837;&#26367;&#25442;&#21521;&#37327;&#65292;&#30418;&#29366;&#23884;&#20837;&#26159;&#19968;&#31181;&#21306;&#22495;&#20026;&#22522;&#30784;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#21487;&#23398;&#20064;&#30340;&#25991;&#27663;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#26597;&#35810;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#30418;&#29366;&#23884;&#20837;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to compactly and robustly represent item-attribute relations arises in many important tasks, such as faceted browsing and recommendation systems. A popular machine learning approach for this task denotes that an item has an attribute by a high dot-product between vectors for the item and attribute -- a representation that is not only dense, but also tends to correct noisy and incomplete data. While this method works well for queries retrieving items by a single attribute (such as \emph{movies that are comedies}), we find that vector embeddings do not so accurately support compositional queries (such as movies that are comedies and British but not romances). To address these set-theoretic compositions, this paper proposes to replace vectors with box embeddings, a region-based representation that can be thought of as learnable Venn diagrams. We introduce a new benchmark dataset for compositional queries, and present experiments and analysis providing insights into the behavior o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;</title><link>http://arxiv.org/abs/2306.04125</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#20132;&#20114;: &#20154;&#31867;&#21644;&#33258;&#21160;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20960;&#20046;&#25152;&#26377;&#22810;&#27169;&#24577;&#38382;&#39064;&#21644;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#22810;&#31181;&#24322;&#26500;&#21644;&#20114;&#32852;&#30340;&#20449;&#21495;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#27169;&#24577;&#21487;&#20197;&#23637;&#29616;&#30340;&#20132;&#20114;&#31867;&#22411;&#65306;&#27599;&#31181;&#27169;&#24577;&#22914;&#20309;&#21333;&#29420;&#25552;&#20379;&#23545;&#20219;&#21153;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#24403;&#23384;&#22312;&#20854;&#20182;&#27169;&#24577;&#26102;&#36825;&#20123;&#20449;&#24687;&#22914;&#20309;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#27880;&#37322;&#32773;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20004;&#31181;&#20998;&#31867;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65306;(1) &#37096;&#20998;&#26631;&#31614;&#65292;&#20854;&#20013;&#19981;&#21516;&#38543;&#26426;&#20998;&#37197;&#30340;&#27880;&#37322;&#32773;&#27880;&#37322;&#32473;&#23450;&#31532;&#19968;&#20010;&#12289;&#31532;&#20108;&#20010;&#21644;&#20004;&#20010;&#27169;&#24577;&#30340;&#26631;&#31614;&#65292;&#20197;&#21450;(2) &#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#20854;&#20013;&#21516;&#19968;&#27880;&#37322;&#32773;&#34987;&#35201;&#27714;&#22312;&#32473;&#20986;&#31532;&#19968;&#20010;&#27169;&#24577;&#20043;&#21069;&#27880;&#37322;&#26631;&#31614;&#65292;&#28982;&#21518;&#32473;&#20986;&#31532;&#20108;&#20010;&#27169;&#24577;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#26126;&#30830;&#22320;&#25512;&#29702;&#20182;&#20204;&#30340;&#31572;&#26696;&#22914;&#20309;&#25913;&#21464;&#65292;&#28982;&#21518;&#25552;&#20986;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#21478;&#19968;&#31181;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;RetroKNN&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#30340;k&#26368;&#36817;&#37051;&#26816;&#32034;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26469;&#25552;&#39640;&#36870;&#21521;&#21512;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04123</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#27169;&#26495;&#26816;&#32034;&#30340;&#36870;&#21521;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis Prediction with Local Template Retrieval. (arXiv:2306.04123v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;RetroKNN&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#30340;k&#26368;&#36817;&#37051;&#26816;&#32034;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26469;&#25552;&#39640;&#36870;&#21521;&#21512;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#33647;&#29289;&#25506;&#32034;&#20013;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#20998;&#23376;&#21453;&#24212;&#29289;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RetroKNN&#65292;&#19968;&#31181;&#22522;&#20110;&#26412;&#22320;&#21453;&#24212;&#27169;&#26495;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#21442;&#25968;&#26816;&#32034;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#27169;&#26495;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#23376;&#27169;&#26495;&#23384;&#20648;&#24211;&#21644;&#19968;&#20010;&#38190;&#27169;&#26495;&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26412;&#22320;&#27169;&#26495;&#65292;&#28982;&#21518;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;k&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#25628;&#32034;&#20174;&#36825;&#20123;&#27169;&#26495;&#20013;&#26816;&#32034;&#12290;&#26816;&#32034;&#21040;&#30340;&#27169;&#26495;&#19982;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30456;&#32467;&#21512;&#20316;&#20026;&#26368;&#32456;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#20197;&#35843;&#25972;&#22522;&#20110;&#38544;&#34255;&#34920;&#31034;&#21644;&#26816;&#32034;&#27169;&#26495;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;KNN&#39044;&#27979;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;USPTO-50K&#21644;USPTO-MIT&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23588;&#20854;&#26159;&#22312;top-1&#31934;&#24230;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which predicts the reactants of a given target molecule, is an essential task for drug discovery. In recent years, the machine learing based retrosynthesis methods have achieved promising results. In this work, we introduce RetroKNN, a local reaction template retrieval method to further boost the performance of template-based systems with non-parametric retrieval. We first build an atom-template store and a bond-template store that contain the local templates in the training data, then retrieve from these templates with a k-nearest-neighbor (KNN) search during inference. The retrieved templates are combined with neural network predictions as the final output. Furthermore, we propose a lightweight adapter to adjust the weights when combing neural network and KNN predictions conditioned on the hidden representation and the retrieved templates. We conduct comprehensive experiments on two widely used benchmarks, the USPTO-50K and USPTO-MIT. Especially for the top-1 accuracy
&lt;/p&gt;</description></item><item><title>MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04120</link><description>&lt;p&gt;
MESSY&#20272;&#35745;&#65306;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04120
&lt;/p&gt;
&lt;p&gt;
MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;MESSY&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#27969;&#30340;&#30697;&#23558;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20174;&#26679;&#26412;&#20013;&#24674;&#22797;&#20026;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#24182;&#23558;ansatz&#20316;&#20026;&#39537;&#21160;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#20989;&#25968;&#30340;&#26679;&#26412;&#19982;&#29468;&#27979;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#30456;&#36830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#29468;&#27979;&#20998;&#24067;&#20855;&#26377;&#26368;&#22823;&#29109;&#24418;&#24335;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#30340;&#26679;&#26412;&#30340;&#30697;&#26500;&#24314;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#39640;&#25928;&#22320;&#25214;&#21040;&#35813;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#24179;&#28369;&#20989;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#26368;&#22823;&#29109;&#27867;&#20989;&#25351;&#25968;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#25628;&#32034;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25104;&#26412;&#19982;&#26679;&#26412;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M$^3$Fair&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#23618;&#38754;&#21644;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#32531;&#35299;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04118</link><description>&lt;p&gt;
M$^3$Fair&#65306;&#22810;&#23618;&#27425;&#22810;&#25935;&#24863;&#23646;&#24615;&#21152;&#26435;&#26041;&#27861;&#20943;&#36731;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
M$^3$Fair: Mitigating Bias in Healthcare Data through Multi-Level and Multi-Sensitive-Attribute Reweighting Method. (arXiv:2306.04118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M$^3$Fair&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#23618;&#38754;&#21644;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#32531;&#35299;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#20998;&#24067;&#22833;&#34913;&#31561;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#21307;&#30103;&#29366;&#20917;&#31561;&#25935;&#24863;&#23646;&#24615;&#36890;&#24120;&#19982;&#27495;&#35270;&#25110;&#20559;&#35265;&#26377;&#20851;&#12290;&#36825;&#20123;&#23646;&#24615;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#33021;&#23545;&#20010;&#20154;&#33719;&#24471;&#30340;&#25252;&#29702;&#36136;&#37327;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#23545;&#20110;&#25552;&#39640;&#20581;&#24247;&#20844;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#21069;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#20854;&#20013;&#65292;&#21152;&#26435;&#65288;RW&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21069;&#22788;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;RW&#35843;&#25972;&#29305;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#26435;&#37325;&#20197;&#35299;&#20915;&#20559;&#24046;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RW&#26041;&#27861;&#22312;&#24179;&#34913;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#22788;&#29702;&#22810;&#23618;&#27425;&#25968;&#25454;&#26041;&#38754;&#25928;&#26524;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; M$^3$Fair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#22810;&#25935;&#24863;&#23646;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#19981;&#21516;&#25968;&#25454;&#23618;&#27425;&#19978;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#12290;M$^3$Fair&#22312;&#32500;&#25345;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#24179;&#34913;&#20102;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;M$^3$Fair&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. However, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. Sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. In healthcare AI, these attributes can play a significant role in determining the quality of care that individuals receive. For example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in US. Therefore, detecting and mitigating bias in data is crucial to enhancing health equity. Bias mitigation methods include pre-processing, in-processing, and post-processing. Among them, Reweighting (RW) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. RW adjusts the weights for samples wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#65292;&#23454;&#29616;&#20102;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.04116</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#36755;&#36816;&#22312;&#19981;&#24179;&#34913;&#30340;&#21333;&#35821;&#35789;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unbalanced Optimal Transport for Unbalanced Word Alignment. (arXiv:2306.04116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#65292;&#23454;&#29616;&#20102;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35821;&#35789;&#23545;&#40784;&#23545;&#20110;&#27169;&#22411;&#21270;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#22320;&#65292;&#31354;&#23545;&#40784;&#26159;&#19968;&#31181;&#26222;&#36941;&#19988;&#20851;&#38190;&#30340;&#29616;&#35937;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#20041;&#19978;&#19981;&#30456;&#20284;&#30340;&#21477;&#23376;&#12290;&#35782;&#21035;&#31354;&#23545;&#40784;&#26412;&#36523;&#23601;&#26377;&#21161;&#20110;&#25512;&#26029;&#21477;&#23376;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#22240;&#20026;&#23427;&#34920;&#26126;&#23384;&#22312;&#20449;&#24687;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#23478;&#26063;&#65288;&#24179;&#34913;&#12289;&#37096;&#20998;&#21644;&#19981;&#24179;&#34913;&#36755;&#36816;&#65289;&#26159;&#33258;&#28982;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#27809;&#26377;&#37327;&#36523;&#23450;&#21046;&#30340;&#25216;&#26415;&#20063;&#33021;&#23454;&#29616;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#28085;&#30422;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#36890;&#29992;OT-based&#23545;&#40784;&#26041;&#27861;&#22312;&#20855;&#26377;&#39640;&#31354;&#23545;&#40784;&#39057;&#29575;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#33021;&#31454;&#20105;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20854;&#32479;&#35745;&#29305;&#24615;&#21644;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04111</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25311;&#29275;&#39039;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20854;&#32479;&#35745;&#29305;&#24615;&#21644;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#35745;&#31639;&#23545;&#20110;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20986;&#33394;&#30340;&#32479;&#35745;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20998;&#24067;&#24335;&#25311;&#29275;&#39039;(DQN)&#26694;&#26550;&#12290;&#22312;DQN&#26041;&#27861;&#20013;&#65292;&#19981;&#38656;&#35201;&#29275;&#39039;&#30697;&#38453;&#27714;&#36870;&#25110;&#36890;&#20449;&#65292;&#36825;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#30456;&#20851;&#26041;&#27861;&#21482;&#20998;&#26512;&#25968;&#20540;&#25910;&#25947;&#65292;&#24182;&#38656;&#35201;&#21457;&#25955;&#30340;&#36845;&#20195;&#27425;&#25968;&#25165;&#33021;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DQN&#26041;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#24182;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#32467;&#26524;&#20272;&#35745;&#22120;&#22312;&#23569;&#37327;&#36845;&#20195;&#19979;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#20998;&#26512;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20165;&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#30456;&#23545;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#26469;&#36991;&#20813;&#22312;&#19981;&#21516;&#30340;&#21021;&#22987;&#22270;&#20687;&#19979;&#20135;&#29983;&#30456;&#21453;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04109</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#30340;&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership inference attack with relative decision boundary distance. (arXiv:2306.04109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20165;&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#30456;&#23545;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#26469;&#36991;&#20813;&#22312;&#19981;&#21516;&#30340;&#21021;&#22987;&#22270;&#20687;&#19979;&#20135;&#29983;&#30456;&#21453;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#27969;&#34892;&#30340;&#38544;&#31169;&#25915;&#20987;&#20043;&#19968;&#65292;&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#26631;&#31614;&#20165;&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;&#26159;&#19968;&#31181;&#21464;&#20307;&#65292;&#21033;&#29992;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#25932;&#25163;&#21482;&#33021;&#35775;&#38382;&#36755;&#20837;&#26679;&#26412;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#21463;&#38543;&#26426;&#21021;&#22987;&#22270;&#20687;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#22240;&#27492;&#21363;&#20351;&#23545;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#26679;&#26412;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#24471;&#21040;&#30456;&#21453;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#26631;&#31614;&#20165;&#35774;&#32622;&#20013;&#30340;&#22810;&#31867;&#33258;&#36866;&#24212;&#25104;&#21592;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#26089;&#26399;&#30340;&#25915;&#20987;&#36845;&#20195;&#20013;&#65292;&#25152;&#26377;&#30446;&#26631;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#24050;&#34987;&#36941;&#21382;&#65292;&#38543;&#21518;&#30340;&#25915;&#20987;&#36845;&#20195;&#20250;&#32487;&#32493;&#20351;&#29992;&#26368;&#30701;&#30340;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#20197;&#33719;&#24471;&#31283;&#23450;&#21644;&#26368;&#20339;&#30340;&#20915;&#31574;&#36793;&#30028;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attack is one of the most popular privacy attacks in machine learning, which aims to predict whether a given sample was contained in the target model's training set. Label-only membership inference attack is a variant that exploits sample robustness and attracts more attention since it assumes a practical scenario in which the adversary only has access to the predicted labels of the input samples. However, since the decision boundary distance, which measures robustness, is strongly affected by the random initial image, the adversary may get opposite results even for the same input samples. In this paper, we propose a new attack method, called muti-class adaptive membership inference attack in the label-only setting. All decision boundary distances for all target classes have been traversed in the early attack iterations, and the subsequent attack iterations continue with the shortest decision boundary distance to obtain a stable and optimal decision boundary distan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04107</link><description>&lt;p&gt;
BeMap&#65306;&#24179;&#34913;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#29992;&#20110;&#20844;&#24179;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
BeMap: Balanced Message Passing for Fair Graph Neural Network. (arXiv:2306.04107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#31216;&#20026;BeMap&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#26469;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#21363;&#28040;&#24687;&#20256;&#36882;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#35777;&#25454;&#26174;&#31034;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#21475;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#36825;&#35201;&#27714;&#32771;&#34385;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#22312;&#20445;&#35777;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#35757;&#32451;&#26399;&#38388;&#24448;&#24448;&#24182;&#19981;&#26126;&#30830;&#32771;&#34385;&#28040;&#24687;&#20256;&#36882;&#22312;GNN&#20013;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#24403;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;1-hop&#37051;&#23621;&#19981;&#24179;&#34913;&#26102;&#65292;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#25918;&#22823;&#20559;&#24046;&#12290;&#22312;&#36825;&#20123;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeMap&#65292;&#19968;&#31181;&#20844;&#24179;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#24863;&#30693;&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#24179;&#34913;&#27599;&#20010;&#33410;&#28857;&#30340;1-hop&#37051;&#23621;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has shown strong empirical performance in many downstream tasks by iteratively aggregating information from the local neighborhood of each node, i.e., message passing. However, concrete evidence has revealed that a graph neural network could be biased against certain demographic groups, which calls for the consideration of algorithmic fairness. Despite the increasing efforts in ensuring algorithmic fairness on graph neural networks, they often do not explicitly consider the induced bias caused by message passing in GNN during training. In this paper, we first investigate the problem of bias amplification in message passing. We empirically and theoretically demonstrate that message passing could amplify the bias when the 1-hop neighbors from different demographic groups are unbalanced. Guided by such analyses, we propose BeMap, a fair message passing method, that leverages a balance-aware sampling strategy to balance the number of the 1-hop neighbors of each n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Phoenix &#19968;&#31181;&#32852;&#37030;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#36328;&#22810;&#20010;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04098</link><description>&lt;p&gt;
Phoenix&#65306;&#19968;&#31181;&#32852;&#37030;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Phoenix: A Federated Generative Diffusion Model. (arXiv:2306.04098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Phoenix &#19968;&#31181;&#32852;&#37030;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#36328;&#22810;&#20010;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#29616;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#31561;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#35273;&#20869;&#23481;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#22411;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#22312;&#25968;&#25454;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#20998;&#25955;&#25216;&#26415;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20010;&#20307;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#36328;&#22810;&#20010;&#25968;&#25454;&#28304;&#35757;&#32451;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#23454;&#29616;&#20248;&#36136;&#22270;&#20687;&#26041;&#38754;&#27604;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; Phoenix &#26041;&#27861;&#26159;&#19968;&#31181;&#26080;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#31574;&#30053;&#25913;&#36827;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21363;&#20351;&#26159;&#22312;&#35757;&#32451;&#32479;&#35745;&#26434;&#36136;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has made impressive strides in enabling users to create diverse and realistic visual content such as images, videos, and audio. However, training generative models on large centralized datasets can pose challenges in terms of data privacy, security, and accessibility. Federated learning (FL) is an approach that uses decentralized techniques to collaboratively train a shared deep learning model while retaining the training data on individual edge devices to preserve data privacy. This paper proposes a novel method for training a Denoising Diffusion Probabilistic Model (DDPM) across multiple data sources using FL techniques. Diffusion models, a newly emerging generative model, show promising results in achieving superior quality images than Generative Adversarial Networks (GANs). Our proposed method Phoenix is an unconditional diffusion model that leverages strategies to improve the data diversity of generated samples even when trained on data with statistical heterogeneity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;DeepONets&#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;X-DeepONet&#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#24182;&#36890;&#36807;&#26681;&#32593;&#32476;&#35299;&#20915;&#20102;&#26631;&#20934;DeepONet&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04096</link><description>&lt;p&gt;
&#19968;&#31181;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#30340;&#26032;&#22411;Deeponet&#27169;&#22411;&#21450;&#20854;&#22312;&#22320;&#38663;&#38663;&#28304;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A novel deeponet model for learning moving-solution operators with applications to earthquake hypocenter localization. (arXiv:2306.04096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;DeepONets&#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;X-DeepONet&#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#24182;&#36890;&#36807;&#26681;&#32593;&#32476;&#35299;&#20915;&#20102;&#26631;&#20934;DeepONet&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#31867;&#27963;&#21160;&#24341;&#36215;&#30340;&#22320;&#38663;&#27963;&#21160;&#23545;&#20844;&#20849;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#22320;&#38663;&#38663;&#28304;&#23450;&#20301;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476; (DeepONets) &#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243; (PDEs) &#30340;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#21033;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#21147;&#37327;&#65292;X-DeepONet &#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#31867;&#20284;&#20110; DeepONet&#65292;X-DeepONet &#21253;&#25324;&#20027;&#24178;&#32593;&#32476;&#21644;&#20998;&#25903;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26681;&#32593;&#32476;&#65292;&#23427;&#19981;&#20165;&#23558;&#26631;&#20934;&#30340; DeepONet &#20056;&#27861;&#31639;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#36824;&#23558;&#21152;&#27861;&#21644;&#20943;&#27861;&#31639;&#23376;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#31227;&#21160;&#22330;&#30340;&#38382;&#39064;&#20013;&#65292;DeepONet &#30340;&#26631;&#20934;&#20056;&#27861;&#25805;&#20316;&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#65292;&#32780;X-DeepONet &#30340;&#26681;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismicity induced by human activities poses a significant threat to public safety, emphasizing the need for accurate and timely earthquake hypocenter localization. In this study, we introduce X-DeepONet, a novel variant of deep operator networks (DeepONets), for learning moving-solution operators of parametric partial differential equations (PDEs), with application to real-time earthquake localization. Leveraging the power of neural operators, X-DeepONet learns to estimate traveltime fields associated with earthquake sources by incorporating information from seismic arrival times and velocity models. Similar to the DeepONet, X-DeepONet includes a trunk net and a branch net. Additionally, we introduce a root network that not only takes the standard DeepONet's multiplication operator as input, it also takes addition and subtraction operators. We show that for problems with moving fields, the standard multiplication operation of DeepONet is insufficient to capture field relocation, while
&lt;/p&gt;</description></item><item><title>Patch-level Routing in Mixture-of-Experts &#21487;&#20197;&#35777;&#26126;&#22312; CNN &#20013;&#20855;&#26377;&#37319;&#26679;&#25928;&#29575;&#65292;&#20854;&#29992;&#20110; patch-level &#36335;&#30001;&#30340;&#20004;&#20010;&#26426;&#21046;&#20998;&#21035;&#26159;&#19987;&#23478;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#20197;&#21450;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.04073</link><description>&lt;p&gt;
Mixture-of-Experts &#20013;&#30340; Patch-level &#36335;&#30001;&#23545;&#20110; CNN &#21487;&#20197;&#35777;&#26126;&#20855;&#26377;&#37319;&#26679;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks. (arXiv:2306.04073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04073
&lt;/p&gt;
&lt;p&gt;
Patch-level Routing in Mixture-of-Experts &#21487;&#20197;&#35777;&#26126;&#22312; CNN &#20013;&#20855;&#26377;&#37319;&#26679;&#25928;&#29575;&#65292;&#20854;&#29992;&#20110; patch-level &#36335;&#30001;&#30340;&#20004;&#20010;&#26426;&#21046;&#20998;&#21035;&#26159;&#19987;&#23478;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#20197;&#21450;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;Mixture-of-Experts&#65288;MoE&#65289;&#26681;&#25454;&#27599;&#20010;&#26679;&#26412;&#25110;&#27599;&#20010;&#26631;&#35760;&#28608;&#27963;&#19968;&#20010;&#25110;&#20960;&#20010;&#19987;&#23478;&#65288;&#23376;&#32593;&#32476;&#65289;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340; Patch-level routing in MoE (pMoE) &#23558;&#27599;&#20010;&#36755;&#20837;&#20998;&#20026; $n$ &#20010; patch&#65288;&#25110; token&#65289; &#24182;&#36890;&#36807;&#20248;&#20808;&#36335;&#30001;&#23558; $l$ &#20010; patch ($l \ll n$) &#21457;&#36865;&#21040;&#27599;&#20010;&#19987;&#23478;&#12290;pMoE &#22312;&#32500;&#25345;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23637;&#29616;&#20986;&#22312;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#24040;&#22823;&#23454;&#38469;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;pMoE &#21644;&#19968;&#33324;&#30340; MoE &#30340;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#21547;&#31946;&#19981;&#28165;&#12290;&#22312;&#20351;&#29992;&#20004;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#28151;&#21512;&#30340;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102; pMoE &#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#38454;&#25968;&#20013; $n/l$ &#30340;&#22240;&#23376;&#35777;&#26126;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#27867;&#21270;&#65288;&#21363;&#26679;&#26412;&#22797;&#26434;&#24230;&#65289;&#65292;&#24182;&#19988;&#32988;&#36807;&#20854;&#20855;&#26377;&#30456;&#21516;&#25110;&#29978;&#33267;&#26356;&#22823;&#23481;&#37327;&#30340;&#21333;&#19987;&#23478;&#23545;&#24212;&#39033;&#12290;pMoE &#30340;&#20248;&#21183;&#20027;&#35201;&#24402;&#22240;&#20110;&#20004;&#20010;&#26426;&#21046;&#65306;1&#65289;&#27599;&#20010;&#19987;&#23478;&#30340;&#26377;&#25928;&#21442;&#25968;&#25968;&#37327;&#22823;&#22823;&#38477;&#20302;&#65292;2&#65289;&#36890;&#36807; Patch-level routing &#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#29305;&#24449;&#31354;&#38388;&#30340;&#21033;&#29992;&#25928;&#29575;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;pMoE &#26159;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#26377;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#65292;&#23545; pMoE &#30340;&#29702;&#35299;&#21487;&#20197;&#28508;&#22312;&#22320;&#25512;&#36827; MoE &#30340;&#29702;&#35770;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \underline{p}atch-level routing in \underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantag
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2306.04072</link><description>&lt;p&gt;
L2&#24402;&#19968;&#21270;&#25216;&#26415;&#22312;&#31616;&#21333;&#39640;&#36136;&#37327;OoD&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;--&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;L2&#24402;&#19968;&#21270;--&#33021;&#22815;&#20135;&#29983;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;&#24403;&#22312;&#27979;&#35797;&#26102;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#30340;L2&#33539;&#25968;&#25104;&#20026;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24778;&#20154;&#30340;&#26367;&#20195;&#32773;&#65292;&#32780;&#24403;&#27809;&#26377;L2&#24402;&#19968;&#21270;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#34892;&#20026;&#21364;&#27809;&#26377;&#37027;&#20040;&#26377;&#25928;&#12290;&#30452;&#35266;&#19978;&#65292;&#29087;&#24713;&#30340;&#22270;&#20687;&#20250;&#20135;&#29983;&#22823;&#30340;&#21521;&#37327;&#65292;&#32780;&#38476;&#29983;&#30340;&#22270;&#20687;&#21017;&#20250;&#20135;&#29983;&#23567;&#30340;&#21521;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#26102;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22312;&#27979;&#35797;&#26102;&#20063;&#27809;&#26377;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26234;&#33021;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#26032;&#31639;&#27861;&#25110;&#20462;&#25913;&#20854;&#20182;&#39046;&#22495;&#30340;&#21512;&#36866;&#31639;&#27861;&#26469;&#26681;&#25454;&#25105;&#20204;&#30340;&#37319;&#26679;&#38656;&#27714;&#35774;&#35745;&#31639;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#31639;&#27861;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#20197;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#37319;&#26679;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.04066</link><description>&lt;p&gt;
&#26234;&#33021;&#37319;&#26679;&#22312;&#20195;&#29702;&#24314;&#27169;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Intelligent sampling for surrogate modeling, hyperparameter optimization, and data analysis. (arXiv:2306.04066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26234;&#33021;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#26032;&#31639;&#27861;&#25110;&#20462;&#25913;&#20854;&#20182;&#39046;&#22495;&#30340;&#21512;&#36866;&#31639;&#27861;&#26469;&#26681;&#25454;&#25105;&#20204;&#30340;&#37319;&#26679;&#38656;&#27714;&#35774;&#35745;&#31639;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#31639;&#27861;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#20197;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#37319;&#26679;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#22270;&#24418;&#23398;&#31561;&#39046;&#22495;&#12290;&#27599;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#26415;&#37117;&#26159;&#20026;&#20102;&#28385;&#36275;&#29305;&#23450;&#30340;&#38480;&#21046;&#32780;&#35774;&#35745;&#30340;&#65292;&#20363;&#22914;&#27599;&#20010;&#32500;&#24230;&#30340;&#33539;&#22260;&#30340;&#22343;&#21248;&#35206;&#30422;&#25110;&#33267;&#23569;&#22312;&#19968;&#23450;&#36317;&#31163;&#20869;&#38543;&#26426;&#37319;&#26679;&#12290;&#24403;&#24212;&#29992;&#31243;&#24207;&#24378;&#21152;&#26032;&#30340;&#38480;&#21046;&#26102;&#65292;&#20363;&#22914;&#38656;&#35201;&#22312;&#38750;&#30697;&#24418;&#22495;&#20013;&#36827;&#34892;&#37319;&#26679;&#25110;&#23558;&#26032;&#26679;&#26412;&#28155;&#21152;&#21040;&#29616;&#26377;&#26679;&#26412;&#38598;&#20013;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#20462;&#25913;&#24403;&#21069;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#20294;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26234;&#33021;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#26032;&#31639;&#27861;&#25110;&#20462;&#25913;&#20854;&#20182;&#39046;&#22495;&#30340;&#21512;&#36866;&#31639;&#27861;&#26469;&#26681;&#25454;&#25105;&#20204;&#30340;&#37319;&#26679;&#38656;&#27714;&#35774;&#35745;&#31639;&#27861;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23450;&#24615;&#21644;&#23450;&#37327;&#27604;&#36739;&#34920;&#26126;&#65292;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#31639;&#27861;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#20197;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#37319;&#26679;&#38656;&#27714;&#65292;&#21253;&#25324;&#20195;&#29702;&#24314;&#27169;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling techniques are used in many fields, including design of experiments, image processing, and graphics. The techniques in each field are designed to meet the constraints specific to that field such as uniform coverage of the range of each dimension or random samples that are at least a certain distance apart from each other. When an application imposes new constraints, for example, by requiring samples in a non-rectangular domain or the addition of new samples to an existing set, a common solution is to modify the algorithm currently in use, often with less than satisfactory results. As an alternative, we propose the concept of intelligent sampling, where we devise algorithms specifically tailored to meet our sampling needs, either by creating new algorithms or by modifying suitable algorithms from other fields. Surprisingly, both qualitative and quantitative comparisons indicate that some relatively simple algorithms can be easily modified to meet the many sampling requirements 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.04064</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#23454;&#29616;&#20998;&#31867;&#25968;&#25454;&#30340;&#21487;&#36716;&#31227;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings. (arXiv:2306.04064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#22330;&#26223;&#19979;&#65292;&#23545;&#20998;&#31867;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#12290;&#28982;&#32780;&#65292;&#20998;&#31867;&#25968;&#25454;&#20855;&#26377;&#31867;&#21035;&#29305;&#24449;&#65292;&#29616;&#26377;&#30340;&#20248;&#21270;&#31243;&#24207;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#21270;&#21040;&#19968;&#20010;&#31354;&#38388;&#20013;&#65292;&#24182;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#25345;&#23545;&#20110;&#24178;&#20928;&#25968;&#25454;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#25991;&#26041;&#27861;&#24212;&#29992;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26102;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training).  In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created u
&lt;/p&gt;</description></item><item><title>RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2306.04054</link><description>&lt;p&gt;
RescueSpeech&#65306;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04054
&lt;/p&gt;
&lt;p&gt;
RescueSpeech&#26159;&#19968;&#20010;&#29992;&#20110;&#25628;&#25937;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#22312;&#26368;&#36817;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20294;&#22312;&#22024;&#26434;&#12289;&#22238;&#22768;&#30340;&#29615;&#22659;&#20013;&#20934;&#30830;&#36716;&#24405;&#23545;&#35805;&#21644;&#24773;&#24863;&#34920;&#36798;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38590;&#24230;&#12290;&#22312;&#25628;&#25937;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#36716;&#24405;&#25937;&#25588;&#38431;&#25104;&#21592;&#20043;&#38388;&#30340;&#23545;&#35805;&#23545;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25628;&#25937;&#22330;&#26223;&#20013;&#35821;&#38899;&#25968;&#25454;&#21644;&#30456;&#20851;&#32972;&#26223;&#22122;&#22768;&#30340;&#31232;&#32570;&#24615;&#20351;&#24471;&#37096;&#32626;&#20581;&#22766;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;&#19968;&#20010;&#21517;&#20026;RescueSpeech&#30340;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#25937;&#25588;&#28436;&#20064;&#30340;&#30495;&#23454;&#35821;&#38899;&#24405;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#31454;&#20105;&#24615;&#35757;&#32451;&#37197;&#26041;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25152;&#36798;&#21040;&#30340;&#24615;&#33021;&#27700;&#24179;&#20173;&#36828;&#26410;&#33021;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04050</link><description>&lt;p&gt;
LLMZip&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25439;&#25991;&#26412;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA-7B&#23545;&#33521;&#35821;&#29109;&#30340;&#28176;&#36817;&#19978;&#30028;&#25552;&#20986;&#20102;&#26032;&#20272;&#35745;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#19982;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#22914;BSC&#12289;ZPAQ&#21644;paq8h&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#36793;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#65292;&#22312;&#27599;&#34892;&#21482;&#26377;&#20004;&#20010;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25554;&#20540;&#31639;&#27861;&#21487;&#20197;&#21487;&#38752;&#24674;&#22797;$X^TX$&#65292;&#36827;&#32780;&#24674;&#22797;$X$&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04049</link><description>&lt;p&gt;
&#20174;&#27599;&#34892;&#20004;&#20010;&#35266;&#27979;&#26469;&#30475;&#30340;&#21333;&#36793;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
One-sided Matrix Completion from Two Observations Per Row. (arXiv:2306.04049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#36793;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#65292;&#22312;&#27599;&#34892;&#21482;&#26377;&#20004;&#20010;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25554;&#20540;&#31639;&#27861;&#21487;&#20197;&#21487;&#38752;&#24674;&#22797;$X^TX$&#65292;&#36827;&#32780;&#24674;&#22797;$X$&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;$X$&#30340;&#19968;&#20123;&#35266;&#27979;&#20540;&#65292;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#26159;&#25512;&#27979;&#32570;&#22833;&#20540;&#30340;&#38382;&#39064;&#65292;&#23427;&#26159;&#24418;&#24335;&#21270;&#25551;&#36848;&#19968;&#31995;&#21015;&#38656;&#35201;&#20272;&#35745;&#32570;&#22833;&#25968;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#24403;&#35266;&#27979;&#21040;&#30340;&#26465;&#30446;&#22826;&#23569;&#32780;&#26080;&#27861;&#23436;&#25104;&#30697;&#38453;&#26102;&#65292;&#21487;&#20197;&#21487;&#38752;&#24674;&#22797;&#22522;&#30784;&#30697;&#38453;&#30340;&#21738;&#20123;&#20854;&#20182;&#26041;&#38754;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#8220;&#21333;&#36793;&#8221;&#30697;&#38453;&#23436;&#25104;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;$X$&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#65292;&#21363;&#20351;&#22312;&#26080;&#27861;&#24674;&#22797;&#24038;&#22855;&#24322;&#21521;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#24403;&#34892;&#25968;&#22823;&#20110;&#21015;&#25968;&#19988;&#35266;&#27979;&#30340;&#24456;&#23569;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#31639;&#27861;&#65292;&#28041;&#21450;&#21040;&#30697;&#38453;$X^TX$&#20013;&#32570;&#22833;&#20540;&#30340;&#25554;&#20540;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#22312;&#27599;&#34892;&#21482;&#26377;&#20004;&#20010;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#21482;&#35201;&#25105;&#20204;&#26377;&#33267;&#23569;$\Omega(r^2 d \log d)$&#34892;&#65292;&#20854;&#20013;$r$&#20026;&#31209;&#65292;$d$&#20026;&#21015;&#25968;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#21487;&#38752;&#22320;&#24674;&#22797;$X^TX$&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#36793;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#20197;&#21450;&#25512;&#33616;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#25152;&#32771;&#34385;&#30340;&#35774;&#32622;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given only a few observed entries from a low-rank matrix $X$, matrix completion is the problem of imputing the missing entries, and it formalizes a wide range of real-world settings that involve estimating missing data. However, when there are too few observed entries to complete the matrix, what other aspects of the underlying matrix can be reliably recovered? We study one such problem setting, that of "one-sided" matrix completion, where our goal is to recover the right singular vectors of $X$, even in the regime where recovering the left singular vectors is impossible, which arises when there are more rows than columns and very few observations. We propose a natural algorithm that involves imputing the missing values of the matrix $X^TX$ and show that even with only two observations per row in $X$, we can provably recover $X^TX$ as long as we have at least $\Omega(r^2 d \log d)$ rows, where $r$ is the rank and $d$ is the number of columns. We evaluate our algorithm on one-sided reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedVal&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#38468;&#21152;&#20449;&#24687;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#20855;&#26377;&#31283;&#20581;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#22312;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.04040</link><description>&lt;p&gt;
FedVal&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#22909;&#22351;
&lt;/p&gt;
&lt;p&gt;
FedVal: Different good or different bad in federated learning. (arXiv:2306.04040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedVal&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#38468;&#21152;&#20449;&#24687;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#20855;&#26377;&#31283;&#20581;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#22312;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#36890;&#36807;&#21508;&#31181;&#27602;&#21270;&#25915;&#20987;&#26469;&#30772;&#22351;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;FL&#22312;&#35299;&#20915;&#22242;&#20307;&#20559;&#35265;&#26041;&#38754;&#20063;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#30830;&#20445;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#33021;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#22788;&#29702;&#65292;&#32780;FL&#31995;&#32479;&#24182;&#27809;&#26377;&#36825;&#20010;&#21151;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedVal&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#31283;&#20581;&#24615;&#21448;&#20855;&#26377;&#20844;&#24179;&#24615;&#65292;&#20854;&#19981;&#38656;&#35201;&#20174;&#23458;&#25143;&#31471;&#33719;&#21462;&#20219;&#20309;&#21487;&#33021;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#21361;&#21450;FL&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26381;&#21153;&#22120;&#31471;&#39564;&#35777;&#26041;&#27861;&#30340;&#21019;&#26032;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#35780;&#20272;&#23458;&#25143;&#31471;&#26356;&#26032;&#24182;&#30830;&#23450;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#20339;&#32858;&#21512;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#27602;&#21270;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#29992;&#20110;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#21644;&#38543;&#21518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are susceptible to attacks from malicious actors who might attempt to corrupt the training model through various poisoning attacks. FL also poses new challenges in addressing group bias, such as ensuring fair performance for different demographic groups. Traditional methods used to address such biases require centralized access to the data, which FL systems do not have. In this paper, we present a novel approach FedVal for both robustness and fairness that does not require any additional information from clients that could raise privacy concerns and consequently compromise the integrity of the FL system. To this end, we propose an innovative score function based on a server-side validation method that assesses client updates and determines the optimal aggregation balance between locally-trained models. Our research shows that this approach not only provides solid protection against poisoning attacks but can also be used to reduce group bias and subsequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;MoL&#65289;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#32467;&#21512;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#65292;&#26412;&#25991;&#25104;&#21151;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.04039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21152;&#36895;&#22120;&#19978;&#30340;&#31070;&#32463;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Neural Retrieval on Accelerators. (arXiv:2306.04039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;MoL&#65289;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#32467;&#21512;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#65292;&#26412;&#25991;&#25104;&#21151;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#38656;&#35201;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#25214;&#20986;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#20854;&#20013;&#65292;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#65288;item&#65289;&#30340;&#30456;&#20284;&#24230;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#12290;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#23558;&#29992;&#25143;&#19982;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#34920;&#31034;&#20026;&#20004;&#20010;&#23398;&#20064;&#23884;&#20837;&#30340;&#28857;&#31215;&#65292;&#21448;&#34987;&#31216;&#20026;&#26368;&#22823;&#20869;&#31215;&#26816;&#32034;&#65288;MIPS&#65289;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#25928;&#29575;&#65292;&#20294;&#26159;&#23427;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#36895;&#22120;&#30340;&#38750;&#28857;&#31215;&#26816;&#32034;&#26041;&#27861;&#65306;&#28151;&#21512;&#23545;&#25968;&#27169;&#22411;&#65288;Mixture of Logits&#65292;MoL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#22522;&#26412;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#24314;&#27169;&#29992;&#25143;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39640;&#38454;&#29992;&#25143;-&#25991;&#26412;&#20132;&#20114;&#20316;&#29992;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#38271;&#23614;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#36824;&#32467;&#21512;&#19968;&#31181;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#65288;h-indexer&#65289;&#23558;MoL&#25193;&#23637;&#21040;&#21333;&#20010;GPU&#19978;&#30340;100M&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04037</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#26512;&#23450;&#37327;&#35780;&#20272;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#24577;&#19979;&#25191;&#34892;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;XAI&#26041;&#27861;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25152;&#38656;&#23646;&#24615;&#30340;&#21508;&#31181;&#31867;&#21035;&#26469;&#23450;&#37327;&#27604;&#36739;XAI&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;XAI&#26041;&#27861;&#20197;&#21152;&#28145;&#23545;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#24037;&#20316;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38236;&#22836;&#20803;&#25968;&#25454;&#23884;&#20837;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;Bokeh&#25928;&#26524;&#20174;&#27169;&#31946;&#21040;&#28165;&#26224;&#21644;&#28165;&#26224;&#21040;&#27169;&#31946;&#36827;&#34892;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33258;&#28982;&#30340;&#25928;&#26524;&#65292;&#24182;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#30340;Bokeh&#25928;&#26524;&#28210;&#26579;&#21644;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04032</link><description>&lt;p&gt;
BokehOrNot&#65306;&#21033;&#29992;&#22270;&#20687;&#21464;&#25442;&#22120;&#21644;&#38236;&#22836;&#20803;&#25968;&#25454;&#23884;&#20837;&#36716;&#25442;Bokeh&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens Metadata Embedding. (arXiv:2306.04032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38236;&#22836;&#20803;&#25968;&#25454;&#23884;&#20837;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;Bokeh&#25928;&#26524;&#20174;&#27169;&#31946;&#21040;&#28165;&#26224;&#21644;&#28165;&#26224;&#21040;&#27169;&#31946;&#36827;&#34892;&#36716;&#25442;&#65292;&#34920;&#29616;&#20986;&#33258;&#28982;&#30340;&#25928;&#26524;&#65292;&#24182;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#30340;Bokeh&#25928;&#26524;&#28210;&#26579;&#21644;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bokeh&#25928;&#26524;&#26159;&#39640;&#31471;&#30456;&#26426;&#19982;&#24191;&#35282;&#38236;&#22836;&#25152;&#20135;&#29983;&#30340;&#19968;&#31181;&#35270;&#35273;&#25928;&#24212;&#65292;&#25552;&#20379;&#20102;&#24841;&#24742;&#30340;&#35270;&#35273;&#20307;&#39564;&#12290;Bokeh&#25928;&#26524;&#30340;&#36716;&#25442;&#20219;&#21153;&#26088;&#22312;&#26681;&#25454;&#21478;&#19968;&#32452;&#38236;&#22836;&#19982;&#20809;&#22280;&#30340;&#32452;&#21512;&#26469;&#20135;&#29983;&#26399;&#26395;&#30340;&#25928;&#26524;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#28210;&#26579;&#29305;&#23450;&#30340;Bokeh&#25928;&#26524;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20027;&#35201;&#26159;&#20174;&#28165;&#26224;&#21040;&#27169;&#31946;&#30340;&#36716;&#25442;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#38236;&#22836;&#20803;&#25968;&#25454;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26032;&#21457;&#24067;&#30340;Bokeh&#25928;&#26524;&#36716;&#25442;&#25968;&#25454;&#38598;&#65288;BETD&#65289;[3]&#20013;&#30340;&#945;&#25513;&#27169;&#35745;&#31639;&#25439;&#22833;&#12290;&#22522;&#20110;&#19978;&#36848;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BokehOrNot&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#38236;&#22836;&#21644;&#20809;&#22280;&#23610;&#23544;&#32452;&#21512;&#30340;&#27169;&#31946;&#21040;&#28165;&#26224;&#21644;&#28165;&#26224;&#21040;&#27169;&#31946;&#30340;Bokeh&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#30340;Bokeh&#28210;&#26579;&#21644;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#65292;&#24182;&#21576;&#29616;&#20986;&#33258;&#28982;&#30340;Bokeh&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bokeh effect is an optical phenomenon that offers a pleasant visual experience, typically generated by high-end cameras with wide aperture lenses. The task of bokeh effect transformation aims to produce a desired effect in one set of lenses and apertures based on another combination. Current models are limited in their ability to render a specific set of bokeh effects, primarily transformations from sharp to blur. In this paper, we propose a novel universal method for embedding lens metadata into the model and introducing a loss calculation method using alpha masks from the newly released Bokeh Effect Transformation Dataset(BETD) [3]. Based on the above techniques, we propose the BokehOrNot model, which is capable of producing both blur-to-sharp and sharp-to-blur bokeh effect with various combinations of lenses and aperture sizes. Our proposed model outperforms current leading bokeh rendering and image restoration models and renders visually natural bokeh effects. Our code is available
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;</title><link>http://arxiv.org/abs/2306.04027</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#27169;&#22411;&#35270;&#35282;&#19979;&#30340;&#24178;&#39044;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#25512;&#24191;&#21040;&#26032;&#30340;&#26465;&#20214;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#20379;&#36275;&#22815;&#22810;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#21487;&#33021;&#26368;&#32456;&#23398;&#20064;&#20174;&#26032;&#30340;&#23454;&#39564;&#26465;&#20214;&#21040;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#32452;&#21512;&#31354;&#38388;&#24456;&#22256;&#38590;&#12290;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#22914;&#26524;&#19981;&#20381;&#36182;&#20110;&#37325;&#30340;&#35268;&#21017;&#21270;&#25110;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#26144;&#23556;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#26159;&#21487;&#38752;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24456;&#38590;&#36777;&#25252;&#25110;&#27979;&#35797;&#12290;&#26412;&#25991;&#20174;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#35821;&#35328;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#22914;&#20309;&#20445;&#35777;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#12290;&#20551;&#35774;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#23427;&#24456;&#26041;&#20415;&#22320;&#22788;&#29702;&#20102;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#21644;&#21367;&#31215;&#21464;&#25442;&#30340;&#36328;&#27169;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#21487;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04021</link><description>&lt;p&gt;
&#21033;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#36328;&#27169;&#24577;&#23450;&#20301;&#30340;&#21367;&#31215;&#21464;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models for Cross-Modal Localization using Convolutional Transformers. (arXiv:2306.04021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#21644;&#21367;&#31215;&#21464;&#25442;&#30340;&#36328;&#27169;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#21487;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;GPS&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#19968;&#20010;&#25645;&#36733;&#26377;&#27979;&#36317;&#20256;&#24863;&#22120;&#30340;&#22320;&#38754;&#36710;&#36742;&#30456;&#23545;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#20301;&#32622;&#12290;&#26412;&#26041;&#27861;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#22320;&#22270;&#26500;&#24314;&#65292;&#36825;&#31181;&#22320;&#22270;&#26159;&#24191;&#27867;&#21487;&#29992;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#65292;&#32780;&#19988;&#20855;&#26377;&#20840;&#38754;&#30340;&#35206;&#30422;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#21464;&#25442;&#30340;&#26041;&#27861;&#65292;&#22312;&#36328;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#24230;&#37327;&#32423;&#21035;&#23450;&#20301;&#65292;&#36825;&#26159;&#30001;&#20110;&#31232;&#30095;&#30340;&#27979;&#36317;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#20016;&#23500;&#30340;&#21355;&#26143;&#22270;&#20687;&#20043;&#38388;&#22806;&#35266;&#24046;&#24322;&#30340;&#26497;&#24230;&#20043;&#22823;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and provide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI,
&lt;/p&gt;</description></item><item><title>Green Steganalyzer&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38544;&#20889;&#20998;&#26512;&#30340;&#22522;&#20110;&#32511;&#33394;&#23398;&#20064;&#33539;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20687;&#32032;&#24322;&#24120;&#39044;&#27979;&#12289;&#23884;&#20837;&#20301;&#32622;&#26816;&#27979;&#21644;&#20915;&#31574;&#34701;&#21512;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#35782;&#21035;&#65292;&#19982;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.04008</link><description>&lt;p&gt;
&#32511;&#33394;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;: Green Steganalyzer
&lt;/p&gt;
&lt;p&gt;
Green Steganalyzer: A Green Learning Approach to Image Steganalysis. (arXiv:2306.04008v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04008
&lt;/p&gt;
&lt;p&gt;
Green Steganalyzer&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38544;&#20889;&#20998;&#26512;&#30340;&#22522;&#20110;&#32511;&#33394;&#23398;&#20064;&#33539;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20687;&#32032;&#24322;&#24120;&#39044;&#27979;&#12289;&#23884;&#20837;&#20301;&#32622;&#26816;&#27979;&#21644;&#20915;&#31574;&#34701;&#21512;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#35782;&#21035;&#65292;&#19982;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32511;&#33394;&#23398;&#20064;&#33539;&#24335;&#30340;&#22270;&#20687;&#38544;&#20889;&#20998;&#26512;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Green Steganalyzer&#65288;GS&#65289;&#12290;GS&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;1&#65289;&#22522;&#20110;&#20687;&#32032;&#30340;&#24322;&#24120;&#39044;&#27979;&#65292;2&#65289;&#23884;&#20837;&#20301;&#32622;&#26816;&#27979;&#65292;3&#65289;&#22270;&#20687;&#32423;&#21035;&#26816;&#27979;&#30340;&#20915;&#31574;&#34701;&#21512;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#34917;&#19969;&#20998;&#35299;&#12289;&#37319;&#29992;Saab&#21464;&#25442;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#39044;&#27979;&#20854;&#20013;&#24515;&#20687;&#32032;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;GS&#22312;&#31532;&#19968;&#20010;&#27169;&#22359;&#20013;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#31532;&#20108;&#20010;&#27169;&#22359;&#20013;&#65292;GS&#20998;&#26512;&#20687;&#32032;&#21644;&#37051;&#22495;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20197;&#25214;&#21040;&#26356;&#39640;&#23884;&#20837;&#27010;&#29575;&#30340;&#20687;&#32032;&#12290;&#22312;&#31532;&#19977;&#20010;&#27169;&#22359;&#20013;&#65292;GS&#32858;&#28966;&#20110;&#26356;&#39640;&#23884;&#20837;&#27010;&#29575;&#30340;&#20687;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24322;&#24120;&#24471;&#20998;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#26368;&#32456;&#30340;&#22270;&#20687;&#32423;&#21035;&#20998;&#31867;&#12290;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;GS&#22312;&#30456;&#21516;&#26816;&#27979;&#24615;&#33021;&#19979;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#27169;&#22411;&#22823;&#23567;&#26356;&#23567;&#65292;&#24182;&#33021;&#23545;S-UNIWARD&#12289;WOW&#21644;HILL&#38544;&#20889;&#26041;&#26696;&#33719;&#24471;&#21487;&#27604;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel learning solution to image steganalysis based on the green learning paradigm, called Green Steganalyzer (GS), is proposed in this work. GS consists of three modules: 1) pixel-based anomaly prediction, 2) embedding location detection, and 3) decision fusion for image-level detection. In the first module, GS decomposes an image into patches, adopts Saab transforms for feature extraction, and conducts self-supervised learning to predict an anomaly score of their center pixel. In the second module, GS analyzes the anomaly scores of a pixel and its neighborhood to find pixels of higher embedding probabilities. In the third module, GS focuses on pixels of higher embedding probabilities and fuses their anomaly scores to make final image-level classification. Compared with state-of-the-art deep-learning models, GS achieves comparable detection performance against S-UNIWARD, WOW and HILL steganography schemes with significantly lower computational complexity and a smaller model size, ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33298;&#23572;&#34917;&#30340;&#38543;&#26426;&#25299;&#25169;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#26469;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#24182;&#22312;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04004</link><description>&lt;p&gt;
&#38754;&#21521;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#38543;&#26426;&#33298;&#23572;&#34917;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Randomized Schur Complement Views for Graph Contrastive Learning. (arXiv:2306.04004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33298;&#23572;&#34917;&#30340;&#38543;&#26426;&#25299;&#25169;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#12290;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#26469;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#24182;&#22312;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33298;&#23572;&#34917;&#30340;&#38543;&#26426;&#25299;&#25169;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#12290;&#32473;&#23450;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#35813;&#25216;&#26415;&#29983;&#25104;&#20854;&#38543;&#26426;&#33298;&#23572;&#34917;&#30340;&#26080;&#20559;&#20272;&#35745;&#65292;&#24182;&#23558;&#30456;&#24212;&#30340;&#22270;&#24418;&#35270;&#20026;&#22686;&#24378;&#35270;&#22270;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35828;&#26126;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#22270;&#25193;&#25955;&#30340;&#32852;&#31995;&#12290;&#19982;&#20197;&#24448;&#30340;&#21162;&#21147;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#21518;&#32493;GCL&#38454;&#27573;&#65288;&#22914;&#32534;&#30721;&#21644;&#23545;&#27604;&#65289;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#26377;&#35745;&#21010;&#22320;&#30740;&#31350;&#22686;&#24378;&#22120;&#30340;&#23454;&#35777;&#26377;&#25928;&#24615;&#12290;&#23545;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22987;&#32456;&#20248;&#20110;&#39044;&#23450;&#20041;&#21644;&#33258;&#36866;&#24212;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a randomized topological augmentor based on Schur complements for Graph Contrastive Learning (GCL). Given a graph laplacian matrix, the technique generates unbiased approximations of its Schur complements and treats the corresponding graphs as augmented views. We discuss the benefits of our approach, provide theoretical justifications and present connections with graph diffusion. Unlike previous efforts, we study the empirical effectiveness of the augmentor in a controlled fashion by varying the design choices for subsequent GCL phases, such as encoding and contrasting. Extensive experiments on node and graph classification benchmarks demonstrate that our technique consistently outperforms pre-defined and adaptive augmentation approaches to achieve state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#25311;&#21512;&#30005;&#30913;&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;&#36890;&#36807;&#19982;&#20844;&#24320;&#21487;&#29992;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#22343;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.04001</link><description>&lt;p&gt;
&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#36827;&#34892;&#30005;&#30913;&#27714;&#35299;&#22120;S&#21442;&#25968;&#26354;&#32447;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
One-Dimensional Deep Image Prior for Curve Fitting of S-Parameters from Electromagnetic Solvers. (arXiv:2306.04001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#25311;&#21512;&#30005;&#30913;&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;&#36890;&#36807;&#19982;&#20844;&#24320;&#21487;&#29992;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#22343;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38598;&#25104;&#30005;&#36335;&#23553;&#35013;&#20013;&#24314;&#27169;&#20449;&#21495;&#23436;&#25972;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#38656;&#35201;&#22312;&#25152;&#38656;&#39057;&#24102;&#20869;&#36827;&#34892;&#22810;&#20010;S&#21442;&#25968;&#27979;&#37327;&#20197;&#33719;&#24471;&#36275;&#22815;&#30340;&#20998;&#36776;&#29575;&#12290;&#20351;&#29992;&#30005;&#30913;&#22330;&#27714;&#35299;&#22120;&#33719;&#24471;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#26041;&#27861;&#26159;&#36873;&#25321;&#25152;&#38656;&#26679;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#36866;&#24403;&#30340;&#25311;&#21512;&#26426;&#21046;&#37325;&#26032;&#21019;&#24314;&#23494;&#38598;&#37319;&#26679;&#30340;&#23485;&#24102;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#19968;&#32500;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;DIP&#65289;&#25311;&#21512;&#26469;&#33258;EM&#27714;&#35299;&#22120;&#30340;S&#21442;&#25968;&#12290;DIP&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20197;&#36866;&#24212;&#20174;&#22122;&#22768;&#25110;&#27424;&#23450;&#27979;&#37327;&#20013;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#28789;&#24863;&#26469;&#33258;&#24179;&#28369;&#26679;&#26465;&#65292;&#20197;&#24809;&#32602;&#19981;&#36830;&#32493;&#30340;&#36339;&#36291;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#26080;&#28304;&#28388;&#27874;&#22120;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;DIP&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#19987;&#26377;&#30340;&#34892;&#19994;&#26631;&#20934;&#25311;&#21512;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key problem when modeling signal integrity for passive filters and interconnects in IC packages is the need for multiple S-parameter measurements within a desired frequency band to obtain adequate resolution. These samples are often computationally expensive to obtain using electromagnetic (EM) field solvers. Therefore, a common approach is to select a small subset of the necessary samples and use an appropriate fitting mechanism to recreate a densely-sampled broadband representation. We present the first deep generative model-based approach to fit S-parameters from EM solvers using one-dimensional Deep Image Prior (DIP). DIP is a technique that optimizes the weights of a randomly-initialized convolutional neural network to fit a signal from noisy or under-determined measurements. We design a custom architecture and propose a novel regularization inspired by smoothing splines that penalizes discontinuous jumps. We experimentally compare DIP to publicly available and proprietary indus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#65288;R$^2$OUDA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;R$^2$MMT&#12290;&#36890;&#36807;R$^2$OUDA&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.03993</link><description>&lt;p&gt;
&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification. (arXiv:2306.03993v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#65288;R$^2$OUDA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;R$^2$MMT&#12290;&#36890;&#36807;R$^2$OUDA&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#30495;&#27491;&#30340;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#20154;&#21592;&#20877;&#35782;&#21035;&#20013;&#30340;&#27969;&#34892;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;OUDA&#65289;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#27969;&#30340;&#32771;&#34385;&#26469;&#24357;&#21512;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#28982;&#26080;&#27861;&#30495;&#27491;&#20195;&#34920;&#30495;&#23454;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;&#22312;&#32447;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;R$^2$OUDA&#65289;&#12290;R$^2$OUDA&#35774;&#32622;&#20102;&#30495;&#27491;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;OUDA&#30340;&#33310;&#21488;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#30340;&#22235;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#31995;&#32479;&#29983;&#25104;&#30340;&#20154;&#21592;&#22270;&#20687;&#12289;&#23376;&#38598;&#20998;&#24067;&#36873;&#25321;&#12289;&#22522;&#20110;&#26102;&#38388;&#30340;&#25968;&#25454;&#27969;&#20998;&#21106;&#21644;&#22522;&#20110;&#20998;&#27573;&#30340;&#26102;&#38388;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#30340;R$^2$OUDA&#35774;&#32622;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29616;&#23454;&#19990;&#30028;&#23454;&#26102;&#22312;&#32447;&#27969;&#24335;&#20114;&#30456;&#24179;&#22343;&#25945;&#23398;&#65288;R$^2$MMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#25668;&#20687;&#22836;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the popularity of Unsupervised Domain Adaptation (UDA) in person re-identification, the recently proposed setting of Online Unsupervised Domain Adaptation (OUDA) attempts to bridge the gap towards practical applications by introducing a consideration of streaming data. However, this still falls short of truly representing real-world applications. This paper defines the setting of Real-world Real-time Online Unsupervised Domain Adaptation (R$^2$OUDA) for Person Re-identification. The R$^2$OUDA setting sets the stage for true real-world real-time OUDA, bringing to light four major limitations found in real-world applications that are often neglected in current research: system generated person images, subset distribution selection, time-based data stream segmentation, and a segment-based time constraint. To address all aspects of this new R$^2$OUDA setting, this paper further proposes Real-World Real-Time Online Streaming Mutual Mean-Teaching (R$^2$MMT), a novel multi-camera sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#20027;&#25191;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#19981;&#21516;&#24066;&#22330;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03985</link><description>&lt;p&gt;
&#26426;&#22120;&#20195;&#29702;&#22312;&#33391;&#24615;&#21644;&#24694;&#24615;&#24773;&#22659;&#19979;&#30340;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Agent Performing Autonomous Stock Trading under Good and Bad Situations. (arXiv:2306.03985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#20027;&#25191;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#19981;&#21516;&#24066;&#22330;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#20132;&#26131;&#26159;&#36130;&#21153;&#31649;&#29702;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24066;&#22330;&#21644;&#32463;&#27982;&#29615;&#22659;&#19981;&#31283;&#23450;&#65292;&#36890;&#24120;&#19981;&#33021;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20174;&#20107;&#32929;&#31080;&#20132;&#26131;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#26469;&#20998;&#26512;&#12289;&#21046;&#23450;&#31574;&#30053;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22914;&#26524;&#19968;&#20010;&#26426;&#22120;&#20195;&#29702;&#33021;&#22815;&#36741;&#21161;&#29978;&#33267;&#25191;&#34892;&#20998;&#26512;&#21644;&#24314;&#27169;&#36807;&#21435;&#25968;&#25454;&#65292;&#28982;&#21518;&#29983;&#25104;&#33258;&#20027;&#20132;&#26131;&#31574;&#30053;&#65292;&#37027;&#23558;&#26159;&#26041;&#20415;&#21644;&#26377;&#25928;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#36798;&#21040;&#30446;&#26631;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#32929;&#31080;&#20132;&#26131;&#29615;&#22659;&#30340;&#31649;&#36947;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;Q&#23398;&#20064;&#12289;&#28145;&#24230;SARSA&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#21160;&#21270;&#32929;&#31080;&#20132;&#26131;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30456;&#23545;&#33391;&#22909;&#65288;2021&#24180;&#20043;&#21069;&#65289;&#21644;&#24694;&#21155;&#65288;2021&#24180;-2022&#24180;&#65289;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trading is one of the popular ways for financial management. However, the market and the environment of economy is unstable and usually not predictable. Furthermore, engaging in stock trading requires time and effort to analyze, create strategies, and make decisions. It would be convenient and effective if an agent could assist or even do the task of analyzing and modeling the past data and then generate a strategy for autonomous trading. Recently, reinforcement learning has been shown to be robust in various tasks that involve achieving a goal with a decision making strategy based on time-series data. In this project, we have developed a pipeline that simulates the stock trading environment and have trained an agent to automate the stock trading process with deep reinforcement learning methods, including deep Q-learning, deep SARSA, and the policy gradient method. We evaluate our platform during relatively good (before 2021) and bad (2021 - 2022) situations. The stocks we've eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.03984</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#35780;&#20272;&#25351;&#26631;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20132;&#20114;&#36136;&#37327;&#23545;&#20110;&#25913;&#36827;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#35780;&#20272;&#21333;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#36136;&#37327;&#65292;&#35201;&#20040;&#20174;&#32456;&#31471;&#29992;&#25143;&#31435;&#21363;&#22312;&#20132;&#20114;&#20043;&#21518;&#25910;&#38598;&#23545;&#35805;&#32423;&#21035;&#30340;&#36136;&#37327;&#27979;&#37327;&#25968;&#25454;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#65288;DQA&#65289;&#12290;DQA&#19987;&#23478;&#27880;&#37322;&#21592;&#35780;&#20272;&#25972;&#20010;&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#24182;&#26631;&#35760;&#23545;&#35805;&#30340;&#30446;&#26631;&#23436;&#25104;&#21644;&#29992;&#25143;&#24773;&#24863;&#31561;&#23646;&#24615;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#23613;&#31649;&#23545;&#35805;&#36136;&#37327;&#19981;&#33021;&#23436;&#20840;&#20998;&#35299;&#25104;&#23545;&#35805;&#32423;&#21035;&#23646;&#24615;&#65292;&#20294;&#26576;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#19982;&#23545;&#35805;&#36136;&#37327;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#23545;&#20110;&#23545;&#35805;&#32423;&#21035;&#36136;&#37327;&#20272;&#35745;&#20219;&#21153;&#65292;&#19968;&#20010;&#22312;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#20248;&#20110;&#20165;&#22522;&#20110;&#32858;&#21512;&#36718;&#27425;&#32423;&#21035;&#29305;&#24449;&#30340;&#26041;&#27861;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;DQA&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#23398;&#20064;&#30340;&#36816;&#31639;&#31526;&#26159;&#21542;&#26159;&#21333;&#23556;&#21644;&#28385;&#23556;&#30340;&#24773;&#20917;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#26465;&#20214;&#12290;&#23427;&#20204;&#25552;&#20379;&#30340;&#21333;&#23556;&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#26377;&#38480;&#31209;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23427;&#20204;&#65292;&#20351;&#24471;&#32593;&#32476;&#20173;&#28982;&#21333;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.03982</link><description>&lt;p&gt;
&#20840;&#29699;&#21487;&#27979;&#21644;&#21487;&#36870;&#31070;&#32463;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
Globally injective and bijective neural operators. (arXiv:2306.03982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#23398;&#20064;&#30340;&#36816;&#31639;&#31526;&#26159;&#21542;&#26159;&#21333;&#23556;&#21644;&#28385;&#23556;&#30340;&#24773;&#20917;&#65292;&#24182;&#32473;&#20986;&#20102;&#31934;&#30830;&#26465;&#20214;&#12290;&#23427;&#20204;&#25552;&#20379;&#30340;&#21333;&#23556;&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#26377;&#38480;&#31209;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23427;&#20204;&#65292;&#20351;&#24471;&#32593;&#32476;&#20173;&#28982;&#21333;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#36816;&#31639;&#23398;&#20064;&#39046;&#22495;&#65292;&#32593;&#32476;&#20174;&#22522;&#26412;&#19978;&#26080;&#38480;&#32500;&#24230;&#30340;&#35270;&#35282;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#36816;&#31639;&#31526;&#65292;&#25105;&#20204;&#38024;&#23545;&#32593;&#32476;&#23398;&#20064;&#30340;&#36816;&#31639;&#31526;&#26159;&#21333;&#23556;&#21644;&#28385;&#23556;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been great interest in operator learning, where networks learn operators between function spaces from an essentially infinite-dimensional perspective. In this work we present results for when the operators learned by these networks are injective and surjective. As a warmup, we combine prior work in both the finite-dimensional ReLU and operator learning setting by giving sharp conditions under which ReLU layers with linear neural operators are injective. We then consider the case the case when the activation function is pointwise bijective and obtain sufficient conditions for the layer to be injective. We remark that this question, while trivial in the finite-rank case, is subtler in the infinite-rank case and is proved using tools from Fredholm theory. Next, we prove that our supplied injective neural operators are universal approximators and that their implementation, with finite-rank neural networks, are still injective. This ensures that injectivity is not `lost' 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#65292;&#24182;&#20855;&#26377;&#26410;&#26469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03976</link><description>&lt;p&gt;
&#20351;&#29992;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI using expressive Boolean formulas. (arXiv:2306.03976v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#65292;&#24182;&#20855;&#26377;&#26410;&#26469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#65292;&#22522;&#20110;&#34920;&#36798;&#24335;&#24067;&#23572;&#20844;&#24335;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20449;&#29992;&#35780;&#20998;&#21644;&#21307;&#30103;&#30149;&#20917;&#30340;&#35786;&#26029;&#12290;&#24067;&#23572;&#20844;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#35268;&#21017;&#65292;&#26681;&#25454;&#35813;&#35268;&#21017;&#23558;&#36755;&#20837;&#25968;&#25454;&#20998;&#31867;&#20026;&#21487;&#35843;&#25972;&#22797;&#26434;&#24230;&#65288;&#25110;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;&#36825;&#26679;&#30340;&#20844;&#24335;&#21487;&#20197;&#21253;&#21547;&#24212;&#29992;&#20110;&#19968;&#20010;&#25110;&#22810;&#20010;&#24067;&#23572;&#21464;&#37327;&#30340;&#20219;&#20309;&#36816;&#31639;&#31526;&#65292;&#22240;&#27492;&#20855;&#26377;&#27604;&#26356;&#20005;&#26684;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#26356;&#39640;&#30340;&#34920;&#36798;&#24615;&#12290;&#20998;&#31867;&#22120;&#20351;&#29992;&#26412;&#22320;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#25628;&#32034;&#21487;&#34892;&#20844;&#24335;&#30340;&#31354;&#38388;&#12290;&#27973;&#30340;&#35268;&#21017;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#25110;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#27714;&#35299;&#22120;&#30830;&#23450;&#65292;&#21487;&#33021;&#30001;&#19987;&#29992;&#30828;&#20214;&#25110;&#37327;&#23376;&#35774;&#22791;&#25552;&#20379;&#21160;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25191;&#34892;&#26412;&#22320;&#20248;&#21270;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25928;&#29575;&#19982;&#36825;&#20123;&#35774;&#22791;&#30340;&#24555;&#36895;&#25805;&#20316;&#30456;&#32467;&#21512;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and implement an interpretable machine learning classification model for Explainable AI (XAI) based on expressive Boolean formulas. Potential applications include credit scoring and diagnosis of medical conditions. The Boolean formula defines a rule with tunable complexity (or interpretability), according to which input data are classified. Such a formula can include any operator that can be applied to one or more Boolean variables, thus providing higher expressivity compared to more rigid rule-based and tree-based approaches. The classifier is trained using native local optimization techniques, efficiently searching the space of feasible formulas. Shallow rules can be determined by fast Integer Linear Programming (ILP) or Quadratic Unconstrained Binary Optimization (QUBO) solvers, potentially powered by special purpose hardware or quantum devices. We combine the expressivity and efficiency of the native local optimizer with the fast operation of these devices by executing n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#38543;&#26426;&#36793;&#38469;&#20284;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#21152;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.03968</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#38543;&#26426;&#36793;&#38469;&#20284;&#28982;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels. (arXiv:2306.03968v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#38543;&#26426;&#36793;&#38469;&#20284;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#21152;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#23545;&#20854;&#26377;&#25928;&#24615;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#20294;&#38656;&#35201;&#20154;&#24037;&#21162;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#33021;&#22815;&#20687;&#20351;&#29992;&#26799;&#24230;&#20248;&#21270;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#19968;&#26679;&#20248;&#21270;&#36825;&#20123;&#36229;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#21333;&#20010;&#36229;&#21442;&#25968;&#26799;&#24230;&#38656;&#35201;&#36890;&#36807;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#38480;&#21046;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#21270;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#19979;&#38480;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20197;&#21069;&#30340;&#20272;&#35745;&#22120;&#19981;&#21516;&#65292;&#36825;&#20123;&#19979;&#38480;&#36866;&#29992;&#20110;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#20801;&#35768;&#22312;&#20272;&#35745;&#31934;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#21270;&#25289;&#26222;&#25289;&#26031;&#30340;&#20989;&#25968;&#31354;&#38388;&#24418;&#24335;&#23548;&#20986;&#20102;&#23427;&#20204;&#65292;&#36825;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20272;&#35745;&#22120;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estimators can significantly accelerate gradient-based hyp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861; PILLAR&#65292;&#21487;&#20197;&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#21487;&#20197;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03962</link><description>&lt;p&gt;
PILLAR&#65306;&#22914;&#20309;&#20351;&#21322;&#31169;&#26377;&#23398;&#20064;&#26356;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
PILLAR: How to make semi-private learning more effective. (arXiv:2306.03962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861; PILLAR&#65292;&#21487;&#20197;&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#21487;&#20197;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#21322;&#31169;&#26377;&#65288;SP&#65289;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#20844;&#20849;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#31169;&#26377;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#20551;&#35774;&#25968;&#25454;&#31526;&#21512;&#19968;&#23450;&#26465;&#20214;&#65292;&#21487;&#20197;&#26126;&#26174;&#38477;&#20302;&#31169;&#26377;&#26631;&#35760;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#20844;&#20849;&#25968;&#25454;&#65288;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#65289;&#19978;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30340;&#20998;&#24067;&#21487;&#33021;&#19982;&#36827;&#34892;SP&#23398;&#20064;&#30340;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#23454;&#35777;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22312;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#65288;\(\epsilon=0.1\))&#21644;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#30340;&#23454;&#39564;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#31867;&#20284;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#29616;&#26377;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Semi-Supervised Semi-Private (SP) learning, the learner has access to both public unlabelled and private labelled data. We propose a computationally efficient algorithm that, under mild assumptions on the data, provably achieves significantly lower private labelled sample complexity and can be efficiently run on real-world datasets. For this purpose, we leverage the features extracted by networks pre-trained on public (labelled or unlabelled) data, whose distribution can significantly differ from the one on which SP learning is performed. To validate its empirical effectiveness, we propose a wide variety of experiments under tight privacy constraints (\(\epsilon=0.1\)) and with a focus on low-data regimes. In all of these settings, our algorithm exhibits significantly improved performance over available baselines that use similar amounts of public data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#27721;&#23383;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;99.4%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03954</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;
&lt;/p&gt;
&lt;p&gt;
Recognition of Handwritten Japanese Characters Using Ensemble of Convolutional Neural Networks. (arXiv:2306.03954v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#26469;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#27721;&#23383;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;99.4%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#26412;&#20070;&#20889;&#31995;&#32479;&#22797;&#26434;&#65292;&#21253;&#25324;&#24179;&#20551;&#21517;&#12289;&#29255;&#20551;&#21517;&#21644;&#27721;&#23383;&#19977;&#31181;&#23383;&#31526;&#31867;&#22411;&#12290;&#27721;&#23383;&#21253;&#25324;&#25968;&#21315;&#20010;&#19981;&#21516;&#30340;&#23383;&#31526;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#23383;&#31526;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#38590;&#24230;&#12290;&#23558;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;&#36716;&#25442;&#20026;&#25968;&#23383;&#25991;&#26412;&#23545;&#20110;&#25968;&#25454;&#20998;&#26512;&#12289;&#32763;&#35793;&#12289;&#23398;&#20064;&#21644;&#25991;&#21270;&#20445;&#25252;&#37117;&#24456;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#23383;&#31526;&#65288;&#27721;&#23383;&#65289;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#19977;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38598;&#21512;&#26469;&#35782;&#21035;&#25163;&#20889;&#26085;&#35821;&#27721;&#23383;&#65292;&#24182;&#21033;&#29992; MNIST&#12289;K-MNIST&#12289;Kuzushiji-49&#65288;K49&#65289;&#20197;&#21450; Kuzushiji-Kanji&#65288;K-Kanji&#65289;&#20013;&#21069;150&#20010;&#37325;&#35201;&#31867;&#21035;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;CNN-ensemble&#26550;&#26500;&#35782;&#21035;&#25163;&#20889;&#23383;&#31526;&#20855;&#26377;&#21487;&#34892;&#24615;&#65292;&#22312;MNIST&#19978;&#23454;&#29616;&#20102;99.4&#65285;&#12289;96.4&#65285;&#12289;95.0&#65285;&#21644;96.4&#65285;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Japanese writing system is complex, with three character types of Hiragana, Katakana, and Kanji. Kanji consists of thousands of unique characters, further adding to the complexity of character identification and literature understanding. Being able to translate handwritten Japanese characters into digital text is useful for data analysis, translation, learning and cultural preservation. In this study, a machine learning approach to analyzing and recognizing handwritten Japanese characters (Kanji) is proposed. The study used an ensemble of three convolutional neural networks (CNNs) for recognizing handwritten Kanji characters and utilized four datasets of MNIST, K-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the Kuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results indicate feasibility of using proposed CNN-ensemble architecture for recognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4% classification accuracy on MNIST
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#21644;&#23548;&#33322;&#65292;&#21516;&#26102;&#32467;&#21512;PID&#25511;&#21046;&#21644;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03951</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor. (arXiv:2306.03951v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#21644;&#23548;&#33322;&#65292;&#21516;&#26102;&#32467;&#21512;PID&#25511;&#21046;&#21644;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#26088;&#22312;&#25506;&#32034;&#32463;&#20856;&#25511;&#21046;&#31639;&#27861;&#65288;&#22914;PID&#65289;&#21644;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#23454;&#29992;&#30340;&#25511;&#21046;&#26426;&#21046;&#26469;&#25511;&#21046;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#25191;&#34892;PID&#35843;&#25972;&#65292;&#27425;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#32463;&#39564;&#65292;&#36890;&#36807;&#19982;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#38598;&#25104;&#23454;&#29616;&#23548;&#33322;&#25511;&#21046;&#12290;&#22312;&#23548;&#33322;&#26041;&#38754;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#26377;&#38480;&#36816;&#21160;&#22522;&#20803;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#31163;&#25955;&#23548;&#33322;&#38382;&#39064;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36830;&#32493;&#23548;&#33322;&#12290; RL&#35757;&#32451;&#30340;&#27169;&#25311;&#23558;&#22312;gym-pybullet-drones&#19978;&#36827;&#34892;&#65292;&#35813;&#24179;&#21488;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#28304;gym&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;stable-baselines3&#25552;&#20379;RL&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of the project is to explore synergies between classical control algorithms such as PID and contemporary reinforcement learning algorithms to come up with a pragmatic control mechanism to control the CrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning using reinforcement learning strategies. The secondary objective is to leverage the learnings from the first task to implement control for navigation by integrating with the lighthouse positioning system. Two approaches are considered for navigation, a discrete navigation problem using Deep Q-Learning with finite predefined motion primitives, and deep reinforcement learning for a continuous navigation approach. Simulations for RL training will be performed on gym-pybullet-drones, an open-source gym-based environment for reinforcement learning, and the RL implementations are provided by stable-baselines3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#37096;&#20998;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03949</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#30340;&#37096;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Partial Inference in Structured Prediction. (arXiv:2306.03949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#37096;&#20998;&#26631;&#31614;&#24674;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#37096;&#20998;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#30740;&#31350;&#22312;&#26631;&#31614;&#22270;&#19978;&#26368;&#22823;&#21270;&#19968;&#31181;&#21253;&#21547;&#19968;&#20803;&#21644;&#20108;&#20803;&#22240;&#23376;&#30340;&#35780;&#20998;&#20989;&#25968;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#20004;&#38454;&#27573;&#20984;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#26631;&#31614;&#24674;&#22797;&#65292;&#20998;&#26512;&#20102;&#22823;&#22810;&#25968;&#26631;&#31614;&#21487;&#24674;&#22797;&#30340;&#26465;&#20214;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#21644;&#21407;&#22987;&#23545;&#20598;&#26500;&#36896;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#37096;&#20998;&#24674;&#22797;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we examine the problem of partial inference in the context of structured prediction. Using a generative model approach, we consider the task of maximizing a score function with unary and pairwise potentials in the space of labels on graphs. Employing a two-stage convex optimization algorithm for label recovery, we analyze the conditions under which a majority of the labels can be recovered. We introduce a novel perspective on the Karush-Kuhn-Tucker (KKT) conditions and primal and dual construction, and provide statistical and topological requirements for partial recovery with provable guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;COVID-19&#30456;&#20851;&#30740;&#31350;&#22312;&#20840;&#29699;&#21644;&#24847;&#22823;&#21033;&#30340;&#20135;&#37327;&#65292;&#21457;&#29616;&#32654;&#22269;&#21644;&#20013;&#22269;&#30340;&#30740;&#31350;&#27963;&#21160;&#26368;&#20026;&#27963;&#36291;&#65292;&#22312;&#21307;&#23398;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#25991;&#29486;&#20135;&#20986;&#22686;&#38271;&#26368;&#24555;&#65292;&#24182;&#25506;&#35752;&#20102;&#20986;&#29256;&#29289;&#25968;&#37327;&#21644;&#27515;&#20129;&#20154;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03941</link><description>&lt;p&gt;
COVID-19&#23545;&#30740;&#31350;&#20135;&#20986;&#20256;&#25773;&#30340;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A scientometric analysis of the effect of COVID-19 on the spread of research outputs. (arXiv:2306.03941v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;COVID-19&#30456;&#20851;&#30740;&#31350;&#22312;&#20840;&#29699;&#21644;&#24847;&#22823;&#21033;&#30340;&#20135;&#37327;&#65292;&#21457;&#29616;&#32654;&#22269;&#21644;&#20013;&#22269;&#30340;&#30740;&#31350;&#27963;&#21160;&#26368;&#20026;&#27963;&#36291;&#65292;&#22312;&#21307;&#23398;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#25991;&#29486;&#20135;&#20986;&#22686;&#38271;&#26368;&#24555;&#65292;&#24182;&#25506;&#35752;&#20102;&#20986;&#29256;&#29289;&#25968;&#37327;&#21644;&#27515;&#20129;&#20154;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2020&#24180;Sars-COV-2&#22823;&#27969;&#34892;&#30340;&#34067;&#24310;&#23545;&#25105;&#20204;&#25152;&#26377;&#20154;&#30340;&#29983;&#27963;&#36712;&#36857;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#24555;&#36895;&#20256;&#25773;&#20063;&#23548;&#33268;&#28041;&#21450;COVID-19&#30340;&#19981;&#21516;&#26041;&#38754;&#20027;&#39064;&#30340;&#30740;&#31350;&#20135;&#37327;&#22686;&#21152;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24847;&#22823;&#21033;&#26159;&#26368;&#26089;&#22823;&#35268;&#27169;&#21367;&#20837;&#35813;&#30149;&#26292;&#21457;&#30340;&#22269;&#23478;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#30740;&#31350;&#20135;&#37327;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20840;&#29699;&#33539;&#22260;&#65288;&#30123;&#24773;&#24320;&#22987;&#21518;&#30340;&#21069;&#20004;&#24180;&#20869;&#20135;&#29983;&#30340;&#20840;&#37096;&#25991;&#29486;&#65289;&#21644;&#26412;&#22320;&#23618;&#38754;&#65288;&#24847;&#22823;&#21033;&#38468;&#23646;&#26426;&#26500;&#20316;&#32773;&#25776;&#20889;&#30340;COVID-19&#25991;&#29486;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32654;&#22269;&#21644;&#20013;&#22269;&#26159;&#21457;&#34920;&#25968;&#37327;&#26368;&#22810;&#30340;&#30740;&#31350;&#27963;&#36291;&#22269;&#23478;&#65292;&#24182;&#19988;&#26426;&#26500;&#20043;&#38388;&#21512;&#20316;&#30340;&#25968;&#37327;&#26681;&#25454;&#22320;&#29702;&#36317;&#31163;&#32780;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21307;&#23398;&#29983;&#29289;&#23398;&#39046;&#22495;&#22312;&#25991;&#29486;&#20135;&#20986;&#26041;&#38754;&#22686;&#38271;&#26368;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20986;&#29256;&#29289;&#25968;&#37327;&#19982;&#27515;&#20129;&#20154;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of the Sars-COV-2 pandemic in 2020 had a huge impact on the life course of all of us. This rapid spread has also caused an increase in the research production in topics related to COVID-19 with regard to different aspects. Italy has, unfortunately, been one of the first countries to be massively involved in the outbreak of the disease. In this paper we present an extensive scientometric analysis of the research production both at global (entire literature produced in the first 2 years after the beginning of the pandemic) and local level (COVID-19 literature produced by authors with an Italian affiliation). Our results showed that US and China are the most active countries in terms of number of publications and that the number of collaborations between institutions varies according to geographical distance. Moreover, we identified the medical-biological as the fields with the greatest growth in terms of literature production. Furthermore, we also better explored the relations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#65292;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03938</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Mechanisms through Orthogonal Neural Networks. (arXiv:2306.03938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#65292;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#26159;&#33021;&#22815;&#20174;&#20302;&#32423;&#24863;&#23448;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#39640;&#32423;&#25277;&#35937;&#12290;&#36825;&#31181;&#25512;&#29702;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#21457;&#29616;&#27169;&#22359;&#21270;&#30340;&#29983;&#25104;&#26426;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20174;&#22833;&#30495;&#30340;&#25968;&#25454;&#28857;&#20013;&#23398;&#20064;&#19968;&#32452;&#29420;&#31435;&#26426;&#21046;&#30340;&#21453;&#21521;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#20010;&#37325;&#35201;&#24369;&#28857;&#22312;&#20110;&#36328;&#27169;&#22359;&#22810;&#26679;&#21270;&#19981;&#36275;&#12290;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#19982;&#26426;&#22120;&#26234;&#33021;&#20043;&#38388;&#30340;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#26159;&#27169;&#24335;&#35782;&#21035;&#31995;&#32479;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#21644;&#20998;&#31163;&#19968;&#32452;&#29420;&#31435;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental feature of human intelligence is the ability to infer high-level abstractions from low-level sensory data. An essential component of such inference is the ability to discover modularized generative mechanisms. Despite many efforts to use statistical learning and pattern recognition for finding disentangled factors, arguably human intelligence remains unmatched in this area.  In this paper, we investigate a problem of learning, in a fully unsupervised manner, the inverse of a set of independent mechanisms from distorted data points. We postulate, and justify this claim with experimental results, that an important weakness of existing machine learning solutions lies in the insufficiency of cross-module diversification. Addressing this crucial discrepancy between human and machine intelligence is an important challenge for pattern recognition systems.  To this end, our work proposes an unsupervised method that discovers and disentangles a set of independent mechanisms from u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03937</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Fl)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36328;&#22810;&#20010;&#21442;&#19982;&#32773;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#32771;&#34385;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29616;&#26377;FL&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#28857;&#30340;&#24433;&#21709;; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#38598;&#20013;&#24335;&#23398;&#20064;&#35774;&#32622;&#20013;&#22823;&#37327;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#25991;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;FL&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#32452;&#35745;&#31639;&#26426;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20165;&#25311;&#21512;&#32447;&#24615;&#20998;&#31867;&#22836;&#26159;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#25311;&#21512;&#20998;&#31867;&#22120;&#21487;&#20197;&#31934;&#30830;&#22320;&#19988;&#27604;&#29616;&#26377;&#25552;&#35758;&#39640;&#25928;&#22320;&#23436;&#25104;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#33719;&#24471;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.03928</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#35774;&#35745;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#24182;&#19981;&#23436;&#32654;&#65292;&#36825;&#20123;&#31995;&#32479;&#36824;&#38656;&#35201;&#35753;&#20154;&#31867;&#19987;&#23478;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#26356;&#26032;&#33258;&#24049;&#30340;&#39044;&#27979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#26377;&#20154;&#35748;&#20026;&#65292;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21487;&#33021;&#20250;&#36991;&#24320;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#26159;&#25552;&#20379;&#21333;&#20010;&#26631;&#31614;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#19968;&#32452;&#26631;&#31614;&#39044;&#27979;&#20540;&#65292;&#21363;&#39044;&#27979;&#38598;&#65292;&#24182;&#24378;&#21046;&#35201;&#27714;&#19987;&#23478;&#20174;&#39044;&#27979;&#38598;&#20013;&#39044;&#27979;&#19968;&#20010;&#26631;&#31614;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#36804;&#20170;&#20173;&#20381;&#36182;&#20110;&#26679;&#24335;&#21270;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25215;&#35834;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#31181;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22810;&#37325;&#32422;&#26463;&#30340;&#23545;&#31216;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#34920;&#31034;&#22823;&#35268;&#27169;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#24314;&#27169;&#31574;&#30053;&#29421;&#31364;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.03911</link><description>&lt;p&gt;
&#22810;&#37325;&#32422;&#26463;&#23545;&#31216;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#20197;&#20934;&#30830;&#34920;&#31034;&#22823;&#35268;&#27169;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-constrained Symmetric Nonnegative Latent Factor Analysis for Accurately Representing Large-scale Undirected Weighted Networks. (arXiv:2306.03911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22810;&#37325;&#32422;&#26463;&#30340;&#23545;&#31216;&#38750;&#36127;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#34920;&#31034;&#22823;&#35268;&#27169;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#24314;&#27169;&#31574;&#30053;&#29421;&#31364;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#22312;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#28041;&#21450;&#20247;&#22810;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#20363;&#22914;&#29983;&#29289;&#20449;&#24687;&#23398;&#24212;&#29992;&#20013;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#12290;&#23545;&#31216;&#39640;&#32500;&#19981;&#23436;&#25972;&#30697;&#38453;&#21487;&#20197;&#24179;&#28369;&#22320;&#25551;&#36848;&#36825;&#26679;&#30340;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#21547;&#33410;&#28857;&#20132;&#20114;&#34892;&#20026;&#21644;&#23616;&#37096;&#22797;&#26434;&#24615;&#31561;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#23545;&#31216;&#39640;&#32500;&#19981;&#23436;&#25972;&#30697;&#38453;&#20013;&#25552;&#21462;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#20998;&#26512;&#27169;&#22411;&#24212;&#35813;&#20180;&#32454;&#32771;&#34385;&#20854;&#23545;&#31216;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#25551;&#36848;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#12290;&#23558;&#26080;&#21521;&#21152;&#26435;&#32593;&#32476;&#34920;&#31034;&#20026;&#28508;&#22312;&#22240;&#23376;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#23545;&#31216;&#24863;&#30693;&#27169;&#22411;&#37329;&#23383;&#22612;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#21033;&#29992;&#21807;&#19968;&#30340;&#28508;&#22312;&#22240;&#23376;&#30697;&#38453;&#26469;&#20005;&#26684;&#34920;&#31034;SHDI&#23545;&#31216;&#24615;&#30340;&#23545;&#31216;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;1&#65289;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65307;2&#65289;&#20854;&#24314;&#27169;&#31574;&#30053;&#29421;&#31364;&#20854;&#34920;&#31034;&#29305;&#24449;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#26356;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;UWN&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Undirected Weighted Network (UWN) is frequently encountered in a big-data-related application concerning the complex interactions among numerous nodes, e.g., a protein interaction network from a bioinformatics application. A Symmetric High-Dimensional and Incomplete (SHDI) matrix can smoothly illustrate such an UWN, which contains rich knowledge like node interaction behaviors and local complexes. To extract desired knowledge from an SHDI matrix, an analysis model should carefully consider its symmetric-topology for describing an UWN's intrinsic symmetry. Representation learning to an UWN borrows the success of a pyramid of symmetry-aware models like a Symmetric Nonnegative Matrix Factorization (SNMF) model whose objective function utilizes a sole Latent Factor (LF) matrix for representing SHDI's symmetry rigorously. However, they suffer from the following drawbacks: 1) their computational complexity is high; and 2) their modeling strategy narrows their representation features, maki
&lt;/p&gt;</description></item><item><title>ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03901</link><description>&lt;p&gt;
ChatDB: &#23558;&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#22686;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03901
&lt;/p&gt;
&lt;p&gt;
ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20869;&#23384;&#26159;&#35745;&#31639;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;LLMs&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#65292;&#24182;&#19988;&#35774;&#35745;&#21463;&#21040;&#29983;&#29289;&#22823;&#33041;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#36817;&#20284;&#24615;&#36136;&#21644;&#38169;&#35823;&#32047;&#31215;&#20542;&#21521;&#65292;&#20256;&#32479;&#31070;&#32463;&#20869;&#23384;&#26426;&#21046;&#19981;&#33021;&#25903;&#25345;LLMs&#27169;&#25311;&#22797;&#26434;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#35745;&#31639;&#26426;&#26550;&#26500;&#20013;&#23547;&#27714;&#28789;&#24863;&#65292;&#20026;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#22686;&#24378;LLMs&#31526;&#21495;&#20869;&#23384;&#12290;&#36825;&#26679;&#30340;&#31526;&#21495;&#20869;&#23384;&#26694;&#26550;&#34987;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;LLM&#21644;&#19968;&#32452;SQL&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;LLM&#29983;&#25104;SQL&#25351;&#20196;&#20197;&#25805;&#20316;SQL&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290; &#39033;&#30446;&#32593;&#31449;&#20301;&#20110;https://chatdatabase.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03833</link><description>&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#20013;&#30340;&#24739;&#32773;&#39044;&#27979;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach. (arXiv:2306.03833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#34987;&#35465;&#20026;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#20013;&#30340;&#25913;&#21464;&#24615;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#36864;&#20986;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#22686;&#21152;&#20581;&#24247;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#25104;&#26412;&#12290;&#21450;&#26102;&#39044;&#27979;&#24739;&#32773;&#30340;&#36864;&#20986;&#20351;&#32929;&#19996;&#33021;&#22815;&#37319;&#21462;&#31215;&#26497;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#24739;&#32773;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#25552;&#39640;&#20445;&#30041;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22312;&#32447;&#21644;&#31163;&#32447;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#30340;&#21307;&#29983;&#24739;&#32773;&#23545;&#35805;&#12289;&#21508;&#20010;&#32929;&#19996;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#32593;&#32476;&#20013;&#23398;&#20064;&#38544;&#24335;&#21644;&#26174;&#24335;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20013;&#22269;&#26368;&#22823;&#30340;&#34394;&#25311;&#20581;&#24247;&#24179;&#21488;&#20043;&#19968;&#21512;&#20316;&#26469;&#35780;&#20272;MDKDP&#12290;MDKDP&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual health has been acclaimed as a transformative force in healthcare delivery. Yet, its dropout issue is critical that leads to poor health outcomes, increased health, societal, and economic costs. Timely prediction of patient dropout enables stakeholders to take proactive steps to address patients' concerns, potentially improving retention rates. In virtual health, the information asymmetries inherent in its delivery format, between different stakeholders, and across different healthcare delivery systems hinder the performance of existing predictive methods. To resolve those information asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout Prediction (MDKDP) framework that learns implicit and explicit knowledge from doctor-patient dialogues and the dynamic and complex networks of various stakeholders in both online and offline healthcare delivery systems. We evaluate MDKDP by partnering with one of the largest virtual health platforms in China. MDKDP improves the 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03360</link><description>&lt;p&gt;
Vid2Act&#65306;&#20026;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#28608;&#27963;&#31163;&#32447;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03360
&lt;/p&gt;
&lt;p&gt;
Vid2Act&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#20256;&#36755;&#39046;&#22495;&#30456;&#20851;&#30340;&#21160;&#24577;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#22312;&#32447;&#20219;&#21153;&#25928;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#36328;&#22495;&#20013;&#20219;&#21153;&#12289;&#21160;&#24577;&#21644;&#34892;&#20026;&#30340;&#22266;&#26377;&#19981;&#21305;&#37197;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;APV&#30340;&#27169;&#22411;&#36991;&#20813;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#20276;&#38543;&#21160;&#20316;&#35760;&#24405;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#22312;&#28304;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#12289;&#19981;&#28041;&#21450;&#25805;&#20316;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Vid2Act&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#21040;&#22312;&#32447;&#29615;&#22659;&#20013;&#20256;&#36755;&#26377;&#20215;&#20540;&#30340;&#21160;&#20316;&#26465;&#20214;&#21160;&#24577;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#21160;&#20316;&#28436;&#31034;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#19981;&#20165;&#23558;&#19990;&#30028;&#27169;&#22411;&#29992;&#20316;&#34892;&#20026;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#65292;&#36824;&#23558;&#20854;&#29992;&#20316;&#27979;&#37327;&#39046;&#22495;&#30456;&#20851;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36827;&#34892;&#21160;&#24577;&#34920;&#31034;&#20256;&#36755;&#21644;&#31574;&#30053;&#20256;&#36755;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22495;&#36873;&#25321;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#35757;&#32451;&#19990;&#30028;&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#26102;&#38388;&#21464;&#21270;&#30340;&#20219;&#21153;&#30456;&#20284;&#24230;&#12290;&#36825;&#20123;&#30456;&#20284;&#24230;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#22320;&#23558;&#26368;&#30456;&#20851;&#30340;&#39046;&#22495;&#30340;&#21160;&#24577;&#20256;&#36755;&#21040;&#22312;&#32447;&#29615;&#22659;&#65292;&#21644;&#65288;ii&#65289;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25351;&#23548;&#20195;&#29702;&#38598;&#20013;&#25191;&#34892;&#20219;&#21153;&#30456;&#20851;&#30340;&#21160;&#20316;&#12290;&#22312;Atari&#21644;DMControl&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20302;&#31209;&#21442;&#25968;&#21270;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03291</link><description>&lt;p&gt;
&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Switching Autoregressive Low-rank Tensor Models. (arXiv:2306.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20302;&#31209;&#21442;&#25968;&#21270;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#20998;&#26512;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23545;&#20855;&#26377;&#26102;&#21464;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20849;&#21516;&#36830;&#32493;&#21644;&#31163;&#25955;&#28508;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#20026;&#36825;&#26679;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#12289;&#39640;&#25928;&#21644;&#23454;&#39564;&#24615;&#26377;&#29992;&#30340;&#25551;&#36848;&#12290;&#24120;&#29992;&#30340;&#27169;&#22411;&#21253;&#25324;&#33258;&#22238;&#24402;&#38544;Markov&#27169;&#22411;&#65288;ARHMM&#65289;&#21644;&#20999;&#25442;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;SLDS&#65289;&#65292;&#23427;&#20204;&#21508;&#26377;&#20248;&#32570;&#28857;&#12290;ARHMM&#20801;&#35768;&#31934;&#30830;&#25512;&#29702;&#21644;&#31616;&#21333;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#20294;&#22312;&#23545;&#38271;&#20381;&#36182;&#20851;&#31995;&#24314;&#27169;&#26102;&#20855;&#26377;&#21442;&#25968;&#23494;&#38598;&#24615;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#28508;&#24577;&#21160;&#21147;&#23398;&#65292;SLDS&#21487;&#20197;&#20197;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#65292;&#20294;&#22256;&#38590;&#30340;&#21442;&#25968;&#20272;&#35745;&#20219;&#21153;&#21644;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#21364;&#26159;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#26041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20999;&#25442;&#33258;&#22238;&#24402;&#20302;&#31209;&#24352;&#37327;&#65288;SALT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20445;&#30041;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;SALT&#23558;ARHMM&#30340;&#24352;&#37327;&#21442;&#25968;&#21270;&#20026;&#20302;&#31209;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#29273;&#31185;CBCT&#25104;&#20687;&#20013;&#22522;&#20110;AI&#30340;&#30149;&#21464;&#26816;&#27979;&#12289;&#38169;&#39052;&#20998;&#31867;&#12289;&#39050;&#39592;&#21402;&#24230;&#27979;&#37327;&#12289;&#29273;&#40831;&#12289;&#39052;&#39592;&#12289;&#19979;&#39052;&#39592;&#12289;&#26631;&#24535;&#12289;&#36718;&#24275;&#21644;&#21693;&#21897;&#27668;&#36947;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#31561;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.03025</link><description>&lt;p&gt;
&#29273;&#31185;&#39046;&#22495;&#38181;&#24418;&#26463;CT&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65306;&#36235;&#21183;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and Practices. (arXiv:2306.03025v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#29273;&#31185;CBCT&#25104;&#20687;&#20013;&#22522;&#20110;AI&#30340;&#30149;&#21464;&#26816;&#27979;&#12289;&#38169;&#39052;&#20998;&#31867;&#12289;&#39050;&#39592;&#21402;&#24230;&#27979;&#37327;&#12289;&#29273;&#40831;&#12289;&#39052;&#39592;&#12289;&#19979;&#39052;&#39592;&#12289;&#26631;&#24535;&#12289;&#36718;&#24275;&#21644;&#21693;&#21897;&#27668;&#36947;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#26159;&#19968;&#31181;&#22312;&#29273;&#31185;&#39046;&#22495;&#24120;&#29992;&#30340;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#20851;&#20110;&#29273;&#40831;&#12289;&#39052;&#39592;&#21450;&#21608;&#22260;&#32452;&#32455;&#30340;&#35814;&#32454;&#19977;&#32500;&#22270;&#20687;&#65292;&#29992;&#20110;&#35786;&#26029;&#21644;&#35268;&#21010;&#27835;&#30103;&#22810;&#31181;&#21475;&#33108;&#30142;&#30149;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#22312;CBCT&#25104;&#20687;&#20013;&#20351;&#24471;&#20854;&#22312;&#35786;&#26029;&#20215;&#20540;&#12289;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#29273;&#31185;CBCT&#25104;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#36235;&#21183;&#21644;&#23454;&#36341;&#65292;&#20027;&#35201;&#20171;&#32461;&#20102;&#22522;&#20110;CBCT&#22270;&#20687;&#30340;&#30149;&#21464;&#26816;&#27979;&#12289;&#38169;&#39052;&#20998;&#31867;&#12289;&#39050;&#39592;&#21402;&#24230;&#27979;&#37327;&#12289;&#29273;&#40831;&#12289;&#39052;&#39592;&#12289;&#19979;&#39052;&#39592;&#12289;&#26631;&#24535;&#12289;&#36718;&#24275;&#21644;&#21693;&#21897;&#27668;&#36947;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#65292;&#28041;&#21450;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cone-beam computed tomography (CBCT) is a popular imaging modality in dentistry for diagnosing and planning treatment for a variety of oral diseases with the ability to produce detailed, three-dimensional images of the teeth, jawbones, and surrounding structures. CBCT imaging has emerged as an essential diagnostic tool in dentistry. CBCT imaging has seen significant improvements in terms of its diagnostic value, as well as its accuracy and efficiency, with the most recent development of artificial intelligence (AI) techniques. This paper reviews recent AI trends and practices in dental CBCT imaging. AI has been used for lesion detection, malocclusion classification, measurement of buccal bone thickness, and classification and segmentation of teeth, alveolar bones, mandibles, landmarks, contours, and pharyngeal airways using CBCT images. Mainly machine learning algorithms, deep learning algorithms, and super-resolution techniques are used for these tasks. This review focuses on the pote
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#26657;&#20934;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#26368;&#22823;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26657;&#20934;&#21644;&#38160;&#24230;&#20043;&#38388;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#24179;&#34913;&#65292;&#32780;&#21518;&#32493;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#36234;&#65292;&#27492;&#22806;&#36824;&#35777;&#26126;&#20102;&#20998;&#20301;&#25968;&#37325;&#26032;&#26657;&#20934;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#35268;&#24615;&#39044;&#27979;&#30340;&#19968;&#20010;&#29305;&#23450;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.02738</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#20013;&#27010;&#29575;&#26657;&#20934;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Study of Probabilistic Calibration in Neural Network Regression. (arXiv:2306.02738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#26657;&#20934;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#26368;&#22823;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26657;&#20934;&#21644;&#38160;&#24230;&#20043;&#38388;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#24179;&#34913;&#65292;&#32780;&#21518;&#32493;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#36234;&#65292;&#27492;&#22806;&#36824;&#35777;&#26126;&#20102;&#20998;&#20301;&#25968;&#37325;&#26032;&#26657;&#20934;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#35268;&#24615;&#39044;&#27979;&#30340;&#19968;&#20010;&#29305;&#23450;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27010;&#29575;&#39044;&#27979;&#23545;&#20110;&#26368;&#20248;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35823;&#26657;&#20934;&#20027;&#35201;&#30740;&#31350;&#20102;&#20998;&#31867;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#20013;&#36739;&#23569;&#25506;&#35752;&#30340;&#35823;&#26657;&#20934;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#26657;&#20934;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#37325;&#26032;&#26657;&#20934;&#12289;&#21512;&#35268;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#27010;&#29575;&#26657;&#20934;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#37325;&#26032;&#26657;&#20934;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26657;&#20934;&#21644;&#38160;&#24230;&#20043;&#38388;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#24179;&#34913;&#12290;&#21518;&#32493;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#27010;&#29575;&#26657;&#20934;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#21512;&#35268;&#24615;&#39044;&#27979;&#30340;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#20998;&#20301;&#25968;&#37325;&#26032;&#26657;&#20934;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#35268;&#24615;&#39044;&#27979;&#30340;&#19968;&#20010;&#29305;&#23450;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#23436;&#20840;&#21487;&#20877;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction. Our study is fully reproducible
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#35813;&#26694;&#26550;&#22312;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02715</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Learning for Intrusion Detection in IoT Networks. (arXiv:2306.02715v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#35813;&#26694;&#26550;&#22312;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#22823;&#24133;&#22686;&#38271;&#21644;&#25915;&#20987;&#21521;&#37327;&#21644;&#23041;&#32961;&#34892;&#20026;&#30340;&#19981;&#26029;&#28436;&#21270;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#12290;&#26032;&#22411;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#20197;&#33719;&#21462;&#23545;&#25935;&#24863;&#25968;&#25454;&#30340;&#35775;&#38382;&#25110;&#25511;&#21046;&#23427;&#20204;&#20197;&#37096;&#32626;&#36827;&#19968;&#27493;&#30340;&#24694;&#24847;&#27963;&#21160;&#12290;&#26816;&#27979;&#26032;&#22411;&#25915;&#20987;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#22522;&#20110;AI&#30340;&#20837;&#20405;&#26816;&#27979;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20197;&#38598;&#20013;&#26041;&#24335;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#21487;&#33021;&#20250;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#21644;&#20445;&#23494;&#24615;&#12290;&#27492;&#22806;&#65292;&#38598;&#20013;&#24335;&#25968;&#25454;&#37319;&#38598;&#31105;&#27490;IDS&#30340;&#25193;&#23637;&#12290;&#22240;&#27492;&#65292;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26397;&#20998;&#25955;&#21270;&#26041;&#21521;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#33021;&#22815;&#22312;&#20445;&#30041;&#25968;&#25454;&#26426;&#23494;&#24615;&#21644;&#23616;&#37096;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;IDS&#37117;&#26159;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#25968;&#25454;&#20998;&#24067;&#26465;&#20214;&#19979;&#35774;&#35745;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20195;&#34920;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65288;FDL&#65289;&#26694;&#26550;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25972;&#21512;&#20102;CNN&#27169;&#22411;&#21644;&#38376;&#25511;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#25910;&#25947;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23616;&#37096;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;FDL&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#23454;&#38469;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast increase of IoT technologies and the ever-evolving attack vectors and threat actors have increased cyber-security risks dramatically. Novel attacks can compromise IoT devices to gain access to sensitive data or control them to deploy further malicious activities. The detection of novel attacks often relies upon AI solutions. A common approach to implementing AI-based IDS in distributed IoT systems is in a centralised manner. However, this approach may violate data privacy and secrecy. In addition, centralised data collection prohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT ecosystems need to move towards a decentralised direction. FL has attracted significant interest in recent years due to its ability to perform collaborative learning while preserving data confidentiality and locality. Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic data distribution conditions. To that end, we design an experiment representative o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;Equity-Transformer&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#65292;&#24182;&#29983;&#25104;&#32771;&#34385;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;Equity-Transformer&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02689</link><description>&lt;p&gt;
&#23558;NP&#22256;&#38590;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20316;&#20026;&#20855;&#26377;&#20844;&#24179;&#32972;&#26223;&#30340;&#39034;&#24207;&#29983;&#25104;&#26469;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. (arXiv:2306.02689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;Equity-Transformer&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#65292;&#24182;&#29983;&#25104;&#32771;&#34385;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;Equity-Transformer&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#25152;&#26377;&#20195;&#29702;&#21830;&#21327;&#21516;&#35775;&#38382;&#25152;&#26377;&#22478;&#24066;&#30340;&#26368;&#22823;&#26053;&#28216;&#38271;&#24230;&#65292;&#21363;&#23436;&#25104;&#26102;&#38388;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#26377;&#24433;&#21709;&#21147;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#34987;&#35748;&#20026;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21327;&#35843;&#20247;&#22810;&#20195;&#29702;&#21830;&#35206;&#30422;&#25968;&#21315;&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#22810;&#20010;&#20195;&#29702;&#21830;&#30340;&#21516;&#26102;&#20915;&#31574;&#24314;&#27169;&#20026;&#39034;&#24207;&#29983;&#25104;&#36807;&#31243;&#65292;&#20801;&#35768;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#22312;&#39034;&#24207;&#36817;&#20284;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;Transformer&#27169;&#22411;Equity-Transformer&#65292;&#23427;&#29983;&#25104;&#32771;&#34385;&#20854;&#20182;&#20195;&#29702;&#21830;&#20043;&#38388;&#20844;&#24179;&#24037;&#20316;&#36127;&#36733;&#30340;&#39034;&#24207;&#21160;&#20316;&#12290;Equity-Transformer&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20854;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26368;&#23567;&#26368;&#22823;&#36335;&#24452;&#38382;&#39064;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Min-max routing problems aim to minimize the maximum tour length among agents as they collaboratively visit all cities, i.e., the completion time. These problems include impactful real-world applications but are known as NP-hard. Existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. This paper proposes a new deep-learning framework to solve large-scale min-max routing problems. We model the simultaneous decision-making of multiple agents as a sequential generation process, allowing the utilization of scalable deep-learning models for sequential decision-making. In the sequentially approximated problem, we propose a scalable contextual Transformer model, Equity-Transformer, which generates sequential actions considering an equitable workload among other agents. The effectiveness of Equity-Transformer is demonstrated through its superior performance in two representative min-max routing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-SAGE&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#20363;&#20803;&#23398;&#20064;&#21644;&#26102;&#38388;&#34920;&#35843;&#25972;&#26469;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#30495;&#23454;&#22320;&#20248;&#21270;&#20102;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.02688</link><description>&lt;p&gt;
Meta-SAGE&#65306;&#29992;&#24341;&#23548;&#25506;&#32034;&#30340;&#35268;&#21010;&#26041;&#27861;&#21644;&#27604;&#20363;&#19968;&#20803;&#23398;&#20064;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#35268;&#27169;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization. (arXiv:2306.02688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-SAGE&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#20363;&#20803;&#23398;&#20064;&#21644;&#26102;&#38388;&#34920;&#35843;&#25972;&#26469;&#36866;&#24212;&#27169;&#22411;&#65292;&#24182;&#30495;&#23454;&#22320;&#20248;&#21270;&#20102;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;Meta-SAGE&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#20219;&#21153;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#26041;&#27861;&#36890;&#36807;&#24314;&#35758;&#20004;&#20010;&#32452;&#20214;&#26469;&#22312;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#35299;&#20915;&#35268;&#27169;&#38382;&#39064;&#65306;&#19968;&#20010;&#26159;&#27604;&#20363;&#20803;&#23398;&#20064;&#22120;&#65288;SML&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20855;&#26377;&#24341;&#23548;&#25506;&#32034;&#21644;&#26102;&#38388;&#34920;&#35843;&#25972;&#21151;&#33021;&#30340;scheduled adaptation with guided exploration&#65288;SAGE&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Meta-SAGE&#20248;&#20110;&#20197;&#21069;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#34920;&#24615;CO&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Meta-SAGE, a novel approach for improving the scalability of deep reinforcement learning models for combinatorial optimization (CO) tasks. Our method adapts pre-trained models to larger-scale problems in test time by suggesting two components: a scale meta-learner (SML) and scheduled adaptation with guided exploration (SAGE). First, SML transforms the context embedding for subsequent adaptation of SAGE based on scale information. Then, SAGE adjusts the model parameters dedicated to the context embedding for a specific instance. SAGE introduces locality bias, which encourages selecting nearby locations to determine the next location. The locality bias gradually decays as the model is adapted to the target instance. Results show that Meta-SAGE outperforms previous adaptation methods and significantly improves scalability in representative CO tasks. Our source code is available at https://github.com/kaist-silab/meta-sage
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.02349</link><description>&lt;p&gt;
bgGLUE&#65306;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark. (arXiv:2306.02349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;bgGLUE&#65288;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#24207;&#21015;&#26631;&#35760;&#12289;&#25991;&#26723;&#32423;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#30340;NLU&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;&#20445;&#21152;&#21033;&#20122;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#36275;&#20102;&#20061;&#20010;&#20219;&#21153;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;bgGLUE&#19982;&#24494;&#35843;&#21644;&#35780;&#20272;&#20195;&#30721;&#19968;&#36215;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#21450;&#22312;https://bgglue.github.io/&#19978;&#25552;&#20379;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#24076;&#26395;&#23427;&#33021;&#20419;&#36827;&#26356;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present bgGLUE(Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io/, and we hope that it will enable further advancements in developi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2306.01951</link><description>&lt;p&gt;
GAD-NR: &#36890;&#36807;&#37051;&#22495;&#37325;&#26500;&#23454;&#29616;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#31038;&#20132;&#23186;&#20307;&#22403;&#22334;&#26816;&#27979;&#21644;&#20854;&#20182;&#21508;&#31181;&#39046;&#22495;&#20013;&#26377;&#24212;&#29992;&#12290;GAD&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#65292;&#23427;&#23558;&#22270;&#24418;&#25968;&#25454;&#32534;&#30721;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#34920;&#31034;&#26469;&#35780;&#20272;&#22270;&#24418;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#30452;&#25509;&#38142;&#25509;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36830;&#25509;&#22270;&#20013;&#30340;&#33410;&#28857;&#34987;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25797;&#38271;&#26816;&#27979;&#32858;&#31867;&#22411;&#32467;&#26500;&#24322;&#24120;&#65292;&#20294;&#23545;&#19981;&#31526;&#21512;&#32858;&#31867;&#30340;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#24322;&#24120;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GAD-NR&#65292;&#23427;&#26159;GAE&#30340;&#19968;&#20010;&#26032;&#21464;&#20307;&#65292;&#34701;&#21512;&#37051;&#22495;&#37325;&#26500;&#36827;&#34892;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#12290;GAD-NR&#30340;&#30446;&#26631;&#26159;&#37325;&#26500;&#33410;&#28857;&#30340;&#25972;&#20010;&#37051;&#22495;&#65292;&#28085;&#30422;&#26412;&#22320;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; UNet &#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#33021;&#22815;&#36890;&#36807;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#21644;&#24212;&#21464;&#22270;&#20687;&#30340;&#20998;&#26512;&#65292;&#31934;&#30830;&#22320;&#25512;&#26029;&#29983;&#29289;&#36719;&#32452;&#32455;&#20013;&#26448;&#26009;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#20248;&#21155;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01204</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340; UNet &#29992;&#20110;&#21457;&#29616;&#24322;&#36136;&#26448;&#26009;&#20013;&#38544;&#34255;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials. (arXiv:2306.01204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340; UNet &#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#33021;&#22815;&#36890;&#36807;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#21644;&#24212;&#21464;&#22270;&#20687;&#30340;&#20998;&#26512;&#65292;&#31934;&#30830;&#22320;&#25512;&#26029;&#29983;&#29289;&#36719;&#32452;&#32455;&#20013;&#26448;&#26009;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#20248;&#21155;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36719;&#32452;&#32455;&#24120;&#24120;&#30001;&#20110;&#32467;&#26500;&#25104;&#20998;&#30340;&#21464;&#21270;&#32780;&#20855;&#26377;&#22797;&#26434;&#30340;&#26426;&#26800;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110; UNet &#30340;&#21453;&#28436;&#24377;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; (El-UNet)&#65292;&#23558;&#24212;&#21464;&#22270;&#20316;&#20026;&#36755;&#20837;&#22270;&#20687;&#12289;&#27491;&#24120;&#24212;&#21147;&#36793;&#30028;&#26465;&#20214;&#21644;&#21306;&#22495;&#29289;&#29702;&#20449;&#24687;&#65292;&#20197;&#25512;&#26029;&#21147;&#23398;&#21442;&#25968;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; El-UNet &#22312;&#20272;&#35745;&#31561;&#21521;&#24615;&#32447;&#24615;&#24377;&#24615;&#30340;&#26410;&#30693;&#21442;&#25968;&#21644;&#24212;&#21147;&#20998;&#24067;&#26041;&#38754;&#65292;&#19982;&#20840;&#36830;&#25509;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#31934;&#24230;&#36824;&#26159;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545; El-UNet &#30340;&#19981;&#21516;&#21464;&#20307;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31354;&#38388;&#25439;&#22833;&#21152;&#26435;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#21453;&#28436;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#31561;&#21521;&#24322;&#36136;&#26448;&#26009;&#26377;&#38480;&#20803;&#27169;&#25311;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;El-UNet &#27604;&#20840;&#36830;&#25509;&#29289;&#29702;&#23398;&#30693;&#35782;&#22686;&#24378;&#30340;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft biological tissues often have complex mechanical properties due to variation in structural components. In this paper, we develop a novel UNet-based neural network model for inversion in elasticity (El-UNet) to infer the spatial distributions of mechanical parameters from strain maps as input images, normal stress boundary conditions, and domain physics information. We show superior performance, both in terms of accuracy and computational cost, by El-UNet compared to fully-connected physics-informed neural networks in estimating unknown parameters and stress distributions for isotropic linear elasticity. We characterize different variations of El-UNet and propose a self-adaptive spatial loss weighting approach. To validate our inversion models, we performed various finite-element simulations of isotropic domains with heterogenous distributions of material parameters to generate synthetic data. El-UNet is faster and more accurate than the fully-connected physics-informed implementat
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#23041;&#22270;&#32467;&#26500;&#29305;&#24449;&#23545;&#22522;&#20110;GNN&#30340;IDS&#26816;&#27979;&#36827;&#34892;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00934
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36923;&#36753;&#35299;&#37322;&#21644;&#21487;&#25191;&#34892;&#21518;&#32493;&#34892;&#21160;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#31995;&#32479;&#26469;&#28304;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROVEXPLAINER&#65292;&#19968;&#31181;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#22797;&#21046;&#22522;&#20110;GNN&#30340;&#23433;&#20840;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#35770;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23433;&#20840;&#39046;&#22495;&#30693;&#35782;&#30340;&#24191;&#27867;&#25968;&#25454;&#30740;&#31350;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#19982;&#31995;&#32479;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#31354;&#38388;&#34892;&#21160;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#20351;&#26816;&#27979;&#32467;&#26524;&#21487;&#29992;&#20154;&#31867;&#35821;&#35328;&#25551;&#36848;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CRS-FL&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;CRS&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#27850;&#26494;&#25277;&#26679;&#30340;&#38543;&#26426;&#31995;&#25968;&#30340;&#25506;&#32034;&#23454;&#29616;&#26356;&#39640;&#30340;&#38646;&#26799;&#24230;&#26080;&#20559;&#27010;&#29575;&#65292;&#26082;&#26377;&#25928;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#21448;&#30830;&#20445;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00674</link><description>&lt;p&gt;
CRS-FL: &#29992;&#20110;&#36890;&#20449;&#39640;&#25928;&#21644;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#30340;&#26465;&#20214;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CRS-FL: Conditional Random Sampling for Communication-Efficient and Privacy-Preserving Federated Learning. (arXiv:2306.00674v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CRS-FL&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;CRS&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#27850;&#26494;&#25277;&#26679;&#30340;&#38543;&#26426;&#31995;&#25968;&#30340;&#25506;&#32034;&#23454;&#29616;&#26356;&#39640;&#30340;&#38646;&#26799;&#24230;&#26080;&#20559;&#27010;&#29575;&#65292;&#26082;&#26377;&#25928;&#20943;&#23569;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#21448;&#30830;&#20445;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#20445;&#25252;&#21442;&#19982;&#32773;&#25968;&#25454;&#38544;&#31169;&#32780;&#22312;&#29289;&#32852;&#32593;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#30740;&#31350;&#20063;&#19981;&#33021;&#22312;&#20445;&#38556;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#25277;&#26679;&#65288;CRS&#65289;&#26041;&#27861;&#30340;CRS-FL&#65292;&#24182;&#23558;&#20854;&#23454;&#29616;&#21040;&#26631;&#20934;FL&#35774;&#32622;&#20013;&#65292;&#20197;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;CRS&#22522;&#20110;&#27850;&#26494;&#25277;&#26679;&#25506;&#32034;&#38543;&#26426;&#31995;&#25968;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#39640;&#30340;&#38646;&#26799;&#24230;&#26080;&#20559;&#27010;&#29575;&#65292;&#24182;&#26377;&#25928;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;CRS&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#20445;&#35777;&#26465;&#20214;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CRS-FL&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#37117;&#27604;&#25552;&#20379;&#30340;&#25216;&#26415;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), a privacy-oriented distributed ML paradigm, is being gaining great interest in Internet of Things because of its capability to protect participants data privacy. Studies have been conducted to address challenges existing in standard FL, including communication efficiency and privacy-preserving. But they cannot achieve the goal of making a tradeoff between communication efficiency and model accuracy while guaranteeing privacy. This paper proposes a Conditional Random Sampling (CRS) method and implements it into the standard FL settings (CRS-FL) to tackle the above-mentioned challenges. CRS explores a stochastic coefficient based on Poisson sampling to achieve a higher probability of obtaining zero-gradient unbiasedly, and then decreases the communication overhead effectively without model accuracy degradation. Moreover, we dig out the relaxation Local Differential Privacy (LDP) guarantee conditions of CRS theoretically. Extensive experiment results indicate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.00421</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Medical Imaging Informatics. (arXiv:2306.00421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#26159;&#23558;&#21307;&#23398;&#25104;&#20687;&#21644;&#20449;&#24687;&#23398;&#30340;&#21407;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#30340;&#33719;&#21462;&#12289;&#31649;&#29702;&#21644;&#35299;&#37322;&#20026;&#30446;&#30340;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#39046;&#22495;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#26412;&#31456;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#23450;&#37327;&#22270;&#20687;&#26631;&#35760;&#21644;&#30142;&#30149;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#28085;&#30422;&#21307;&#23398;&#25104;&#20687;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#26412;&#31456;&#20026;&#29702;&#35299;&#20449;&#24687;&#23398;&#22312;&#21307;&#23398;&#20013;&#30340;&#20316;&#29992;&#21450;&#20854;&#23545;&#24739;&#32773;&#25252;&#29702;&#30340;&#28508;&#22312;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging informatics is a rapidly growing field that combines the principles of medical imaging and informatics to improve the acquisition, management, and interpretation of medical images. This chapter introduces the basic concepts of medical imaging informatics, including image processing, feature engineering, and machine learning. It also discusses the recent advancements in computer vision and deep learning technologies and how they are used to develop new quantitative image markers and prediction models for disease detection, diagnosis, and prognosis prediction. By covering the basic knowledge of medical imaging informatics, this chapter provides a foundation for understanding the role of informatics in medicine and its potential impact on patient care.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00390</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#29992;&#20110;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;TTS&#65289;&#25968;&#25454;&#26159;&#39640;&#32500;&#31354;&#38388;&#20013;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33324;&#21270;&#65292;&#26159;&#29616;&#23454;&#22330;&#26223;&#20013;&#19975;&#33021;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#30417;&#27979;&#31995;&#32479;&#20013;&#65288;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#31354;&#27668;&#27745;&#26579;&#29289;&#65289;&#12290;&#19982;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30456;&#27604;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#20184;&#20986;&#30340;&#21162;&#21147;&#36739;&#23569;&#12290;&#30001;&#20110;&#20854;&#39640;&#32500;&#21644;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#27491;&#30830;&#22788;&#29702;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;TTS&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#21629;&#21517;&#20026;GMRL&#65292;&#21363;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#23454;&#38469;TTS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#35745;&#31639;&#30340;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#36798;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19982;&#19987;&#29992;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00088</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#35745;&#31639;&#33258;&#21160;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning. (arXiv:2306.00088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#35745;&#31639;&#30340;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#36798;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19982;&#19987;&#29992;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#34987;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#26512;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#27714;&#35299;&#20851;&#31995;&#35745;&#31639;&#20013;&#30340;&#33258;&#21160;&#24494;&#20998;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#36816;&#34892;&#33258;&#21160;&#24494;&#20998;&#20851;&#31995;&#31639;&#27861;&#30340;&#20851;&#31995;&#24341;&#25806;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#29992;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relational data model was designed to facilitate large-scale data management and analytics. We consider the problem of how to differentiate computations expressed relationally. We show experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets, and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19903</link><description>&lt;p&gt;
&#20351;&#29992;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#23884;&#20837;&#24402;&#19968;&#21270;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#36890;&#24120;&#24573;&#30053;&#20102;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#30340;&#37325;&#35201;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#19987;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#8212;&#8212;SUbgraph-sPEcific FactoR Embedded Normalization&#65288;SuperNorm&#65289;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#35813;&#26041;&#26696;&#26126;&#30830;&#32771;&#34385;&#20102;&#27599;&#20010;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20869;&#37096;&#36830;&#25509;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;BatchNorm&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#23884;&#20837;&#20102;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#65292;&#24182;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#20197;&#25552;&#39640;&#21306;&#20998;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#65292;&#25351;&#20986;&#36890;&#36807;&#25913;&#21892;&#30340;SuperNorm&#65292;&#20219;&#24847;GNN&#33267;&#23569;&#19982;1-WL&#27979;&#35797;&#19968;&#26679;&#33021;&#22815;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GAN&#26469;&#35757;&#32451;Learnable-MPC&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#19981;&#33021;&#30456;&#21516;&#26102;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19111</link><description>&lt;p&gt;
GAN-MPC:&#20351;&#29992;&#38750;&#30456;&#21516;&#19987;&#23478;&#30340;&#28436;&#31034;&#35757;&#32451;&#20855;&#26377;&#21442;&#25968;&#21270;&#25104;&#26412;&#20989;&#25968;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
GAN-MPC: Training Model Predictive Controllers with Parameterized Cost Functions using Demonstrations from Non-identical Experts. (arXiv:2305.19111v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;GAN&#26469;&#35757;&#32451;Learnable-MPC&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#19981;&#33021;&#30456;&#21516;&#26102;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26159;&#26426;&#22120;&#20154;&#23454;&#38469;&#24212;&#29992;&#20013;&#36712;&#36857;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;MPC&#31574;&#30053;&#21487;&#20197;&#22312;&#36816;&#21160;&#21160;&#21147;&#23398;&#21644;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#36712;&#36857;&#21442;&#25968;&#65292;&#24182;&#22312;&#23433;&#20840;&#20445;&#38556;&#12289;&#26368;&#20248;&#24615;&#12289;&#27867;&#21270;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35828;&#26126;&#24615;&#26041;&#38754;&#25552;&#20379;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#34892;&#20026;&#26159;&#22797;&#26434;&#30340;&#65292;&#25163;&#24037;&#21046;&#20316;MPC&#30446;&#26631;&#20989;&#25968;&#26159;&#22256;&#38590;&#30340;&#12290;&#19968;&#31181;&#21517;&#20026;&#21487;&#23398;&#20064;MPC&#30340;&#29305;&#27530;&#31867;&#21035;&#30340;MPC&#31574;&#30053;&#21033;&#29992;&#26469;&#33258;&#19987;&#23478;&#28436;&#31034;&#30340;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#27714;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#20195;&#29702;&#30456;&#21516;&#65292;&#36825;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24403;&#28436;&#31034;&#32773;&#21644;&#27169;&#20223;&#32773;&#27809;&#26377;&#20849;&#20139;&#30456;&#21516;&#21160;&#21147;&#23398;&#19988;&#29366;&#24577;&#31354;&#38388;&#21487;&#33021;&#37096;&#20998;&#37325;&#21472;&#26102;&#35757;&#32451;&#21487;&#23398;&#20064;MPC&#31574;&#30053;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#26368;&#23567;&#21270;Jensen-Shannon&#36317;&#31163;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) is a popular approach for trajectory optimization in practical robotics applications. MPC policies can optimize trajectory parameters under kinodynamic and safety constraints and provide guarantees on safety, optimality, generalizability, interpretability, and explainability. However, some behaviors are complex and it is difficult to hand-craft an MPC objective function. A special class of MPC policies called Learnable-MPC addresses this difficulty using imitation learning from expert demonstrations. However, they require the demonstrator and the imitator agents to be identical which is hard to satisfy in many real world applications of robotics. In this paper, we address the practical problem of training Learnable-MPC policies when the demonstrator and the imitator do not share the same dynamics and their state spaces may have a partial overlap. We propose a novel approach that uses a generative adversarial network (GAN) to minimize the Jensen-Shannon di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2305.18088</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#30340;&#33647;&#29289;&#37325;&#29992;&#20197;&#38774;&#21521;COVID-19 3CL Protease
&lt;/p&gt;
&lt;p&gt;
Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#20581;&#24247;&#21361;&#26426;&#65292;&#36843;&#20999;&#38656;&#35201;&#24555;&#36895;&#37492;&#23450;&#28508;&#22312;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#33647;&#29289;&#37325;&#29992;&#26159;&#30465;&#26102;&#30465;&#21147;&#30340;&#21807;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Zinc&#25968;&#25454;&#24211;&#23545;&#20840;&#29699;&#24050;&#25209;&#20934;&#65288;&#21253;&#25324;FDA&#25209;&#20934;&#65289;&#30340;5903&#31181;&#33647;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#20316;&#20026;&#28508;&#22312;&#30340;COVID-19&#27835;&#30103;&#33647;&#29289;&#65292;&#20197;&#38774;&#21521;SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#12290;&#25105;&#20204;&#20351;&#29992;Autodock-Vina&#36827;&#34892;&#20998;&#23376;&#23545;&#25509;&#65292;&#26816;&#26597;&#33647;&#29289;&#20998;&#23376;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20915;&#31574;&#26641;&#12289;&#39069;&#22806;&#26641;&#12289;MLP&#12289;KNN&#12289;XGBoost&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#24314;&#27169;&#32467;&#21512;&#33647;&#29289;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#12290;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17341</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31354;&#38388;&#30340;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#26469;&#25913;&#36827;&#38544;&#31169;&#20445;&#25252;PCA
&lt;/p&gt;
&lt;p&gt;
Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20197;&#24448;&#26041;&#27861;&#65292;&#20854;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#19978;&#22343;&#26377;&#25552;&#21319;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#21644;&#39640;&#25928;&#21516;&#24577;&#30005;&#36335;&#65292;&#35745;&#31639;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36817;&#20284;&#25968;&#20540;&#31639;&#26415;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;PCA&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#34987;&#31216;&#20026;PowerMethod&#30340;PCA&#24120;&#35268;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20197;&#21327;&#26041;&#24046;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19982;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#20027;&#25104;&#20998;&#23545;&#24212;&#30340;&#36817;&#20284;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;Pandas CSCML 21&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20197;&#19979;&#20248;&#21270;&#65306;&#65288;i&#65289;&#20248;&#21270;&#20102;&#21516;&#24577;&#30697;&#38453;&#20056;&#27861;&#25216;&#26415;&#65288;Jiang&#31561;&#20154;SIGSAC 2018&#65289;&#65292;&#35813;&#25216;&#26415;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35745;&#31639;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#21516;&#24577;&#30005;&#36335;&#26469;&#21516;&#24577;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#29992;&#20110;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11930</link><description>&lt;p&gt;
PyTorch&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#8212;&#8212;&#38754;&#21521;spotPython&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#65288;&#25110;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#30340;&#30446;&#26631;&#26159;&#20248;&#21270;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#26426;&#22120;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;spotPython&#26159;&#30693;&#21517;&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;SPOT&#30340;Python&#29256;&#26412;&#65292;SPOT&#24050;&#32463;&#22312;R&#32534;&#31243;&#29615;&#22659;&#20013;&#20026;&#32479;&#35745;&#20998;&#26512;&#24320;&#21457;&#20102;&#21313;&#24180;&#20197;&#19978;&#12290;PyTorch&#26159;&#19968;&#31181;&#22522;&#20110;GPU&#21644;CPU&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#24352;&#37327;&#24211;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;spotPython&#36229;&#21442;&#25968;&#35843;&#35856;&#22120;&#38598;&#25104;&#21040;PyTorch&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#12290;&#20197;CIFAR10&#22270;&#20687;&#20998;&#31867;&#22120;&#20026;&#20363;&#65292;&#20171;&#32461;&#20102;spotPython&#20197;&#21450;&#19982;Ray Tune&#30340;&#31616;&#30701;&#27604;&#36739;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;spotPython&#30340;&#20351;&#29992;&#32463;&#39564;&#65292;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;hook&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.05089</link><description>&lt;p&gt;
&#21487;&#32422;&#30340;&#21452;&#26354;&#27491;&#20999;&#32593;&#32476;&#30340;&#21151;&#33021;&#31561;&#20215;&#21644;&#36335;&#24452;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks. (arXiv:2305.05089v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#38656;&#35201;&#28548;&#28165;&#23398;&#20064;&#21457;&#29983;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#32467;&#26500;&#12290;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#23454;&#29616;&#30456;&#21516;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#30340;&#21442;&#25968;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#21487;&#32422;&#21442;&#25968;&#65292;&#20854;&#21151;&#33021;&#31561;&#20215;&#31867;&#30001;&#32593;&#32476;&#21333;&#20803;&#20043;&#38388;&#30340;&#20887;&#20313;&#36896;&#25104;&#65292;&#22240;&#32780;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#12290;&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22810;&#25968;&#20887;&#20313;&#21333;&#20803;&#30340;&#21442;&#25968;&#65292;&#36825;&#20123;&#38598;&#21512;&#30340;&#30452;&#24452;&#26368;&#22810;&#20026;7&#32447;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's functional equivalence class is the set of parameters implementing the same input--output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of reducible parameters, with richer functional equivalence classes caused by redundancies among the network's units.  In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02757</link><description>&lt;p&gt;
&#19981;&#20805;&#20998;&#26631;&#27880;&#19979;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#65288;MDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21407;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;(MDL)&#25351;&#21516;&#26102;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#25110;&#19968;&#32452;&#27169;&#22411;&#12290;&#20256;&#32479;&#26041;&#27861;&#24378;&#35843;&#22495;&#20849;&#20139;&#20449;&#24687;&#30340;&#25552;&#21462;&#21644;&#22495;&#31169;&#26377;&#20449;&#24687;&#30340;&#20445;&#30041;&#65292;&#36981;&#24490;&#20849;&#20139;-&#31169;&#26377;&#26550;&#26500;(SP&#27169;&#22411;)&#65292;&#36825;&#27604;&#21333;&#39046;&#22495;&#23398;&#20064;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#26377;&#38480;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#20256;&#32479;&#30417;&#30563;MDL&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;(MDCL)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#19981;&#20805;&#20998;&#27880;&#37322;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MDCL&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#22495;&#38388;&#35821;&#20041;&#23545;&#40784;&#21644;&#22495;&#20869;&#23545;&#27604;&#12290;&#21069;&#32773;&#26088;&#22312;&#23558;&#19981;&#21516;&#39046;&#22495;&#20013;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#24050;&#26631;&#27880;&#23454;&#20363;&#22312;&#20849;&#20139;&#30340;&#38544;&#31354;&#38388;&#20013;&#23545;&#40784;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#22312;&#27599;&#20010;&#39046;&#22495;&#20869;&#26368;&#22823;&#21270;&#20998;&#31163;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22810;&#39046;&#22495;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;MDCL&#26041;&#27861;&#22312;&#21508;&#31181;&#27880;&#37322;&#26041;&#26696;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;MDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#19979;&#19968;&#20010;token&#65292;&#20026;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13840</link><description>&lt;p&gt;
&#38754;&#21521;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#30340;Verilog&#33258;&#21160;&#23436;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation. (arXiv:2304.13840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#19979;&#19968;&#20010;token&#65292;&#20026;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#28385;&#36275;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#30005;&#23376;&#35774;&#22791;&#30340;&#35774;&#35745;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290; Verilog&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25968;&#23383;&#30005;&#36335;&#35774;&#35745;&#21644;&#39564;&#35777;&#30340;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;EDA&#24037;&#20855;&#36827;&#34892;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#32534;&#20889;&#20195;&#30721;&#26159;&#19968;&#39033;&#37325;&#22797;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#20854;&#27425;&#25552;&#20379;&#20102;&#20174;&#24320;&#28304;&#23384;&#20648;&#24211;&#33719;&#21462;&#30340;Verilog&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#19978;&#65292;&#28982;&#21518;&#22312;&#30456;&#20284;&#20110;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;&#27604;&#36739;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#35757;&#32451;&#19981;&#21516;&#23376;&#38598;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#25552;&#35758;&#30340;Verilog&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;BLEU&#65292;ROUGE-L&#21644;chrF&#24471;&#20998;&#65292;&#24182;&#19988;&#26377;&#25928;&#39044;&#27979;&#20102;&#37096;&#20998;&#32534;&#20889;&#30340;Verilog&#35821;&#21477;&#30340;&#19979;&#19968;&#20010;token&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative Electronic Design Automation (EDA) solutions are important to meet the design requirements for increasingly complex electronic devices. Verilog, a hardware description language, is widely used for the design and verification of digital circuits and is synthesized using specific EDA tools. However, writing code is a repetitive and time-intensive task. This paper proposes, primarily, a novel deep learning framework for training a Verilog autocompletion model and, secondarily, a Verilog dataset of files and snippets obtained from open-source repositories. The framework involves integrating models pretrained on general programming language data and finetuning them on a dataset curated to be similar to a target downstream task. This is validated by comparing different pretrained models trained on different subsets of the proposed Verilog dataset using multiple evaluation metrics. These experiments demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF sco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06051</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#26234;&#33021;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598; - Open-TransMind
&lt;/p&gt;
&lt;p&gt;
Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06051
&lt;/p&gt;
&lt;p&gt;
Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#35745;&#31639;&#33021;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#36234;&#26469;&#36234;&#22810;&#30340;&#34892;&#19994;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#22312;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#20197;&#19979;&#20856;&#22411;&#25361;&#25112;&#65306;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#65292;&#26088;&#22312;&#22686;&#21152;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#24230;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#35813;&#25361;&#25112;&#20998;&#20026;&#20004;&#20010;&#36187;&#36947;&#65306;&#20840;&#33021;&#22411;&#21644;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20004;&#20010;&#36187;&#36947;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#32447;&#21644;&#22522;&#20934;&#25968;&#25454;&#65292;&#31216;&#20026;Open-TransMind&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#24212;&#29992;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;&#25991;&#29486;&#20998;&#20026;&#30417;&#35270;&#21644;&#25805;&#20316;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.06009</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing. (arXiv:2304.06009v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#24212;&#29992;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;&#25991;&#29486;&#20998;&#20026;&#30417;&#35270;&#21644;&#25805;&#20316;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#30340;&#33258;&#21160;&#21270;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#28508;&#21147;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25152;&#26377;&#25991;&#29486;&#37117;&#34987;&#20998;&#31867;&#20026;&#24212;&#29992;&#21644;&#29992;&#20110;&#35299;&#20915;&#20219;&#21153;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#12290;&#20851;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#25991;&#29486;&#20998;&#20026;&#20004;&#20010;&#39046;&#22495;&#65306;&#30417;&#35270;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#24182;&#38142;&#25509;&#21040;&#36866;&#29992;&#20110;&#29289;&#27969;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#21147;&#20173;&#28982;&#24040;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#23545;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#24456;&#26377;&#24110;&#21161;&#65292;&#22240;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#24320;&#21457;&#21644;&#27979;&#35797;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision applications in transportation logistics and warehousing have a huge potential for process automation. We present a structured literature review on research in the field to help leverage this potential. All literature is categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the computer vision techniques that are used. Regarding applications, we subdivide the literature in two areas: Monitoring, i.e. observing and retrieving relevant information from the environment, and manipulation, where approaches are used to analyze and interact with the environment. In addition to that, we point out directions for future research and link to recent developments in computer vision that are suitable for application in logistics. Finally, we present an overview of existing datasets and industrial solutions. We conclude that while already many research areas have been investigated, there is still huge potential for future research. The results of our analysis are als
&lt;/p&gt;</description></item><item><title>ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.05977</link><description>&lt;p&gt;
ImageReward&#65306;&#23398;&#20064;&#21644;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;
&lt;/p&gt;
&lt;p&gt;
ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05977
&lt;/p&gt;
&lt;p&gt;
ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;ImageReward&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#22522;&#20110;&#25105;&#20204;&#30340;&#31995;&#32479;&#27880;&#37322;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20998;&#21644;&#25490;&#21517;&#32452;&#20214;&#65292;&#36804;&#20170;&#24050;&#25910;&#38598;&#20102;137k&#30340;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#38598;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;ImageReward&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#20363;&#22914;&#27604;CLIP&#39640;38.6\%&#65289;&#65292;&#22240;&#27492;&#23427;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;\texttt {image-reward}&#31243;&#24207;&#21253;&#20844;&#24320;&#25552;&#20379;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/THUDM/ImageReward}&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04027</link><description>&lt;p&gt;
NeBLa: &#20351;&#29992;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#37325;&#24314;&#21475;&#33108;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;X&#32447;&#29255;&#65288;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#65292;PX&#65289;&#26159;&#24120;&#29992;&#20110;&#29273;&#31185;&#26816;&#26597;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;3D&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#30456;&#27604;&#65292;PX&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;PX&#21482;&#25552;&#20379;&#21475;&#33108;&#32467;&#26500;&#30340;&#20108;&#32500;&#25153;&#24179;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#30340;PX&#22270;&#20687;&#20272;&#35745;3D&#21475;&#33108;&#32467;&#26500;&#12290;&#30001;&#20110;PX&#21644;CBCT&#25968;&#25454;&#30340;&#21305;&#37197;&#19981;&#22810;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#20102;&#20174;CBCT&#27169;&#25311;&#30340;PX&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#20102;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#32447;&#37319;&#26679;&#26041;&#27861;&#65292;&#21463;&#21040;&#20840;&#26223;&#25918;&#23556;&#32447;&#25104;&#20687;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#21860;&#37202;-&#20848;&#20271;&#29305;&#23450;&#24459;&#23548;&#20986;&#28210;&#26579;&#20989;&#25968;&#29983;&#25104;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#36716;&#25442;&#27169;&#22359;&#65292;&#29983;&#25104;&#27169;&#22359;&#21644;&#31934;&#28860;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23558;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;&#35757;&#32451;&#22270;&#20687;&#39118;&#26684;&#12290;&#29983;&#25104;&#27169;&#22359;&#21033;&#29992;&#23556;&#32447;&#37319;&#26679;&#26041;&#27861;&#24471;&#21040;&#30340;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#32422;&#26463;&#19979;&#30340;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#32467;&#26500;&#12290;&#31934;&#28860;&#27169;&#22359;&#25913;&#21892;&#20102;3D&#32467;&#26500;&#30340;&#24179;&#28369;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#29273;&#31185;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.17727</link><description>&lt;p&gt;
BOLT&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17727
&lt;/p&gt;
&lt;p&gt;
BOLT&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#27169;&#22411;&#24182;&#25277;&#35937;&#25481;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#26222;&#36890;CPU&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#23545;&#20110;&#27665;&#20027;&#21270;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#30446;&#21069;&#65292;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#24191;&#27867;&#20351;&#29992;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#20363;&#22914;GPU&#65289;&#65292;&#36825;&#20123;&#21152;&#36895;&#22120;&#20165;&#38480;&#20110;&#23569;&#25968;&#20855;&#26377;&#30456;&#24403;&#36130;&#21153;&#36164;&#28304;&#30340;&#26426;&#26500;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#24102;&#26469;&#24778;&#20154;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BOLT&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#26631;&#20934;CPU&#30828;&#20214;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;BOLT&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#39640;&#32423;API&#65292;&#29992;&#20110;&#26500;&#24314;&#27169;&#22411;&#65292;&#35813;API&#23545;&#20110;&#29616;&#26377;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#29087;&#24713;&#30340;&#12290;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#19987;&#29992;&#36229;&#21442;&#25968;&#65292;BOLT&#20063;&#25277;&#35937;&#25481;&#20102;&#31232;&#30095;&#32593;&#32476;&#35757;&#32451;&#30340;&#31639;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17152</link><description>&lt;p&gt;
&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#36890;&#36807;&#38543;&#26426;&#36974;&#30422;&#22270;&#20687;&#34917;&#19969;&#21644;&#37325;&#24314;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MAE&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20173;&#28982;&#26159;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#19981;&#21516;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;MAE&#30340;&#26222;&#36941;&#28151;&#21512;&#22686;&#24378;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26420;&#32032;&#28151;&#21512;&#23558;&#30001;&#20110;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#28304;&#35782;&#21035;&#26041;&#27861;&#65292;&#19968;&#31181;&#36741;&#21161;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#19981;&#20165;&#36890;&#36807;&#26126;&#30830;&#35201;&#27714;&#27599;&#20010;&#34917;&#19969;&#35782;&#21035;&#21516;&#28304;&#34917;&#19969;&#26469;&#32531;&#35299;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25191;&#34892;&#38754;&#21521;&#23545;&#35937;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#23494;&#38598;&#24863;&#30693;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#22312;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;</title><link>http://arxiv.org/abs/2303.14068</link><description>&lt;p&gt;
&#22522;&#20110;CNN-LSTM&#26550;&#26500;&#30340; AIS &#25968;&#25454;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data. (arXiv:2303.14068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#33337;&#33334;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#27915;&#30417;&#27979;&#20013;&#65292;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#33337;&#33334;&#36816;&#21160;&#27169;&#24335;&#23545;&#20110;&#21450;&#26102;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#38656;&#35201;&#20351;&#29992;&#36712;&#36857;&#20851;&#32852;&#31639;&#27861;&#65292;&#23558;&#30001;&#36816;&#21160;&#21442;&#25968;&#32452;&#25104;&#30340;&#26102;&#24207;&#35266;&#27979;&#32467;&#26524;&#19982;&#30456;&#24212;&#30340;&#33337;&#21482;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;1D CNN-LSTM&#32467;&#26500;&#30340;&#36712;&#36857;&#20851;&#32852;&#26694;&#26550;&#65292;&#23558;&#36825;&#19968;&#36861;&#36394;&#20219;&#21153;&#35270;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
In marine surveillance, distinguishing between normal and anomalous vessel movement patterns is critical for identifying potential threats in a timely manner. Once detected, it is important to monitor and track these vessels until a necessary intervention occurs. To achieve this, track association algorithms are used, which take sequential observations comprising geological and motion parameters of the vessels and associate them with respective vessels. The spatial and temporal variations inherent in these sequential observations make the association task challenging for traditional multi-object tracking algorithms. Additionally, the presence of overlapping tracks and missing data can further complicate the trajectory tracking process. To address these challenges, in this study, we approach this tracking task as a multivariate time series problem and introduce a 1D CNN-LSTM architecture-based framework for track association. This special neural network architecture can capture the spat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#29992;&#20110;&#30830;&#23450;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FPUS23&#21487;&#20197;&#20026;&#20020;&#24202;&#36229;&#22768;&#30417;&#27979;&#24037;&#20316;&#27969;&#31243;&#24102;&#26469;&#25913;&#21892;&#65292;&#20197;&#21450;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2303.07852</link><description>&lt;p&gt;
&#19968;&#20221;&#24102;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#30340;&#32650;&#27700;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features. (arXiv:2303.07852v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#29992;&#20110;&#30830;&#23450;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FPUS23&#21487;&#20197;&#20026;&#20020;&#24202;&#36229;&#22768;&#30417;&#27979;&#24037;&#20316;&#27969;&#31243;&#24102;&#26469;&#25913;&#21892;&#65292;&#20197;&#21450;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#26159;&#35780;&#20272;&#32974;&#20799;&#22312;&#22922;&#23072;&#26399;&#38388;&#30340;&#29983;&#38271;&#65292;&#21457;&#23637;&#21644;&#25972;&#20307;&#20581;&#24247;&#29366;&#20917;&#26368;&#31361;&#20986;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#20026;&#20102;&#25913;&#21892;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#24182;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#23427;&#21487;&#29992;&#20110;&#30830;&#23450;&#65288;1&#65289;&#29992;&#20110;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#35745;&#37327;&#20540;&#30340;&#27491;&#30830;&#35786;&#26029;&#24179;&#38754;&#65292;&#65288;2&#65289;&#32974;&#20799;&#26041;&#21521;&#65292;&#65288;3&#65289;&#23427;&#20204;&#30340;&#35299;&#21078;&#32467;&#26500;&#21644;&#65288;4&#65289;23&#21608;&#23381;&#40836;&#26102;&#32974;&#20799;&#27169;&#22411;&#35299;&#21078;&#23398;&#30340;&#36793;&#30028;&#26694;&#12290;&#25972;&#20010;&#25968;&#25454;&#38598;&#30001;15,728&#24352;&#22270;&#20687;&#32452;&#25104;&#65292;&#29992;&#20110;&#35757;&#32451;&#24314;&#31435;&#22312;ResNet34&#39592;&#24178;&#32593;&#32476;&#19978;&#30340;&#22235;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19978;&#36848;&#32974;&#20799;&#29305;&#24449;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;FPUS23&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound imaging is one of the most prominent technologies to evaluate the growth, progression, and overall health of a fetus during its gestation. However, the interpretation of the data obtained from such studies is best left to expert physicians and technicians who are trained and well-versed in analyzing such images. To improve the clinical workflow and potentially develop an at-home ultrasound-based fetal monitoring platform, we present a novel fetus phantom ultrasound dataset, FPUS23, which can be used to identify (1) the correct diagnostic planes for estimating fetal biometric values, (2) fetus orientation, (3) their anatomical features, and (4) bounding boxes of the fetus phantom anatomies at 23 weeks gestation. The entire dataset is composed of 15,728 images, which are used to train four different Deep Neural Network models, built upon a ResNet34 backbone, for detecting aforementioned fetus features and use-cases. We have also evaluated the models trained using our FPUS23 da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#24182;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.04756</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#65306;&#26377;&#38480;&#25968;&#25454;&#20013;&#26041;&#24046;&#32553;&#20943;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-learning Control Variates: Variance Reduction with Limited Data. (arXiv:2303.04756v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#24182;&#23545;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#21464;&#37327;&#26159;&#20943;&#23567;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#22120;&#26041;&#24046;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#22312;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#26377;&#25928;&#30340;&#25511;&#21046;&#21464;&#37327;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#24403;&#38656;&#35201;&#35745;&#31639;&#22823;&#37327;&#30456;&#20851;&#31215;&#20998;&#26102;&#65292;&#21363;&#20351;&#27599;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#25968;&#24456;&#23569;&#65292;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#31215;&#20998;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20803;&#23398;&#20064;CV&#65288;Meta-CVs&#65289;&#26041;&#27861;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#30334;&#20010;&#25110;&#25968;&#21315;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;Meta-CVs&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30830;&#23450;&#20102;Meta-CVs&#25104;&#21151;&#35757;&#32451;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control variates can be a powerful tool to reduce the variance of Monte Carlo estimators, but constructing effective control variates can be challenging when the number of samples is small. In this paper, we show that when a large number of related integrals need to be computed, it is possible to leverage the similarity between these integration tasks to improve performance even when the number of samples per task is very small. Our approach, called meta learning CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our empirical assessment indicates that Meta-CVs can lead to significant variance reduction in such settings, and our theoretical analysis establishes general conditions under which Meta-CVs can be successfully trained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.04562</link><description>&lt;p&gt;
&#36845;&#20195;&#20462;&#27491;&#30340;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22806;&#25512;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24207;&#21015;&#12290;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#65292;&#36825;&#20010;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#27604;&#29616;&#26377;&#24207;&#21015;&#26356;&#22909;&#65288;&#20363;&#22914;&#26356;&#31283;&#23450;&#65289;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#12290;&#22240;&#27492;&#65292;&#25353;&#29031;&#23450;&#20041;&#65292;&#30446;&#26631;&#24207;&#21015;&#21450;&#20854;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#65292;&#25361;&#25112;&#29616;&#26377;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36845;&#20195;&#25511;&#21046;&#22806;&#25512;&#65288;ICE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#24207;&#21015;&#36827;&#34892;&#23616;&#37096;&#32534;&#36753;&#26469;&#23454;&#29616;&#22806;&#25512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#24207;&#21015;&#23545;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28436;&#31034;&#24494;&#23567;&#30340;&#23646;&#24615;&#20540;&#25913;&#36827;&#12290;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#26512;&#65289;&#21644;&#20004;&#20010;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#65288;ACE2&#31283;&#23450;&#24615;&#21644;AAV&#36866;&#24212;&#24615;&#65289;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ICE&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VeSSAL&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#27969;&#24335;&#35774;&#32622;&#19979;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#25552;&#21069;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#35775;&#38382;&#26435;&#38480;&#65292;&#21487;&#20197;&#20174;&#36935;&#21040;&#30340;&#28857;&#30340;&#26679;&#26412;&#32452;&#20013;&#25277;&#26679;&#26597;&#35810;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#26597;&#35810;&#26679;&#26412;&#22810;&#26679;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#26597;&#35810;&#29575;&#65292;&#25299;&#23637;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2303.02535</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Streaming Active Learning with Deep Neural Networks. (arXiv:2303.02535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VeSSAL&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#27969;&#24335;&#35774;&#32622;&#19979;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#25552;&#21069;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#35775;&#38382;&#26435;&#38480;&#65292;&#21487;&#20197;&#20174;&#36935;&#21040;&#30340;&#28857;&#30340;&#26679;&#26412;&#32452;&#20013;&#25277;&#26679;&#26597;&#35810;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#26597;&#35810;&#26679;&#26412;&#22810;&#26679;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#26597;&#35810;&#29575;&#65292;&#25299;&#23637;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26368;&#33258;&#28982;&#30340;&#24418;&#24335;&#25110;&#35768;&#26159;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#20551;&#23450;&#25552;&#21069;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#35775;&#38382;&#26435;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;VeSSAL&#65292;&#29992;&#20110;&#22312;&#27969;&#24335;&#35774;&#32622;&#19979;&#25209;&#37327;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#36935;&#21040;&#30340;&#28857;&#30340;&#26679;&#26412;&#32452;&#25277;&#26679;&#26597;&#35810;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#26597;&#35810;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#26597;&#35810;&#29575;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#33539;&#22260;&#25193;&#22823;&#21040;&#26356;&#21152;&#23454;&#38469;&#30340;&#20027;&#21160;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#27604;&#22914;&#28041;&#21450;HCI&#21644;&#22823;&#22411;&#35010;&#21464;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is perhaps most naturally posed as an online learning problem. However, prior active learning approaches with deep neural networks assume offline access to the entire dataset ahead of time. This paper proposes VeSSAL, a new algorithm for batch active learning with deep neural networks in streaming settings, which samples groups of points to query for labels at the moment they are encountered. Our approach trades off between uncertainty and diversity of queried samples to match a desired query rate without requiring any hand-tuned hyperparameters. Altogether, we expand the applicability of deep neural networks to realistic active learning scenarios, such as applications relevant to HCI and large, fractured datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;SQRL&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#32479;&#35745;&#35268;&#21017;&#65307;&#22312;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#31561;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#36829;&#21453;&#36825;&#20123;&#35268;&#21017;&#65292;&#20294;&#26159;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#65292;&#36829;&#35268;&#34892;&#20026;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01433</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#33021;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30340;&#32479;&#35745;&#35268;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Machine Learning Models Learn Statistical Rules Inferred from Data?. (arXiv:2303.01433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;SQRL&#65292;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#32479;&#35745;&#35268;&#21017;&#65307;&#22312;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#31561;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#36829;&#21453;&#36825;&#20123;&#35268;&#21017;&#65292;&#20294;&#26159;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#23545;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#65292;&#36829;&#35268;&#34892;&#20026;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#22823;&#37327;&#25968;&#25454;&#20013;&#38544;&#34255;&#19968;&#20123;&#37325;&#35201;&#30340;&#38169;&#35823;&#65292;&#32780;&#36825;&#20123;&#38169;&#35823;&#36890;&#24120;&#36829;&#21453;&#20102;&#20154;&#31867;&#30452;&#35273;&#30340;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#20197;&#20154;&#31867;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#35268;&#21017;&#24448;&#24448;&#19981;&#26131;&#25193;&#23637;&#25110;&#27491;&#24335;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;SQRL&#65292;&#23427;&#23558;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#19982;&#32479;&#35745;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#30417;&#30563;&#21363;&#21487;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#36825;&#20123;&#35268;&#21017;&#65292;&#24182;&#37327;&#21270;&#27169;&#22411;&#24050;&#32463;&#23398;&#20064;&#21040;&#36825;&#20123;&#35268;&#21017;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#35843;&#25972;&#27169;&#22411;&#20197;&#20943;&#23569;&#35268;&#21017;&#36829;&#35268;&#24182;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;SQRL&#21487;&#20197;&#22312;&#35270;&#35273;&#12289;&#34920;&#26684;&#21644;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22810;&#36798;30&#19975;&#26465;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#36829;&#21453;&#20102;&#36825;&#20123;&#35268;&#21017;&#39640;&#36798;158K&#27425;&#12290;&#32780;&#27979;&#35797;&#26102;&#38388;&#30340;&#36866;&#24212;&#21487;&#20197;&#23558;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#20943;&#23569;&#39640;&#36798;68.7%&#65292;&#24182;&#25552;&#39640;&#30456;&#23545;&#24615;&#33021;&#39640;&#36798;32%&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7% with relative performance improvement up to 32%. SQRL is available a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#23558;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#37325;&#26500;&#20026;&#27010;&#29575;&#20998;&#24067;$Q$&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20174;&#32780;&#35299;&#20915;&#23545;&#27491;&#21017;&#21270;&#27969;&#30340;&#38480;&#21046;&#65292;&#22312;&#24314;&#27169;&#19978;&#23454;&#29616;&#26080;&#32541;&#36830;&#25509;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#27169;&#22411;&#65288;&#22914;&#27491;&#21017;&#21270;&#27969;&#65289;&#23545;Q&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2302.12235</link><description>&lt;p&gt;
Q-Flow&#65306;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#37327;&#30340;&#24320;&#25918;&#37327;&#23376;&#21160;&#21147;&#23398;&#24494;&#20998;&#26041;&#31243;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows. (arXiv:2302.12235v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#23558;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#37325;&#26500;&#20026;&#27010;&#29575;&#20998;&#24067;$Q$&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20174;&#32780;&#35299;&#20915;&#23545;&#27491;&#21017;&#21270;&#27969;&#30340;&#38480;&#21046;&#65292;&#22312;&#24314;&#27169;&#19978;&#23454;&#29616;&#26080;&#32541;&#36830;&#25509;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#27169;&#22411;&#65288;&#22914;&#27491;&#21017;&#21270;&#27969;&#65289;&#23545;Q&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#24577;&#21487;&#20197;&#22312;&#22522;&#30784;&#29289;&#29702;&#23398;&#20197;&#21450;&#24212;&#29992;&#20110;&#37327;&#23376;&#24037;&#31243;&#21644;&#37327;&#23376;&#35745;&#31639;&#26041;&#38754;&#23454;&#29616;&#31361;&#30772;&#12290;&#30001;&#20110;&#23494;&#24230;&#30697;&#38453;$\rho$ &#26159;&#25551;&#36848;&#27492;&#31867;&#31995;&#32479;&#21160;&#24577;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20854;&#32500;&#25968;&#24456;&#39640;&#65292;&#22240;&#27492;&#23450;&#21046;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24314;&#27169;$\rho$&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;$\rho$&#30340;&#22797;&#25968;&#24615;&#36136;&#21644;&#27491;&#21017;&#21270;&#38480;&#21046;&#65292;&#20197;&#21450;&#20854;&#22797;&#26434;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#38459;&#30861;&#20102;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#19982;&#28145;&#24230;&#29983;&#25104;&#24335;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#20043;&#38388;&#30340;&#27969;&#30021;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#37325;&#26500;&#20026;&#30456;&#24212;&#27010;&#29575;&#20998;&#24067; $Q$ &#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#21363;Husimi Q&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#24335;&#27169;&#22411;&#65288;&#22914;&#27491;&#21017;&#21270;&#27969;&#65289;&#26080;&#32541;&#22320;&#23545;Q&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#27491;&#21017;&#21270;&#27969;&#21160;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the dynamics of open quantum systems can enable breakthroughs both in fundamental physics and applications to quantum engineering and quantum computation. Since the density matrix $\rho$, which is the fundamental description for the dynamics of such systems, is high-dimensional, customized deep generative neural networks have been instrumental in modeling $\rho$. However, the complex-valued nature and normalization constraints of $\rho$, as well as its complicated dynamics, prohibit a seamless connection between open quantum systems and the recent advances in deep generative modeling. Here we lift that limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution $Q$, the Husimi Q function. Thus, we model the Q function seamlessly with off-the-shelf deep generative models such as normalizing flows. Additionally, we develop novel methods for learning normalizing flow evolution govern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#27861;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#22312;&#30495;&#23454;&#30340;&#20449;&#29992;&#36151;&#27454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08077</link><description>&lt;p&gt;
&#24102;&#26377;&#25935;&#24863;&#23646;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Group Fairness with Uncertainty in Sensitive Attributes. (arXiv:2302.08077v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#27861;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#22312;&#30495;&#23454;&#30340;&#20449;&#29992;&#36151;&#27454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#20943;&#23569;&#38024;&#23545;&#23569;&#25968;&#32676;&#20307;&#30340;&#26377;&#20559;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#36866;&#24403;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25935;&#24863;&#23646;&#24615;&#36890;&#24120;&#20250;&#32570;&#22833;&#25110;&#23384;&#22312;&#22122;&#22768;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#22312;&#19981;&#30830;&#23450;&#30340;&#25935;&#24863;&#23646;&#24615;&#19978;&#23454;&#26045;&#20844;&#24179;&#32422;&#26463;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#35757;&#32451;&#26102;&#27809;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#25152;&#36798;&#21040;&#30340;&#20844;&#24179;&#27700;&#24179;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21161;&#27861;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#30446;&#26631;&#30340;&#20844;&#24179;&#27700;&#24179;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#39640;&#26031;&#20998;&#26512;&#36827;&#34892;&#24341;&#23548;&#65292;&#23454;&#29616;&#8220;&#29420;&#31435;&#27010;&#24565;&#8221;&#30340;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20108;&#27425;&#32422;&#26463;&#20108;&#27425;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#20855;&#26377;&#19981;&#30830;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#20005;&#26684;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20449;&#29992;&#36151;&#27454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#24403;&#25935;&#24863;&#23646;&#24615;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a fair predictive model is crucial to mitigate biased decisions against minority groups in high-stakes applications. A common approach to learn such a model involves solving an optimization problem that maximizes the predictive power of the model under an appropriate group fairness constraint. However, in practice, sensitive attributes are often missing or noisy resulting in uncertainty. We demonstrate that solely enforcing fairness constraints on uncertain sensitive attributes can fall significantly short in achieving the level of fairness of models trained without uncertainty. To overcome this limitation, we propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. The algorithm is guided by a Gaussian analysis for the independence notion of fairness where we propose a robust quadratically constrained quadratic problem to ensure a strict fairness guarantee with uncertain sensitive attributes. Our algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#24748;&#23830;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#23427;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#19982;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.07348</link><description>&lt;p&gt;
&#24748;&#23830;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cliff-Learning. (arXiv:2302.07348v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#31216;&#20026;&#24748;&#23830;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#23427;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#19982;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#22312;&#20302;&#19979;&#28216;&#25968;&#25454;&#29366;&#24577;&#19979;&#30340;&#25968;&#25454;&#32553;&#25918;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24748;&#23830;&#23398;&#20064;&#12290;&#24748;&#23830;&#23398;&#20064;&#26159;&#25351;&#22312;&#25968;&#25454;&#32553;&#25918;&#27861;&#21017;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#24615;&#33021;&#30340;&#25552;&#21319;&#36895;&#24230;&#24555;&#20110;&#24130;&#24459;&#36895;&#24230;&#30340;&#29616;&#35937;&#65288;&#21363;&#22312;&#23545;&#25968;&#32553;&#25918;&#22270;&#19978;&#30340;&#20985;&#24418;&#21306;&#22495;&#65289;&#12290;&#25105;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24748;&#23830;&#23398;&#20064;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#24182;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#29609;&#20855;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24748;&#23830;&#23398;&#20064;&#30340;&#31243;&#24230;&#21453;&#26144;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#25152;&#23398;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the data-scaling of transfer learning from foundation models in the low-downstream-data regime. We observe an intriguing phenomenon which we call cliff-learning. Cliff-learning refers to regions of data-scaling laws where performance improves at a faster than power law rate (i.e. regions of concavity on a log-log scaling plot). We conduct an in-depth investigation of foundation-model cliff-learning and study toy models of the phenomenon. We observe that the degree of cliff-learning reflects the degree of compatibility between the priors of a learning algorithm and the task being learned.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#23574;&#24230;&#19982;&#27867;&#21270;&#20851;&#31995;&#24182;&#19981;&#23494;&#20999;&#30456;&#20851;&#65292;&#32780;&#19982;&#26576;&#20123;&#35757;&#32451;&#21442;&#25968;&#21576;&#27491;&#30456;&#20851;&#25110;&#36127;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2302.07011</link><description>&lt;p&gt;
&#23574;&#24230;&#19982;&#27867;&#21270;&#20851;&#31995;&#30340;&#29616;&#20195;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Modern Look at the Relationship between Sharpness and Generalization. (arXiv:2302.07011v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07011
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23574;&#24230;&#19982;&#27867;&#21270;&#20851;&#31995;&#24182;&#19981;&#23494;&#20999;&#30456;&#20851;&#65292;&#32780;&#19982;&#26576;&#20123;&#35757;&#32451;&#21442;&#25968;&#21576;&#27491;&#30456;&#20851;&#25110;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#24230;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#19982;&#28145;&#24230;&#32593;&#32476;&#27867;&#21270;&#30456;&#20851;&#30340;&#37327;&#65292;&#24403;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#23574;&#24230;&#24182;&#19981;&#26159;&#31070;&#32463;&#32593;&#32476;&#37325;&#21442;&#25968;&#21270;&#19981;&#21464;&#30340;&#65292;&#24182;&#22240;&#27492;&#25552;&#20986;&#20102;&#37325;&#21442;&#25968;&#21270;&#19981;&#21464;&#23574;&#24230;&#23450;&#20041;&#65292;&#26368;&#33879;&#21517;&#30340;&#26159;&#33258;&#36866;&#24212;&#23574;&#24230;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#32034;&#21508;&#31181;&#33258;&#36866;&#24212;&#23574;&#24230;&#23450;&#20041;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;Imagenet&#21644;CIFAR-10&#21040;&#24494;&#35843;Imagenet&#21644;MNLI&#30340;CLIP&#21644;BERT&#30340;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#21464;&#24418;&#22120;&#65292;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20854;&#23574;&#24230;&#30340;&#20102;&#35299;&#20173;&#24456;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23574;&#24230;&#19982;&#27867;&#21270;&#20851;&#31995;&#19981;&#22823;&#65292;&#32780;&#19982;&#19968;&#20123;&#35757;&#32451;&#21442;&#25968;&#65288;&#22914;&#23398;&#20064;&#29575;&#65289;&#21576;&#27491;&#30456;&#20851;&#25110;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21407;&#21017;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.04445</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#31227;&#21160;&#35775;&#38382;&#30340;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems. (arXiv:2302.04445v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21407;&#21017;&#20197;&#25552;&#39640;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#22810;&#26234;&#33021;&#20307;&#28436;&#21592;-&#35780;&#35770;&#32593;&#32476;&#65288;QMACN&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#31227;&#21160;&#35775;&#38382;&#31995;&#32479;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#12290;&#22312;&#20419;&#36827;&#22810;&#20010;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#21327;&#20316;&#26041;&#38754;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#25216;&#26415;&#30340;&#24212;&#29992;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#38598;&#20307;&#23398;&#20064;&#65292;&#22312;&#20849;&#20139;&#29615;&#22659;&#20013;&#20248;&#21270;&#23427;&#20204;&#30340;&#34892;&#21160;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#20102;&#37327;&#23376;&#35745;&#31639;&#65288;QC&#65289;&#30340;&#21407;&#21017;&#65292;&#20197;&#22686;&#24378;&#28041;&#21450;&#30340;&#26080;&#20154;&#26426;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29420;&#29305;&#35745;&#31639;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;QC&#24341;&#20837;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#36817;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel algorithm, named quantum multi-agent actor-critic networks (QMACN) for autonomously constructing a robust mobile access system employing multiple unmanned aerial vehicles (UAVs). In the context of facilitating collaboration among multiple unmanned aerial vehicles (UAVs), the application of multi-agent reinforcement learning (MARL) techniques is regarded as a promising approach. These methods enable UAVs to learn collectively, optimizing their actions within a shared environment, ultimately leading to more efficient cooperative behavior. Furthermore, the principles of a quantum computing (QC) are employed in our study to enhance the training process and inference capabilities of the UAVs involved. By leveraging the unique computational advantages of quantum computing, our approach aims to boost the overall effectiveness of the UAV system. However, employing a QC introduces scalability challenges due to the near intermediate-scale quantum (NISQ) limitation ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#21487;&#20197;&#20197;&#19982;&#21407;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03542</link><description>&lt;p&gt;
&#20004;&#31181;&#25439;&#22833;&#27604;&#19968;&#31181;&#26356;&#22909;&#65306;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;&#20195;&#29702;&#21152;&#24555;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy. (arXiv:2302.03542v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#21487;&#20197;&#20197;&#19982;&#21407;&#20989;&#25968;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#30340;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#20989;&#25968;&#20316;&#20026;&#20195;&#29702;&#65292;&#26469;&#26368;&#23567;&#21270;&#19968;&#20010;&#38590;&#20197;&#35745;&#31639;&#26799;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#20195;&#29702;&#30340;&#36817;&#20284;&#36817;&#31471;&#28857;&#36845;&#20195;&#65292;&#32467;&#21512;&#26469;&#33258;&#30446;&#26631;&#20989;&#25968;&#30340;&#30456;&#23545;&#36739;&#23569;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;$\delta$-&#24179;&#28369;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20197;&#19982;$\delta$-&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#19978;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#21305;&#37197;&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#12289;&#29289;&#29702;&#27169;&#25311;&#22120;&#12289;&#28151;&#21512;&#20844;&#20849;&#21644;&#31169;&#20154;&#25968;&#25454;&#31561;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2302.03162</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24418;&#27700;&#21360;&#20445;&#25252;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#26377;&#21147;&#25903;&#25345;&#32773;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#27169;&#22411;&#25552;&#20379;&#20813;&#36153;&#25110;&#32463;&#27982;&#23454;&#24800;&#30340; API &#35775;&#38382;&#65292;&#36825;&#20351;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#20351;&#29992;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#20363;&#22914;&#35789;&#27719;&#27700;&#21360;&#21644;&#21516;&#20041;&#35789;&#26367;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26126;&#26174;&#30340;&#23545;&#31574;&#22914;&#8220;&#21516;&#20041;&#35789;&#38543;&#26426;&#21270;&#8221;&#31561;&#25152;&#25269;&#28040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; GINSEW&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#33976;&#39311;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25506;&#27979;&#23244;&#30097;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#31192;&#23494;&#28040;&#24687;&#26159;&#21542;&#30001;&#21463;&#20445;&#25252;&#30340;&#27169;&#22411;&#33976;&#39311;&#32780;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GINSEW &#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.02713</link><description>&lt;p&gt;
&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36890;&#36807;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#20808;&#39564;&#20998;&#24067;&#24182;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#38598;&#25104;&#39044;&#27979;&#21644;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#36739;&#20302;&#23574;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21518;&#39564;&#25512;&#35770;&#23545;&#20110;&#23574;&#24230;/&#25153;&#24179;&#21270;&#24182;&#19981;&#20855;&#22791;&#24847;&#35782;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20174;&#20854;&#37319;&#26679;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#23574;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#25153;&#24179;&#21270;&#23545;&#21518;&#39564;&#36827;&#34892;&#20102;&#29702;&#35770;&#12289;&#36125;&#21494;&#26031;&#35774;&#23450;&#21644;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#25153;&#24179;&#21270;&#24847;&#20041;&#19978;&#25512;&#26029;&#30340;&#27169;&#22411;&#20197;&#21450;&#20272;&#35745;&#35813;&#25153;&#24179;&#21270;&#24847;&#20041;&#21518;&#39564;&#30340;&#26368;&#20339;&#36817;&#20284;&#21518;&#39564;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22806;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#26631;&#20934;&#35757;&#32451;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#21644;&#24178;&#20928;&#24615;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#32531;&#35299;&#36825;&#31181;&#24046;&#24322;&#24182;&#20419;&#36827;&#34920;&#31034;&#25910;&#25947;&#21040;&#19968;&#20010;&#36890;&#29992;&#38598;&#21512;&#20013;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#23398;&#20064;&#26041;&#26696;&#12290;&#22686;&#21152;&#23545;&#25239;&#21644;&#24178;&#20928;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21487;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02502</link><description>&lt;p&gt;
&#20174;&#23545;&#25239;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#40065;&#26834;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Robust Contrastive Learning from the Adversarial Perspective. (arXiv:2302.02502v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02502
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#26631;&#20934;&#35757;&#32451;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#21644;&#24178;&#20928;&#24615;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#32531;&#35299;&#36825;&#31181;&#24046;&#24322;&#24182;&#20419;&#36827;&#34920;&#31034;&#25910;&#25947;&#21040;&#19968;&#20010;&#36890;&#29992;&#38598;&#21512;&#20013;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#23398;&#20064;&#26041;&#26696;&#12290;&#22686;&#21152;&#23545;&#25239;&#21644;&#24178;&#20928;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#21487;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#25239;&#35757;&#32451;&#23545;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26631;&#20934;&#35757;&#32451;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#21644;&#24178;&#20928;&#24615;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#36825;&#31181;&#24046;&#24322;&#24471;&#21040;&#20102;&#26174;&#33879;&#32531;&#35299;&#24182;&#20419;&#36827;&#20102;&#34920;&#31034;&#25910;&#25947;&#21040;&#19968;&#20010;&#36890;&#29992;&#38598;&#21512;&#20013;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#23398;&#20064;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#23545;&#25239;&#21644;&#24178;&#20928;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#26411;&#31471;&#38468;&#36817;&#65292;&#21487;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#35774;&#35745;&#21644;&#35757;&#32451;&#39640;&#25928;&#32780;&#40065;&#26834;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;\url{https://github.com/softsys4ai/CL-Robustness}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
To advance the understanding of robust deep learning, we delve into the effects of adversarial training on self-supervised and supervised contrastive learning alongside supervised learning. Our analysis uncovers significant disparities between adversarial and clean representations in standard-trained networks across various learning algorithms. Remarkably, adversarial training mitigates these disparities and fosters the convergence of representations toward a universal set, regardless of the learning scheme used. Additionally, increasing the similarity between adversarial and clean representations, particularly near the end of the network, enhances network robustness. These findings offer valuable insights for designing and training effective and robust deep learning networks. Our code is released at \textcolor{magenta}{\url{https://github.com/softsys4ai/CL-Robustness}}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36870;&#21521;&#21487;&#36776;&#35782;&#21452;&#23556;&#22240;&#26524;&#27169;&#22411;&#65292;&#30830;&#31435;&#20102;&#20854;&#22312;&#19977;&#31181;&#24120;&#35265;&#22240;&#26524;&#32467;&#26500;&#19979;&#30340;&#36870;&#21521;&#21487;&#36776;&#35782;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#30340;&#36870;&#21521;&#39044;&#27979;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.02228</link><description>&lt;p&gt;
&#36870;&#21521;&#21487;&#36776;&#35782;&#21452;&#23556;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Identifiability of Bijective Causal Models. (arXiv:2302.02228v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36870;&#21521;&#21487;&#36776;&#35782;&#21452;&#23556;&#22240;&#26524;&#27169;&#22411;&#65292;&#30830;&#31435;&#20102;&#20854;&#22312;&#19977;&#31181;&#24120;&#35265;&#22240;&#26524;&#32467;&#26500;&#19979;&#30340;&#36870;&#21521;&#21487;&#36776;&#35782;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#30340;&#36870;&#21521;&#39044;&#27979;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;&#21452;&#23556;&#29983;&#25104;&#26426;&#21046;&#65288;BGM&#65289;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#36870;&#21521;&#21487;&#36776;&#35782;&#24615;&#65292;&#20854;&#20013;BGM&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#20110;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30830;&#31435;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#20855;&#26377;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#22240;&#26524;&#32467;&#26500;&#30340;&#36870;&#21521;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;BGM&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#29983;&#25104;&#24314;&#27169;&#12290;&#23398;&#20064;&#21040;&#30340;BGM&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#36870;&#21521;&#39044;&#27979;&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#35273;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#35270;&#39057;&#27969;&#23186;&#20307;&#20223;&#30495;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#25910;&#25947;&#30340;&#20445;&#35777;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01633</link><description>&lt;p&gt;
&#24322;&#26500;&#25968;&#25454;&#19978;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Sequencial Split Learning on Heterogeneous Data. (arXiv:2302.01633v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;&#20998;&#21106;&#23398;&#20064;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#25910;&#25947;&#30340;&#20445;&#35777;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#20248;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#31181;&#27969;&#34892;&#33539;&#20363;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#21368;&#36733;&#21040;&#26381;&#21153;&#22120;&#65292;SL&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#28145;&#23618;&#27169;&#22411;&#35757;&#32451;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20173;&#32570;&#20047;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#39034;&#24207;SL&#65288;SSL&#65292;&#36827;&#34892;&#39034;&#24207;&#27169;&#22411;&#35757;&#32451;&#30340;SL&#22522;&#26412;&#24773;&#24418;&#65289;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#23545;&#20110;&#24378;&#21270;/&#19968;&#33324;/&#38750;&#20984;&#30446;&#26631;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#20445;&#35777;&#34920;&#26126;&#65292;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#65292;SSL&#27604;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65292;FL&#20013;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#65289;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#26497;&#31471;&#24322;&#26500;&#25968;&#25454;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20010;&#21453;&#30452;&#35273;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) and Split Learning (SL) are two popular paradigms of distributed machine learning. By offloading the computation-intensive portions to the server, SL is promising for deep model training on resource-constrained devices, yet still lacking of rigorous convergence analysis. In this paper, we derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL that conducts the model training in sequence) for strongly/general/non-convex objectives on heterogeneous data. Notably, the derived guarantees suggest that SSL is better than Federated Averaging (FedAvg, the most popular algorithm in FL) on heterogeneous data. We validate the counterintuitive analysis result empirically on extremely heterogeneous data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#20869;&#22312;&#29366;&#24577;&#30340;&#20108;&#20803;&#32452;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#21152;&#36895;&#23545;&#27604;&#28023;&#27604;&#23398;&#20064;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#21333;&#20010;&#25512;&#26029;&#38454;&#27573;&#65292;&#21487;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2302.01228</link><description>&lt;p&gt;
&#21452;&#21521;&#20256;&#25773;&#65306;&#29992;&#20108;&#20803;&#31070;&#32463;&#20803;&#21152;&#36895;&#23545;&#27604;&#28023;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic Neurons. (arXiv:2302.01228v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#20869;&#22312;&#29366;&#24577;&#30340;&#20108;&#20803;&#32452;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#21152;&#36895;&#23545;&#27604;&#28023;&#27604;&#23398;&#20064;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#21333;&#20010;&#25512;&#26029;&#38454;&#27573;&#65292;&#21487;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27963;&#21160;&#24046;&#24322;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#23545;&#27604;&#28023;&#27604;&#23398;&#20064;&#21644;&#24179;&#34913;&#20256;&#25773;&#65292;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#29983;&#29289;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#26367;&#20195;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#30340;&#25968;&#23383;&#33455;&#29255;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#26114;&#36149;&#30340;&#25512;&#26029;&#38382;&#39064;&#20004;&#27425;&#65292;&#20351;&#36825;&#20123;&#26041;&#27861;&#27604;&#21453;&#21521;&#20256;&#25773;&#24930;&#20004;&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#12290;&#22312;&#27169;&#25311;&#24773;&#20917;&#19979;&#65292;&#24179;&#34913;&#20256;&#25773;&#21487;&#33021;&#26159;&#24555;&#36895;&#21644;&#33021;&#37327;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#26159;&#29366;&#24577;&#20173;&#28982;&#38656;&#35201;&#25512;&#26029;&#21644;&#23384;&#20648;&#20004;&#27425;&#12290;&#21463;&#25552;&#21319;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23460;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#23460;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#31216;&#20026;&#21452;&#21521;&#20256;&#25773;&#65292;&#20854;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#20869;&#22312;&#29366;&#24577;&#32452;&#25104;&#30340;&#20108;&#20803;&#32452;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#36825;&#20123;&#20869;&#22312;&#29366;&#24577;&#36890;&#36807;&#23427;&#20204;&#30340;&#24046;&#24322;&#21644;&#24179;&#22343;&#20540;&#32534;&#30721;&#35823;&#24046;/&#27963;&#21160;&#30340;&#20108;&#20803;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#26159;&#21482;&#38656;&#35201;&#21333;&#20010;&#25512;&#26029;&#38454;&#27573;&#65292;&#24182;&#19988;&#25512;&#26029;&#21487;&#20197;&#30001;&#31070;&#32463;&#20803;&#26412;&#36523;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activity difference based learning algorithms-such as contrastive Hebbian learning and equilibrium propagation-have been proposed as biologically plausible alternatives to error back-propagation. However, on traditional digital chips these algorithms suffer from having to solve a costly inference problem twice, making these approaches more than two orders of magnitude slower than back-propagation. In the analog realm equilibrium propagation may be promising for fast and energy efficient learning, but states still need to be inferred and stored twice. Inspired by lifted neural networks and compartmental neuron models we propose a simple energy based compartmental neuron model, termed dual propagation, in which each neuron is a dyad with two intrinsic states. At inference time these intrinsic states encode the error/activity duality through their difference and their mean respectively. The advantage of this method is that only a single inference phase is needed and that inference can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;&#65292;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.00860</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interventional and Counterfactual Inference with Diffusion Models. (arXiv:2302.00860v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;&#65292;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#22240;&#26524;&#20805;&#20998;&#35774;&#32622;&#20013;&#22238;&#31572;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#26469;&#23398;&#20064;&#29983;&#25104;&#29420;&#29305;&#30340;&#28508;&#22312;&#32534;&#30721;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#36825;&#20123;&#32534;&#30721;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24178;&#39044;&#19979;&#30452;&#25509;&#37319;&#26679;&#21644;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#37324;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#33410;&#28857;&#32534;&#30721;&#20026;&#19968;&#20010;&#20195;&#34920;&#22806;&#29983;&#22122;&#22768;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22238;&#31572;&#22240;&#26524;&#26597;&#35810;&#26041;&#38754;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#20026;&#20998;&#26512;&#19968;&#33324;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#65292;&#36825;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#22806;&#30340;&#35774;&#32622;&#21487;&#33021;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings. These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2302.00141</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; Bellman Errors &#29992;&#20110;&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270; Bellman Errors&#65292;&#21457;&#29616;&#20043;&#21069;&#30340;Bellman Errors &#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340; MSBE &#20272;&#35745;&#22120;&#65292;&#22312;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#36873;&#25321;&#65288;OMS&#65289;&#21363;&#22312;&#21482;&#26377;&#24050;&#35760;&#24405;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20174;&#20247;&#22810;&#31574;&#30053;&#20013;&#36873;&#25321;&#26368;&#20339;&#31574;&#30053;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#24212;&#29992;&#31163;&#32447;RL&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#32463;&#36807;&#24191;&#27867;&#25506;&#35752;&#30340;&#24819;&#27861;&#26159;&#26681;&#25454;&#30456;&#20851;Q&#20989;&#25968;&#30340;&#22343;&#26041;Bellman&#35823;&#24046;&#65288;MSBE&#65289;&#36873;&#25321;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#30340;OMS&#24615;&#33021;&#65292;&#23548;&#33268;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25918;&#24323;&#27492;&#24819;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#20043;&#21069;&#30340;&#32467;&#26524;&#20351;&#29992;Bellman&#35823;&#24046;&#26102;&#20250;&#30475;&#21040;&#24754;&#35266;&#30340;&#32467;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#22522;&#20110;Bellman&#35823;&#24046;&#30340;OMS&#31639;&#27861;&#23558;&#34920;&#29616;&#33391;&#22909;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;MSBE&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#19981;&#21516;&#30340;&#31163;&#25955;&#25511;&#21046;&#20219;&#21153;&#65288;&#21253;&#25324; Atari &#28216;&#25103;&#65289;&#19978;&#33719;&#24471;&#20102;&#20986;&#33394;&#30340;OMS&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377; MCMC &#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#24555;&#36895;&#29256;&#26412;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#23383;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#20840;&#38754;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.13778</link><description>&lt;p&gt;
&#24102;&#26377;MCMC&#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Bayesian Linear Regression with MCMC. (arXiv:2301.13778v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377; MCMC &#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#24555;&#36895;&#29256;&#26412;&#65292;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#23383;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#20840;&#38754;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#25317;&#26377;&#37096;&#20998;&#25968;&#25454;&#24182;&#20998;&#20139;&#20854;&#37096;&#20998;&#30340;&#26576;&#20123;&#24635;&#32467;&#32479;&#35745;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#31169;&#19979;&#20849;&#20139;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#32447;&#24615;&#22238;&#24402;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#20043;&#38388;&#30340;&#26377;&#29992;&#20998;&#24067;&#20851;&#31995;&#12290;&#22238;&#24402;&#31995;&#25968;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#20027;&#35201;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#36827;&#34892;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#29256;&#26412;&#65292;&#20197;&#22312;&#19968;&#27425;&#36845;&#20195;&#20013;&#25191;&#34892;&#36125;&#21494;&#26031;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#25552;&#20379;&#20102;&#25968;&#23383;&#23454;&#39564;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#20840;&#38754;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Bayesian inference framework for distributed differentially private linear regression. We consider a distributed setting where multiple parties hold parts of the data and share certain summary statistics of their portions in privacy-preserving noise. We develop a novel generative statistical model for privately shared statistics, which exploits a useful distributional relation between the summary statistics of linear regression. Bayesian estimation of the regression coefficients is conducted mainly using Markov chain Monte Carlo algorithms, while we also provide a fast version to perform Bayesian estimation in one iteration. The proposed methods have computational advantages over their competitors. We provide numerical results on both real and simulated data, which demonstrate that the proposed algorithms provide well-rounded estimation and prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25214;&#21040;&#26368;&#20248;&#21270;&#30340;&#30123;&#24773;&#35745;&#21010;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2301.12802</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35268;&#21010;&#22810;&#31181;&#20256;&#26579;&#30149;&#24178;&#39044;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Planning Multiple Epidemic Interventions with Reinforcement Learning. (arXiv:2301.12802v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25214;&#21040;&#26368;&#20248;&#21270;&#30340;&#30123;&#24773;&#35745;&#21010;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27969;&#34892;&#30149;&#38656;&#35201;&#21046;&#23450;&#35745;&#21010;&#65292;&#25551;&#36848;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#19981;&#21516;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#35201;&#27714;&#20329;&#25140;&#21475;&#32617;&#12289;&#25509;&#31181;&#30123;&#33495;&#12289;&#20851;&#38381;&#23398;&#26657;&#25110;&#24037;&#20316;&#22330;&#25152;&#31561;&#12290;&#26368;&#20248;&#30340;&#35745;&#21010;&#23558;&#20197;&#26368;&#23567;&#30340;&#29983;&#21629;&#25439;&#22833;&#12289;&#30142;&#30149;&#36127;&#25285;&#21644;&#32463;&#27982;&#25104;&#26412;&#36943;&#21046;&#30123;&#24773;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#23547;&#25214;&#26368;&#20248;&#35745;&#21010;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#34920;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#31034;&#23545;&#20110;&#20219;&#20309;&#30001;&#24120;&#24494;&#20998;&#26041;&#31243;&#23450;&#20041;&#30340;&#30142;&#30149;&#27169;&#22411;&#19978;&#30340;&#22810;&#20010;&#36830;&#32493;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#21644;SAC&#65289;&#65292;&#20197;&#22312;&#36830;&#32493;&#21644;&#22797;&#26434;&#30340;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#26368;&#23567;&#21270;&#30142;&#30149;&#21644;&#32463;&#27982;&#25104;&#26412;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating an epidemic entails finding a plan that describes when and how to apply different interventions, such as mask-wearing mandates, vaccinations, school or workplace closures. An optimal plan will curb an epidemic with minimal loss of life, disease burden, and economic cost. Finding an optimal plan is an intractable computational problem in realistic settings. Policy-makers, however, would greatly benefit from tools that can efficiently search for plans that minimize disease and economic costs especially when considering multiple possible interventions over a continuous and complex action space given a continuous and equally complex state space. We formulate this problem as a Markov decision process. Our formulation is unique in its ability to represent multiple continuous interventions over any disease model defined by ordinary differential equations. We illustrate how to effectively apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and SAC) to search fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;Genie&#65292;&#21487;&#20197;&#20351;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21644;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#26032;&#39062;&#12289;&#21487;&#35774;&#35745;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#26032;&#30340;&#34507;&#30333;&#36136;&#27835;&#30103;&#21644;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2301.12485</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#23450;&#21521;&#27688;&#22522;&#37240;&#20113;&#29983;&#25104;&#26032;&#39062;&#12289;&#21487;&#35774;&#35745;&#21644;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds. (arXiv:2301.12485v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;Genie&#65292;&#21487;&#20197;&#20351;&#29992;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21644;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#26032;&#39062;&#12289;&#21487;&#35774;&#35745;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#26032;&#30340;&#34507;&#30333;&#36136;&#27835;&#30103;&#21644;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22312;&#32454;&#32990;&#20013;&#21457;&#25381;&#30528;&#21508;&#31181;&#21151;&#33021;&#36807;&#31243;&#12290;&#21019;&#36896;&#20855;&#26377;&#35774;&#35745;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#26032;&#34507;&#30333;&#36136;&#23558;&#26377;&#21161;&#20110;&#24037;&#31243;&#21270;&#32454;&#32990;&#34892;&#20026;&#21644;&#24320;&#21457;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#27835;&#30103;&#21644;&#26448;&#26009;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#35774;&#35745;&#26088;&#22312;&#23547;&#25214;&#21487;&#35774;&#35745;&#65288;&#21487;&#20197;&#30001;&#34507;&#30333;&#36136;&#24207;&#21015;&#23454;&#29616;&#30340;&#32467;&#26500;&#65289;&#65292;&#26032;&#39062;&#65288;&#19982;&#22825;&#28982;&#34507;&#30333;&#36136;&#26377;&#19981;&#21516;&#30340;&#20960;&#20309;&#24418;&#29366;&#65289;&#21644;&#22810;&#26679;&#21270;&#65288;&#28085;&#30422;&#24191;&#27867;&#20960;&#20309;&#24418;&#29366;&#33539;&#22260;&#65289;&#30340;&#32467;&#26500;&#12290;&#23613;&#31649;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#30340;&#36827;&#23637;&#20351;&#24471;&#39044;&#27979;&#26032;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#32467;&#26500;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#32452;&#21512;&#31354;&#38388;&#38480;&#21046;&#20102;&#25628;&#32034;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#21547;&#22320;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22122;&#22768;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;Genie&#30340;&#34507;&#30333;&#36136;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of prote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;</title><link>http://arxiv.org/abs/2301.12366</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;&#38750;&#24179;&#31283;&#36830;&#32493;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#20915;&#31574;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#37117;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#33021;&#22815;&#22788;&#29702;&#21464;&#21270;&#30340;&#36172;&#21338;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#20026;&#20102;&#20445;&#25252;&#38750;&#24179;&#28369;&#21464;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#20165;&#21463;&#21040;&#24635;&#21464;&#24046;&#25110;&#26102;&#38388;&#19978;&#30340;Lipschitz&#24615;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#23427;&#20204;&#20445;&#35777;$\tilde \Theta(T^{2/3})$&#30340;&#36951;&#25022;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29615;&#22659;&#32463;&#24120;&#20197;&#24179;&#31283;&#30340;&#26041;&#24335;&#25913;&#21464;&#65292;&#22240;&#27492;&#36825;&#31181;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#20135;&#29983;&#27604;&#24517;&#35201;&#26356;&#39640;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#21464;&#21270;&#29575;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20551;&#35774;&#33218;&#30340;&#24179;&#22343;&#22238;&#25253;&#26159;&#19968;&#20010;$\beta$-H\''older&#20989;&#25968;&#65292;&#21363;&#23427;&#26159;$(\beta-1)$&#27425;Lipschitz&#36830;&#32493;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23545;&#20110;$\beta=2$&#65292;&#23427;&#30340;&#36951;&#25022;&#20026;$\tilde O(T^{3/5})$&#65292;&#20174;&#32780;&#39318;&#27425;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20043;&#38388;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20219;&#24847;$\Omg(T^{(\beta+1)/(2\beta+1)})$&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#36825;&#20010;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.11355</link><description>&lt;p&gt;
&#29992;&#20110;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#30340;&#21018;&#20307;&#27969;
&lt;/p&gt;
&lt;p&gt;
Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;(NF)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#39640;&#24230;&#28789;&#27963;&#21644;&#34920;&#29616;&#21147;&#65292;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#65292;&#20363;&#22914;&#26230;&#20307;&#20013;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;:&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#65292;&#20174;&#32780;&#21487;&#20197;&#25429;&#25417;&#21018;&#20307;&#30340;&#36830;&#32493;&#26059;&#36716;&#36816;&#21160;;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#20301;&#22235;&#20803;&#25968;&#30340;&#21452;&#35206;&#30422;&#29305;&#24615;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#23450;&#20041;&#19968;&#20010;&#36866;&#24403;&#30340;&#23494;&#24230;&#12290;&#36825;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#25110;&#22522;&#20110;&#28909;&#21147;&#23398;&#30446;&#26631;&#23494;&#24230;&#30340;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#20998;&#23376;&#31034;&#20363;&#30340;Boltzmann&#29983;&#25104;&#22120;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#21363;&#22235;&#38754;&#20307;&#31995;&#32479;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#32593;&#26684;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21019;&#26032;&#22320;&#23558;&#27010;&#29575;&#27979;&#24230;&#36171;&#20104;&#31354;&#38388;&#22495;&#65292;&#24418;&#25104;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25968;&#25454;&#21463;&#22122;&#22768;&#24178;&#25200;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11040</link><description>&lt;p&gt;
&#38754;&#21521;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38543;&#26426;&#32593;&#26684;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Random Grid Neural Processes for Parametric Partial Differential Equations. (arXiv:2301.11040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#32593;&#26684;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21019;&#26032;&#22320;&#23558;&#27010;&#29575;&#27979;&#24230;&#36171;&#20104;&#31354;&#38388;&#22495;&#65292;&#24418;&#25104;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25968;&#25454;&#21463;&#22122;&#22768;&#24178;&#25200;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#26032;&#30340;&#22522;&#20110;&#21487;&#25193;&#23637;&#21464;&#20998;&#31070;&#32463;&#36807;&#31243;&#30340;&#20855;&#26377;&#31354;&#38388;&#38543;&#26426;&#29289;&#29702;&#23398;&#21644;&#25968;&#25454;&#20449;&#24687;&#30340;&#28145;&#24230;&#28508;&#22312;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27010;&#29575;&#27979;&#24230;&#36171;&#20104;&#31354;&#38388;&#22495;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#37197;&#28857;&#32593;&#26684;&#35270;&#20026;&#38543;&#26426;&#21464;&#37327;&#26469;&#36793;&#38469;&#21270;&#12290;&#36890;&#36807;&#36866;&#24212;&#36825;&#31181;&#31354;&#38388;&#32479;&#35745;&#35270;&#22270;&#65292;&#25105;&#20204;&#20197;&#20135;&#29983;&#35299;&#22330;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20316;&#20026;&#32467;&#26524;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#36825;&#20123;&#38543;&#26426;&#32593;&#26684;&#30340;&#23454;&#29616;&#20026;&#21453;&#21521;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid Invariant Convolutional Networks(GICNets)&#30340;&#26032;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#23558;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#32435;&#20837;&#25105;&#20204;&#30340;&#29289;&#29702;&#30693;&#35782;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#36827;&#23545;&#19968;&#20123;&#21487;&#33021;&#26377;&#21487;&#29992;&#25968;&#25454;&#20294;&#27979;&#37327;&#32467;&#26524;&#34987;&#22122;&#22768;&#27745;&#26579;&#30340;&#38382;&#39064;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of spatially stochastic physics and data informed deep latent models for parametric partial differential equations (PDEs) which operate through scalable variational neural processes. We achieve this by assigning probability measures to the spatial domain, which allows us to treat collocation grids probabilistically as random variables to be marginalised out. Adapting this spatial statistics view, we solve forward and inverse problems for parametric PDEs in a way that leads to the construction of Gaussian process models of solution fields. The implementation of these random grids poses a unique set of challenges for inverse physics informed deep learning frameworks and we propose a new architecture called Grid Invariant Convolutional Networks (GICNets) to overcome these challenges. We further show how to incorporate noisy data in a principled manner into our physics informed model to improve predictions for problems where data may be available but whose measurem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#22312;&#28065;&#27969;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#35774;&#35745;&#26469;&#25104;&#21151;&#37325;&#24314;&#27969;&#22330;&#12290;&#35813;&#30740;&#31350;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#20540;&#21644;&#23454;&#39564;&#27969;&#21160;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2301.10937</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#65306;&#27969;&#20307;&#27969;&#21160;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Super-Resolution Analysis via Machine Learning: A Survey for Fluid Flows. (arXiv:2301.10937v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#22312;&#28065;&#27969;&#27969;&#21160;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#35774;&#35745;&#26469;&#25104;&#21151;&#37325;&#24314;&#27969;&#22330;&#12290;&#35813;&#30740;&#31350;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#20540;&#21644;&#23454;&#39564;&#27969;&#21160;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#26500;&#29992;&#20110;&#28065;&#27969;&#27969;&#21160;&#30340;&#30740;&#31350;&#12290;&#36229;&#20998;&#36776;&#29575;&#26088;&#22312;&#20174;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#20013;&#33719;&#21462;&#39640;&#20998;&#36776;&#29575;&#30340;&#27969;&#22330;&#65292;&#36890;&#24120;&#29992;&#20110;&#22270;&#20687;&#37325;&#26500;&#12290;&#38500;&#20102;&#35843;&#30740;&#21508;&#31181;&#26368;&#26032;&#30340;&#36229;&#20998;&#36776;&#24212;&#29992;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#32500;&#34928;&#20943;&#21516;&#24615;&#28237;&#27969;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#35774;&#35745;&#20351;&#24471;&#20174;&#31354;&#38388;&#21463;&#38480;&#27979;&#37327;&#20013;&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#28065;&#27969;&#27969;&#22330;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#22312;&#27969;&#20307;&#27969;&#21160;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#23637;&#26395;&#12290;&#26412;&#30740;&#31350;&#24471;&#20986;&#30340;&#35265;&#35299;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#20540;&#21644;&#23454;&#39564;&#27969;&#21160;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys machine-learning-based super-resolution reconstruction for vortical flows. Super resolution aims to find the high-resolution flow fields from low-resolution data and is generally an approach used in image reconstruction. In addition to surveying a variety of recent super-resolution applications, we provide case studies of super-resolution analysis for an example of two-dimensional decaying isotropic turbulence. We demonstrate that physics-inspired model designs enable successful reconstruction of vortical flows from spatially limited measurements. We also discuss the challenges and outlooks of machine-learning-based super-resolution analysis for fluid flow applications. The insights gained from this study can be leveraged for super-resolution analysis of numerical and experimental flow data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#25552;&#21319;&#30340;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#25216;&#26415;&#20197;&#21450;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08951</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#20013;&#24515;&#34920;&#24449;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#30340;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#35270;&#39057;&#20998;&#35299;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#25552;&#21319;&#30340;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#25216;&#26415;&#20197;&#21450;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#20174;&#22810;&#20010;&#35270;&#35282;&#24863;&#30693;&#19990;&#30028;&#26102;&#65292;&#21363;&#20351;&#26576;&#20010;&#23545;&#35937;&#34987;&#23436;&#20840;&#36974;&#25377;&#65292;&#20063;&#33021;&#20197;&#32452;&#21512;&#26041;&#24335;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35266;&#23519;&#22810;&#20010;&#35270;&#35282;&#20043;&#21518;&#24819;&#35937;&#26032;&#35270;&#35282;&#12290;&#26368;&#36817;&#22810;&#35270;&#22270;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;&#30340;&#26174;&#30528;&#36827;&#27493;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;1&#65289;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#30340;&#24418;&#29366;&#26080;&#27861;&#34987;&#20934;&#30830;&#37325;&#24314;&#12290;2&#65289;&#26032;&#35270;&#35282;&#39044;&#27979;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#35270;&#35282;&#27880;&#37322;&#65292;&#32780;&#19981;&#26159;&#35270;&#22270;&#34920;&#24449;&#20013;&#30340;&#38544;&#24335;&#35268;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#30340;&#26102;&#38388;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#20026;&#20102;&#20934;&#30830;&#37325;&#24314;&#23545;&#35937;&#30340;&#23436;&#25972;&#24418;&#29366;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#23545;&#35937;&#21644;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#20043;&#38388;&#30340;&#35299;&#32806;&#65292;&#20854;&#20013;&#26102;&#38388;&#26465;&#20214;&#30340;&#35270;&#22270;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;Transformer&#19968;&#36215;&#32852;&#21512;&#25512;&#26029;&#65292;&#28982;&#21518;&#36755;&#20837;&#21040;Slot Attention Networks (SANs)&#30340;&#39034;&#24207;&#25193;&#23637;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#38544;&#24335;&#35270;&#35282;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#26174;&#24335;&#35270;&#35282;&#27880;&#37322;&#30340;&#38656;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37096;&#20998;&#21644;&#23436;&#20840;&#36974;&#25377;&#23545;&#35937;&#23436;&#25104;&#21644;&#26032;&#35270;&#35282;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When perceiving the world from multiple viewpoints, humans have the ability to reason about the complete objects in a compositional manner even when an object is completely occluded from certain viewpoints. Meanwhile, humans are able to imagine novel views after observing multiple viewpoints. Recent remarkable advances in multi-view object-centric learning still leaves some unresolved problems: 1) The shapes of partially or completely occluded objects can not be well reconstructed. 2) The novel viewpoint prediction depends on expensive viewpoint annotations rather than implicit rules in view representations. In this paper, we introduce a time-conditioned generative model for videos. To reconstruct the complete shape of an object accurately, we enhance the disentanglement between the latent representations of objects and views, where the latent representations of time-conditioned views are jointly inferred with a Transformer and then are input to a sequential extension of Slot Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.08839</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#26080;&#27861;&#22312;&#25805;&#20316;&#26399;&#38388;&#25345;&#32493;&#39564;&#35777;CNN&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20154;&#21592;&#21644;&#30417;&#31649;&#26426;&#26500;&#38590;&#20197;&#23545;&#20351;&#29992;CNN&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#37096;&#32626;&#33719;&#24471;&#20449;&#24515;&#12290;&#22312;&#25805;&#20316;&#26399;&#38388;&#65292;&#20102;&#35299;CNN&#30340;&#39044;&#27979;&#20309;&#26102;&#21487;&#20449;&#25110;&#21487;&#30097;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#26412;&#26041;&#27861;&#26159;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35780;&#20272;&#39044;&#27979;&#26159;&#21542;&#21487;&#20449;&#25110;&#21487;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#26469;&#33258;&#40657;&#30418;&#35745;&#31639;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#24456;&#38590;&#23558;&#21487;&#20449;&#24230;&#24402;&#22240;&#20110;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#36879;&#26126;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25552;&#20379;CNN&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
&lt;/p&gt;</description></item><item><title>Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.05062</link><description>&lt;p&gt;
Tracr: &#32534;&#35793;&#21464;&#21387;&#22120;&#27169;&#22411;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05062
&lt;/p&gt;
&lt;p&gt;
Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32534;&#35793;&#22120;Tracr&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#25191;&#34892;&#22810;&#27493;&#31639;&#27861;&#30340;&#21464;&#21387;&#22120;&#20013;&#30340;&#8220;&#21472;&#21152;&#8221;&#12290;&#27492;&#22806;&#65292;Tracr&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#30495;&#23454;&#22522;&#20934;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#21464;&#21387;&#22120;&#23398;&#20064;&#30340;&#8220;&#31243;&#24207;&#8221;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#35299;&#37322;&#26159;&#21542;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#26816;&#26597;&#21253;&#25324;&#35745;&#31639;&#20196;&#29260;&#39057;&#29575;&#12289;&#25490;&#24207;&#21644;&#25324;&#21495;&#26816;&#26597;&#22312;&#20869;&#30340;&#31243;&#24207;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;https://github.com/deepmind/tracr&#25552;&#20379;&#20102;Tracr&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.00437</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22604;&#38519;:&#20174;&#24179;&#34913;&#21040;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00437
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#35777;&#26126;&#23545;&#20110;&#22343;&#26041;&#35823;&#24046;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#31070;&#32463;&#22604;&#38519;&#30340;&#29305;&#24615;&#65292;&#21363;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#32780;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#39030;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22797;&#26434;&#31995;&#32479;&#22312;&#35757;&#32451;&#21040;&#25910;&#25947;&#26102;&#65292;&#23427;&#20204;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#32467;&#26500;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#23849;&#28291;&#20026;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#26159;&#31561;&#35282;&#32039;&#26694;&#26550;(simplex Equiangular Tight Frame)&#30340;&#39030;&#28857;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#22604;&#38519;(NC)&#12290;&#26368;&#36817;&#30340;&#35770;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#31616;&#21270;&#30340;&#8220;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#8221;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#20013;&#20986;&#29616;&#20102;$\mathcal{NC}$&#12290;&#22312;&#36825;&#20010;&#35821;&#22659;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24120;&#29992;&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#20132;&#21449;&#29109;(CE)&#25439;&#22833;&#19979;&#65292;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#20250;&#21457;&#29983;$\mathcal{NC}$&#29616;&#35937;&#65292;&#34920;&#26126;&#20840;&#23616;&#35299;&#22312;&#19981;&#21516;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;$\mathcal{NC}$&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#24490;&#29615;&#40065;&#26834;&#31574;&#30053;&#26799;&#24230;&#65288;DRPG&#65289;&#31639;&#27861;&#65292;&#26159;&#39318;&#20010;&#36890;&#29992;&#30340;RMDP&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35843;&#22320;&#20943;&#23569;&#36817;&#20284;&#35823;&#24046;&#26469;&#20445;&#35777;&#22312;&#34920;&#26684;RMDP&#20013;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10439</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#40065;&#26834;MDP&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient in Robust MDPs with Global Convergence Guarantee. (arXiv:2212.10439v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#24490;&#29615;&#40065;&#26834;&#31574;&#30053;&#26799;&#24230;&#65288;DRPG&#65289;&#31639;&#27861;&#65292;&#26159;&#39318;&#20010;&#36890;&#29992;&#30340;RMDP&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#35843;&#22320;&#20943;&#23569;&#36817;&#20284;&#35823;&#24046;&#26469;&#20445;&#35777;&#22312;&#34920;&#26684;RMDP&#20013;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#38752;&#31574;&#30053;&#20197;&#24212;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#35768;&#22810;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#21464;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;RMDP&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;RMDP&#23545;&#20110;&#22823;&#22411;&#23454;&#38469;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#24490;&#29615;&#40065;&#26834;&#31574;&#30053;&#26799;&#24230;&#65288;DRPG&#65289;&#65292;&#36825;&#26159;&#39318;&#20010;&#36890;&#29992;&#30340;RMDP&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#40065;&#26834;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30456;&#27604;&#65292;DRPG&#21333;&#35843;&#22320;&#20943;&#23569;&#36817;&#20284;&#35823;&#24046;&#65292;&#20197;&#20445;&#35777;&#22312;&#34920;&#26684;RMDP&#20013;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#36716;&#31227;&#26680;&#65292;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#35299;&#20915;&#20869;&#37096;&#24490;&#29615;&#40065;&#26834;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26032;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#24182;&#30830;&#35748;&#20102;&#23427;&#30340;&#20840;&#23616;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#20197;&#26377;&#25928;&#22320;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#19978;&#30340;&#27491;&#21453;&#21521;&#20256;&#25773;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.09567</link><description>&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization. (arXiv:2212.09567v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#20197;&#26377;&#25928;&#22320;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#36890;&#36807;&#26597;&#35810;&#35745;&#31639;&#26641;&#19978;&#30340;&#27491;&#21453;&#21521;&#20256;&#25773;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#23884;&#20837;&#24335;&#26041;&#27861;&#38656;&#35201;&#35757;&#32451;&#22797;&#26434;&#26597;&#35810;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20998;&#24067;&#26597;&#35810;&#32467;&#26500;&#20043;&#22806;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#31471;&#21040;&#31471;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#26694;&#26550;&#21270;&#65292;&#21482;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#26368;&#20248;&#35299;&#21482;&#33021;&#34987;&#36817;&#20284;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; QTO&#65288;&#26597;&#35810;&#35745;&#31639;&#26641;&#20248;&#21270;&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#31934;&#30830;&#30340;&#26368;&#20248;&#35299;&#12290;QTO&#36890;&#36807;&#22312;&#26641;&#29366;&#35745;&#31639;&#22270;&#65288;&#21363;&#26597;&#35810;&#35745;&#31639;&#26641;&#65289;&#19978;&#36827;&#34892;&#27491;&#21453;&#21521;&#20256;&#25773;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;QTO&#21033;&#29992;&#20102;&#26597;&#35810;&#35745;&#31639;&#26641;&#20013;&#32534;&#30721;&#30340;&#29420;&#31435;&#24615;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20165;&#28041;&#21450;&#23616;&#37096;&#35745;&#31639;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;QTO&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains sta
&lt;/p&gt;</description></item><item><title>&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#32479;&#35745;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#22806;&#25512;&#21306;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#21518;&#32493;&#23398;&#20064;&#30340;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.02457</link><description>&lt;p&gt;
&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#31069;&#31119;&#21644;&#35781;&#21650;&#65306;&#23545;&#25239;&#23398;&#20064;&#21160;&#24577;&#12289;&#26041;&#21521;&#25910;&#25947;&#21644;&#24179;&#34913;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics, Directional Convergence, and Equilibria. (arXiv:2212.02457v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02457
&lt;/p&gt;
&lt;p&gt;
&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#32479;&#35745;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#22806;&#25512;&#21306;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#21518;&#32493;&#23398;&#20064;&#30340;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21464;&#37327;&#20998;&#24067;&#36716;&#31227;&#21644;&#23545;&#25239;&#25200;&#21160;&#23545;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#27979;&#35797;&#21327;&#21464;&#37327;&#20998;&#24067;&#20013;&#30340;&#36731;&#24494;&#36716;&#31227;&#33021;&#26174;&#33879;&#24433;&#21709;&#22522;&#20110;&#35757;&#32451;&#20998;&#24067;&#23398;&#20064;&#30340;&#32479;&#35745;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#22806;&#25512;&#21457;&#29983;&#26102;&#65292;&#21363;&#21327;&#21464;&#37327;&#36716;&#31227;&#21040;&#35757;&#32451;&#20998;&#24067;&#31232;&#32570;&#30340;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#20250;&#38477;&#20302;&#65292;&#22240;&#27492;&#65292;&#23398;&#20064;&#27169;&#22411;&#20449;&#24687;&#24456;&#23569;&#12290;&#20026;&#20102;&#31283;&#20581;&#24615;&#21644;&#27491;&#21017;&#21270;&#32771;&#34385;&#65292;&#24314;&#35758;&#37319;&#29992;&#23545;&#25239;&#25200;&#21160;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#23545;&#32473;&#23450;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#22806;&#25512;&#21306;&#22495;&#36827;&#34892;&#20180;&#32454;&#30740;&#31350;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#30340;&#35774;&#32622;&#20013;&#31934;&#30830;&#21051;&#30011;&#20102;&#22806;&#25512;&#21306;&#22495;&#65292;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#20102;&#23545;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23545;&#38543;&#21518;&#30340;&#24179;&#34913;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate distribution shifts and adversarial perturbations present robustness challenges to the conventional statistical learning framework: mild shifts in the test covariate distribution can significantly affect the performance of the statistical model learned based on the training distribution. The model performance typically deteriorates when extrapolation happens: namely, covariates shift to a region where the training distribution is scarce, and naturally, the learned model has little information. For robustness and regularization considerations, adversarial perturbation techniques are proposed as a remedy; however, careful study needs to be carried out about what extrapolation region adversarial covariate shift will focus on, given a learned model. This paper precisely characterizes the extrapolation region, examining both regression and classification in an infinite-dimensional setting. We study the implications of adversarial covariate shifts to subsequent learning of the equi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SinDDM&#30340;&#21333;&#22270;&#20687;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21333;&#24352;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20219;&#24847;&#32500;&#24230;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#36890;&#36807;&#22806;&#37096;&#30417;&#30563;&#36827;&#34892;&#25351;&#23548;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26679;&#24335;&#36716;&#25442;&#21644;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2211.16582</link><description>&lt;p&gt;
SinDDM: &#19968;&#31181;&#21333;&#22270;&#20687;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SinDDM: A Single Image Denoising Diffusion Model. (arXiv:2211.16582v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SinDDM&#30340;&#21333;&#22270;&#20687;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21333;&#24352;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20219;&#24847;&#32500;&#24230;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#36890;&#36807;&#22806;&#37096;&#30417;&#30563;&#36827;&#34892;&#25351;&#23548;&#65292;&#21487;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26679;&#24335;&#36716;&#25442;&#21644;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#20462;&#22797;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;DDMs&#38656;&#35201;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#19978;&#35757;&#32451;DDM&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;SinDDM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#25193;&#25955;&#36807;&#31243;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#30340;&#20869;&#37096;&#32479;&#35745;&#20449;&#24687;&#12290;&#20026;&#20102;&#25512;&#21160;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#36731;&#37327;&#32423;&#21435;&#22122;&#22120;&#65292;&#23427;&#21463;&#22122;&#22768;&#27700;&#24179;&#21644;&#23610;&#24230;&#30340;&#35843;&#33410;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#20801;&#35768;&#20197;&#31895;&#30053;&#21040;&#32454;&#33410;&#30340;&#26041;&#24335;&#29983;&#25104;&#20219;&#24847;&#32500;&#24230;&#30340;&#26679;&#26412;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#35828;&#26126;&#30340;&#65292;SinDDM&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#22312;&#21253;&#25324;&#26679;&#24335;&#36716;&#25442;&#21644;&#35856;&#35843;&#22312;&#20869;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#21463;&#21040;&#22806;&#37096;&#30417;&#30563;&#30340;&#25351;&#23548;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#20174;&#21333;&#24352;&#22270;&#20687;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#33073;&#32806;&#34920;&#31034;&#19982;&#31232;&#30095;&#22522;&#39044;&#27979;&#22120;&#30456;&#32467;&#21512;&#21487;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#26041;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.14666</link><description>&lt;p&gt;
&#33073;&#32806;&#34920;&#31034;&#19982;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65306;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning. (arXiv:2211.14666v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#33073;&#32806;&#34920;&#31034;&#19982;&#31232;&#30095;&#22522;&#39044;&#27979;&#22120;&#30456;&#32467;&#21512;&#21487;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#26041;&#27861;&#26469;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33073;&#32806;&#34920;&#31034;&#32463;&#24120;&#34987;&#35748;&#20026;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#65292;&#20294;&#30446;&#21069;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#19982;&#31232;&#30095;&#22522;&#39044;&#27979;&#22120;&#30456;&#32467;&#21512;&#30340;&#33073;&#32806;&#34920;&#31034;&#21487;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#25552;&#20379;&#20102;&#26368;&#22823;&#31232;&#30095;&#22522;&#39044;&#27979;&#22120;&#20135;&#29983;&#33073;&#32806;&#34920;&#31034;&#30340;&#26465;&#20214;&#12290;&#21463;&#36825;&#20010;&#29702;&#35770;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#20419;&#36827;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#23398;&#20064;&#33073;&#32806;&#34920;&#31034;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#22522;&#20110;&#32452; Lasso &#22810;&#31867; SVM &#22522;&#39044;&#27979;&#22120;&#30340;&#20803;&#23398;&#20064;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#23545;&#20598;&#20844;&#24335;&#12290;&#23427;&#22312;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#32780;&#27599;&#20010;&#20219;&#21153;&#20165;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#30450;&#30446;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#23548;&#33268;&#29123;&#23613;&#12289;&#27969;&#22833;&#21644;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2211.13585</link><description>&lt;p&gt;
&#23398;&#20064;&#24314;&#35758;&#20241;&#24687;&#65306;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement. (arXiv:2211.13585v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25345;&#32493;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#30450;&#30446;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#23548;&#33268;&#29123;&#23613;&#12289;&#27969;&#22833;&#21644;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#29992;&#25143;&#21442;&#19982;&#24230;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#65292;&#20294;&#26159;&#30450;&#30446;&#22320;&#25512;&#21160;&#29992;&#25143;&#22686;&#21152;&#28040;&#36153;&#20250;&#22686;&#21152;&#29123;&#23613;&#12289;&#27969;&#22833;&#29978;&#33267;&#25104;&#30270;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#20419;&#36827;&#25968;&#23383;&#31119;&#21033;&#65292;&#29616;&#22312;&#22823;&#22810;&#25968;&#24179;&#21488;&#37117;&#25552;&#20379;&#23450;&#26399;&#25552;&#31034;&#29992;&#25143;&#20241;&#24687;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20123;&#26381;&#21153;&#24517;&#39035;&#25163;&#21160;&#35774;&#32622;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#29992;&#25143;&#21644;&#31995;&#32479;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20241;&#24687;&#22312;&#25512;&#33616;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20419;&#36827;&#21644;&#32500;&#25345;&#38271;&#26399;&#21442;&#19982;&#30340;&#26368;&#20248;&#20241;&#24687;&#31574;&#30053;&#12290;&#22522;&#20110;&#25512;&#33616;&#21160;&#24577;&#26131;&#21463;&#21040;&#31215;&#26497;&#21644;&#28040;&#26497;&#21453;&#39304;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#34920;&#31034;&#20026;&#19968;&#20010; Lotka-Volterra &#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#20013;&#20241;&#24687;&#20943;&#23569;&#20026;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#26469;&#23398;&#20064;&#19982;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#65292;&#23454;&#39564;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.02918</link><description>&lt;p&gt;
&#22522;&#20110;&#31579;&#36873;&#30340;&#19968;&#33324;&#26041;&#27861;&#26469;&#23398;&#20064;&#35748;&#30693;&#22270;&#30340;&#21512;&#29702;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Filtering-based General Approach to Learning Rational Constraints of Epistemic Graphs. (arXiv:2211.02918v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#26469;&#23398;&#20064;&#19982;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#65292;&#23454;&#39564;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#22270;&#26159;&#27010;&#29575;&#35770;&#35777;&#30340;&#35748;&#30693;&#26041;&#27861;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;Hunter&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#27425;&#25512;&#24191;&#26694;&#26550;&#26469;&#20174;&#20247;&#21253;&#25968;&#25454;&#20013;&#23398;&#20064;&#35748;&#30693;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#35748;&#30693;&#32422;&#26463;&#21482;&#21453;&#26144;&#20102;&#29992;&#25143;&#20174;&#25968;&#25454;&#20013;&#30340;&#20449;&#24565;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#35748;&#30693;&#22270;&#20013;&#32534;&#30721;&#30340;&#21512;&#29702;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#21069;&#26694;&#26550;&#21482;&#33021;&#29983;&#25104;&#21453;&#26144;&#20195;&#29702;&#20154;&#20449;&#8203;&#8203;&#20219;&#31243;&#24230;&#32780;&#19981;&#26159;&#26159;&#21542;&#30456;&#20449;&#19968;&#20010;&#35770;&#28857;&#30340;&#35748;&#30693;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31579;&#36873;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#37325;&#25512;&#24191;&#27493;&#39588;&#20174;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#19968;&#32452;&#19982;&#20854;&#35748;&#30693;&#22270;&#19968;&#33268;&#30340;&#21512;&#29702;&#35268;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21512;&#29702;&#35268;&#21017;&#65292;&#20197;&#21453;&#26144;&#35748;&#30693;&#22270;&#20013;&#30340;&#21512;&#29702;&#24615;&#32771;&#34385;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21512;&#29702;&#35268;&#21017;&#30340;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epistemic graphs are a generalization of the epistemic approach to probabilistic argumentation. Hunter proposed a 2-way generalization framework to learn epistemic constraints from crowd-sourcing data. However, the learnt epistemic constraints only reflect users' beliefs from data, without considering the rationality encoded in epistemic graphs. Meanwhile, the current framework can only generate epistemic constraints that reflect whether an agent believes an argument, but not the degree to which it believes in it. The major challenge to achieving this effect is that the computational complexity will increase sharply when expanding the variety of constraints, which may lead to unacceptable time performance. To address these problems, we propose a filtering-based approach using a multiple-way generalization step to generate a set of rational rules which are consistent with their epistemic graphs from a dataset. This approach is able to learn a wider variety of rational rules that reflect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.00181</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#29699;&#30340;&#23481;&#37327;&#38543;&#21322;&#24452;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#33021;&#22815;&#23558;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#23884;&#20837;&#20854;&#20013;&#32780;&#19981;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#25968;&#22686;&#38271;&#30340;&#24615;&#36136;&#24120;&#24120;&#23548;&#33268;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#36229;&#20960;&#20309;&#23398;&#20064;&#27169;&#22411;&#26377;&#26102;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;NaN&#38382;&#39064;&#21644;&#28014;&#28857;&#31639;&#26415;&#20013;&#36935;&#21040;&#26080;&#27861;&#34920;&#31034;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;&#8212;&#8212;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#65292;&#22312;64&#20301;&#31639;&#26415;&#31995;&#32479;&#19979;&#65292;Poincar\'e&#29699;&#30456;&#23545;&#20110;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#33021;&#21147;&#26469;&#27491;&#30830;&#34920;&#31034;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;Lorentz&#27169;&#22411;&#20248;&#20110;Poincar\'e&#29699;&#30340;&#20248;&#36234;&#24615;&#12290;&#37492;&#20110;&#20004;&#31181;&#27169;&#22411;&#30340;&#25968;&#20540;&#38480;&#21046;&#65292;&#25105;&#20204;&#30830;&#23450;&#19968;&#31181;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#20043;&#22806;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2210.15304</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#21518;&#65292;GNN&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#65292;&#36825;&#20419;&#20351;&#38656;&#35201;&#26041;&#27861;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;GNN&#35299;&#37322;&#22120;&#24320;&#22987;&#20986;&#29616;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#65292;&#19968;&#20123;&#26159;&#26032;&#39062;&#30340;&#65292;&#19968;&#20123;&#26159;&#20174;&#20854;&#20182;&#39046;&#22495;&#25913;&#32534;&#32780;&#26469;&#30340;&#12290;&#20026;&#20102;&#25972;&#29702;&#36825;&#31181;&#28023;&#37327;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#26041;&#38754;&#23545;&#19981;&#21516;&#30340;&#35299;&#37322;&#22120;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#24037;&#20316;&#27809;&#26377;&#23581;&#35797;&#25552;&#20379;&#20851;&#20110;&#19981;&#21516;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#26356;&#25110;&#19981;&#26131;&#35299;&#37322;&#30340;&#27934;&#23519;&#65292;&#20063;&#27809;&#26377;&#35828;&#26126;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#35299;&#37322;&#22120;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#31995;&#32479;&#24615;&#23454;&#39564;&#30740;&#31350;&#65292;&#23545;&#20843;&#20010;&#20195;&#34920;&#24615;&#20307;&#31995;&#32467;&#26500;&#19978;&#35757;&#32451;&#30340;&#21313;&#31181;&#35299;&#37322;&#22120;&#22312;&#20845;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22635;&#34917;&#20102;&#36825;&#20123;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#26041;&#27861;GCBS&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.12874</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25490;&#21015;&#20248;&#21270;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#26041;&#27861;GCBS&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25366;&#25496;&#30340;&#30828;&#36127;&#20363;&#26469;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#25209;&#22788;&#29702;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#21152;&#20102;&#19982;&#25366;&#25496;&#36127;&#20363;&#25968;&#25104;&#27604;&#20363;&#30340;&#32426;&#20803;&#38271;&#24230;&#65292;&#24182;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;&#26368;&#36817;&#37051;&#23621;&#32034;&#24341;&#25110;&#20174;&#26368;&#36817;&#30340;&#25209;&#27425;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#26367;&#20195;&#26041;&#26696;&#65306;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#65288;GCBS&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#36817;&#20284;&#25209;&#22788;&#29702;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#19978;&#30028;&#20102;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#36317;$\mathcal{L}^{Global} - \mathcal{L}^{Train}$&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;GCBS&#25913;&#21892;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#20195;&#30721;&#25628;&#32034;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;GCBS&#26131;&#20110;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23569;&#37327;&#38468;&#21152;&#20195;&#30721;&#65292;&#19981;&#38656;&#35201;&#32500;&#25252;&#22806;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#22914;&#26368;&#36817;&#37051;&#23621;&#32034;&#24341;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbo
&lt;/p&gt;</description></item><item><title>AnalogVNN&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#20809;&#30005;&#22122;&#22768;&#12289;&#26377;&#38480;&#31934;&#24230;&#21644;&#20449;&#21495;&#24402;&#19968;&#21270;&#31561;&#24433;&#21709;&#65292;&#20351;&#29992;&#26694;&#26550;&#21487;&#35757;&#32451;&#21644;&#20248;&#21270;&#32447;&#24615;/&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24471;&#20986;&#22312;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20013;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#24402;&#19968;&#21270;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#38477;&#20302;&#31934;&#24230;&#21644;&#22122;&#38899;&#31561;&#22240;&#32032;&#65292;&#23454;&#29616;&#25968;&#23383;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21521;&#20854;&#27169;&#25311;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2210.10048</link><description>&lt;p&gt;
AnalogVNN&#65306;&#23436;&#20840;&#27169;&#22359;&#21270;&#30340;&#20809;&#23376;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21644;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnalogVNN: A fully modular framework for modeling and optimizing photonic neural networks. (arXiv:2210.10048v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10048
&lt;/p&gt;
&lt;p&gt;
AnalogVNN&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#20809;&#30005;&#22122;&#22768;&#12289;&#26377;&#38480;&#31934;&#24230;&#21644;&#20449;&#21495;&#24402;&#19968;&#21270;&#31561;&#24433;&#21709;&#65292;&#20351;&#29992;&#26694;&#26550;&#21487;&#35757;&#32451;&#21644;&#20248;&#21270;&#32447;&#24615;/&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24471;&#20986;&#22312;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20013;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#24402;&#19968;&#21270;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#38477;&#20302;&#31934;&#24230;&#21644;&#22122;&#38899;&#31561;&#22240;&#32032;&#65292;&#23454;&#29616;&#25968;&#23383;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21521;&#20854;&#27169;&#25311;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AnalogVNN &#26159;&#19968;&#20010;&#22522;&#20110; PyTorch &#26500;&#24314;&#30340;&#20223;&#30495;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#20809;&#23376;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#20013;&#23384;&#22312;&#30340;&#20809;&#30005;&#22122;&#22768;&#12289;&#26377;&#38480;&#31934;&#24230;&#21644;&#20449;&#21495;&#24402;&#19968;&#21270;&#31561;&#24433;&#21709;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26694;&#26550;&#35757;&#32451;&#21644;&#20248;&#21270;&#20855;&#26377;&#26368;&#22810; 9 &#23618;&#21644;&#32422; 1.7 &#30334;&#19975;&#21442;&#25968;&#30340;&#32447;&#24615;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#24402;&#19968;&#21270;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#38477;&#20302;&#31934;&#24230;&#21644;&#22122;&#38899;&#23545;&#27169;&#25311;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36981;&#24490; PyTorch &#20013;&#23384;&#22312;&#30340;&#30456;&#21516;&#23618;&#32467;&#26500;&#35774;&#35745;&#65292;AnalogVNN &#26694;&#26550;&#20801;&#35768;&#29992;&#25143;&#20165;&#20351;&#29992;&#20960;&#34892;&#20195;&#30721;&#23558;&#22823;&#22810;&#25968;&#25968;&#23383;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36716;&#25442;&#20026;&#20854;&#27169;&#25311;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992; PyTorch &#25552;&#20379;&#30340;&#24320;&#28304;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644; GPU &#21152;&#36895;&#24211;&#12290;&#20195;&#30721;&#21487;&#22312; https://analogvnn.github.io &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
AnalogVNN, a simulation framework built on PyTorch which can simulate the effects of optoelectronic noise, limited precision, and signal normalization present in photonic neural network accelerators. We use this framework to train and optimize linear and convolutional neural networks with up to 9 layers and ~1.7 million parameters, while gaining insights into how normalization, activation function, reduced precision, and noise influence accuracy in analog photonic neural networks. By following the same layer structure design present in PyTorch, the AnalogVNN framework allows users to convert most digital neural network models to their analog counterparts with just a few lines of code, taking full advantage of the open-source optimization, deep learning, and GPU acceleration libraries available through PyTorch. Code is available at https://analogvnn.github.io
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#30340;&#26032;&#22411;&#33609;&#22270;&#26041;&#26696;&#65292;&#36890;&#36807;&#33410;&#32422;&#36890;&#20449;&#25104;&#26412;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#65292;&#20294;&#26159;&#38656;&#35201;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#22788;&#29702;&#20197;&#20445;&#25252;&#23616;&#37096;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2210.08371</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#24102;&#23485;&#36890;&#36947;&#21644;&#28431;&#27934;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#33609;&#22270;&#25216;&#26415;&#65306;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability. (arXiv:2210.08371v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#30340;&#26032;&#22411;&#33609;&#22270;&#26041;&#26696;&#65292;&#36890;&#36807;&#33410;&#32422;&#36890;&#20449;&#25104;&#26412;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#65292;&#20294;&#26159;&#38656;&#35201;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#22788;&#29702;&#20197;&#20445;&#25252;&#23616;&#37096;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33609;&#22270;&#25216;&#26415;&#26159;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#30784;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#22823;&#38382;&#39064;&#36827;&#34892;&#38543;&#26426;&#21387;&#32553;&#21040;&#26356;&#20302;&#32500;&#24230;&#30340;&#26041;&#24335;&#23454;&#29616;&#36816;&#34892;&#26102;&#21644;&#20869;&#23384;&#30340;&#33410;&#30465;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#26041;&#27861;&#33609;&#22270;&#26041;&#26696;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#20197;&#33410;&#32422;&#20998;&#24067;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#22522;&#20110;&#19968;&#20010;&#39640;&#32500;&#24230; $d$ &#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#20195;&#29702;&#36890;&#36807;&#33609;&#22270;&#30697;&#38453; $R\in \mathbb{R}^{s\times d}$ &#22788;&#29702;&#36825;&#20123;&#21387;&#32553;&#20449;&#24687;&#65292;&#22312;&#20256;&#36882;&#26102;&#21482;&#20256;&#36755;&#33609;&#22270;&#21518;&#30340;&#20449;&#24687;&#65292;&#32780;&#25509;&#25910;&#26041;&#20250;&#36890;&#36807;&#21453;&#33609;&#22270;&#30697;&#38453; $R^\top$ &#36827;&#34892;&#36824;&#21407;&#12290;&#21033;&#29992;&#36825;&#31181;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#36739;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#27492;&#38543;&#26426;&#33609;&#22270;&#24182;&#19981;&#33021;&#30452;&#25509;&#20445;&#25252;&#23616;&#37096;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#21518;&#65292;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sketching is one of the most fundamental tools in large-scale machine learning. It enables runtime and memory saving via randomly compressing the original large problem into lower dimensions. In this paper, we propose a novel sketching scheme for the first order method in large-scale distributed learning setting, such that the communication costs between distributed agents are saved while the convergence of the algorithms is still guaranteed. Given gradient information in a high dimension $d$, the agent passes the compressed information processed by a sketching matrix $R\in \mathbb{R}^{s\times d}$ with $s\ll d$, and the receiver de-compressed via the de-sketching matrix $R^\top$ to ``recover'' the information in original dimension. Using such a framework, we develop algorithms for federated learning with lower communication costs. However, such random sketching does not protect the privacy of local data directly. We show that the gradient leakage problem still exists after applying the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;SGD&#20351;&#29992;&#22823;&#27493;&#38271;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#27493;&#38271;&#35843;&#24230;&#65292;&#26799;&#24230;&#21644;&#22122;&#22768;&#30456;&#20114;&#20316;&#29992;&#65292;&#20849;&#21516;&#39537;&#21160;SGD&#21160;&#24577;&#31359;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#24179;&#38754;&#65292;&#20174;&#32780;&#21457;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.05337</link><description>&lt;p&gt;
SGD&#20351;&#29992;&#22823;&#27493;&#38271;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
SGD with Large Step Sizes Learns Sparse Features. (arXiv:2210.05337v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;SGD&#20351;&#29992;&#22823;&#27493;&#38271;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#27493;&#38271;&#35843;&#24230;&#65292;&#26799;&#24230;&#21644;&#22122;&#22768;&#30456;&#20114;&#20316;&#29992;&#65292;&#20849;&#21516;&#39537;&#21160;SGD&#21160;&#24577;&#31359;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#24179;&#38754;&#65292;&#20174;&#32780;&#21457;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#24120;&#29992;&#30340;&#22823;&#27493;&#38271;&#20250;&#23548;&#33268;&#36845;&#20195;&#20174;&#23665;&#35895;&#30340;&#19968;&#20391;&#36339;&#21040;&#21478;&#19968;&#20391;&#23548;&#33268;&#25439;&#22833;&#31283;&#23450;&#65292;&#21516;&#26102;&#36825;&#31181;&#31283;&#23450;&#24615;&#20250;&#24341;&#36215;&#19968;&#20010;&#38544;&#21547;&#30340;&#12289;&#22402;&#30452;&#20110;&#36339;&#36291;&#26041;&#21521;&#30340;&#38543;&#26426;&#21160;&#24577;&#65292;&#23558;&#20854;&#20559;&#21521;&#20110;&#31232;&#30095;&#39044;&#27979;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#38271;&#26102;&#38388;&#20351;&#29992;&#22823;&#27493;&#38271;&#21487;&#20445;&#25345;SGD&#22312;&#25439;&#22833;&#24179;&#38754;&#20013;&#30340;&#39640;&#24230;&#65292;&#36827;&#32780;&#33021;&#26356;&#22909;&#22320;&#23454;&#29616;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#21457;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#37324;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#26174;&#24335;&#27491;&#21017;&#21270;&#65292;&#22240;&#27492;&#27491;&#21017;&#21270;&#25928;&#26524;&#23436;&#20840;&#26469;&#33258;&#20110;&#21463;&#27493;&#38271;&#35843;&#24230;&#24433;&#21709;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36890;&#36807;&#27493;&#38271;&#35843;&#24230;&#65292;&#26799;&#24230;&#21644;&#22122;&#22768;&#22914;&#20309;&#20849;&#21516;&#39537;&#21160;SGD&#21160;&#24577;&#31359;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#24179;&#38754;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24130;&#24459;&#27493;&#38271;&#35843;&#24230;&#21305;&#37197;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#29702;&#35770;&#39044;&#27979;&#24182;&#23548;&#33268;&#26368;&#31283;&#20581;&#21644;&#26368;&#31232;&#30095;&#30340;&#34920;&#31034;&#26469;&#35777;&#26126;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward sparse predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;</title><link>http://arxiv.org/abs/2209.12573</link><description>&lt;p&gt;
&#25968;&#23383;&#38899;&#39057;&#21462;&#35777;&#65306;&#30450;&#30446;&#26816;&#27979;&#20154;&#31867;&#35821;&#38899;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30450;&#30446;&#26816;&#27979;&#36755;&#20837;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#38899;&#39057;&#27450;&#35784;&#38382;&#39064;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#36825;&#31181;&#20998;&#31867;&#22120;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#27169;&#20223;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#20294;&#21516;&#26102;&#20063;&#24456;&#23481;&#26131;&#34987;&#35823;&#29992;&#26469;&#27450;&#39575;&#20154;&#20204;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#65292;&#30456;&#20851;&#25216;&#26415;&#29616;&#22312;&#23545;&#20960;&#20046;&#25152;&#26377;&#20154;&#37117;&#21487;&#29992;&#65292;&#36825;&#20351;&#24471;&#29359;&#32618;&#21644;&#20266;&#36896;&#21464;&#24471;&#26356;&#21152;&#31616;&#21333;&#12290;&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#30450;&#30446;&#20998;&#31867;&#36755;&#20837;&#38899;&#39057;&#20026;&#30495;&#23454;&#25110;&#32773;&#27169;&#20223;&#65307;&#8220;&#30450;&#30446;&#8221;&#25351;&#30340;&#26159;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25110;&#30495;&#23454;&#26469;&#28304;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20223;&#21046;&#38899;&#39057;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#32452;&#37325;&#35201;&#29305;&#24449;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#34987;&#29992;&#20110;&#27979;&#35797;&#19981;&#21516;&#38899;&#39057;&#30340;&#30456;&#21516;&#29305;&#24449;&#38598;&#12290;&#25968;&#25454;&#25552;&#21462;&#33258;&#20004;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20026;&#36825;&#39033;&#24037;&#20316;&#32780;&#32534;&#20889;;&#19968;&#20010;&#20840;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#38463;&#25289;&#20271;&#35821;&#21152;&#33521;&#35821;&#65289;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24050;&#36890;&#36807;GitHub&#20197;&#21407;&#22987;&#24418;&#24335;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#65292;&#32593;&#22336;&#20026;https://github.com/SaSs7/Datas
&lt;/p&gt;
&lt;p&gt;
Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LrMM&#30340;&#20302;&#31209;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Lloyd&#31639;&#27861;&#21644;&#20302;&#31209;&#36924;&#36817;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22312;&#36739;&#24369;&#30340;&#38480;&#21046;&#19979;&#22788;&#29702;&#30697;&#38453;&#35266;&#27979;&#20540;&#65292;&#22312;&#36798;&#21040;&#26368;&#23567;&#21270;&#28176;&#36827;&#27425;&#20248;&#30340;&#32858;&#31867;&#35823;&#24046;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#12290;</title><link>http://arxiv.org/abs/2207.04600</link><description>&lt;p&gt;
&#20302;&#31209;&#28151;&#21512;&#27169;&#22411;&#30340;Lloyd&#31639;&#27861;&#26368;&#20248;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Optimal Clustering by Lloyd Algorithm for Low-Rank Mixture Model. (arXiv:2207.04600v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LrMM&#30340;&#20302;&#31209;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Lloyd&#31639;&#27861;&#21644;&#20302;&#31209;&#36924;&#36817;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22312;&#36739;&#24369;&#30340;&#38480;&#21046;&#19979;&#22788;&#29702;&#30697;&#38453;&#35266;&#27979;&#20540;&#65292;&#22312;&#36798;&#21040;&#26368;&#23567;&#21270;&#28176;&#36827;&#27425;&#20248;&#30340;&#32858;&#31867;&#35823;&#24046;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30697;&#38453;&#35266;&#27979;&#20540;&#30340;&#32858;&#31867;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#28151;&#21512;&#27169;&#22411;(LrMM)&#65292;&#35813;&#27169;&#22411;&#20174;&#32463;&#20856;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMM)&#20013;&#28436;&#21270;&#32780;&#26469;&#65292;&#29992;&#20110;&#22788;&#29702;&#30697;&#38453;&#35266;&#27979;&#20540;&#65292;&#20551;&#35774;&#31181;&#32676;&#20013;&#24515;&#30697;&#38453;&#30340;&#20302;&#31209;&#24615;&#12290;&#36890;&#36807;&#38598;&#25104;Lloyd&#31639;&#27861;&#21644;&#20302;&#31209;&#36924;&#36817;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#19968;&#26086;&#21021;&#22987;&#21270;&#33391;&#22909;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#24555;&#36895;&#65292;&#20855;&#26377;&#19968;&#31181;&#25351;&#25968;&#22411;&#30340;&#32858;&#31867;&#35823;&#24046;&#29575;&#65292;&#36798;&#21040;&#26368;&#23567;&#21270;&#30340;&#28176;&#36827;&#27425;&#20248;&#24615;&#12290;&#19982;GMM&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#24352;&#37327;&#30340;&#35889;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#32858;&#31867;&#12290;&#26368;&#23567;&#21270;&#28176;&#36827;&#27425;&#20248;&#30340;&#32858;&#31867;&#35823;&#24046;&#29575;&#30001;&#20998;&#31163;&#24378;&#24230;&#20915;&#23450;&#65292;&#21363;&#31181;&#32676;&#20013;&#24515;&#30697;&#38453;&#20043;&#38388;&#30340;&#26368;&#23567;&#36317;&#31163;&#12290;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20998;&#31163;&#24378;&#24230;&#30340;&#35201;&#27714;&#19978;&#20855;&#26377;&#36739;&#24369;&#30340;&#38480;&#21046;&#12290;&#20294;&#19982;GMM&#19981;&#21516;&#30340;&#26159;&#65292;LrMM&#30340;&#35745;&#31639;&#38590;&#24230;&#34987;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the computational and statistical limits in clustering matrix-valued observations. We propose a low-rank mixture model (LrMM), adapted from the classical Gaussian mixture model (GMM) to treat matrix-valued observations, which assumes low-rankness for population center matrices. A computationally efficient clustering method is designed by integrating Lloyd's algorithm and low-rank approximation. Once well-initialized, the algorithm converges fast and achieves an exponential-type clustering error rate that is minimax optimal. Meanwhile, we show that a tensor-based spectral method delivers a good initial clustering. Comparable to GMM, the minimax optimal clustering error rate is decided by the separation strength, i.e., the minimal distance between population center matrices. By exploiting low-rankness, the proposed algorithm is blessed with a weaker requirement on the separation strength. Unlike GMM, however, the computational difficulty of LrMM is characterized b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEPT&#30340;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#12290;&#36890;&#36807;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#24182;&#33021;&#22815;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2207.02434</link><description>&lt;p&gt;
&#26089;&#26399;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;&#27874;&#26031;&#35821;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Early Discovery of Emerging Entities in Persian Twitter with Semantic Similarity. (arXiv:2207.02434v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEPT&#30340;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#12290;&#36890;&#36807;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#24182;&#33021;&#22815;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#65288;EEs&#65289;&#26159;&#25351;&#22312;&#23427;&#20204;&#34987;&#35748;&#21487;&#20043;&#21069;&#23601;&#25214;&#21040;&#23427;&#20204;&#30340;&#36807;&#31243;&#12290;&#36825;&#20123;&#23454;&#20307;&#23545;&#20010;&#20154;&#12289;&#20844;&#21496;&#21644;&#25919;&#24220;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#35768;&#22810;&#23454;&#20307;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21457;&#29616;&#65292;&#20363;&#22914;Twitter&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#20123;&#23454;&#20307;&#24050;&#25104;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19968;&#26679;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#36825;&#19968;&#38382;&#39064;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EEPT&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;EEs&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#33021;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering emerging entities (EEs) is the problem of finding entities before their establishment. These entities can be critical for individuals, companies, and governments. Many of these entities can be discovered on social media platforms, e.g. Twitter. These identities have been the spot of research in academia and industry in recent years. Similar to any machine learning problem, data availability is one of the major challenges in this problem. This paper proposes EEPT. That is an online clustering method able to discover EEs without any need for training on a dataset. Additionally, due to the lack of a proper evaluation metric, this paper uses a new metric to evaluate the results. The results show that EEPT is promising and finds significant entities before their establishment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#65292;&#21457;&#29616;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#65292;&#20984;&#20989;&#25968;&#31867;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#65292;&#32780;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.12977</link><description>&lt;p&gt;
&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust PAC Learnability of Real-Valued Functions. (arXiv:2206.12977v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23454;&#20540;&#20989;&#25968;&#20013;&#23545;&#25239;&#40065;&#26834;PAC&#23398;&#20064;&#24615;&#65292;&#21457;&#29616;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#65292;&#20984;&#20989;&#25968;&#31867;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#65292;&#32780;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;$\ell_p$&#25439;&#22833;&#21644;&#20219;&#24847;&#25200;&#21160;&#38598;&#30340;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#27979;&#35797;&#26102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21738;&#20123;&#20989;&#25968;&#31867;&#26159;PAC&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#26377;&#38480;&#32982;&#25240;&#23556;&#32500;&#30340;&#31867;&#26082;&#21487;&#20197;&#22312;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#35774;&#32622;&#20013;&#34987;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20984;&#20989;&#25968;&#31867;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#20197;&#27491;&#30830;&#23398;&#20064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#20123;&#38750;&#20984;&#20989;&#25968;&#31867;&#26174;&#28982;&#38656;&#35201;&#19981;&#27491;&#24403;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#22522;&#20110;&#26500;&#24314;&#19968;&#20010;&#30001;&#32982;&#25240;&#23556;&#32500;&#20915;&#23450;&#22823;&#23567;&#30340;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#23454;&#20540;&#20989;&#25968;&#30340;&#19981;&#21487;&#30693;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study robustness to test-time adversarial attacks in the regression setting with $\ell_p$ losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample compression scheme for real-valued functions, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36923;&#36753;&#21644;&#8221;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#37327;&#19978;&#26174;&#33879;&#38477;&#20302;&#65292;&#21516;&#26102;&#32463;&#36807;&#35777;&#26126;&#22312;&#23485;&#30340;&#26368;&#32456;&#8220;&#35835;&#20986;&#8221;&#23618;&#30340;&#32593;&#32476;&#20013;&#21021;&#22987;&#21270;&#21518;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;eNTK&#12290;</title><link>http://arxiv.org/abs/2206.12543</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#24555;&#36895;&#19988;&#26377;&#26681;&#25454;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel. (arXiv:2206.12543v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36923;&#36753;&#21644;&#8221;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#37327;&#19978;&#26174;&#33879;&#38477;&#20302;&#65292;&#21516;&#26102;&#32463;&#36807;&#35777;&#26126;&#22312;&#23485;&#30340;&#26368;&#32456;&#8220;&#35835;&#20986;&#8221;&#23618;&#30340;&#32593;&#32476;&#20013;&#21021;&#22987;&#21270;&#21518;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;eNTK&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;eNTK&#65289;&#21487;&#20197;&#24456;&#22909;&#22320;&#29702;&#35299;&#32473;&#23450;&#32593;&#32476;&#30340;&#34920;&#31034;&#65306;&#23427;&#20204;&#36890;&#24120;&#27604;&#26080;&#38480;&#23485;NTK&#35745;&#31639;&#20415;&#23452;&#24471;&#22810;&#65292;&#36866;&#29992;&#33539;&#22260;&#26356;&#24191;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;O&#20010;&#36755;&#20986;&#21333;&#20803;&#65288;&#20363;&#22914;O&#31867;&#20998;&#31867;&#22120;&#65289;&#30340;&#32593;&#32476;&#65292;N&#20010;&#36755;&#20837;&#30340;eNTK&#30340;&#22823;&#23567;&#20026;$NO\times NO$&#65292;&#38656;&#35201;$O((NO)^2)$&#30340;&#20869;&#23384;&#21644;&#39640;&#36798;$O((NO)^3)$&#30340;&#35745;&#31639;&#37327;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#23569;&#25968;&#20960;&#20010;&#36817;&#20284;&#20540;&#20043;&#19968;&#65292;&#21487;&#20197;&#20135;&#29983;$N\times N$&#20869;&#26680;&#30697;&#38453;&#65292;&#20174;&#32780;&#33410;&#30465;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#65292;&#20294;&#27809;&#26377;&#25110;&#26497;&#23569;&#26377;&#29702;&#35770;&#20381;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20013;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36923;&#36753;&#21644;&#8221;&#65292;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#23485;&#30340;&#26368;&#32456;&#8220;&#35835;&#20986;&#8221;&#23618;&#30340;&#32593;&#32476;&#65292;&#22312;&#21021;&#22987;&#21270;&#26102;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;eNTK&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20010;&#36817;&#20284;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#20013;&#30340;&#21508;&#31181;&#29992;&#36884;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size $NO \times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call "sum of logits", converges to the true eNTK at initialization for any network with a wide final "readout" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#26657;&#20934;&#30340;&#19987;&#23478;&#20056;&#31215;&#26041;&#27861;(BalPoE)&#26469;&#35299;&#20915;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#20855;&#26377;&#19981;&#21516;&#27979;&#35797;&#30446;&#26631;&#20998;&#24067;&#30340;&#19987;&#23478;&#20197;&#23454;&#29616;&#26080;&#20559;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.05260</link><description>&lt;p&gt;
&#20026;&#38271;&#23614;&#35782;&#21035;&#35774;&#35745;&#30340;&#24179;&#34913;&#26657;&#20934;&#19987;&#23478;&#20056;&#31215;
&lt;/p&gt;
&lt;p&gt;
Balanced Product of Calibrated Experts for Long-Tailed Recognition. (arXiv:2206.05260v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#26657;&#20934;&#30340;&#19987;&#23478;&#20056;&#31215;&#26041;&#27861;(BalPoE)&#26469;&#35299;&#20915;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#20855;&#26377;&#19981;&#21516;&#27979;&#35797;&#30446;&#26631;&#20998;&#24067;&#30340;&#19987;&#23478;&#20197;&#23454;&#29616;&#26080;&#20559;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#35782;&#21035;&#38382;&#39064;&#37117;&#34987;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#25152;&#34920;&#24449;&#12290;&#30001;&#20110;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#20998;&#24067;&#20351;&#34920;&#31034;&#23398;&#20064;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22914;&#26524;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#65292;&#20363;&#22914;&#22343;&#21248;&#20998;&#24067;&#19982;&#38271;&#23614;&#20998;&#24067;&#65292;&#23601;&#38656;&#35201;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23398;&#20064;&#22810;&#20010;&#19981;&#21516;&#19987;&#23478;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22836;&#37096;&#21644;&#23614;&#37096;&#31867;&#21035;&#20013;&#19987;&#38376;&#21270;&#19981;&#21516;&#30340;&#19987;&#23478;&#65292;&#26469;&#40723;&#21169;&#38598;&#21512;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#35843;&#25972;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#38598;&#21512;&#20013;&#65292;&#24418;&#25104;&#24179;&#34913;&#19987;&#23478;&#20056;&#31215;(BalPoE)&#12290;BalPoE&#32452;&#21512;&#20102;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#27979;&#35797;&#30446;&#26631;&#20998;&#24067;&#30340;&#19987;&#23478;&#65292;&#25512;&#24191;&#20102;&#20960;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27491;&#30830;&#22320;&#23450;&#20041;&#36825;&#20123;&#20998;&#24067;&#24182;&#32452;&#21512;&#19987;&#23478;&#20197;&#23454;&#29616;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35777;&#26126;&#35813;&#38598;&#21512;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ens
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#65288;NDPs&#65289;&#65292;&#36890;&#36807;&#26377;&#38480;&#36793;&#32536;&#23398;&#20064;&#20174;&#20016;&#23500;&#30340;&#20989;&#25968;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;NDPs &#21487;&#20197;&#25429;&#33719;&#25509;&#36817;&#30495;&#23454;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#20855;&#26377;&#36229;&#36234;&#31070;&#32463;&#36807;&#31243;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#22238;&#24402;&#12289;&#38544;&#24335;&#36229;&#21442;&#25968;&#36793;&#32536;&#21270;&#12289;&#38750;&#39640;&#26031;&#21518;&#39564;&#39044;&#27979;&#21644;&#20840;&#23616;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.03992</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Diffusion Processes. (arXiv:2206.03992v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#65288;NDPs&#65289;&#65292;&#36890;&#36807;&#26377;&#38480;&#36793;&#32536;&#23398;&#20064;&#20174;&#20016;&#23500;&#30340;&#20989;&#25968;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;NDPs &#21487;&#20197;&#25429;&#33719;&#25509;&#36817;&#30495;&#23454;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#20855;&#26377;&#36229;&#36234;&#31070;&#32463;&#36807;&#31243;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#22238;&#24402;&#12289;&#38544;&#24335;&#36229;&#21442;&#25968;&#36793;&#32536;&#21270;&#12289;&#38750;&#39640;&#26031;&#21518;&#39564;&#39044;&#27979;&#21644;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20989;&#25968;&#20803;&#23398;&#20064;&#20998;&#24067;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#22686;&#24378;&#21644;&#25512;&#26029;&#22797;&#26434;&#24615;&#38477;&#20302;&#30340;&#20248;&#28857;&#12290;&#22312;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#65288;NDPs&#65289;&#65292;&#36890;&#36807;&#26377;&#38480;&#36793;&#32536;&#23398;&#20064;&#20174;&#20016;&#23500;&#30340;&#20989;&#25968;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23450;&#20041;&#27880;&#24847;&#21147;&#22359;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#38543;&#26426;&#36807;&#31243;&#30340;&#23646;&#24615;&#65288;&#22914;&#21487;&#20132;&#25442;&#24615;&#65289;&#30452;&#25509;&#32435;&#20837; NDP &#30340;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#26126;&#20102; NDPs &#21487;&#20197;&#25429;&#33719;&#25509;&#36817;&#30495;&#23454;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#34920;&#26126;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#27169;&#25311;&#39640;&#26031;&#36807;&#31243;&#30340;&#34892;&#20026;&#24182;&#36229;&#36234;&#31070;&#32463;&#36807;&#31243;&#30340;&#34920;&#29616;&#12290;NDPs &#21487;&#20197;&#36827;&#34892;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#22238;&#24402;&#12289;&#38544;&#24335;&#36229;&#21442;&#25968;&#36793;&#32536;&#21270;&#12289;&#38750;&#39640;&#26031;&#21518;&#39564;&#39044;&#27979;&#21644;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DevFormer&#65292;&#19968;&#31181;&#29992;&#20110;&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#30340;&#23545;&#31216;Transformer&#65292;&#24341;&#20837;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#21160;&#20316;&#32622;&#25442;&#23545;&#31216;&#24615;&#31561;&#24378;&#24402;&#32435;&#20559;&#32622;&#20197;&#26377;&#25928;&#25429;&#25417;&#30828;&#20214;&#19978;&#19979;&#25991;&#65292;&#33021;&#26377;&#25928;&#22320;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#23454;&#29616;&#35774;&#35745;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#20998;&#31163;&#30005;&#23481;&#22120;&#25918;&#32622;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#21487;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#32452;&#20214;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2205.13225</link><description>&lt;p&gt;
DevFormer: &#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30828;&#20214;&#24067;&#23616;&#30340;&#23545;&#31216;Transformer
&lt;/p&gt;
&lt;p&gt;
DevFormer: A Symmetric Transformer for Context-Aware Device Placement. (arXiv:2205.13225v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DevFormer&#65292;&#19968;&#31181;&#29992;&#20110;&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#30340;&#23545;&#31216;Transformer&#65292;&#24341;&#20837;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#21160;&#20316;&#32622;&#25442;&#23545;&#31216;&#24615;&#31561;&#24378;&#24402;&#32435;&#20559;&#32622;&#20197;&#26377;&#25928;&#25429;&#25417;&#30828;&#20214;&#19978;&#19979;&#25991;&#65292;&#33021;&#26377;&#25928;&#22320;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#23454;&#29616;&#35774;&#35745;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#20998;&#31163;&#30005;&#23481;&#22120;&#25918;&#32622;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#20247;&#65292;&#21487;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#32452;&#20214;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DevFormer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#30828;&#20214;&#35774;&#35745;&#20248;&#21270;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#21160;&#20316;&#32622;&#25442;&#23545;&#31216;&#24615;&#31561;&#24378;&#24402;&#32435;&#20559;&#32622;&#26469;&#26377;&#25928;&#25429;&#25417;&#30828;&#20214;&#19978;&#19979;&#25991;&#24182;&#20351;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#23454;&#29616;&#26377;&#25928;&#30340;&#35774;&#35745;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;DevFoemer&#24212;&#29992;&#20110;&#20998;&#31163;&#30005;&#23481;&#22120;&#25918;&#32622;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#30828;&#20214;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21487;&#22312;&#20943;&#23569;&#36229;&#36807;30&#65285;&#30340;&#32452;&#20214;&#25968;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#22522;&#20110;&#31163;&#32447;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21327;&#20316;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Boosting Tail Neural Network&#65288;BTNN&#65289;&#26469;&#25913;&#21892;&#23454;&#26102;&#33258;&#23450;&#20041;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24369;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#33719;&#24471;&#20102;&#26174;&#33879;&#34920;&#29616;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2205.12933</link><description>&lt;p&gt;
&#23454;&#26102;&#33258;&#23450;&#20041;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#30340; Boosting Tail Neural Network
&lt;/p&gt;
&lt;p&gt;
Boosting Tail Neural Network for Realtime Custom Keyword Spotting. (arXiv:2205.12933v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Boosting Tail Neural Network&#65288;BTNN&#65289;&#26469;&#25913;&#21892;&#23454;&#26102;&#33258;&#23450;&#20041;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24369;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#33719;&#24471;&#20102;&#26174;&#33879;&#34920;&#29616;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Boosting Tail Neural Network&#65288;BTNN&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#23454;&#26102;&#33258;&#23450;&#20041;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;RCKS&#65289;&#30340;&#24615;&#33021;&#12290;&#21463;&#22823;&#33041;&#31185;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#25209;&#24369;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21796;&#37266;&#29575;&#21644;&#35823;&#25253;&#29575;&#26041;&#38754;&#24615;&#33021;&#26356;&#22909;&#65292;&#22312;&#19982;&#20165;&#20351;&#29992;&#19968;&#20010;&#24378;&#20998;&#31867;&#22120;&#30340;&#20256;&#32479;&#31639;&#27861;&#30456;&#27604;&#36739;&#26102;&#65292;&#33719;&#24471;&#20102;18&#65285;&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a Boosting Tail Neural Network (BTNN) for improving the performance of Realtime Custom Keyword Spotting (RCKS) that is still an industrial challenge for demanding powerful classification ability with limited computation resources. Inspired by Brain Science that a brain is only partly activated for a nerve simulation and numerous machine learning algorithms are developed to use a batch of weak classifiers to resolve arduous problems, which are often proved to be effective. We show that this method is helpful to the RCKS problem. The proposed approach achieve better performances in terms of wakeup rate and false alarm.  In our experiments compared with those traditional algorithms that use only one strong classifier, it gets 18\% relative improvement. We also point out that this approach may be promising in future ASR exploration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;dboost&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20026;&#8220;&#39044;&#27979;&#65292;&#28982;&#21518;&#20248;&#21270;&#8221;&#38382;&#39064;&#35774;&#35745;&#30340;&#26234;&#33021;&#26799;&#24230;&#25552;&#21319;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#20984;&#20108;&#27425;&#38181;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#33258;&#23450;&#20041;&#19981;&#21160;&#28857;&#26144;&#23556;&#30340;&#38544;&#24335;&#24494;&#20998;&#26469;&#25191;&#34892;&#26799;&#24230;&#25552;&#21319;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2204.06895</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#20984;&#38181;&#39044;&#27979;&#21644;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gradient boosting for convex cone predict and optimize problems. (arXiv:2204.06895v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;dboost&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20026;&#8220;&#39044;&#27979;&#65292;&#28982;&#21518;&#20248;&#21270;&#8221;&#38382;&#39064;&#35774;&#35745;&#30340;&#26234;&#33021;&#26799;&#24230;&#25552;&#21319;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#20984;&#20108;&#27425;&#38181;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#33258;&#23450;&#20041;&#19981;&#21160;&#28857;&#26144;&#23556;&#30340;&#38544;&#24335;&#24494;&#20998;&#26469;&#25191;&#34892;&#26799;&#24230;&#25552;&#21319;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#29420;&#31435;&#20110;&#20915;&#31574;&#20248;&#21270;&#36827;&#34892;&#20248;&#21270;&#12290;&#26234;&#33021;&#39044;&#27979;&#20248;&#21270;&#65288;SPO&#65289;&#26694;&#26550;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#20197;&#26368;&#23567;&#21270;&#19979;&#28216;&#20915;&#31574;&#36951;&#25022;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;dboost&#65292;&#38024;&#23545;&#8220;&#39044;&#27979;&#65292;&#28982;&#21518;&#20248;&#21270;&#8221;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#26234;&#33021;&#26799;&#24230;&#25552;&#21319;&#23454;&#29616;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#20984;&#20108;&#27425;&#38181;&#35268;&#21010;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#19981;&#21160;&#28857;&#26144;&#23556;&#30340;&#38544;&#24335;&#24494;&#20998;&#26469;&#25191;&#34892;&#26799;&#24230;&#25552;&#21319;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;SPO&#26041;&#27861;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;dboost&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#26679;&#26412;&#22806;&#20915;&#31574;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction models are typically optimized independently from decision optimization. A smart predict then optimize (SPO) framework optimizes prediction models to minimize downstream decision regret. In this paper we present dboost, the first general purpose implementation of smart gradient boosting for `predict, then optimize' problems. The framework supports convex quadratic cone programming and gradient boosting is performed by implicit differentiation of a custom fixed-point mapping. Experiments comparing with state-of-the-art SPO methods show that dboost can further reduce out-of-sample decision regret.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2203.12026</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;&#22312;ADAS&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20223;&#30495;&#38598;&#25104;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#27979;&#35797;&#26041;&#27861;Deeper&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#19982;&#31454;&#36187;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#30340;&#27604;&#36739;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;Deeper&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#23454;&#29616;&#30340;&#20223;&#30495;&#38598;&#25104;&#27979;&#35797;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#27979;&#35797;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#30340;&#25925;&#38556;&#21457;&#29616;&#27979;&#35797;&#22330;&#26223;&#12290;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#32452;&#26032;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;-&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#65288;&#956;+&#955;&#65289;&#21644;&#65288;&#956;&#65292;&#955;&#65289;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#20197;&#21450;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#65292;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#36136;&#37327;&#31181;&#23376;&#31181;&#32676;&#20197;&#21450;&#20026;&#24314;&#27169;&#27979;&#35797;&#22330;&#26223;&#20351;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#20132;&#21449;&#21644;&#31361;&#21464;&#25805;&#20316;&#12290;&#20026;&#20102;&#23637;&#31034;Deeper&#20013;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#19982;SBST 2021&#30340;&#20116;&#20010;&#21442;&#36187;&#24037;&#20855;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26032;&#29256;&#26412;&#20013;&#65292;Deeper&#20013;&#30340;&#26032;&#27979;&#35797;&#29983;&#25104;&#22120;&#19981;&#20165;&#22312;&#20197;&#21069;&#30340;&#29256;&#26412;&#19978;&#26377;&#20102;&#24456;&#22823;&#25552;&#21319;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22870;&#21169;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#25919;&#31574;&#20248;&#21270;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#27604;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20197;&#20854;&#19981;&#21464;&#24615;&#20026;&#20381;&#25454;&#65292;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.07475</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#20013;&#30340;&#19981;&#21464;&#24615;&#21450;&#22870;&#21169;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22870;&#21169;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#25919;&#31574;&#20248;&#21270;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#27604;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20197;&#20854;&#19981;&#21464;&#24615;&#20026;&#20381;&#25454;&#65292;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#20219;&#21153;&#65292;&#25163;&#21160;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#20063;&#20250;&#26377;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#24456;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#20989;&#25968;&#21482;&#33021;&#34987;&#37096;&#20998;&#22320;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#22870;&#21169;&#23398;&#20064;&#25968;&#25454;&#28304;&#65288;&#21253;&#25324;&#19987;&#23478;&#28436;&#31034;&#21644;&#36712;&#36857;&#27604;&#36739;&#65289;&#19979;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36825;&#31181;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#23545;&#20110;&#20960;&#39033;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#25919;&#31574;&#20248;&#21270;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20854;&#19981;&#21464;&#24615;&#23545;&#27604;&#25968;&#25454;&#28304;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#25968;&#25454;&#28304;&#30340;&#35774;&#35745;&#21644;&#36873;&#25321;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#35299;&#37322;&#20026;&#35299;&#20915;&#32447;&#24615;PDE&#25110;&#32447;&#24615;BSDE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07960</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21644;&#29366;&#24577;&#19979;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#65288;&#38543;&#26426;&#22330;&#26223;&#20013;&#65289;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning with Continuous Time and State in the Stochastic Setting. (arXiv:2202.07960v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#30340;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#35299;&#37322;&#20026;&#35299;&#20915;&#32447;&#24615;PDE&#25110;&#32447;&#24615;BSDE&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#35266;&#23519;&#26469;&#23398;&#20064;&#19982;&#26410;&#21463;&#25511;&#21046;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#36880;&#28176;&#20943;&#23569;&#30340;&#26102;&#38388;&#27493;&#38271;&#30340;&#33879;&#21517;TD&#65288;0&#65289;&#26041;&#27861;&#30340;&#21407;&#22987;&#21464;&#20307;&#12290;&#19968;&#31181;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#12290;&#23545;&#20110;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25110;&#32773;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#36817;&#20284;&#35299;&#20915;&#32447;&#24615;PDE&#65288;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#25110;&#32447;&#24615;BSDE&#65288;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65289;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of continuous-time policy evaluation. This consists in learning through observations the value function associated with an uncontrolled continuous-time stochastic dynamic and a reward function. We propose two original variants of the well-known TD(0) method using vanishing time steps. One is model-free and the other is model-based. For both methods, we prove theoretical convergence rates that we subsequently verify through numerical simulations. Alternatively, those methods can be interpreted as novel reinforcement learning approaches for approximating solutions of linear PDEs (partial differential equations) or linear BSDEs (backward stochastic differential equations).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD-NEGF&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;NEGF&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#23376;&#36755;&#36816;&#27169;&#25311;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#21453;&#21521;&#35774;&#35745;&#31561;&#39046;&#22495;&#65292;&#24182;&#20855;&#26377;&#21152;&#36895;&#26448;&#26009;&#35774;&#35745;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.05098</link><description>&lt;p&gt;
AD-NEGF&#65306;&#19968;&#31181;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#21453;&#38382;&#39064;&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#37327;&#23376;&#36755;&#36816;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AD-NEGF: An End-to-End Differentiable Quantum Transport Simulator for Sensitivity Analysis and Inverse Problems. (arXiv:2202.05098v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AD-NEGF&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;NEGF&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#23376;&#36755;&#36816;&#27169;&#25311;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#21453;&#21521;&#35774;&#35745;&#31561;&#39046;&#22495;&#65292;&#24182;&#20855;&#26377;&#21152;&#36895;&#26448;&#26009;&#35774;&#35745;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;70&#24180;&#20195;&#25552;&#20986;&#20197;&#26469;&#65292;&#38750;&#24179;&#34913;&#26684;&#26519;&#20989;&#25968;&#65288;NEGF&#65289;&#26041;&#27861;&#24050;&#34987;&#20844;&#35748;&#20026;&#37327;&#23376;&#36755;&#36816;&#27169;&#25311;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#23613;&#31649;&#23427;&#22312;&#27169;&#25311;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20294;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#26080;&#27861;&#25215;&#21463;&#39640;&#36890;&#37327;&#27169;&#25311;&#20219;&#21153;&#65292;&#20363;&#22914;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#21453;&#21521;&#35774;&#35745;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;AD-NEGF&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;NEGF&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#23376;&#36755;&#36816;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;PyTorch&#23454;&#29616;&#25972;&#20010;&#25968;&#20540;&#36807;&#31243;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#23618;&#25216;&#26415;&#35774;&#35745;&#20102;&#33258;&#23450;&#20041;&#30340;&#21453;&#21521;&#20256;&#36882;&#65292;&#20197;&#25552;&#20379;&#21487;&#36127;&#25285;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#35777;&#27491;&#21521;&#27169;&#25311;&#30340;&#27491;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32463;&#36807;&#24212;&#29992;&#39564;&#35777;&#65292;&#29992;&#20110;&#35745;&#31639;&#19981;&#21516;&#29289;&#29702;&#37327;&#12289;&#23454;&#39564;&#21442;&#25968;&#25311;&#21512;&#21644;&#25530;&#26434;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#21152;&#36895;&#26448;&#26009;&#35774;&#35745;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since proposed in the 70s, the Non-Equilibrium Green Function (NEGF) method has been recognized as a standard approach to quantum transport simulations. Although it achieves superiority in simulation accuracy, the tremendous computational cost makes it unbearable for high-throughput simulation tasks such as sensitivity analysis, inverse design, etc. In this work, we propose AD-NEGF, to our best knowledge the first end-to-end differentiable NEGF model for quantum transport simulations. We implement the entire numerical process in PyTorch, and design customized backward pass with implicit layer techniques, which provides gradient information at an affordable cost while guaranteeing the correctness of the forward simulation. The proposed model is validated with applications in calculating differential physical quantities, empirical parameter fitting, and doping optimization, which demonstrates its capacity to accelerate the material design process by conducting gradient-based parameter op
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.10635</link><description>&lt;p&gt;
HeterPS&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#24322;&#26500;&#29615;&#22659;&#19979;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Paddle-HeterPS&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#22810;&#23618;&#27425;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#35768;&#22810;&#23618;&#21644;&#22823;&#37327;&#21442;&#25968;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;DNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#22788;&#29702;&#20855;&#26377;&#35768;&#22810;&#31232;&#30095;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20250;&#20135;&#29983;&#39640;&#24310;&#36831;&#21644;I/O&#25104;&#26412;&#65292;&#32780;&#26576;&#20123;&#23618;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#31867;&#22411;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22914;CPU&#21644;GPU&#31561;&#65292;&#20063;&#21487;&#29992;&#20110;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22810;&#23618;&#27425;&#22320;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#23545;&#35757;&#32451;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#36890;&#36807;&#24322;&#26500;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26694;&#26550;Paddle-Heterogeneous Parameter Server&#65288;Paddle-HeterPS&#65289;&#65292;&#30001;&#20998;&#24067;&#24335;&#26550;&#26500;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#24230;&#26041;&#27861;&#32452;&#25104;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;Paddle-HeterPS&#30340;&#20248;&#28857;&#26377;&#19977;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#21152;&#36895;&#20316;&#29992;&#65292;&#36825;&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2110.05854</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#30340;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#38754;&#30721;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#21152;&#36895;&#20316;&#29992;&#65292;&#36825;&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#30721;&#32416;&#38169;&#25552;&#20379;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30340;&#39640;&#24230;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#24403;&#20316;&#20026;&#31283;&#23450;&#30721;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#34920;&#38754;&#30721;&#35745;&#31639;&#21253;&#25324;&#19968;&#20010;&#32508;&#21512;&#24449;&#35299;&#30721;&#27493;&#39588;&#65292;&#20854;&#20013;&#20351;&#29992;&#27979;&#37327;&#30340;&#31283;&#23450;&#30721;&#31639;&#31526;&#26469;&#30830;&#23450;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#20013;&#30340;&#38169;&#35823;&#30340;&#36866;&#24403;&#32416;&#27491;&#12290;&#35793;&#30721;&#31639;&#27861;&#32463;&#21382;&#20102;&#23454;&#36136;&#24615;&#30340;&#21457;&#23637;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#32435;&#20837;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#12290;&#23613;&#31649;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#22522;&#20110;ML&#30340;&#32508;&#21512;&#24449;&#35793;&#30721;&#22120;&#20173;&#28982;&#23616;&#38480;&#20110;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#23567;&#35268;&#27169;&#28436;&#31034;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#38656;&#35201;&#36827;&#34892;&#26230;&#26684;&#25163;&#26415;&#21644;&#32534;&#32455;&#30340;&#36793;&#30028;&#26465;&#20214;&#21644;&#21508;&#31181;&#24418;&#29366;&#30340;&#34920;&#38754;&#30721;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#21487;&#25193;&#23637;&#24555;&#36895;&#32508;&#21512;&#24449;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#35299;&#30721;&#20219;&#24847;&#24418;&#29366;&#21644;&#22823;&#23567;&#30340;&#34920;&#38754;&#30721;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#37327;&#23376;&#27604;&#29305;&#21463;&#21040;&#26497;&#21270;&#35823;&#24046;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;5000&#19975;&#20010;&#38543;&#26426;&#37327;&#23376;&#38169;&#35823;&#23454;&#20363;&#30340;&#20005;&#26684;&#35757;&#32451;&#65292;ANN&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35793;&#30721;&#31639;&#27861;&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#25110;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#21576;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35793;&#30721;&#26041;&#27861;&#23454;&#29616;&#22823;&#35268;&#27169;&#37327;&#23376;&#32416;&#38169;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface code error correction offers a highly promising pathway to achieve scalable fault-tolerant quantum computing. When operated as stabilizer codes, surface code computations consist of a syndrome decoding step where measured stabilizer operators are used to determine appropriate corrections for errors in physical qubits. Decoding algorithms have undergone substantial development, with recent work incorporating machine learning (ML) techniques. Despite promising initial results, the ML-based syndrome decoders are still limited to small scale demonstrations with low latency and are incapable of handling surface codes with boundary conditions and various shapes needed for lattice surgery and braiding. Here, we report the development of an artificial neural network (ANN) based scalable and fast syndrome decoder capable of decoding surface codes of arbitrary shape and size with data qubits suffering from the depolarizing error model. Based on rigorous training over 50 million random qu
&lt;/p&gt;</description></item><item><title>&#26680;&#32454;&#21270;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#21387;&#32553;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;$n$&#28857;&#36817;&#20284;&#30340;&#20998;&#24067;&#21387;&#32553;&#21040;&#20855;&#26377;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#65292;&#20854;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2105.05842</link><description>&lt;p&gt;
&#26680;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernel Thinning. (arXiv:2105.05842v9 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.05842
&lt;/p&gt;
&lt;p&gt;
&#26680;&#32454;&#21270;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#21387;&#32553;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;$n$&#28857;&#36817;&#20284;&#30340;&#20998;&#24067;&#21387;&#32553;&#21040;&#20855;&#26377;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#65292;&#20854;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#26680;&#32454;&#21270;&#65292;&#19968;&#31181;&#27604;&#29420;&#31435;&#21516;&#20998;&#24067;&#37319;&#26679;&#25110;&#26631;&#20934;&#32454;&#21270;&#26356;&#26377;&#25928;&#22320;&#21387;&#32553;&#20998;&#24067;$\mathbb{P}$&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#20877;&#29983;&#26680;$\mathbf{k}_{\star}$&#21644;$\mathcal{O}(n^2)$&#26102;&#38388;&#65292;&#26680;&#32454;&#21270;&#23558;&#19968;&#20010;$n$&#28857;&#36817;&#20284;&#30340;$\mathbb{P}$&#21387;&#32553;&#25104;&#19968;&#20010;&#20855;&#26377;&#19982;&#30456;&#20851;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#21487;&#27604;&#36739;&#26368;&#22351;&#31215;&#20998;&#35823;&#24046;&#30340;$\sqrt{n}$&#28857;&#36817;&#20284;&#12290;&#22312;&#27010;&#29575;&#19978;&#65292;&#32039;&#25903;&#25745;&#30340;$\mathbb{P}$&#30340;&#31215;&#20998;&#35823;&#24046;&#26368;&#22823;&#24046;&#21035;&#20026;$\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$&#65292;&#22312;$\mathbb{R}^d$&#19978;&#30340;&#20122;&#25351;&#25968;$\mathbb{P}$&#20026;$\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26469;&#33258;$\mathbb{P}$&#30340;&#31561;&#22823;&#23567;i.i.d.&#26679;&#26412;&#38754;&#20020;$\Omega(n^{-1/4})$&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20122;&#25351;&#25968;&#20445;&#35777;&#31867;&#20284;&#20110;&#22312;$[0,1]^d$&#19978;&#22343;&#21248;$\mathbb{P}$&#30340;&#32463;&#20856;&#20934;&#33945;&#29305;&#21345;&#32599;&#35823;&#24046;&#29575;&#65292;&#20294;&#36866;&#29992;&#20110;$\mathbb{R}^d$&#19978;&#30340;&#19968;&#33324;&#20998;&#24067;&#21644;&#19968;&#20010;&#22823;
&lt;/p&gt;
&lt;p&gt;
We introduce kernel thinning, a new procedure for compressing a distribution $\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\mathbf{k}_{\star}$ and $\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\mathbb{P}$ into a $\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. The maximum discrepancy in integration error is $\mathcal{O}_d(n^{-1/2}\sqrt{\log n})$ in probability for compactly supported $\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} (\log n)^{(d+1)/2}\sqrt{\log\log n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\mathbb{R}^d$ and a wid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2009.02476</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#25945;&#23398;&#30740;&#31350;&#25945;&#25480;&#24378;&#21270;&#23398;&#20064;&#32773;&#26102;&#20154;&#31867;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.02476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#65292;&#21457;&#29616;&#20154;&#20204;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#21644;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#36827;&#23637;&#35843;&#25972;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25104;&#21151;&#25945;&#23398;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#32773;&#23398;&#20064;&#26041;&#24335;&#36827;&#34892;&#20551;&#35774;&#65292;&#21363;&#23398;&#20064;&#32773;&#22914;&#20309;&#20351;&#29992;&#26469;&#33258;&#19990;&#30028;&#30340;&#32463;&#39564;&#26469;&#26356;&#26032;&#20854;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22870;&#21169;&#21644;&#24809;&#32602;&#26041;&#27861;&#19979;&#65292;&#20154;&#20204;&#23545;&#20110;&#23398;&#20064;&#32773;&#30340;&#26399;&#26395;&#20551;&#35774;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861; Q-learning&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#32771;&#23519;&#20154;&#20204;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36798;&#21040;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#26631;&#20934;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26426;&#22120;&#25945;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#26469;&#27169;&#25311;&#23398;&#20064;&#32773;&#22312;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23398;&#20064;&#39044;&#27979;&#21453;&#39304;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#22312;&#25945;&#25480;&#29702;&#24819;&#21270;&#30340;&#25506;&#32034;&#21033;&#29992;&#20219;&#21153;&#26102;&#65292;&#20154;&#20204;&#23545;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#21644;&#25240;&#25187;&#29575;&#26377;&#21738;&#20123;&#20551;&#35774;&#65311;&#22312;&#34892;&#20026;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#30456;&#23545;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#25945;&#23548; Q-&#23398;&#20064;&#32773;&#36825;&#39033;&#20219;&#21153;&#12290;&#20154;&#20204;&#20542;&#21521;&#20110;&#20551;&#35774;&#23398;&#20064;&#32773;&#20855;&#26377;&#39640;&#30340;&#25240;&#25187;&#29575;&#65292;&#24182;&#39640;&#24230;&#37325;&#35270;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#20250;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#36827;&#23637;&#35843;&#25972;&#33258;&#24049;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#20998;&#23618;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#24674;&#22797;&#20102;&#25429;&#25417;&#22810;&#20041;&#24615;&#30340;&#33021;&#21147;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#39640;&#26031;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#20041;&#35789;&#26816;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30340;&#23618;&#27425;&#27169;&#22411;&#20855;&#26377;&#26356;&#21152;&#31616;&#27905;&#30340;&#20027;&#39064;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2002.10855</link><description>&lt;p&gt;
&#39640;&#26031;&#20998;&#23618;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65306;&#20877;&#29616;&#22810;&#20041;&#35789;
&lt;/p&gt;
&lt;p&gt;
Gaussian Hierarchical Latent Dirichlet Allocation: Bringing Polysemy Back. (arXiv:2002.10855v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.10855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#20998;&#23618;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#24674;&#22797;&#20102;&#25429;&#25417;&#22810;&#20041;&#24615;&#30340;&#33021;&#21147;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#39640;&#26031;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#22810;&#20041;&#35789;&#26816;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30340;&#23618;&#27425;&#27169;&#22411;&#20855;&#26377;&#26356;&#21152;&#31616;&#27905;&#30340;&#20027;&#39064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21457;&#29616;&#19968;&#32452;&#25991;&#26723;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20004;&#31181;&#20856;&#22411;&#30340;&#27169;&#22411;&#26159;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#21644;&#39640;&#26031;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65292;&#21069;&#32773;&#20351;&#29992;&#21333;&#35789;&#19978;&#30340;&#22810;&#39033;&#24335;&#20998;&#24067;&#65292;&#21518;&#32773;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#35789;&#23884;&#20837;&#21521;&#37327;&#19978;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#28508;&#22312;&#20027;&#39064;&#34920;&#31034;&#12290;&#19982;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30456;&#27604;&#65292;&#39640;&#26031;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#22312;&#25429;&#25417;&#8220;&#38134;&#34892;&#8221;&#31561;&#35789;&#30340;&#22810;&#20041;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#26031;&#20998;&#23618;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#36890;&#36807;&#20026;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#32473;&#23450;&#25991;&#26723;&#30340;&#20027;&#39064;&#38598;&#21512;&#24341;&#20837;&#23618;&#27425;&#32467;&#26500;&#65292;&#21487;&#20197;&#24674;&#22797;&#25429;&#25417;&#22810;&#20041;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#39640;&#26031;&#20998;&#23618;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30456;&#23545;&#20110;&#22522;&#20110;&#39640;&#26031;&#30340;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20041;&#35789;&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#22522;&#20110;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30340;&#23618;&#27425;&#27169;&#22411;&#26356;&#20026;&#31616;&#27905;&#30340;&#20027;&#39064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are widely used to discover the latent representation of a set of documents. The two canonical models are latent Dirichlet allocation, and Gaussian latent Dirichlet allocation, where the former uses multinomial distributions over words, and the latter uses multivariate Gaussian distributions over pre-trained word embedding vectors as the latent topic representations, respectively. Compared with latent Dirichlet allocation, Gaussian latent Dirichlet allocation is limited in the sense that it does not capture the polysemy of a word such as ``bank.'' In this paper, we show that Gaussian latent Dirichlet allocation could recover the ability to capture polysemy by introducing a hierarchical structure in the set of topics that the model can use to represent a given document. Our Gaussian hierarchical latent Dirichlet allocation significantly improves polysemy detection compared with Gaussian-based models and provides more parsimonious topic representations compared with hierarch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31209;&#19968;&#26356;&#26032;&#30340;ROIPCA&#21644;fROIPCA&#20004;&#31181;&#22312;&#32447;PCA&#31639;&#27861;&#65292;&#22312;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#20934;&#30830;&#24615;&#22909;&#12289;&#36816;&#34892;&#26102;&#38388;&#30701;&#12290;&#20854;&#20013;fROIPCA&#20026;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/1911.11049</link><description>&lt;p&gt;
ROIPCA&#65306;&#19968;&#31181;&#22522;&#20110;&#31209;&#19968;&#26356;&#26032;&#30340;&#22312;&#32447;&#20869;&#23384;&#21463;&#38480;PCA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ROIPCA: An online memory-restricted PCA algorithm based on rank-one updates. (arXiv:1911.11049v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.11049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31209;&#19968;&#26356;&#26032;&#30340;ROIPCA&#21644;fROIPCA&#20004;&#31181;&#22312;&#32447;PCA&#31639;&#27861;&#65292;&#22312;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#20934;&#30830;&#24615;&#22909;&#12289;&#36816;&#34892;&#26102;&#38388;&#30701;&#12290;&#20854;&#20013;fROIPCA&#20026;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#22522;&#26412;&#30340;&#31639;&#27861;&#12290;&#20854;&#20869;&#23384;&#21463;&#38480;&#30340;&#22312;&#32447;&#29256;&#26412;&#22312;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#20013;&#25968;&#25454;&#36807;&#22823;&#32780;&#26080;&#27861;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#65292;&#25110;&#32773;&#25968;&#25454;&#21040;&#36798;&#26102;&#20026;&#19968;&#31995;&#21015;&#39033;&#30446;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROIPCA&#21644;fROIPCA&#20004;&#31181;&#22522;&#20110;&#31209;&#19968;&#26356;&#26032;&#30340;&#22312;&#32447;PCA&#31639;&#27861;&#12290;&#34429;&#28982;ROIPCA&#36890;&#24120;&#26356;&#20934;&#30830;&#65292;&#20294;fROIPCA&#26356;&#24555;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;fROIPCA&#19982;&#29616;&#26377;&#27969;&#34892;&#30340;&#22312;&#32447;PCA&#26799;&#24230;&#31639;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#29305;&#21035;&#35777;&#26126;&#20102;fROIPCA&#23454;&#38469;&#19978;&#26159;&#20855;&#26377;&#26368;&#20248;&#23398;&#20064;&#29575;&#30340;&#26799;&#24230;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal components analysis (PCA) is a fundamental algorithm in data analysis. Its memory-restricted online versions are useful in many modern applications, where the data are too large to fit in memory, or when data arrive as a stream of items. In this paper, we propose ROIPCA and fROIPCA, two online PCA algorithms that are based on rank-one updates. While ROIPCA is typically more accurate, fROIPCA is faster and has comparable accuracy. We show the relation between fROIPCA and an existing popular gradient algorithm for online PCA, and in particular, prove that fROIPCA is in fact a gradient algorithm with an optimal learning rate. We demonstrate numerically the advantages of our algorithms over existing state-of-the-art algorithms in terms of accuracy and runtime.
&lt;/p&gt;</description></item></channel></rss>