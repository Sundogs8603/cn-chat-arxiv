<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#36923;&#36753;&#22238;&#24402;&#31639;&#27861;&#65292;&#30456;&#27604;CPU&#23454;&#29616;&#22312;&#25191;&#34892;&#26102;&#38388;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.10037</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65306;CPU&#21644;GPU&#23454;&#29616;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison. (arXiv:2308.10037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10037
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#36923;&#36753;&#22238;&#24402;&#31639;&#27861;&#65292;&#30456;&#27604;CPU&#23454;&#29616;&#22312;&#25191;&#34892;&#26102;&#38388;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#24212;&#23545;&#22823;&#25968;&#25454;&#38598;&#20013;&#23545;&#26356;&#24555;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#26159;&#23545;X. Zou&#31561;&#20154;&#25552;&#20986;&#30340;&#24182;&#34892;&#26799;&#24230;&#19979;&#38477;&#36923;&#36753;&#22238;&#24402;&#31639;&#27861;&#30340;&#30452;&#25509;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;CPU&#30340;&#23454;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;GPU&#30340;LR&#22312;&#25191;&#34892;&#26102;&#38388;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#30340;f1&#20998;&#25968;&#12290;&#23545;&#20110;&#23454;&#26102;&#39044;&#27979;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#35782;&#21035;&#12289;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#21644;&#27450;&#35784;&#26816;&#27979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#19968;&#20010;&#21487;&#30452;&#25509;&#20351;&#29992;&#30340;Python&#24211;&#20013;&#23454;&#29616;&#65292;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#21462;&#65306;https://github.com/NechbaMohammed/SwiftLogisticReg
&lt;/p&gt;
&lt;p&gt;
We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#25991;&#26469;&#35782;&#21035;&#36710;&#36742;&#21163;&#25345;&#20107;&#20214;&#12290;&#36890;&#36807;&#24212;&#29992;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#21644;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;CBLOF&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;F1-Score&#31561;&#25351;&#26631;&#19978;&#31245;&#24494;&#20248;&#20110;KNN&#26041;&#27861;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#39318;&#36873;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10036</link><description>&lt;p&gt;
&#29992;&#20110;&#30830;&#23450;&#36710;&#36742;&#21163;&#25345;&#25512;&#25991;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets. (arXiv:2308.10036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#25991;&#26469;&#35782;&#21035;&#36710;&#36742;&#21163;&#25345;&#20107;&#20214;&#12290;&#36890;&#36807;&#24212;&#29992;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#21644;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;CBLOF&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;F1-Score&#31561;&#25351;&#26631;&#19978;&#31245;&#24494;&#20248;&#20110;KNN&#26041;&#27861;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21335;&#38750;&#65292;&#36710;&#36742;&#21163;&#25345;&#38382;&#39064;&#26085;&#30410;&#20005;&#37325;&#12290;&#36825;&#23548;&#33268;&#26053;&#34892;&#32773;&#26102;&#21051;&#25285;&#24515;&#25104;&#20026;&#36825;&#31181;&#20107;&#20214;&#30340;&#21463;&#23475;&#32773;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#25512;&#25991;&#26469;&#35782;&#21035;&#21163;&#25345;&#20107;&#20214;&#65292;&#37319;&#29992;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#33719;&#21462;&#21253;&#21547;&#20851;&#38190;&#23383;"&#21163;&#25345;"&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#36827;&#34892;&#23384;&#20648;&#21644;&#22788;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#65306;1&#65289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#65307;2&#65289;&#22522;&#20110;&#32858;&#31867;&#30340;&#24322;&#24120;&#22240;&#23376;&#65288;CBLOF&#65289;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;KNN&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20026;89%&#65292;&#32780;CBLOF&#30340;&#20934;&#30830;&#29575;&#20026;90%&#12290;CBLOF&#26041;&#27861;&#36824;&#33021;&#22815;&#33719;&#24471;0.8&#30340;F1-Score&#65292;&#32780;KNN&#30340;&#24471;&#20998;&#20026;0.78&#12290;&#22240;&#27492;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#26377;&#36731;&#24494;&#24046;&#24322;&#65292;&#26377;&#21033;&#20110;CBLOF&#65292;&#34987;&#36873;&#20026;&#30830;&#23450;&#30456;&#20851;&#21163;&#25345;&#20107;&#20214;&#30340;&#39318;&#36873;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#29942;&#39048;&#23485;&#24230;&#23545;&#22522;&#20110;StarGAN&#30340;&#21809;&#27468;&#25216;&#24039;&#36716;&#25442;&#21512;&#25104;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#21457;&#29616;&#65292;&#26356;&#23485;&#30340;&#29942;&#39048;&#21487;&#20197;&#25552;&#39640;&#21457;&#38899;&#28165;&#26224;&#24230;&#65292;&#20294;&#19981;&#19968;&#23450;&#33021;&#22815;&#22686;&#21152;&#19982;&#30446;&#26631;&#22768;&#38899;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.10021</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#29942;&#39048;&#23485;&#24230;&#30340;&#22522;&#20110;StarGAN&#30340;&#21809;&#27468;&#25216;&#24039;&#36716;&#25442;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion. (arXiv:2308.10021v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#29942;&#39048;&#23485;&#24230;&#23545;&#22522;&#20110;StarGAN&#30340;&#21809;&#27468;&#25216;&#24039;&#36716;&#25442;&#21512;&#25104;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#21457;&#29616;&#65292;&#26356;&#23485;&#30340;&#29942;&#39048;&#21487;&#20197;&#25552;&#39640;&#21457;&#38899;&#28165;&#26224;&#24230;&#65292;&#20294;&#19981;&#19968;&#23450;&#33021;&#22815;&#22686;&#21152;&#19982;&#30446;&#26631;&#22768;&#38899;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21809;&#27468;&#25216;&#24039;&#36716;&#25442;&#65288;STC&#65289;&#26159;&#25351;&#22312;&#20445;&#25345;&#21407;&#22987;&#27468;&#25163;&#36523;&#20221;&#12289;&#26059;&#24459;&#21644;&#35821;&#35328;&#32452;&#25104;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#19968;&#31181;&#22768;&#20048;&#25216;&#24039;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#22768;&#20048;&#25216;&#24039;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;STC&#30740;&#31350;&#20197;&#21450;&#19968;&#33324;&#30340;&#27468;&#22768;&#36716;&#25442;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36827;&#34892;&#36716;&#25442;&#65292;&#20294;CAE&#30340;&#29942;&#39048;&#23485;&#24230;&#23545;&#21512;&#25104;&#36136;&#37327;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#22810;&#22495;STC&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#20102;WORLD&#22768;&#30721;&#22120;&#34920;&#24449;&#21644;CAE&#26550;&#26500;&#12290;&#25105;&#20204;&#25913;&#21464;&#20102;CAE&#30340;&#29942;&#39048;&#23485;&#24230;&#65292;&#24182;&#20027;&#35266;&#35780;&#20272;&#20102;&#36716;&#25442;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#21253;&#25324;&#22235;&#20301;&#27468;&#25163;&#21644;&#22235;&#31181;&#21809;&#27468;&#25216;&#24039;&#65288;&#33016;&#22768;&#12289;&#20551;&#22768;&#12289;&#22070;&#21713;&#22768;&#21644;&#21736;&#38899;&#22768;&#65289;&#30340;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#23485;&#30340;&#29942;&#39048;&#23545;&#20110;&#21457;&#38899;&#28165;&#26224;&#24230;&#26356;&#22909;&#65292;&#20294;&#19981;&#19968;&#23450;&#23548;&#33268;&#26356;&#25509;&#36817;&#30446;&#26631;&#22768;&#38899;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#21305;&#37197;&#30340;&#21322;&#38544;&#24335;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SIVI-SM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21322;&#38544;&#24335;&#21464;&#20998;&#23478;&#26063;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;&#19981;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#23494;&#24230;&#26469;&#23454;&#29616;&#19982;MCMC&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#36125;&#21494;&#26031;&#25512;&#26029;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;ELBO&#30340;SIVI&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.10014</link><description>&lt;p&gt;
&#36890;&#36807;&#24471;&#20998;&#21305;&#37197;&#23454;&#29616;&#30340;&#21322;&#38544;&#24335;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Semi-Implicit Variational Inference via Score Matching. (arXiv:2308.10014v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#21305;&#37197;&#30340;&#21322;&#38544;&#24335;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SIVI-SM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#21322;&#38544;&#24335;&#21464;&#20998;&#23478;&#26063;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;&#19981;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#23494;&#24230;&#26469;&#23454;&#29616;&#19982;MCMC&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#36125;&#21494;&#26031;&#25512;&#26029;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;ELBO&#30340;SIVI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#38544;&#24335;&#21464;&#20998;&#25512;&#26029;&#65288;SIVI&#65289;&#36890;&#36807;&#32771;&#34385;&#20197;&#23618;&#27425;&#26041;&#24335;&#23450;&#20041;&#30340;&#38544;&#24335;&#21464;&#20998;&#20998;&#24067;&#65292;&#26497;&#22823;&#22320;&#20016;&#23500;&#20102;&#21464;&#20998;&#23478;&#26063;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21464;&#20998;&#20998;&#24067;&#30340;&#19981;&#21487;&#35745;&#31639;&#23494;&#24230;&#65292;&#24403;&#21069;&#30340;SIVI&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26367;&#20195;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#20869;&#24490;&#29615;MCMC&#36816;&#34892;&#20197;&#36827;&#34892;&#26080;&#20559;ELBO&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SIVI-SM&#65292;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#21305;&#37197;&#30340;&#26367;&#20195;&#35757;&#32451;&#30446;&#26631;&#30340;SIVI&#26032;&#26041;&#27861;&#12290;&#21033;&#29992;&#21322;&#38544;&#24335;&#21464;&#20998;&#23478;&#26063;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#20801;&#35768;&#20351;&#29992;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#33258;&#28982;&#22788;&#29702;&#19981;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#23494;&#24230;&#30340;&#26368;&#23567;&#26368;&#22823;&#24418;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;SIVI-SM&#22312;&#21508;&#31181;&#36125;&#21494;&#26031;&#25512;&#26029;&#20219;&#21153;&#20013;&#20960;&#20046;&#19982;MCMC&#30340;&#20934;&#30830;&#24615;&#19968;&#33268;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20110;ELBO&#30340;SIVI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09971</link><description>&lt;p&gt;
&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#36873;&#25321;&#24615;&#28304;&#20219;&#21153;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#21363;&#20351;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#21518;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#24615;&#33021;&#20173;&#28982;&#20445;&#30041;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#34987;&#35270;&#20026;&#25152;&#26377;&#32773;&#30340;&#31169;&#26377;&#36130;&#20135;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#23547;&#27714;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#24191;&#20041;&#24615;&#33021;&#30340;&#29420;&#21344;&#26435;&#26159;&#21512;&#29702;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25286;&#21368;&#24335;&#36801;&#31227;&#23398;&#20064;(DTL)&#30340;&#26032;&#33539;&#24335;&#65292;&#23427;&#21482;&#22788;&#29702;&#28304;&#20219;&#21153;&#32780;&#19981;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#30693;&#35782;&#30340;&#36951;&#24536;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#26799;&#24230;&#30896;&#25758;&#25439;&#22833;(GC&#25439;&#22833;)&#12290;GC&#25439;&#22833;&#36890;&#36807;&#24341;&#23548;&#19981;&#21516;mini-batches&#30340;&#26799;&#24230;&#21521;&#37327;&#26397;&#19981;&#21516;&#26041;&#21521;&#21069;&#36827;&#65292;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#28304;&#30693;&#35782;&#12290;&#27169;&#22411;&#26159;&#21542;&#25104;&#21151;&#36951;&#24536;&#28304;&#20219;&#21153;&#36890;&#36807;&#8220;&#32972;&#39534;&#24335;&#23398;&#20064;&#20934;&#30830;&#24615;&#8221;(PL&#20934;&#30830;&#24615;)&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09970</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09970
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20869;&#24515;&#29420;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65288;IMMO&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#28151;&#21512;&#34701;&#21512;&#21644;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#38656;&#35201;AI&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;LLM&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20043;&#38388;&#30340;&#28151;&#21512;&#34701;&#21512;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#39318;&#20808;&#34987;VLM&#36716;&#21270;&#20026;&#35821;&#35328;&#25551;&#36848;&#65292;&#25104;&#20026;LLM&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#30340;&#36755;&#20837;&#65307;&#65288;2&#65289;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#65292;&#20854;&#20013;&#35270;&#35273;&#36755;&#20837;&#34987;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30417;&#30563;&#24494;&#35843;&#23558;&#20854;&#25237;&#24433;&#21040;LLM&#30340;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24456;&#38590;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#29305;&#24449;&#23545;&#40784;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20869;&#24515;&#29420;&#30333;&#22810;&#27169;&#24577;&#20248;&#21270;&#65288;IMMO&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#24605;&#32500;&#36807;&#31243;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating in
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09965</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09965
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#23545;&#40784;&#30340;&#30072;&#21464;&#24863;&#30693;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36935;&#21040;&#26410;&#30693;&#23545;&#35937;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#26631;&#20934;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#37197;&#22791;&#24322;&#24120;&#24863;&#30693;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#21512;&#25104;&#30340;&#31163;&#32676;&#20998;&#24067;&#65288;OoD&#65289;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20943;&#23567;OoD&#25968;&#25454;&#21644;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#22521;&#35757;&#36807;&#31243;&#20013;&#21487;&#33021;&#20316;&#20026;&#26126;&#26174;&#25463;&#24452;&#30340;&#39118;&#26684;&#24046;&#24322;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;OoD&#21512;&#25104;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24494;&#35843;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#22320;&#23548;&#33268;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#8220;&#32473;&#23450;&#31867;&#21035;&#20043;&#22806;&#8221;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21033;&#29992;&#27599;&#20010;&#20687;&#32032;&#30340;OoD&#20998;&#25968;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#12290;&#36890;&#36807;&#26368;&#23567;&#30340;&#24494;&#35843;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#20351;&#24471;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#24322;&#24120;&#20998;&#21106;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#31649;&#29702;ML&#27169;&#22411;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#21160;&#24577;&#27169;&#22411;&#20999;&#25442;&#20197;&#25552;&#39640;&#25972;&#20307;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.09960</link><description>&lt;p&gt;
&#36890;&#36807;QoS&#24863;&#30693;&#30340;&#27169;&#22411;&#20999;&#25442;&#23454;&#29616;&#33258;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching. (arXiv:2308.09960v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09960
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#31649;&#29702;ML&#27169;&#22411;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#21160;&#24577;&#27169;&#22411;&#20999;&#25442;&#20197;&#25552;&#39640;&#25972;&#20307;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#23548;&#33268;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#31995;&#32479;&#65288;MLS&#65289;&#30340;&#20852;&#36215;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#36827;&#36825;&#20123;MLS&#36827;&#20837;&#29983;&#20135;&#20013;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24433;&#21709;&#25972;&#20307;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#21508;&#31181;&#36816;&#34892;&#26102;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#26469;&#33258;ML&#27169;&#22411;&#12289;&#36719;&#20214;&#32452;&#20214;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#33258;&#36866;&#24212;&#25216;&#26415;&#22312;&#31649;&#29702;&#36816;&#34892;&#26102;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;MLS&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24179;&#34913;&#22120;&#30340;&#27010;&#24565;&#65292;&#37325;&#28857;&#35299;&#20915;&#19982;ML&#27169;&#22411;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#31649;&#29702;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdaMLS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36825;&#19968;&#27010;&#24565;&#24182;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;MAPE-K&#24490;&#29615;&#20197;&#23454;&#29616;&#25345;&#32493;&#30340;MLS&#33258;&#36866;&#24212;&#12290;AdaMLS&#37319;&#29992;&#36731;&#37327;&#32423;&#26080;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#27169;&#22411;&#20999;&#25442;&#65292;&#20174;&#32780;&#30830;&#20445;&#19968;&#33268;&#30340;Qo
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent Qo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#38024;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#26432;&#27602;&#20135;&#21697;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20808;&#21069;&#26816;&#27979;&#21040;&#30340;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#20248;&#21270;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#65292;&#21516;&#26102;&#29983;&#25104;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#21487;&#25104;&#21151;&#29992;&#20110;&#20854;&#20182;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.09958</link><description>&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Adversarial Learning Techniques for Malware Detection. (arXiv:2308.09958v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#38024;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#26432;&#27602;&#20135;&#21697;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20808;&#21069;&#26816;&#27979;&#21040;&#30340;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#20248;&#21270;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#65292;&#21516;&#26102;&#29983;&#25104;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#21487;&#25104;&#21151;&#29992;&#20110;&#20854;&#20182;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#21160;&#21270;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#26174;&#31034;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#29983;&#25104;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#30340;Windows&#21487;&#25191;&#34892;&#25991;&#20214;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#20851;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#24212;&#29992;&#20110;&#36873;&#25321;&#30340;&#26432;&#27602;&#20135;&#21697;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20808;&#21069;&#26816;&#27979;&#21040;&#30340;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#20248;&#21270;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#23558;&#25991;&#20214;&#38169;&#35823;&#20998;&#31867;&#20026;&#33391;&#24615;&#12290;&#24050;&#30693;&#29983;&#25104;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#21487;&#25104;&#21151;&#29992;&#20110;&#20854;&#20182;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21033;&#29992;&#22522;&#22240;&#32452;&#21512;&#21487;&#20197;&#22686;&#21152;&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#23545;&#25239;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of gene
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#22240;&#26524;&#24615;&#30340;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#26377;&#21407;&#21017;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#26435;&#37325;&#26469;&#26368;&#23567;&#21270;&#38169;&#20998;&#31867;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20445;&#25345;&#20102;&#21407;&#22987;&#24615;&#33021;&#21644;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09955</link><description>&lt;p&gt;
&#26159;&#20462;&#21098;&#36824;&#26159;&#19981;&#20462;&#21098;&#65306;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#22240;&#26524;&#24615;&#30340;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#26377;&#21407;&#21017;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks. (arXiv:2308.09955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09955
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#22240;&#26524;&#24615;&#30340;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#26377;&#21407;&#21017;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#26435;&#37325;&#26469;&#26368;&#23567;&#21270;&#38169;&#20998;&#31867;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20445;&#25345;&#20102;&#21407;&#22987;&#24615;&#33021;&#21644;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36890;&#36807;&#21435;&#38500;&#26435;&#37325;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#26469;&#20943;&#23567;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65288;&#20462;&#21098;&#65289;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#65292;&#20462;&#21098;&#36890;&#24120;&#26159;&#36890;&#36807;&#22522;&#20110;&#22823;&#23567;&#31561;&#26631;&#20934;&#23545;&#26435;&#37325;&#36827;&#34892;&#25490;&#24207;&#25110;&#24809;&#32602;&#65292;&#24182;&#22312;&#37325;&#26032;&#35757;&#32451;&#21097;&#20313;&#26435;&#37325;&#20043;&#21069;&#21435;&#38500;&#20302;&#25490;&#21517;&#26435;&#37325;&#26469;&#23454;&#29616;&#30340;&#12290;&#20462;&#21098;&#31574;&#30053;&#20063;&#21487;&#20197;&#28041;&#21450;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#31070;&#32463;&#20803;&#65292;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#32593;&#32476;&#23610;&#23544;&#20943;&#23567;&#12290;&#25105;&#20204;&#23558;&#20462;&#21098;&#38382;&#39064;&#23450;&#24335;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#26435;&#37325;&#26469;&#26368;&#23567;&#21270;&#38169;&#20998;&#31867;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#20013;&#30340;&#28151;&#27788;&#27010;&#24565;&#65288;&#26446;&#20122;&#26222;&#35834;&#22827;&#25351;&#25968;&#65289;&#65292;&#36890;&#36807;&#26435;&#37325;&#26356;&#26032;&#24182;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#26469;&#35782;&#21035;&#36896;&#25104;&#38169;&#20998;&#31867;&#30340;&#22240;&#26524;&#24615;&#26435;&#37325;&#12290;&#36825;&#26679;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#20445;&#25345;&#20102;&#21407;&#22987;&#24615;&#33021;&#24182;&#20445;&#30041;&#20102;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09952</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#23547;&#25214;&#20986;&#29616;&#65306;&#21551;&#21457;&#20110;&#22240;&#26524;&#20986;&#29616;&#30340;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding emergence in data: causal emergence inspired dynamics learning. (arXiv:2308.09952v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24494;&#35266;&#23618;&#38754;&#30340;&#35266;&#27979;&#25968;&#25454;&#26080;&#27861;&#30452;&#25509;&#25429;&#25417;&#21040;&#20986;&#29616;&#34892;&#20026;&#21644;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23439;&#35266;&#23618;&#38754;&#20986;&#29616;&#21160;&#21147;&#23398;&#24182;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#37327;&#21270;&#20986;&#29616;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21253;&#21547;&#20986;&#29616;&#28508;&#22312;&#31354;&#38388;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#20449;&#24687;&#65288;EI&#65289;&#26469;&#33719;&#24471;&#19968;&#20010;&#20855;&#26377;&#26356;&#24378;&#22240;&#26524;&#25928;&#26524;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#23545;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#19981;&#20165;&#25104;&#21151;&#25429;&#25417;&#21040;&#20986;&#29616;&#27169;&#24335;&#65292;&#36824;&#23398;&#20064;&#20102;&#31895;&#31890;&#21270;&#31574;&#30053;&#24182;&#37327;&#21270;&#20102;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20986;&#29616;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#19981;&#21516;&#29615;&#22659;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26694;&#26550;&#22312;&#24314;&#27169;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09947</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Study on the effectiveness of AutoML in detecting cardiovascular disease. (arXiv:2308.09947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#21512;&#24182;&#30340;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#22312;&#24739;&#26377;&#24930;&#24615;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#32773;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#19988;&#26159;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#21253;&#25324;&#22312;&#21171;&#21160;&#24180;&#40836;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21457;&#23637;&#21644;&#24212;&#29992;&#20197;&#24739;&#32773;&#20026;&#23548;&#21521;&#30340;&#31995;&#32479;&#30340;&#30456;&#20851;&#24615;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#12290;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21487;&#20197;&#31616;&#21270;&#21644;&#21152;&#36895;AI/ML&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#22312;&#24739;&#32773;&#23548;&#21521;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#26159;&#20851;&#38190;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#21307;&#30103;&#19987;&#23478;&#30340;&#24212;&#29992;&#29992;&#25143;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26694;&#26550;&#21644;&#19977;&#31181;&#22330;&#26223;&#65292;&#20801;&#35768;&#23558;&#26469;&#33258;UCI&#26426;&#22120;&#23398;&#20064;&#20179;&#24211;&#30340;&#20116;&#20010;&#24515;&#34880;&#31649;&#30142;&#30149;&#25351;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#24182;&#65292;&#22312;&#26816;&#27979;&#36825;&#31867;&#30142;&#30149;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#12290;&#30740;&#31350;&#20013;&#35843;&#26597;&#20102;&#19968;&#20010;&#20351;&#29992;&#21644;&#20248;&#21270;&#20102;&#36229;&#21442;&#25968;&#30340;AutoML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thir
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#21644;&#25104;&#21151;&#27835;&#30103;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24050;&#26377;&#25991;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.09945</link><description>&lt;p&gt;
&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#26816;&#27979;&#21644;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy. (arXiv:2308.09945v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#21644;&#25104;&#21151;&#27835;&#30103;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24050;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26159;&#31958;&#23615;&#30149;&#30340;&#20005;&#37325;&#24182;&#21457;&#30151;&#65292;&#22914;&#26524;&#19981;&#21450;&#26102;&#27835;&#30103;&#21487;&#33021;&#23548;&#33268;&#27704;&#20037;&#24615;&#22833;&#26126;&#12290;&#23545;&#35813;&#30142;&#30149;&#30340;&#26089;&#26399;&#21644;&#20934;&#30830;&#35786;&#26029;&#23545;&#20110;&#25104;&#21151;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#20174;&#20844;&#24320;&#26469;&#28304;&#33719;&#24471;&#30340;APTOS 2019&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;APTOS 2019&#19978;&#65292;&#23427;&#22312;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#26816;&#27979;&#21644;&#20998;&#32423;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#24050;&#26377;&#30340;&#25991;&#29486;&#12290;&#22312;&#20108;&#20998;&#31867;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;98.50&#65285;&#65292;&#25935;&#24863;&#24615;&#36798;&#21040;99.46&#65285;&#65292;&#29305;&#24322;&#24615;&#36798;&#21040;97.51&#65285;&#12290;&#22312;&#20998;&#32423;&#20013;&#65292;&#23427;&#36798;&#21040;&#20102;93.00&#65285;&#30340;&#24179;&#26041;&#21152;&#26435;Kappa&#20540;&#65292;&#20934;&#30830;&#29575;&#36824;&#26159;&#24456;&#39640;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#24320;&#25918;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#24378;OOD&#20462;&#21098;&#12289;&#21160;&#24577;&#21407;&#22411;&#25193;&#23637;&#21644;&#20998;&#24067;&#23545;&#40784;&#31561;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#21463;&#21040;&#24378;OOD&#25968;&#25454;&#27745;&#26579;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09942</link><description>&lt;p&gt;
&#20851;&#20110;&#24320;&#25918;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#31283;&#20581;&#24615;&#65306;&#33258;&#25105;&#35757;&#32451;&#19982;&#21160;&#24577;&#21407;&#22411;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion. (arXiv:2308.09942v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#24320;&#25918;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#24378;OOD&#20462;&#21098;&#12289;&#21160;&#24577;&#21407;&#22411;&#25193;&#23637;&#21644;&#20998;&#24067;&#23545;&#40784;&#31561;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#21463;&#21040;&#24378;OOD&#25968;&#25454;&#27745;&#26579;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20998;&#24067;&#19982;&#20302;&#24310;&#36831;&#30340;&#38382;&#39064;&#20419;&#20351;&#20102;&#23545;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;/&#36866;&#24212;&#65288;TTT/TTA&#65289;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#27880;&#37325;&#25552;&#39640;&#22312;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#19979;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#21463;&#21040;&#24378;&#26377;&#21147;&#30340;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#27745;&#26579;&#26102;&#26080;&#27861;&#20445;&#25345;&#24615;&#33021;&#65292;&#21363;&#24320;&#25918;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#65288;OWTTT&#65289;&#12290;&#36825;&#31181;&#22833;&#36133;&#20027;&#35201;&#26159;&#30001;&#20110;&#26080;&#27861;&#21306;&#20998;&#24378;ODD&#26679;&#26412;&#21644;&#24120;&#35268;&#24369;OOD&#26679;&#26412;&#12290;&#20026;&#20102;&#25552;&#39640;OWTTT&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#30340;&#24378;OOD&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#33258;&#25105;&#35757;&#32451;TTT&#26041;&#27861;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25193;&#23637;&#21407;&#22411;&#30340;&#26041;&#24335;&#65292;&#20197;&#34920;&#31034;&#24378;OOD&#26679;&#26412;&#65292;&#20197;&#25913;&#21892;&#24369;/&#24378;OOD&#25968;&#25454;&#30340;&#20998;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29992;&#20998;&#24067;&#23545;&#40784;&#26469;&#35268;&#33539;&#21270;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#19982;&#21160;&#24577;&#21407;&#22411;&#25193;&#23637;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21327;&#21516;&#26426;&#21046;&#30340;&#22810;&#21464;&#37327;&#30417;&#27979;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#25429;&#25417;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21450;&#20854;&#21382;&#21490;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#26381;&#21153;&#30340;&#23454;&#29992;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.09937</link><description>&lt;p&gt;
&#22312;&#32447;&#26381;&#21153;&#30340;&#22810;&#21464;&#37327;&#30417;&#27979;&#25351;&#26631;&#23454;&#29992;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services. (arXiv:2308.09937v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21327;&#21516;&#26426;&#21046;&#30340;&#22810;&#21464;&#37327;&#30417;&#27979;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#25429;&#25417;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21450;&#20854;&#21382;&#21490;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#26381;&#21153;&#30340;&#23454;&#29992;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#22312;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#37327;&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#65292;&#23545;&#22810;&#21464;&#37327;&#30417;&#27979;&#25351;&#26631;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21450;&#20854;&#21382;&#21490;&#27169;&#24335;&#22312;&#36861;&#27714;&#21450;&#26102;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#39640;&#25928;&#25429;&#25417;&#36825;&#31181;&#20449;&#24687;&#65292;&#26080;&#27861;&#28385;&#36275;&#24037;&#19994;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#26426;&#21046;&#30340;&#22810;&#21464;&#37327;&#30417;&#27979;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;CMAnomaly&#12290;&#25152;&#25552;&#20986;&#30340;&#21327;&#21516;&#26426;&#21046;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#25429;&#25417;&#29305;&#24449;&#21644;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#25104;&#23545;&#20132;&#20114;&#30340;&#26426;&#21046;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#27169;&#22411;&#26469;&#21033;&#29992;&#30417;&#27979;&#25351;&#26631;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20854;&#21382;&#21490;&#27169;&#24335;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensiv
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;East&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#26469;&#20248;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#23433;&#20840;&#21327;&#35758;&#26469;&#22788;&#29702;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09923</link><description>&lt;p&gt;
East: &#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09923
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;East&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#26469;&#20248;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#23433;&#20840;&#21327;&#35758;&#26469;&#22788;&#29702;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#26381;&#21153;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#30340;&#36755;&#20837;&#20250;&#27844;&#28431;&#32473;&#27169;&#22411;&#25552;&#20379;&#21830;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;Transformer&#25512;&#29702;&#22312;&#36825;&#31867;&#26381;&#21153;&#20013;&#38656;&#27714;&#37327;&#22823;&#12290;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;Transformer&#25512;&#29702;&#26469;&#35828;&#65292;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#23433;&#20840;&#21327;&#35758;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30740;&#31350;&#24471;&#19981;&#22810;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#23454;&#29992;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#23433;&#20840;&#21327;&#35758;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#32780;&#35328;&#24456;&#22256;&#38590;&#20294;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\emph{East}&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23433;&#20840;Transformer&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24536;&#21364;&#20998;&#27573;&#22810;&#39033;&#24335;&#27714;&#20540;&#31639;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#28608;&#27963;&#20989;&#25968;&#65292;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#23558;GELU&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36890;&#20449;&#37327;&#20998;&#21035;&#20943;&#23569;&#20102;1.5&#20493;&#21644;2.5&#20493;&#20197;&#19978;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;softmax&#21644;&#23618;&#24402;&#19968;&#21270;&#30340;&#23433;&#20840;&#21327;&#35758;&#65292;&#20197;&#24544;&#23454;&#22320;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#35775;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09909</link><description>&lt;p&gt;
&#19981;&#20877;&#37325;&#22797;&#25506;&#32034;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#35775;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20869;&#22312;&#21160;&#26426;&#24050;&#32463;&#25104;&#20026;&#25506;&#32034;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#35745;&#31639;&#35768;&#22810;&#20869;&#22312;&#22870;&#21169;&#20381;&#36182;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#20272;&#35745;&#21464;&#20998;&#21518;&#39564;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#31070;&#32463;&#32479;&#35745;&#36924;&#36817;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25361;&#25112;&#23450;&#20026;"&#22238;&#35775;"&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#21453;&#22797;&#25506;&#32034;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#26377;&#38480;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22870;&#21169;&#32553;&#25918;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#31283;&#23450;&#20808;&#21069;&#25506;&#32034;&#21306;&#22495;&#20869;&#30340;&#20869;&#22312;&#22870;&#21169;&#30340;&#26126;&#26174;&#27874;&#21160;&#65292;&#24182;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#26377;&#25928;&#22320;&#25233;&#21046;&#22238;&#35775;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#24378;&#35843;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20687;Google Research Football&#21644;StarCraft II&#24494;&#25805;&#20316;&#20219;&#21153;&#36825;&#26679;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#35774;&#32622;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21478;&#19968;&#20010;&#21253;&#21547;&#32570;&#22833;&#27979;&#37327;&#20540;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22635;&#34917;&#36328;&#25968;&#25454;&#38598;&#33041;&#37096;&#27979;&#37327;&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09907</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#22635;&#34917;&#33041;&#37096;&#27979;&#37327;&#20540;
&lt;/p&gt;
&lt;p&gt;
Imputing Brain Measurements Across Data Sets via Graph Neural Networks. (arXiv:2308.09907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21478;&#19968;&#20010;&#21253;&#21547;&#32570;&#22833;&#27979;&#37327;&#20540;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22635;&#34917;&#36328;&#25968;&#25454;&#38598;&#33041;&#37096;&#27979;&#37327;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#32467;&#26500;&#24615;MRI&#25968;&#25454;&#38598;&#21487;&#33021;&#19981;&#21253;&#21547;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33041;&#37096;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#20855;&#20307;&#27979;&#37327;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22635;&#34917;&#32570;&#22833;&#27979;&#37327;&#20540;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#25554;&#34917;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#21253;&#21547;&#32570;&#22833;&#27979;&#37327;&#20540;&#24182;&#19982;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#20849;&#20139;&#26576;&#20123;ROI&#27979;&#37327;&#30340;&#21478;&#19968;&#20010;&#65288;&#20844;&#24320;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#20849;&#20139;&#30340;&#27979;&#37327;&#20013;&#39044;&#27979;&#32570;&#22833;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones an
&lt;/p&gt;</description></item><item><title>DPMAC&#31639;&#27861;&#22312;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#20102;&#24046;&#20998;&#38544;&#31169;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20195;&#29702;&#37197;&#22791;&#26412;&#22320;&#28040;&#24687;&#21457;&#36865;&#22120;&#24182;&#26681;&#25454;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#35843;&#25972;&#28040;&#24687;&#20998;&#24067;&#65292;&#20445;&#25252;&#20102;&#20010;&#20307;&#20195;&#29702;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.09902</link><description>&lt;p&gt;
DPMAC&#65306;&#24046;&#20998;&#38544;&#31169;&#36890;&#20449;&#22312;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.09902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09902
&lt;/p&gt;
&lt;p&gt;
DPMAC&#31639;&#27861;&#22312;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#20102;&#24046;&#20998;&#38544;&#31169;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20195;&#29702;&#37197;&#22791;&#26412;&#22320;&#28040;&#24687;&#21457;&#36865;&#22120;&#24182;&#26681;&#25454;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#35843;&#25972;&#28040;&#24687;&#20998;&#24067;&#65292;&#20445;&#25252;&#20102;&#20010;&#20307;&#20195;&#29702;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#23545;&#20110;&#20154;&#31867;&#31038;&#20250;&#21644;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;(MARL)&#20013;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#20154;&#31867;&#22312;&#19982;&#20182;&#20154;&#20132;&#27969;&#26102;&#20063;&#24076;&#26395;&#20445;&#25252;&#33258;&#24049;&#30340;&#38544;&#31169;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#32771;&#34385;&#22312;&#29616;&#26377;&#30340;MARL&#30740;&#31350;&#20013;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#22810;&#20195;&#29702;&#36890;&#20449;(DPMAC)&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20195;&#29702;&#37197;&#22791;&#20855;&#26377;&#20005;&#26684;(&#949;,&#948;)-&#24046;&#20998;&#38544;&#31169;(DP)&#20445;&#35777;&#30340;&#26412;&#22320;&#28040;&#24687;&#21457;&#36865;&#22120;&#26469;&#20445;&#25252;&#20010;&#20307;&#20195;&#29702;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#19982;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#24120;&#29992;&#30340;&#30452;&#25509;&#32473;&#28040;&#24687;&#21152;&#20837;&#39044;&#23450;&#20041;&#30340;DP&#22122;&#22768;&#19981;&#21516;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20195;&#29702;&#37319;&#29992;&#20102;&#19968;&#20010;&#38543;&#26426;&#28040;&#24687;&#21457;&#36865;&#22120;&#65292;&#24182;&#23558;DP&#35201;&#27714;&#34701;&#20837;&#21457;&#36865;&#22120;&#20013;&#65292;&#33258;&#21160;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#28040;&#24687;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;DP&#22122;&#22768;&#24102;&#26469;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#20316;MARL&#20013;&#23384;&#22312;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#22635;&#34917;&#39044;&#27979;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#26032;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22270;&#20998;&#26512;&#30340;&#24739;&#32773;&#20998;&#23618;&#24314;&#27169;&#65292;&#21482;&#20351;&#29992;&#30456;&#20284;&#24739;&#32773;&#30340;&#20449;&#24687;&#65292;&#22635;&#34917;&#32570;&#22833;&#20540;&#21644;&#39044;&#27979;&#27515;&#20129;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.09896</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#20837;&#38498;&#27515;&#20129;&#39118;&#38505;&#24314;&#27169;&#32593;&#32476;&#30340;&#22635;&#34917;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs. (arXiv:2308.09896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#22635;&#34917;&#39044;&#27979;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#26032;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22270;&#20998;&#26512;&#30340;&#24739;&#32773;&#20998;&#23618;&#24314;&#27169;&#65292;&#21482;&#20351;&#29992;&#30456;&#20284;&#24739;&#32773;&#30340;&#20449;&#24687;&#65292;&#22635;&#34917;&#32570;&#22833;&#20540;&#21644;&#39044;&#27979;&#27515;&#20129;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30005;&#23376;&#30149;&#21382;&#65288;EHRs&#65289;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#21463;&#21040;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#26679;&#30340;&#39044;&#27979;&#23558;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#26089;&#26399;&#35686;&#21578;&#65292;&#20197;&#20415;&#21450;&#26102;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#12290;&#36825;&#20010;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;EHR&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#19981;&#35268;&#21017;&#30340;&#65292;&#19981;&#20165;&#23384;&#22312;&#35768;&#22810;&#32570;&#22833;&#20540;&#65292;&#32780;&#19988;&#21307;&#30103;&#35760;&#24405;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#20063;&#19981;&#19968;&#33268;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#21464;&#37327;&#30456;&#20851;&#24615;&#26469;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24182;&#24314;&#31435;&#26102;&#38388;&#34928;&#20943;&#26426;&#21046;&#26469;&#22788;&#29702;&#36825;&#31181;&#19981;&#35268;&#21017;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#22635;&#34917;&#39044;&#27979;&#32593;&#32476;&#65292;&#29992;&#20110;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22635;&#34917;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#22270;&#20998;&#26512;&#30340;&#24739;&#32773;&#20998;&#23618;&#24314;&#27169;&#65292;&#20197;&#23545;&#30456;&#20284;&#24739;&#32773;&#36827;&#34892;&#20998;&#32452;&#12290;&#36825;&#20351;&#24471;&#21482;&#20351;&#29992;&#30456;&#20284;&#24739;&#32773;&#30340;&#20449;&#24687;&#65292;&#38500;&#20102;&#20010;&#20154;&#24773;&#22659;&#20449;&#24687;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual inf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.09895</link><description>&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20195;&#30721;LLMs&#65288;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#24320;&#22987;&#23545;&#32534;&#31243;&#23454;&#36341;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20195;&#30721;LLMs&#36824;&#25104;&#20026;&#32534;&#31243;&#35821;&#35328;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20195;&#30721;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#20805;&#20998;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#12289;Python&#25110;JavaScript&#65289;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20687;OCaml&#21644;Racket&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#25552;&#39640;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#29992;&#20110;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;MultiPL-T&#65292;&#23427;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#23545;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#19982;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.09892</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection. (arXiv:2308.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09892
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#23545;&#20020;&#24202;&#35843;&#26597;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#19982;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#22823;&#37327;&#29305;&#24449;&#65292;&#32780;&#31034;&#20363;&#25968;&#37327;&#30456;&#23545;&#36739;&#20302;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35797;&#22270;&#20174;&#35843;&#26597;&#25968;&#25454;&#39044;&#27979;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#29305;&#24449;&#36873;&#25321;&#65292;&#23427;&#35797;&#22270;&#36873;&#25321;&#19968;&#20010;&#26368;&#20339;&#30340;&#29305;&#24449;&#23376;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#20449;&#24687;&#26469;&#28304;&#26159;&#29305;&#24449;&#30340;&#25991;&#26412;&#21517;&#31216;&#65292;&#36825;&#21487;&#33021;&#22312;&#35821;&#20041;&#19978;&#25351;&#31034;&#21738;&#20123;&#29305;&#24449;&#19982;&#30446;&#26631;&#32467;&#26524;&#30456;&#20851;&#12290;&#21487;&#20197;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35780;&#20272;&#29305;&#24449;&#21517;&#31216;&#21644;&#30446;&#26631;&#21517;&#31216;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#29983;&#25104;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20998;&#25968;&#65292;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;STS&#30452;&#25509;&#36873;&#25321;&#29305;&#24449;&#21644;&#22312;&#26368;&#23567;&#20887;&#20313;&#26368;&#22823;&#30456;&#20851;&#65288;mRMR&#65289;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#21021;&#27493;&#35843;&#26597;&#30340;&#32467;&#26524;&#35780;&#20272;&#20102;STS&#20316;&#20026;&#29305;&#24449;&#36873;&#25321;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.09890</link><description>&lt;p&gt;
&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;: &#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#20195;&#30721;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#65292;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22240;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;ICL&#25216;&#26415;&#22312;&#19981;&#26356;&#26032;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#21040;&#25552;&#31034;&#20013;&#21363;&#21487;&#23454;&#29616;&#22522;&#20110;&#35268;&#21017;&#30340;&#39640;&#20934;&#30830;&#24615;&#25512;&#29702;&#12290;&#34429;&#28982;ICL&#26159;&#19968;&#20010;&#21457;&#23637;&#20013;&#30340;&#39046;&#22495;&#65292;&#36824;&#26377;&#35768;&#22810;&#26410;&#35299;&#31572;&#30340;&#38382;&#39064;&#65292;&#20294;LLMs&#26412;&#36523;&#20316;&#20026;&#25512;&#29702;&#27169;&#22411;&#20284;&#20046;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#26126;&#30830;&#25351;&#20986;"&#24402;&#32435;&#20559;&#24046;"&#30340;&#25512;&#29702;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20195;&#30721;&#29983;&#25104;&#20063;&#26159;LLMs&#30340;&#19968;&#39033;&#37325;&#35201;&#24212;&#29992;&#12290;&#20195;&#30721;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#22823;&#22823;&#25552;&#39640;&#65292;&#20351;&#24471;&#21363;&#20351;&#38750;&#24037;&#31243;&#24072;&#20063;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#25191;&#34892;&#25152;&#38656;&#20219;&#21153;&#30340;&#20195;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24402;&#32435;&#20559;&#24046;&#23398;&#20064;&#65288;IBL&#65289;&#8221;&#65292;&#23427;&#32467;&#21512;&#20102;ICL&#21644;&#20195;&#30721;&#29983;&#25104;&#30340;&#25216;&#26415;&#12290;IBL&#30340;&#24605;&#24819;&#24456;&#30452;&#35266;&#12290;&#19982;ICL&#31867;&#20284;&#65292;IBL&#23558;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#36890;&#29992;&#23545;&#25239;&#27700;&#21360;&#65288;DUAW&#65289;&#65292;&#36890;&#36807;&#24178;&#25200;&#23450;&#21046;&#36807;&#31243;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20445;&#25252;&#21508;&#31181;&#29256;&#26412;&#30340;SD&#27169;&#22411;&#20013;&#30340;&#20247;&#22810;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#30452;&#25509;&#22788;&#29702;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#30340;&#24517;&#35201;&#24615;&#65292;&#20445;&#25252;&#22270;&#20687;&#30340;&#20445;&#23494;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09889</link><description>&lt;p&gt;
DUAW&#65306;&#38024;&#23545;&#31283;&#23450;&#25193;&#25955;&#23450;&#21046;&#30340;&#26080;&#25968;&#25454;&#36890;&#29992;&#23545;&#25239;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization. (arXiv:2308.09889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09889
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#36890;&#29992;&#23545;&#25239;&#27700;&#21360;&#65288;DUAW&#65289;&#65292;&#36890;&#36807;&#24178;&#25200;&#23450;&#21046;&#36807;&#31243;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20445;&#25252;&#21508;&#31181;&#29256;&#26412;&#30340;SD&#27169;&#22411;&#20013;&#30340;&#20247;&#22810;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#30452;&#25509;&#22788;&#29702;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#30340;&#24517;&#35201;&#24615;&#65292;&#20445;&#25252;&#22270;&#20687;&#30340;&#20445;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#23450;&#21046;&#26041;&#27861;&#20351;&#29992;&#25143;&#33021;&#22815;&#20010;&#24615;&#21270;SD&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;AI&#33402;&#26415;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#20801;&#35768;&#20010;&#20154;&#20174;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#20013;&#25220;&#34989;&#29305;&#23450;&#30340;&#39118;&#26684;&#25110;&#20027;&#39064;&#65292;&#36825;&#24341;&#36215;&#20102;&#23545;&#28508;&#22312;&#29256;&#26435;&#20405;&#26435;&#30340;&#37325;&#22823;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24418;&#30340;&#26080;&#25968;&#25454;&#36890;&#29992;&#23545;&#25239;&#27700;&#21360;&#65288;DUAW&#65289;&#65292;&#26088;&#22312;&#20445;&#25252;&#21508;&#31181;&#29256;&#26412;&#30340;SD&#27169;&#22411;&#20013;&#30340;&#20247;&#22810;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#12290;&#39318;&#20808;&#65292;DUAW&#34987;&#35774;&#35745;&#29992;&#20110;&#24178;&#25200;SD&#23450;&#21046;&#36807;&#31243;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#20854;&#27425;&#65292;DUAW&#22312;&#26080;&#25968;&#25454;&#30340;&#29615;&#22659;&#19979;&#25805;&#20316;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#39044;&#35757;&#32451;&#30340;SD&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#30452;&#25509;&#22788;&#29702;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#30340;&#24517;&#35201;&#24615;&#65292;&#20174;&#32780;&#20445;&#25252;&#23427;&#20204;&#30340;&#20445;&#23494;&#24615;&#12290;&#19968;&#26086;&#21046;&#20316;&#23436;&#25104;&#65292;DUAW&#21487;&#20197;&#26080;&#27861;&#23519;&#35273;&#22320;&#38598;&#25104;&#21040;&#22823;&#37327;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20687;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted image
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#26469;&#20272;&#35745;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;UEEG-MCMC&#21644;BEEG-AP&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09888</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design. (arXiv:2308.09888v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#26469;&#20272;&#35745;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;UEEG-MCMC&#21644;BEEG-AP&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26088;&#22312;&#25214;&#21040;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26368;&#20339;&#23454;&#39564;&#26465;&#20214;&#65292;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#20248;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#20248;&#21270;EIG&#65292;&#24448;&#24448;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#65292;&#22240;&#27492;&#20272;&#35745;EIG&#30340;&#26799;&#24230;&#33021;&#21147;&#23545;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#20272;&#35745;EIG&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;EIG&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;EIG&#26799;&#24230;&#30340;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;EIG&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;UEEG-MCMC&#21033;&#29992;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#29983;&#25104;&#30340;&#21518;&#39564;&#26679;&#26412;&#26469;&#20272;&#35745;EIG&#26799;&#24230;&#65292;&#32780;BEEG-AP&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#21453;&#22797;&#20351;&#29992;&#21442;&#25968;&#26679;&#26412;&#26469;&#23454;&#29616;&#39640;&#27169;&#25311;&#25928;&#29575;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#33021;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31574;&#30053;&#21644;&#21305;&#37197;&#22522;&#20934;&#30340;&#26367;&#20195;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#20013;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09887</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibrating Uncertainty for Semi-Supervised Crowd Counting. (arXiv:2308.09887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31574;&#30053;&#21644;&#21305;&#37197;&#22522;&#20934;&#30340;&#26367;&#20195;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#20013;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#20851;&#38190;&#26159;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26657;&#20934;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26377;&#30417;&#30563;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31574;&#30053;&#36890;&#36807;&#19968;&#20010;&#26367;&#20195;&#20989;&#25968;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#30830;&#20445;&#20102;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#30830;&#23450;&#24615;&#24471;&#21040;&#33391;&#22909;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21305;&#37197;&#30340;&#20197;&#34917;&#19969;&#20026;&#21333;&#20301;&#30340;&#26367;&#20195;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#36817;&#20284;&#20154;&#32676;&#35745;&#25968;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27880;&#37325;&#32454;&#33410;&#65292;&#21516;&#26102;&#20445;&#25345;&#36866;&#24403;&#30340;&#32454;&#31890;&#24230;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#28385;&#36275;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#20013;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.09884</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26694;&#26550;&#65306;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case. (arXiv:2308.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09884
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#28385;&#36275;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#20013;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#20840;&#29699;&#20851;&#27880;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;LLMs&#30340;&#26377;&#25928;&#24615;&#21487;&#24402;&#22240;&#20110;&#20854;&#29992;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21363;transformers&#12290;Transformer&#27169;&#22411;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#39034;&#24207;&#30340;&#65292;&#21487;&#20197;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#12290;&#39044;&#27979;&#23398;&#39046;&#22495;&#23545;&#31995;&#32479;&#20581;&#24247;&#31649;&#29702;&#21644;&#36866;&#24403;&#30340;&#32500;&#25252;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#26426;&#22120;&#21097;&#20313;&#21487;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#36827;&#34892;&#21487;&#38752;&#20272;&#35745;&#20855;&#26377;&#33410;&#30465;&#25104;&#26412;&#30340;&#28508;&#21147;&#12290;&#36825;&#21253;&#25324;&#36991;&#20813;&#26426;&#22120;&#31361;&#28982;&#25925;&#38556;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#35774;&#22791;&#65292;&#24182;&#20316;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-Transformer&#26550;&#26500;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all f
&lt;/p&gt;</description></item><item><title>Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.09883</link><description>&lt;p&gt;
Flamingo: &#22810;&#36718;&#21333;&#26381;&#21153;&#22120;&#23433;&#20840;&#32858;&#21512;&#21450;&#20854;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09883
&lt;/p&gt;
&lt;p&gt;
Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Flamingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#22312;&#23433;&#20840;&#32858;&#21512;&#20013;&#65292;&#26381;&#21153;&#22120;&#23545;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#36755;&#20837;&#36827;&#34892;&#27714;&#21644;&#65292;&#24182;&#22312;&#19981;&#20102;&#35299;&#20010;&#20307;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#32467;&#26524;&#65292;&#20165;&#33021;&#25512;&#26029;&#20986;&#26368;&#32456;&#24635;&#21644;&#12290;Flamingo&#19987;&#27880;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#36718;&#35774;&#32622;&#65292;&#20854;&#20013;&#25191;&#34892;&#22810;&#20010;&#36830;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#27714;&#21644;&#65288;&#24179;&#22343;&#65289;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#21327;&#35758;&#65288;&#20363;&#22914;Bell&#31561;&#20154;&#30340;CCS '20&#65289;&#20165;&#36866;&#29992;&#20110;&#21333;&#36718;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#37325;&#22797;&#35813;&#21327;&#35758;&#26469;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#30340;&#35774;&#32622;&#12290;Flamingo&#28040;&#38500;&#20102;&#20043;&#21069;&#21327;&#35758;&#27599;&#36718;&#35774;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;&#20197;&#30830;&#20445;&#22914;&#26524;&#23458;&#25143;&#31471;&#22312;&#27714;&#21644;&#36807;&#31243;&#20013;&#31163;&#24320;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;Flamingo&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#25152;&#35859;&#30340;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#65292;&#27492;&#27010;&#24565;&#30001;Bell&#31561;&#20154;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21453;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26367;&#20195;&#26426;&#21046;&#21644;&#23450;&#20041;&#34394;&#20551;&#26631;&#31614;&#26469;&#35299;&#20915;&#29983;&#25104;&#22120;&#21453;&#23398;&#20064;&#24102;&#26469;&#30340;&#36830;&#32493;&#24615;&#21644;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09881</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks Unlearning. (arXiv:2308.09881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21453;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26367;&#20195;&#26426;&#21046;&#21644;&#23450;&#20041;&#34394;&#20551;&#26631;&#31614;&#26469;&#35299;&#20915;&#29983;&#25104;&#22120;&#21453;&#23398;&#20064;&#24102;&#26469;&#30340;&#36830;&#32493;&#24615;&#21644;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21450;&#25968;&#25454;&#28389;&#29992;&#19985;&#38395;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#20154;&#20204;&#23545;&#20010;&#20154;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#24182;&#20513;&#23548;&#26377;&#26435;&#21033;&#21024;&#38500;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#26426;&#22120;&#21453;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#25830;&#38500;&#35757;&#32451;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#22312;&#20998;&#31867;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#21253;&#25324;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#29420;&#29305;&#26550;&#26500;&#65292;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#30740;&#31350;&#36824;&#23384;&#22312;&#38480;&#21046;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#22120;&#30340;&#21453;&#23398;&#20064;&#65292;&#22240;&#20026;&#35813;&#36807;&#31243;&#21487;&#33021;&#20250;&#25200;&#20081;&#28508;&#22312;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#36825;&#31181;&#25200;&#20081;&#21487;&#33021;&#20250;&#22312;&#21453;&#23398;&#20064;&#21518;&#38477;&#20302;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#22914;&#20309;&#20026;&#21453;&#23398;&#20064;&#22270;&#20687;&#23450;&#20041;&#21028;&#21035;&#22120;&#24212;&#25191;&#34892;&#30340;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26367;&#20195;&#26426;&#21046;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#20010;&#34394;&#20551;&#26631;&#31614;&#26469;&#26377;&#25928;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09878</link><description>&lt;p&gt;
DatasetEquity: &#25152;&#26377;&#26679;&#26412;&#26159;&#21542;&#24179;&#31561;&#65311;&#22312;&#25968;&#25454;&#38598;&#20013;&#36861;&#27714;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets. (arXiv:2308.09878v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#19981;&#24179;&#34913;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24402;&#22240;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#25104;&#26412;&#12289;&#26631;&#31614;&#30340;&#22256;&#38590;&#20197;&#21450;&#25968;&#25454;&#30340;&#22320;&#29702;&#20998;&#24067;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#22270;&#20687;&#22806;&#35266;&#24341;&#36215;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#19982;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#30340;&#20998;&#31867;&#20998;&#24067;&#30456;&#27604;&#65292;&#22270;&#20687;&#22806;&#35266;&#23637;&#31034;&#20102;&#36229;&#20986;&#31867;&#21035;&#26631;&#31614;&#25152;&#25552;&#20379;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#20174;&#21407;&#22987;&#20687;&#32032;&#20013;&#25552;&#21462;&#30340;&#28145;&#23618;&#24863;&#30693;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#22806;&#35266;&#30340;&#28145;&#23618;&#24863;&#30693;&#23884;&#20837;&#21644;&#32858;&#31867;&#35745;&#31639;&#26679;&#26412;&#30340;&#20284;&#28982;&#24615;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20284;&#28982;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;\textbf{&#24191;&#20041;&#32858;&#28966;&#25439;&#22833;}&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed \textbf{Generalized Focal Loss} function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness a
&lt;/p&gt;</description></item><item><title>Skill Transformer&#26159;&#19968;&#20010;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#21644;&#25216;&#33021;&#27169;&#22359;&#21270;&#65292;&#23427;&#33021;&#22815;&#22312;&#26032;&#22330;&#26223;&#20013;&#23454;&#29616;&#24378;&#22823;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#25511;&#21046;&#33021;&#21147;&#65292;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;2.5&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.09873</link><description>&lt;p&gt;
Skill Transformer: &#31227;&#21160;&#25805;&#20316;&#30340;&#21333;&#20307;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skill Transformer: A Monolithic Policy for Mobile Manipulation. (arXiv:2308.09873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09873
&lt;/p&gt;
&lt;p&gt;
Skill Transformer&#26159;&#19968;&#20010;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#21644;&#25216;&#33021;&#27169;&#22359;&#21270;&#65292;&#23427;&#33021;&#22815;&#22312;&#26032;&#22330;&#26223;&#20013;&#23454;&#29616;&#24378;&#22823;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#25511;&#21046;&#33021;&#21147;&#65292;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#65292;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;2.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Skill Transformer&#65292;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#21644;&#25216;&#33021;&#27169;&#22359;&#21270;&#26469;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#21644;&#24863;&#30693;&#35266;&#23519;&#30340;&#26465;&#20214;&#19979;&#65292;Skill Transformer&#34987;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#20197;&#39044;&#27979;&#39640;&#32423;&#25216;&#33021;&#65288;&#22914;&#23548;&#33322;&#12289;&#25342;&#21462;&#12289;&#25918;&#32622;&#65289;&#21644;&#25972;&#20307;&#20302;&#32423;&#21160;&#20316;&#65288;&#22914;&#24213;&#30424;&#21644;&#25163;&#33218;&#36816;&#21160;&#65289;&#65292;&#37319;&#29992;transformer&#26550;&#26500;&#21644;&#35299;&#20915;&#23436;&#25972;&#20219;&#21153;&#30340;&#31034;&#33539;&#36712;&#36857;&#12290;&#36890;&#36807;&#25216;&#33021;&#39044;&#27979;&#27169;&#22359;&#65292;&#23427;&#20445;&#25345;&#20102;&#25972;&#20307;&#20219;&#21153;&#30340;&#32452;&#21512;&#24615;&#21644;&#27169;&#22359;&#24615;&#65292;&#21516;&#26102;&#25512;&#29702;&#20302;&#32423;&#21160;&#20316;&#24182;&#36991;&#20813;&#20102;&#27169;&#22359;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#20132;&#25509;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#30340;&#37325;&#26032;&#25490;&#21015;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;Skill Transformer&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#26032;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#25511;&#21046;&#33021;&#21147;&#65292;&#22312;&#22256;&#38590;&#30340;&#37325;&#26032;&#25490;&#21015;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#29575;&#27604;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;2.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09858</link><description>&lt;p&gt;
&#24352;&#37327;&#21387;&#32553;&#30340;&#21453;&#21521;&#20256;&#25773;&#20813;&#36153;&#35757;&#32451;&#65288;&#29289;&#29702;&#20449;&#24687;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30828;&#20214;&#21644;&#36719;&#20214;&#36164;&#28304;&#26469;&#25903;&#25345;&#33258;&#21160;&#24494;&#20998;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;BP&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#19978;&#24066;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;BP&#30340;&#26694;&#26550;&#65292;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#25773;&#23601;&#21487;&#20197;&#35757;&#32451;&#23454;&#38469;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#22823;&#20110;&#20197;&#21069;ZO&#26041;&#27861;&#33021;&#21147;&#30340;&#32593;&#32476;&#23610;&#23544;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;ZO&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31232;&#30095;&#26684;&#26041;&#27861;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;BP-free&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#25913;&#21464;&#20869;&#37096;&#23618;&#28608;&#27963;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32416;&#27491;&#20998;&#24067;&#25913;&#21464;&#23454;&#29616;&#35757;&#32451;&#21518;&#30340;&#21518;&#38376;&#20943;&#32531;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09850</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#22797;&#31070;&#32463;&#28608;&#27963;&#30340;&#20998;&#24067;&#26469;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Mitigation by Correcting the Distribution of Neural Activations. (arXiv:2308.09850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#25913;&#21464;&#20869;&#37096;&#23618;&#28608;&#27963;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32416;&#27491;&#20998;&#24067;&#25913;&#21464;&#23454;&#29616;&#35757;&#32451;&#21518;&#30340;&#21518;&#38376;&#20943;&#32531;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#65288;&#26408;&#39532;&#65289;&#25915;&#20987;&#26159;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#37325;&#35201;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#24335;&#65292;&#24403;&#25915;&#20987;&#32773;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#20986;&#29616;&#26102;&#65292;&#27979;&#35797;&#23454;&#20363;&#20250;&#34987;&#65288;&#38169;&#35823;&#65289;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#20998;&#26512;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#25104;&#21151;&#30340;&#25915;&#20987;&#20250;&#23548;&#33268;&#21518;&#38376;&#35302;&#21457;&#23454;&#20363;&#20869;&#37096;&#23618;&#28608;&#27963;&#30340;&#20998;&#24067;&#21457;&#29983;&#25913;&#21464;&#65292;&#19982;&#24178;&#20928;&#23454;&#20363;&#30340;&#20998;&#24067;&#19981;&#21516;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;&#32416;&#27491;&#20102;&#36825;&#31181;&#20998;&#24067;&#25913;&#21464;&#65292;&#24102;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#23454;&#20363;&#23558;&#27491;&#30830;&#20998;&#31867;&#20026;&#23427;&#20204;&#21407;&#22987;&#30340;&#28304;&#31867;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#24037;&#31243;&#30340;&#35302;&#21457;&#22120;&#26469;&#36890;&#36807;&#32416;&#27491;&#20998;&#24067;&#25913;&#21464;&#23454;&#29616;&#35757;&#32451;&#21518;&#30340;&#21518;&#38376;&#20943;&#36731;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#25913;&#21464;DNN&#30340;&#20219;&#20309;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#26159;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#20943;&#36731;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09842</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26522;&#20030;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#26159;&#20445;&#35777;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31995;&#32479;&#30340;&#20449;&#20219;&#30340;&#20851;&#38190;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AllDNN-Verification&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#23433;&#20840;&#23646;&#24615;&#21644;&#19968;&#20010;DNN&#65292;&#26522;&#20030;&#23646;&#24615;&#36755;&#20837;&#22495;&#30340;&#25152;&#26377;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#23646;&#24615;&#25104;&#31435;&#30340;&#21306;&#22495;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;#P&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#21483;&#20570;epsilon-ProVe&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32479;&#35745;&#39044;&#27979;&#23481;&#38480;&#38480;&#21046;&#33719;&#24471;&#21487;&#25511;&#20302;&#20272;&#30340;&#36755;&#20986;&#21487;&#36798;&#38598;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#36825;&#31181;&#26032;&#22411;&#30340;DNN&#39564;&#35777;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.09835</link><description>&lt;p&gt;
&#36890;&#36807;&#28857;&#21644;&#24418;&#29366;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#21512;&#25104;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#65292;&#36890;&#36807;&#28857;&#27880;&#37322;&#21644;&#24418;&#29366;&#20808;&#39564;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#19988;&#20173;&#33021;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#29992;&#20110;&#20998;&#21106;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#29983;&#25104;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#38656;&#35201;&#23494;&#38598;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#25104;&#26412;&#39640;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#19982;&#23436;&#25972;&#26631;&#27880;&#25152;&#25551;&#36848;&#30340;&#23545;&#35937;&#30340;&#23436;&#25972;&#36718;&#24275;&#30456;&#27604;&#65292;&#28857;&#27880;&#37322;&#65292;&#29305;&#21035;&#26159;&#23545;&#35937;&#36136;&#24515;&#65292;&#26356;&#23481;&#26131;&#33719;&#21462;&#65292;&#24182;&#19988;&#20173;&#28982;&#20026;&#21518;&#32493;&#20998;&#21106;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#28857;&#27880;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#32479;&#19968;&#27969;&#31243;&#36827;&#34892;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#28857;&#27880;&#37322;&#24182;&#20351;&#29992;&#24418;&#29366;&#20808;&#39564;&#32422;&#26463;&#37319;&#26679;&#19968;&#20010;&#20266;&#23494;&#38598;&#20998;&#21106;&#25513;&#30721;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20197;&#38750;&#37197;&#23545;&#30340;&#26041;&#24335;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#20266;&#25513;&#30721;&#36716;&#21270;&#20026;&#30495;&#23454;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#32423;&#19968;&#33268;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#65307;&#65288;3&#65289;&#20266;&#25513;&#30721;&#21644;&#21512;&#25104;&#22270;&#20687;&#20849;&#21516;&#26500;&#25104;&#20102;&#35757;&#32451;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;DNNs&#23398;&#20064;&#26412;&#22320;&#36335;&#30001;&#31574;&#30053;&#65292;&#21482;&#38656;&#20174;&#21333;&#20010;&#22270;&#24418;&#33719;&#24471;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#65292;&#21363;&#21487;&#25512;&#24191;&#21040;&#25152;&#26377;&#38543;&#26426;&#22270;&#24418;&#30340;&#26080;&#32447;&#32593;&#32476;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#36138;&#23146;&#36716;&#21457;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#22270;&#24418;&#23376;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.09829</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21333;&#19968;&#22270;&#24418;&#36275;&#20197;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#36817;&#26368;&#30701;&#36335;&#24452;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks. (arXiv:2308.09829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;DNNs&#23398;&#20064;&#26412;&#22320;&#36335;&#30001;&#31574;&#30053;&#65292;&#21482;&#38656;&#20174;&#21333;&#20010;&#22270;&#24418;&#33719;&#24471;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#65292;&#21363;&#21487;&#25512;&#24191;&#21040;&#25152;&#26377;&#38543;&#26426;&#22270;&#24418;&#30340;&#26080;&#32447;&#32593;&#32476;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#36138;&#23146;&#36716;&#21457;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#22270;&#24418;&#23376;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26412;&#22320;&#36335;&#30001;&#31574;&#30053;&#65292;&#23427;&#21482;&#38656;&#35201;&#20174;&#21333;&#20010;&#22270;&#24418;&#20013;&#33719;&#24471;&#30340;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#65292;&#21516;&#26102;&#33021;&#22815;&#25512;&#24191;&#21040;&#26631;&#20934;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#38543;&#26426;&#22270;&#24418;&#30340;&#26080;&#32447;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26469;&#35299;&#20915;&#20840;&#23616;&#36817;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#36825;&#20123;DNNs&#33021;&#22815;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#22320;&#23398;&#20064;&#20165;&#32771;&#34385;&#33410;&#28857;&#29366;&#24577;&#21644;&#30456;&#37051;&#33410;&#28857;&#29366;&#24577;&#30340;&#26412;&#22320;&#36335;&#30001;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#20854;&#20013;&#19968;&#20010;DNN&#23398;&#20064;&#30340;&#31574;&#30053;&#23436;&#20840;&#31526;&#21512;&#36138;&#23146;&#36716;&#21457;&#30340;&#24615;&#33021;&#65292;&#21478;&#19968;&#20010;&#21017;&#26222;&#36941;&#20248;&#20110;&#36138;&#23146;&#36716;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#20174;&#22810;&#20010;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20102;&#32593;&#32476;&#39046;&#22495;&#30693;&#35782;&#65306;&#39318;&#20808;&#65292;&#22312;&#36873;&#25321;&#36755;&#20837;&#29305;&#24449;&#26102;&#65307;&#20854;&#27425;&#65292;&#22312;&#36873;&#25321;&#8220;&#31181;&#23376;&#22270;&#8221;&#21450;&#20854;&#26368;&#30701;&#36335;&#24452;&#30340;&#23376;&#37319;&#26679;&#26102;&#12290;&#39046;&#22495;&#30693;&#35782;&#30340;&#21033;&#29992;&#20026;&#23398;&#20064;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#35828;&#26126;&#20102;&#20026;&#20160;&#20040;&#31181;&#23376;&#22270;&#21644;&#33410;&#28857;&#23376;&#37319;&#26679;&#36275;&#20197;&#23454;&#29616;&#26377;&#25928;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#12290;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.09804</link><description>&lt;p&gt;
VL-PET&#65306;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#23454;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VL-PET&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26694;&#26550;&#65292;&#36890;&#36807;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#27169;&#22411;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#20840;&#38754;&#24494;&#35843;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#23384;&#20648;&#26041;&#38754;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#65288;VL&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#25216;&#26415;&#65292;&#23558;&#27169;&#22359;&#21270;&#20462;&#25913;&#65288;&#20363;&#22914;Adapter&#21644;LoRA&#65289;&#38598;&#25104;&#21040;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#20013;&#12290;&#36890;&#36807;&#35843;&#25972;&#19968;&#23567;&#32452;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#27169;&#22359;&#21270;&#20462;&#25913;&#21644;&#24573;&#35270;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#21151;&#33021;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#32780;&#29616;&#26377;&#30340;PET&#25216;&#26415;&#65288;&#20363;&#22914;VL-Adapter&#65289;&#24573;&#35270;&#20102;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-and-Language Parameter-Efficient Tuning&#65288;VL-PET&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#24230;&#25511;&#21046;&#26426;&#21046;&#23545;&#27169;&#22359;&#21270;&#20462;&#25913;&#36827;&#34892;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;&#30001;&#36825;&#31181;&#26426;&#21046;&#29983;&#25104;&#30340;&#19981;&#21516;&#31890;&#24230;&#25511;&#21046;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#22810;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;VL-PET&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#21644;&#36716;&#25442;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36873;&#25321;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09791</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification. (arXiv:2308.09791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#39640;&#32500;&#22522;&#22240;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#21644;&#36716;&#25442;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#36873;&#25321;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#24182;&#22312;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Horse Herd&#20248;&#21270;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;&#24180;&#40836;&#30340;&#39532;&#30340;&#34892;&#20026;&#30340;&#26032;&#22411;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;HOA&#34987;&#24341;&#20837;&#26469;&#35299;&#20915;&#22797;&#26434;&#21644;&#39640;&#32500;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Horse Herd&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#36827;&#21046;&#29256;&#26412;&#65288;BHOA&#65289;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#38382;&#39064;&#21644;&#36873;&#25321;&#26174;&#33879;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;BHOA&#21644;&#26368;&#23567;&#20887;&#20313;&#26368;&#22823;&#30456;&#20851;&#65288;MRMR&#65289;&#36807;&#28388;&#26041;&#27861;&#30340;&#26032;&#22411;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#12290;&#36825;&#31181;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#25321;&#33021;&#22815;&#20135;&#29983;&#19968;&#32452;&#30456;&#20851;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#26377;&#30410;&#29305;&#24449;&#23376;&#38598;&#12290;&#30001;&#20110;&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#20989;&#25968;&#65288;TF&#65289;&#65292;&#31216;&#20026;X&#24418;&#29366;TF&#65292;&#23558;&#36830;&#32493;&#38382;&#39064;&#36716;&#25442;&#20026;&#20108;&#36827;&#21046;&#25628;&#32034;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#34987;&#29992;&#20110;&#26816;&#39564;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#21313;&#20010;&#24494;&#38453;&#21015;&#25968;&#25454;&#38598;&#65288;&#21363;&#28107;&#24052;&#30244;&#12289;&#21069;&#21015;&#33146;&#31561;&#65289;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#30340;&#32593;&#32476;&#24178;&#25200;&#12290;&#36890;&#36807;&#32771;&#34385;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#24314;&#31435;&#36866;&#21512;&#30340;&#26333;&#20809;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#23454;&#39564;&#21644;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09790</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#32593;&#32476;&#24178;&#25200;&#30340;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing. (arXiv:2308.09790v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#30340;&#32593;&#32476;&#24178;&#25200;&#12290;&#36890;&#36807;&#32771;&#34385;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#24314;&#31435;&#36866;&#21512;&#30340;&#26333;&#20809;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#23454;&#39564;&#21644;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32593;&#32476;&#24178;&#25200;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#25511;&#21046;&#23454;&#39564;&#25110;"A/B&#27979;&#35797;"&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#25551;&#36848;&#24322;&#36136;&#32593;&#32476;&#24178;&#25200;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;"&#26333;&#20809;&#26144;&#23556;"&#30830;&#23450;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#22240;&#26524;&#32593;&#32476;&#27169;&#24335;"&#65292;&#24182;&#37319;&#29992;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24314;&#31435;&#26368;&#36866;&#21512;&#21453;&#26144;&#28508;&#22312;&#32593;&#32476;&#24178;&#25200;&#27169;&#24335;&#30340;&#26333;&#20809;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#21512;&#25104;&#23454;&#39564;&#21644;&#19968;&#20010;&#28041;&#21450;100-200&#19975;Instagram&#29992;&#25143;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35774;&#35745;&#30340;&#38598;&#32676;&#38543;&#26426;&#21270;&#21644;&#22522;&#20110;&#20998;&#26512;&#30340;&#37051;&#22495;&#26333;&#20809;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09780</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#65292;&#36890;&#36807;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#20449;&#24687;&#26356;&#26032;&#26469;&#25429;&#25417;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20844;&#21496;&#22312;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#23558;&#30003;&#35831;&#21738;&#20123;&#31867;&#22411;&#30340;&#19987;&#21033;&#33021;&#22815;&#25581;&#31034;&#20986;&#23427;&#20204;&#30340;&#21457;&#23637;&#25112;&#30053;&#65292;&#24182;&#24110;&#21161;&#20854;&#25552;&#21069;&#21457;&#29616;&#28508;&#22312;&#30340;&#21512;&#20316;&#20249;&#20276;&#25110;&#31454;&#20105;&#23545;&#25163;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#20844;&#21496;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#21644;&#23545;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#20851;&#32852;&#30340;&#24314;&#27169;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#40092;&#26377;&#28041;&#21450;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19987;&#21033;&#30003;&#35831;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20844;&#21496;&#21644;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#21487;&#35760;&#24518;&#34920;&#31034;&#22522;&#30784;&#19978;&#12290;&#24403;&#35266;&#23519;&#21040;&#19968;&#20010;&#26032;&#30340;&#19987;&#21033;&#26102;&#65292;&#30456;&#20851;&#20844;&#21496;&#21644;&#20998;&#31867;&#20195;&#30721;&#30340;&#34920;&#31034;&#26681;&#25454;&#21382;&#21490;&#35760;&#24518;&#21644;&#24403;&#21069;&#32534;&#30721;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#19987;&#21033;&#20998;&#31867;&#20195;&#30721;&#30340;&#35821;&#20041;&#25509;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.09766</link><description>&lt;p&gt;
&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#27700;&#36164;&#28304;&#20013;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources. (arXiv:2308.09766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#21160;&#24577;&#29615;&#22659;&#21464;&#37327;&#30340;&#39044;&#27979;&#20173;&#28982;&#26159;&#27700;&#36164;&#28304;&#31185;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#19990;&#30028;&#19978;&#22823;&#37096;&#20998;&#30340;&#28129;&#27700;&#36164;&#28304;&#27809;&#26377;&#36866;&#24403;&#30340;&#30417;&#27979;&#20851;&#38190;&#29615;&#22659;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#32780;&#23545;&#27827;&#27969;&#27969;&#37327;&#21644;&#27700;&#36136;&#31561;&#27700;&#25991;&#21464;&#37327;&#36827;&#34892;&#24191;&#27867;&#39044;&#27979;&#30340;&#38656;&#27714;&#30001;&#20110;&#27668;&#20505;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#36234;&#26469;&#36234;&#36843;&#20999;&#65292;&#24182;&#24433;&#21709;&#30528;&#27700;&#36164;&#28304;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#36807;&#31243;&#21644;&#32463;&#39564;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#27700;&#25991;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#36234;&#26469;&#36234;&#20248;&#36234;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#21644;&#20854;&#20182;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#30456;&#20851;&#26368;&#26032;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#21033;&#29992;&#26032;&#20852;&#26041;&#27861;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;&#38598;&#27700;&#21306;&#29305;&#24449;&#34701;&#21512;&#26041;&#38754;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.09764</link><description>&lt;p&gt;
&#21435;&#38500;&#32972;&#26223;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#23578;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#65292;&#22312;&#24066;&#22330;&#19978;&#20855;&#26377;&#24456;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#30001;&#20110;&#26381;&#35013;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#20197;&#21450;&#21508;&#31181;&#22330;&#26223;&#21644;&#32972;&#26223;&#30340;&#23384;&#22312;&#65292;&#26102;&#23578;&#29702;&#35299;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#21435;&#38500;&#26102;&#23578;&#22270;&#20687;&#20013;&#30340;&#32972;&#26223;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26102;&#23578;&#25968;&#25454;&#36827;&#34892;&#32972;&#26223;&#21435;&#38500;&#12290;&#34987;&#21435;&#38500;&#32972;&#26223;&#30340;&#26102;&#23578;&#22270;&#20687;&#19982;&#26102;&#23578;&#25968;&#25454;&#38598;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#12289;&#27169;&#22411;&#21021;&#22987;&#21270;&#12289;&#19982;&#20854;&#20182;&#35757;&#32451;&#25216;&#24039;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#30446;&#26631;&#20219;&#21153;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32972;&#26223;&#21435;&#38500;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22686;&#24191;&#26500;&#24314;&#20102;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#27719;&#24635;&#65292;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#25968;&#25454;&#24182;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#25512;&#26029;&#65292;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.09751</link><description>&lt;p&gt;
&#23431;&#23449;&#23398;&#20013;&#30340;&#25968;&#25454;&#21387;&#32553;&#19982;&#25512;&#26029;&#65306;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data Compression and Inference in Cosmology with Self-Supervised Machine Learning. (arXiv:2308.09751v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22686;&#24191;&#26500;&#24314;&#20102;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#27719;&#24635;&#65292;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#25968;&#25454;&#24182;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#25512;&#26029;&#65292;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21644;&#21363;&#23558;&#21040;&#26469;&#30340;&#23431;&#23449;&#23398;&#35843;&#26597;&#25152;&#20135;&#29983;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#20449;&#24687;&#25439;&#22833;&#26377;&#25928;&#22320;&#27719;&#24635;&#25968;&#25454;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#20197;&#26032;&#39062;&#26041;&#24335;&#26500;&#24314;&#20195;&#34920;&#24615;&#25968;&#25454;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#25311;&#30340;&#22686;&#24191;&#25216;&#26415;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#25552;&#20379;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#27719;&#24635;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#31934;&#30830;&#21644;&#20934;&#30830;&#30340;&#21442;&#25968;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#33539;&#20363;&#22914;&#20309;&#26500;&#24314;&#23545;&#39044;&#23450;&#31995;&#32479;&#25928;&#24212;&#19981;&#25935;&#24863;&#30340;&#27719;&#24635;&#34920;&#31034;&#65292;&#20363;&#22914;&#24357;&#25955;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20026;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#21387;&#32553;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#30340;&#36827;&#23637;&#36712;&#36857;&#24182;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.09735</link><description>&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#36827;&#23637;&#36712;&#36857;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Interpretable Progression Trajectory Analysis of Chronic Disease. (arXiv:2308.09735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#30340;&#36827;&#23637;&#36712;&#36857;&#24182;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24378;&#35843;&#20102;&#20934;&#30830;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#36712;&#36857;&#21644;&#30693;&#24773;&#20020;&#24202;&#20915;&#31574;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#24739;&#32773;&#29305;&#24449;&#20013;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#32570;&#20047;&#25552;&#20379;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;&#21644;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#20915;&#31574;&#36741;&#21161;&#30340;&#35282;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;CTP&#27169;&#22411;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#36712;&#36857;&#21644;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23558;&#22240;&#26524;&#22270;&#32467;&#21512;&#21040;&#39044;&#27979;&#36807;&#31243;&#20013;&#65292;CTP&#30830;&#20445;&#31062;&#20808;&#29305;&#24449;&#19981;&#21463;&#23545;&#21518;&#20195;&#29305;&#24449;&#30340;&#27835;&#30103;&#24433;&#21709;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#30340;&#25506;&#32034;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.09734</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments. (arXiv:2308.09734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#31574;&#30053;&#24341;&#23548;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#30340;&#25506;&#32034;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#31867;&#38656;&#35201;&#28385;&#36275;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#36807;&#31243;&#29305;&#24615;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#38745;&#24577;&#29615;&#22659;&#26102;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21457;&#23637;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#25506;&#32034;&#23450;&#20041;&#30340;&#30446;&#26631;&#20559;&#22909;&#31354;&#38388;&#65292;&#21516;&#26102;&#28436;&#21270;&#31574;&#30053;&#35206;&#30422;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#31283;&#20581;&#22320;&#22312;&#32447;&#28436;&#21270;&#19968;&#20010;&#20984;&#35206;&#30422;&#31574;&#30053;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the prop
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#30340;&#25216;&#33021;&#38598;&#65292;&#20351;&#24471;&#31574;&#30053;&#35206;&#30422;&#38598;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25345;&#32493;&#28436;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09733</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20869;&#22312;&#21160;&#26426;&#23618;&#27425;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#30340;&#25216;&#33021;&#38598;&#65292;&#20351;&#24471;&#31574;&#30053;&#35206;&#30422;&#38598;&#33021;&#22815;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25345;&#32493;&#28436;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26159;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#36825;&#20123;&#20989;&#25968;&#26080;&#27861;&#22312;&#27809;&#26377;&#22949;&#21327;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#38382;&#39064;&#26080;&#27861;&#20687;&#20256;&#32479;&#24773;&#20917;&#19979;&#37027;&#26679;&#36890;&#36807;&#21333;&#20010;&#26368;&#20248;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21457;&#23637;&#20102;&#19968;&#20010;&#21487;&#20197;&#28385;&#36275;&#35299;&#20915;&#38382;&#39064;&#20013;&#25152;&#26377;&#21487;&#33021;&#20559;&#22909;&#30340;&#26368;&#20248;&#31574;&#30053;&#35206;&#30422;&#38598;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23558;&#20854;&#35206;&#30422;&#38598;&#25512;&#24191;&#21040;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#24037;&#20316;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20998;&#24067;&#30340;&#21442;&#25968;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#38480;&#21046;&#23548;&#33268;&#20102;&#36827;&#21270;&#31574;&#30053;&#38598;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#38656;&#35201;&#23398;&#20064;&#19968;&#32452;&#36890;&#29992;&#30340;&#25216;&#33021;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#26102;&#24341;&#23548;&#31574;&#30053;&#35206;&#30422;&#38598;&#30340;&#28436;&#21464;&#65292;&#20174;&#32780;&#20419;&#36827;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.09732</link><description>&lt;p&gt;
Baird&#21453;&#20363;&#24050;&#35299;&#20915;&#65306;&#20197;&#35843;&#35797;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#31639;&#27861;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;Baird&#21453;&#20363;&#19978;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baird&#21453;&#20363;&#26159;&#30001;Leemon Baird&#22312;1995&#24180;&#25552;&#20986;&#30340;&#65292;&#39318;&#20808;&#29992;&#20110;&#35777;&#26126;Temporal Difference (TD(0))&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#21457;&#25955;&#12290;&#20174;&#37027;&#26102;&#36215;&#65292;&#23427;&#32463;&#24120;&#34987;&#29992;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#31163;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#12290;&#26799;&#24230;TD&#31639;&#27861;&#35299;&#20915;&#20102;TD&#22312;Baird&#21453;&#20363;&#19978;&#30340;&#21457;&#25955;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#25910;&#25947;&#20173;&#28982;&#38750;&#24120;&#32531;&#24930;&#65292;&#32780;&#19988;&#32531;&#24930;&#30340;&#26412;&#36136;&#36824;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#29305;&#21035;&#29702;&#35299;&#20026;&#20160;&#20040;TDC&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#24930;&#65292;&#24182;&#25552;&#20379;&#35843;&#35797;&#20998;&#26512;&#26469;&#29702;&#35299;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#25216;&#26415;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#36817;&#30340;Impression GTD&#31639;&#27861;&#22312;&#36825;&#20010;&#20363;&#23376;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#25910;&#25947;&#38750;&#24120;&#24555;&#65292;&#20107;&#23454;&#19978;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;Baird&#21453;&#20363;&#36890;&#36807;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31639;&#27861;&#35299;&#20915;&#20102;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#21040;TD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.09731</link><description>&lt;p&gt;
ChatGPT-HealthPrompt. &#21033;&#29992;ChatGPT&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#21457;&#25381;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#35774;&#35745;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#20197;&#39046;&#22495;&#30693;&#35782;&#20026;&#22522;&#30784;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25552;&#21462;&#20851;&#38190;&#35265;&#35299;&#24182;&#36741;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#19968;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#32467;&#21512;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#65292;&#24182;&#39564;&#35777;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#65292;&#37325;&#28857;&#20851;&#27880;OpenAI&#30340;ChatGPT&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#20351;&#29992;&#65292;&#31574;&#30053;&#24615;&#22320;&#35774;&#35745;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#20174;&#39640;&#24615;&#33021;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25552;&#31034;&#35774;&#35745;&#20013;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#21307;&#30103;&#19987;&#23478;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20851;&#38190;&#35265;&#35299;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#39046;&#22495;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#22312;&#21019;&#24314;&#26356;&#20855;&#27934;&#23519;&#21147;&#30340;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#36890;&#36807;&#27604;&#36739;OpenAI&#30340;ChatGPT&#19982;&#20256;&#32479;&#30340;supervised&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2308.09730</link><description>&lt;p&gt;
&#22522;&#20110;COVID-19&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#25104;&#20687;&#30340;AI&#35786;&#26029;&#65306;&#20197;&#30149;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#26032;&#22411;&#20896;&#29366;&#30149;&#27602;&#65288;COVID-19&#65289;&#30340;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#35768;&#22810;&#25253;&#36947;&#31216;&#20854;&#24615;&#33021;&#20960;&#20046;&#23436;&#32654;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#30340;&#21464;&#24322;&#24615;&#21644;&#28508;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#24341;&#21457;&#20102;&#23545;&#20020;&#24202;&#36866;&#29992;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#28041;&#21450;&#20351;&#29992;&#20020;&#24202;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;COVID-19&#35786;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#65292;&#20197;&#35780;&#20272;AI&#24615;&#33021;&#21463;&#30142;&#30149;&#33539;&#22260;&#12289;&#36752;&#23556;&#21058;&#37327;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#33016;&#37096;&#25918;&#23556;&#25668;&#24433;&#65288;CXR&#65289;&#25104;&#20687;&#27169;&#24577;&#31561;&#20960;&#20010;&#24739;&#32773;&#21644;&#29289;&#29702;&#24615;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#21253;&#25324;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#24739;&#30149;&#29575;&#65289;&#24378;&#28872;&#24433;&#21709;&#20102;AI&#30340;&#24615;&#33021;&#65292;&#23548;&#33268;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#19979;&#38477;&#20102;&#39640;&#36798;20&#65285;&#65292;&#19988;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#20256;&#25773;&#21644;&#20195;&#25968;&#20960;&#20309;&#20013;&#21452;&#25968;&#27010;&#24565;&#30340;&#24555;&#36895;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU&#19978;&#23454;&#29616;&#19982;CUDA&#21152;&#36895;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09728</link><description>&lt;p&gt;
&#36890;&#36807;&#21069;&#21521;&#20256;&#25773;&#35823;&#24046;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning representations by forward-propagating errors. (arXiv:2308.09728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#20256;&#25773;&#21644;&#20195;&#25968;&#20960;&#20309;&#20013;&#21452;&#25968;&#27010;&#24565;&#30340;&#24555;&#36895;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;CPU&#19978;&#23454;&#29616;&#19982;CUDA&#21152;&#36895;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26159;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BP&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#35828;&#22826;&#24930;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26159;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#19978;&#36827;&#34892;&#65292;&#20351;&#29992;&#35745;&#31639;&#32479;&#19968;&#35774;&#22791;&#26550;&#26500;&#65288;CUDA&#65289;&#32534;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;CPU&#19978;&#24555;&#36895;&#23454;&#29616;CUDA&#21152;&#36895;&#30340;&#36731;&#37327;&#32423;&#24555;&#36895;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#22312;&#20195;&#25968;&#20960;&#20309;&#20013;&#20351;&#29992;&#21452;&#25968;&#27010;&#24565;&#30340;&#21069;&#21521;&#20256;&#25773;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back-propagation (BP) is widely used learning algorithm for neural network optimization. However, BP requires enormous computation cost and is too slow to train in central processing unit (CPU). Therefore current neural network optimizaiton is performed in graphical processing unit (GPU) with compute unified device architecture (CUDA) programming. In this paper, we propose a light, fast learning algorithm on CPU that is fast as CUDA acceleration on GPU. This algorithm is based on forward-propagating method, using concept of dual number in algebraic geometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#26469;&#36827;&#34892;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;&#25968;&#25454;&#20016;&#23500;&#22478;&#24066;&#30340;&#21407;&#22987;&#20132;&#36890;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#29983;&#25104;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#65292;&#21487;&#20197;&#25913;&#21892;&#25968;&#25454;&#31232;&#32570;&#22478;&#24066;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09727</link><description>&lt;p&gt;
&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#65306;&#36890;&#36807;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-city Few-Shot Traffic Forecasting via Traffic Pattern Bank. (arXiv:2308.09727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#26469;&#36827;&#34892;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;&#25968;&#25454;&#20016;&#23500;&#22478;&#24066;&#30340;&#21407;&#22987;&#20132;&#36890;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#29983;&#25104;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#65292;&#21487;&#20197;&#25913;&#21892;&#25968;&#25454;&#31232;&#32570;&#22478;&#24066;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26381;&#21153;&#12290;&#21033;&#29992;&#28145;&#24230;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20132;&#36890;&#20256;&#24863;&#22120;&#25110;&#36710;&#36742;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#28982;&#32780;&#19968;&#20123;&#22478;&#24066;&#21487;&#33021;&#32570;&#20047;&#35774;&#22791;&#25903;&#25345;&#65292;&#22240;&#27492;&#21487;&#29992;&#30340;&#25968;&#25454;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20174;&#25968;&#25454;&#20016;&#23500;&#30340;&#22478;&#24066;&#20013;&#23398;&#20064;&#65292;&#24182;&#23558;&#30693;&#35782;&#36716;&#31227;&#32473;&#25968;&#25454;&#31232;&#32570;&#30340;&#22478;&#24066;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#26469;&#36827;&#34892;&#36328;&#22478;&#24066;&#23569;&#26679;&#26412;&#20132;&#36890;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#22240;&#20026;&#20132;&#36890;&#27169;&#24335;&#22312;&#19981;&#21516;&#22478;&#24066;&#38388;&#26159;&#30456;&#20284;&#30340;&#12290;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20132;&#36890;&#34917;&#19969;&#32534;&#30721;&#22120;&#23558;&#25968;&#25454;&#20016;&#23500;&#30340;&#22478;&#24066;&#30340;&#21407;&#22987;&#20132;&#36890;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#65292;&#36890;&#36807;&#32858;&#31867;&#29983;&#25104;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#12290;&#28982;&#21518;&#65292;&#25968;&#25454;&#31232;&#32570;&#22478;&#24066;&#30340;&#20132;&#36890;&#25968;&#25454;&#21487;&#20197;&#26597;&#35810;&#20132;&#36890;&#27169;&#24335;&#23384;&#20648;&#24211;&#65292;&#24182;&#26500;&#24314;&#23427;&#20204;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#20851;&#31995;&#65292;&#20803;&#30693;&#35782;&#34987;&#32858;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is a critical service in Intelligent Transportation Systems (ITS). Utilizing deep models to tackle this task relies heavily on data from traffic sensors or vehicle devices, while some cities might lack device support and thus have few available data. So, it is necessary to learn from data-rich cities and transfer the knowledge to data-scarce cities in order to improve the performance of traffic forecasting. To address this problem, we propose a cross-city few-shot traffic forecasting framework via Traffic Pattern Bank (TPB) due to that the traffic patterns are similar across cities. TPB utilizes a pre-trained traffic patch encoder to project raw traffic data from data-rich cities into high-dimensional space, from which a traffic pattern bank is generated through clustering. Then, the traffic data of the data-scarce city could query the traffic pattern bank and explicit relations between them are constructed. The metaknowledge is aggregated based on these relations a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#30446;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#22312;&#25968;&#23383;&#20581;&#24247;&#31561;&#39046;&#22495;&#20013;&#21487;&#20197;&#25552;&#39640;&#25968;&#20493;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09726</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#65306;&#21463;&#25968;&#23383;&#20581;&#24247;&#21551;&#21457;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#20844;&#24179;&#30446;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#22312;&#25968;&#23383;&#20581;&#24247;&#31561;&#39046;&#22495;&#20013;&#21487;&#20197;&#25552;&#39640;&#25968;&#20493;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36481;&#21160;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#26159;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36830;&#32493;&#29615;&#22659;&#20013;&#36827;&#34892;&#31639;&#27861;&#20915;&#31574;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;RMABs&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#12289;&#27835;&#30103;&#23433;&#25490;&#12289;&#21453;&#20599;&#29454;&#31561;&#25935;&#24863;&#20915;&#31574;&#65292;&#32780;&#26412;&#30740;&#31350;&#30340;&#21160;&#26426;&#27491;&#26159;&#25968;&#23383;&#20581;&#24247;&#12290;&#22312;&#36825;&#20123;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#24517;&#39035;&#25913;&#21892;&#32467;&#26524;&#24182;&#36991;&#20813;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24046;&#36317;&#65288;&#20363;&#22914;&#65292;&#30830;&#20445;&#20581;&#24247;&#20844;&#24179;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;RMABs&#30340;&#20844;&#24179;&#30446;&#26631;&#65288;ERMABs&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#24179;&#25991;&#29486;&#20013;&#20004;&#31181;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#30446;&#26631;&#65292;&#26368;&#23567;&#21270;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#26368;&#22823;&#21270;&#32435;&#20160;&#31119;&#21033;&#12290;&#25105;&#20204;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#30446;&#26631;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#8212;&#8212;&#23545;&#20110;&#21069;&#32773;&#20351;&#29992;&#20102;&#27700;&#20301;&#22635;&#20805;&#31639;&#27861;&#65292;&#23545;&#20110;&#21518;&#32773;&#20351;&#29992;&#20102;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#36138;&#24515;&#31639;&#27861;&#26469;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#22823;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#27169;&#25311;&#39046;&#22495;&#65288;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#25968;&#23383;&#20581;&#24247;&#27169;&#22411;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21487;&#20197;&#27604;&#24403;&#21069;&#26041;&#27861;&#25552;&#39640;&#25968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09725</link><description>&lt;p&gt;
MoCLIM: &#29992;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#21644;&#32452;&#23398;&#25512;&#29702;&#24314;&#27169;&#23454;&#29616;&#20934;&#30830;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#21307;&#23398;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#30284;&#30151;&#20122;&#22411;&#30340;&#29983;&#21270;&#26426;&#21046;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22522;&#20110;&#32452;&#23398;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#32452;&#23398;&#35760;&#24405;&#20102;&#30284;&#30151;&#20013;&#22810;&#27493;&#39588;&#36807;&#31243;&#30340;&#29983;&#21270;&#20135;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#32467;&#26524;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;MoCLIM&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;MoCLIM&#29420;&#31435;&#22320;&#20174;&#19981;&#21516;&#30340;&#32452;&#23398;&#27169;&#24335;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32452;&#23398;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#25152;&#24471;&#21040;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#22312;&#32473;&#23450;&#30284;&#30151;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#20122;&#22411;&#24456;&#22909;&#22320;&#32858;&#31867;&#21040;&#36739;&#20302;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#23545;&#27604;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#32452;&#38469;&#25512;&#29702;&#30340;&#25237;&#24433;&#12290;&#22312;&#20845;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#39640;&#32500;&#30284;&#30151;&#25968;&#25454;&#25311;&#21512;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#22312;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#20013;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09724</link><description>&lt;p&gt;
&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#29992;&#20110;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer. (arXiv:2308.09724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09724
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#22312;&#20132;&#21449;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#20013;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#20197;&#20840;&#23616;&#26041;&#24335;&#23545;&#40784;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#40784;&#21518;&#65292;&#27599;&#20010;&#28304;&#26679;&#26412;&#37117;&#26399;&#26395;&#19982;&#20219;&#20309;&#30446;&#26631;&#26679;&#26412;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20840;&#23616;&#23545;&#40784;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#25110;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#36866;&#24212;&#65288;KISA&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;KISA&#26368;&#23567;&#21270;&#20849;&#20139;&#39044;&#26399;&#25439;&#22833;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#36825;&#26159;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#25104;&#21151;&#30340;&#21069;&#25552;&#12290;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#21551;&#21457;&#30340;&#23376;&#39046;&#22495;&#21010;&#20998;&#38382;&#39064;&#65292;&#36825;&#22312;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#36866;&#24212;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art deep domain adaptation techniques align source and target samples in a global fashion. That is, after alignment, each source sample is expected to become similar to any target sample. However, global alignment may not always be optimal or necessary in practice. For example, consider cross-domain fraud detection, where there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may yield better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable such fine-grained domain adaption, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. In particular, (1) We provide the theoretical insight that KISA minimizes the shared expected loss which is the premise for the success of domain adaptation methods. (2) We propose the knowledge-inspired subdomain division problem that plays a crucial role in fine-grained doma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#21152;&#36895;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36136;&#37327;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.09723</link><description>&lt;p&gt;
FineQuant: &#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;LLMs&#35299;&#38145;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#21152;&#36895;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#23545;&#20110;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#30001;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#24102;&#23485;&#29942;&#39048;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20165;&#22522;&#20110;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;LLMs&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#21152;&#36895;&#25512;&#29702;&#12290;&#20026;&#20102;&#30830;&#20445;&#36136;&#37327;&#38477;&#20302;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#30340;Mixture-of-Experts&#65288;MoE&#65289;&#21644;&#23494;&#38598;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#19982;LLMs&#37327;&#21270;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25214;&#21040;&#26435;&#37325;&#32454;&#31890;&#24230;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved state-of-the-art performance across various language tasks but pose challenges for practical deployment due to their substantial memory requirements. Furthermore, the latest generative models suffer from high inference costs caused by the memory bandwidth bottleneck in the auto-regressive decoding process. To address these issues, we propose an efficient weight-only quantization method that reduces memory consumption and accelerates inference for LLMs. To ensure minimal quality degradation, we introduce a simple and effective heuristic approach that utilizes only the model weights of a pre-trained model. This approach is applicable to both Mixture-of-Experts (MoE) and dense models without requiring additional fine-tuning. To demonstrate the effectiveness of our proposed method, we first analyze the challenges and issues associated with LLM quantization. Subsequently, we present our heuristic approach, which adaptively finds the granularity of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09722</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#20449;&#30340;&#22522;&#20110;LSTM-Autoencoder&#32593;&#32476;&#30340;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#27861;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#23475;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#20167;&#24680;&#35328;&#35770;&#30340;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#36825;&#20123;&#21487;&#24597;&#30340;&#20869;&#23481;&#21487;&#33021;&#23548;&#33268;&#25233;&#37057;&#21644;&#19982;&#33258;&#26432;&#26377;&#20851;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#21487;&#20449;LSTM-Autoencoder&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#23637;&#31034;&#20102;&#19968;&#31181;&#35299;&#20915;&#25968;&#25454;&#21487;&#29992;&#24615;&#22256;&#38590;&#30340;&#21069;&#27839;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#31561;&#20960;&#31181;&#35821;&#35328;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#30340;&#21407;&#22240;&#65292;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#20256;&#32479;&#27169;&#22411;&#65288;&#21253;&#25324;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#65292;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTM&#65289;&#65292;LSTM-Autoencoder&#65292;Word2vec&#65292;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;2&#65288;GPT-2&#65289;&#27169;&#22411;&#65289;&#23545;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#24615;&#30340;&#20405;&#29359;&#35780;&#35770;&#35782;&#21035;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#39046;&#22495;&#20013;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#32570;&#38519;&#30340;&#26032;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09721</link><description>&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#20855;&#20307;&#23454;&#26045;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
A new solution and concrete implementation steps for Artificial General Intelligence. (arXiv:2308.09721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#39046;&#22495;&#20013;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#32570;&#38519;&#30340;&#26032;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#36890;&#24120;&#37319;&#29992;&#8220;&#27880;&#24847;&#26426;&#21046;+&#28145;&#24230;&#23398;&#20064;&#8221;+&#8220;&#24378;&#21270;&#23398;&#20064;&#8221;&#30340;&#25216;&#26415;&#36335;&#24452;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25472;&#36215;&#20102;&#22823;&#27169;&#22411;&#30340;&#25216;&#26415;&#28010;&#28526;&#12290;&#20294;&#22312;&#28041;&#21450;&#19982;&#23454;&#38469;&#29615;&#22659;&#20132;&#20114;&#30340;&#39046;&#22495;&#65292;&#22914;&#20859;&#32769;&#25252;&#29702;&#12289;&#23478;&#24237;&#20445;&#22982;&#12289;&#20892;&#19994;&#29983;&#20135;&#21644;&#36710;&#36742;&#39550;&#39542;&#31561;&#65292;&#35797;&#38169;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#35797;&#38169;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#24456;&#38590;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#39046;&#22495;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#25105;&#20204;&#38656;&#35201;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#30340;&#32570;&#38519;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#28010;&#28526;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#27714;&#21644;&#27744;&#21270;&#26041;&#27861;&#65288;GSP&#65289;&#29992;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#12290;GSP&#36890;&#36807;&#36873;&#25321;&#35821;&#20041;&#23454;&#20307;&#30340;&#23376;&#38598;&#65292;&#23398;&#20064;&#24573;&#30053;&#26080;&#29992;&#20449;&#24687;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09228</link><description>&lt;p&gt;
&#27867;&#21270;&#30340;&#27714;&#21644;&#27744;&#21270;&#29992;&#20110;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Sum Pooling for Metric Learning. (arXiv:2308.09228v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#27714;&#21644;&#27744;&#21270;&#26041;&#27861;&#65288;GSP&#65289;&#29992;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#12290;GSP&#36890;&#36807;&#36873;&#25321;&#35821;&#20041;&#23454;&#20307;&#30340;&#23376;&#38598;&#65292;&#23398;&#20064;&#24573;&#30053;&#26080;&#29992;&#20449;&#24687;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#24120;&#35265;&#26550;&#26500;&#36873;&#25321;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21518;&#36319;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;GAP&#26159;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;&#20449;&#24687;&#32858;&#21512;&#26041;&#24335;&#12290;&#23545;&#20110;GAP&#30340;&#26377;&#25928;&#24615;&#65292;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;&#23558;&#27599;&#20010;&#29305;&#24449;&#21521;&#37327;&#35270;&#20026;&#34920;&#31034;&#19981;&#21516;&#35821;&#20041;&#23454;&#20307;&#30340;&#38598;&#21512;&#65292;&#32780;GAP&#21017;&#26159;&#23427;&#20204;&#30340;&#20984;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#27867;&#21270;&#20102;GAP&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#27867;&#21270;&#27714;&#21644;&#27744;&#21270;&#26041;&#27861;&#65288;GSP&#65289;&#12290;GSP&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#33021;&#21147;&#25913;&#36827;&#20102;GAP&#65306;i&#65289;&#33021;&#22815;&#36873;&#25321;&#35821;&#20041;&#23454;&#20307;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#24573;&#30053;&#26080;&#29992;&#20449;&#24687;&#65307;ii&#65289;&#23398;&#20064;&#19982;&#27599;&#20010;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#23545;&#24212;&#30340;&#26435;&#37325;&#12290;&#24418;&#24335;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29109;&#24179;&#28369;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#26159;GAP&#30340;&#20005;&#26684;&#27867;&#21270;&#65292;&#21363;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#29616;&#20250;&#24471;&#21040;GAP&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#35299;&#26512;&#26799;&#24230;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#20316;&#20026;&#30452;&#25509;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08998</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#22686;&#24378; (ReST) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064; (RLHF)&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064; (RL) &#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784; LLM&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22686;&#24378;&#33258;&#23398;&#20064; (ReST)&#12290;&#32473;&#23450;&#21021;&#22987;&#30340;LLM&#31574;&#30053;&#65292;ReST&#36890;&#36807;&#20174;&#31574;&#30053;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#20135;&#29983;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;LLM&#31574;&#30053;&#12290;ReST&#27604;&#20856;&#22411;&#30340;&#22312;&#32447;RLHF&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#31163;&#32447;&#29983;&#25104;&#30340;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;ReST&#26159;&#36866;&#29992;&#20110;&#25152;&#26377;&#29983;&#25104;&#23398;&#20064;&#35774;&#32622;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ReST&#21487;&#20197;&#20197;&#35745;&#31639;&#21644;&#37319;&#26679;&#39640;&#25928;&#30340;&#26041;&#24335;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#22312;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#19978;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.08708</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24847;&#35782;&#65306;&#26469;&#33258;&#24847;&#35782;&#31185;&#23398;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#35880;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26469;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35782;&#38382;&#39064;&#12290;&#30740;&#31350;&#20013;&#23545;&#20960;&#31181;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#25512;&#23548;&#20986;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23578;&#19981;&#20855;&#22791;&#24847;&#35782;&#65292;&#20294;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#26080;&#26126;&#26174;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25110;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#33021;&#20855;&#26377;&#24847;&#35782;&#25104;&#20026;&#31185;&#23398;&#30028;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#20063;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#25285;&#24551;&#12290;&#26412;&#25253;&#21578;&#25552;&#20986;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#20005;&#35880;&#19988;&#32463;&#39564;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26041;&#27861;&#65306;&#26681;&#25454;&#25105;&#20204;&#30446;&#21069;&#26368;&#21487;&#20449;&#30340;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#31185;&#23398;&#24847;&#35782;&#29702;&#35770;&#65292;&#21253;&#25324;&#24490;&#29615;&#22788;&#29702;&#29702;&#35770;&#12289;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#12289;&#39640;&#38454;&#29702;&#35770;&#12289;&#39044;&#27979;&#22788;&#29702;&#29702;&#35770;&#21644;&#27880;&#24847;&#27169;&#24335;&#29702;&#35770;&#12290;&#20174;&#36825;&#20123;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20123;&#24847;&#35782;&#30340;&#8220;&#25351;&#31034;&#24615;&#29305;&#24449;&#8221;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#20855;&#22791;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25351;&#31034;&#24615;&#29305;&#24449;&#26469;&#35780;&#20272;&#20102;&#20960;&#20010;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#31995;&#32479;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#24847;&#35782;&#65292;&#20294;&#21516;&#26102;&#20063;&#26174;&#31034;&#20986;&#27809;&#26377;&#26126;&#26174;&#30340;&#24314;&#31435;&#20855;&#26377;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07575</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#38590;&#28857;&#22312;&#20110;&#19981;&#20165;&#38656;&#35201;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21576;&#29616;&#35270;&#35273;&#32454;&#33410;&#65292;&#36824;&#38656;&#35201;&#23545;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20026;&#27599;&#20010;&#21477;&#23376;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20294;&#22312;&#32473;&#23450;&#27573;&#33853;&#20013;&#32534;&#30721;&#19978;&#19979;&#25991;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#35828;&#26381;&#21147;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#35282;&#33394;&#25110;&#36866;&#24403;&#30340;&#22330;&#26223;&#32972;&#26223;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#65292;&#29992;&#20110;&#21452;&#21521;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#29983;&#25104;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25512;&#29702;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;Pororo-SV&#21644;Flintstones-SV&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;FID&#12289;&#23383;&#31526;...
&lt;/p&gt;
&lt;p&gt;
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
&lt;/p&gt;</description></item><item><title>AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07221</link><description>&lt;p&gt;
AudioFormer: &#36890;&#36807;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#38899;&#39057;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07221
&lt;/p&gt;
&lt;p&gt;
AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#26469;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#30340;&#24418;&#24335;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411; (MLM)&#65292;&#20174;&#32780;&#33719;&#24471;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604; (MPC) &#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21516;&#19968;&#38899;&#39057;&#36755;&#20837;&#20013;&#22810;&#20010;&#31163;&#25955;&#22768;&#23398;&#20195;&#30721;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#35270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MPC&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.06394</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#39044;&#38450;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#27867;&#21270;&#36328;&#22810;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#35814;&#32454;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LVLM&#65288;InstructBLIP&#65289;&#20173;&#28982;&#23384;&#22312;&#30528;&#24778;&#20154;&#30340;30%&#30340;&#24187;&#35273;&#25991;&#26412;&#65292;&#21253;&#25324;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#19981;&#24544;&#23454;&#30340;&#25551;&#36848;&#21644;&#19981;&#20934;&#30830;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;M-HalDetect&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#21644;&#39044;&#38450;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;M-HalDetect&#21253;&#21547;&#20102;16k&#20010;&#32454;&#31890;&#24230;&#30340;VQA&#31034;&#20363;&#26631;&#31614;&#65292;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#21482;&#32771;&#34385;&#23545;&#35937;&#24187;&#35273;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#20102;&#23454;&#20307;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.06300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#34880;&#32454;&#32990;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06300
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34880;&#28082;&#20027;&#35201;&#21253;&#25324;&#34880;&#27974;&#12289;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#12290;&#34880;&#32454;&#32990;&#20026;&#36523;&#20307;&#32454;&#32990;&#25552;&#20379;&#27687;&#27668;&#65292;&#28363;&#20859;&#23427;&#20204;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24863;&#26579;&#65292;&#22686;&#24378;&#20813;&#30123;&#21147;&#24182;&#20419;&#36827;&#20957;&#34880;&#12290;&#20154;&#30340;&#20581;&#24247;&#29366;&#20917;&#21487;&#20197;&#20174;&#34880;&#32454;&#32990;&#20013;&#21453;&#26144;&#20986;&#26469;&#12290;&#19968;&#20010;&#20154;&#34987;&#35786;&#26029;&#20986;&#26576;&#31181;&#30142;&#30149;&#30340;&#26426;&#20250;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#34880;&#32454;&#32990;&#31867;&#22411;&#21644;&#35745;&#25968;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#34880;&#32454;&#32990;&#20998;&#31867;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#30142;&#30149;&#65292;&#21253;&#25324;&#30284;&#30151;&#12289;&#39592;&#39635;&#25439;&#20260;&#12289;&#33391;&#24615;&#32959;&#30244;&#21644;&#23427;&#20204;&#30340;&#29983;&#38271;&#12290;&#36825;&#31181;&#20998;&#31867;&#21487;&#20197;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#21306;&#20998;&#19981;&#21516;&#30340;&#34880;&#32454;&#32990;&#29255;&#27573;&#65292;&#20197;&#20415;&#30830;&#23450;&#30142;&#30149;&#30340;&#21407;&#22240;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23558;&#20154;&#31867;&#34880;&#32454;&#32990;&#65288;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#65289;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#20122;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;CNN&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#34880;&#32454;&#32990;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#30333;&#32454;&#32990;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#21644;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06296</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Classification of White Blood Cells Using Machine Learning Models. (arXiv:2308.06296v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#30333;&#32454;&#32990;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#21644;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#21644;&#20998;&#31867;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#26681;&#25454;&#35745;&#31639;&#21644;&#20107;&#23454;&#26469;&#35786;&#26029;&#34880;&#28082;&#30284;&#30151;&#21644;&#33041;&#32959;&#30244;&#12290;&#26412;&#32508;&#36848;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#30333;&#32454;&#32990;&#20998;&#31867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#29616;&#20195;&#25216;&#26415;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#38024;&#23545;&#26412;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#34880;&#28034;&#29255;&#22270;&#20687;&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12289;X&#23556;&#32447;&#21644;&#31867;&#20284;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#30340;&#20027;&#35201;&#24433;&#21709;&#22312;&#20110;&#23545;&#24212;&#29992;&#20110;&#30333;&#32454;&#32990;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#36825;&#31181;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#65292;&#20363;&#22914;&#26368;&#24120;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#26368;&#20339;&#34920;&#29616;&#30340;&#30333;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#36817;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#20351;&#29992;ML&#21644;DL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning (ML) and deep learning (DL) models contribute to exceptional medical image analysis improvement. The models enhance the prediction and improve the accuracy by prediction and classification. It helps the hematologist to diagnose the blood cancer and brain tumor based on calculations and facts. This review focuses on an in-depth analysis of modern techniques applied in the domain of medical image analysis of white blood cell classification. For this review, the methodologies are discussed that have used blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains. The main impact of this review is to present a detailed analysis of machine learning techniques applied for the classification of white blood cells (WBCs). This analysis provides valuable insight, such as the most widely used techniques and best-performing white blood cell classification methods. It was found that in recent decades researchers have been using ML and DL f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdaSPS&#21644;AdaSLS&#20004;&#31181;&#26032;&#30340;&#21464;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SGD&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#32447;&#24615;&#21644;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06058</link><description>&lt;p&gt;
&#24102;&#26377;Polyak&#27493;&#38271;&#21644;&#32447;&#24615;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;SGD: &#40065;&#26834;&#25910;&#25947;&#21644;&#26041;&#24046;&#20943;&#23567;
&lt;/p&gt;
&lt;p&gt;
Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction. (arXiv:2308.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdaSPS&#21644;AdaSLS&#20004;&#31181;&#26032;&#30340;&#21464;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SGD&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#24182;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#32447;&#24615;&#21644;&#20122;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;Polyak&#27493;&#38271; (SPS) &#21644;&#38543;&#26426;&#32447;&#24615;&#25628;&#32034; (SLS) &#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#19979;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#21482;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#30340;&#37051;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#27604;&#21021;&#22987;&#29468;&#27979;&#26356;&#24046;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20154;&#20026;&#20943;&#23567;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064; (Orvieto et al. [2022])&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#20984;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SPS&#21644;SLS&#21464;&#31181;&#65292;&#20998;&#21035;&#31216;&#20026;AdaSPS&#21644;AdaSLS&#65292;&#23427;&#20204;&#22312;&#38750;&#25554;&#20540;&#29615;&#22659;&#20013;&#20445;&#35777;&#25910;&#25947;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#26102;&#20445;&#25345;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#20122;&#32447;&#24615;&#21644;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;AdaSLS&#19981;&#38656;&#35201;&#23545;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#30340;&#20102;&#35299;&#65292;&#32780;AdaSPS&#21482;&#38656;&#35201;&#26368;&#20248;&#20989;&#25968;&#20540;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value 
&lt;/p&gt;</description></item><item><title>AdaER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2308.03810</link><description>&lt;p&gt;
AdaER: &#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning. (arXiv:2308.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03810
&lt;/p&gt;
&lt;p&gt;
AdaER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#23398;&#20064;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32773;&#20250;&#20197;&#39034;&#24207;&#26041;&#24335;&#25345;&#32493;&#33719;&#21462;&#26032;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#35757;&#32451;&#25968;&#25454;&#30340;&#38750;&#31283;&#24577;&#24615;&#32473;&#36825;&#20010;&#36807;&#31243;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25351;&#30340;&#26159;&#22312;&#24341;&#20837;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#32463;&#39564;&#37325;&#25773;(ER)&#65292;&#24050;&#34987;&#25552;&#20986;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#65292;&#23588;&#20854;&#22312;&#22686;&#37327;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#32780;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;(AdaER)&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;AdaER&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35760;&#24518;&#37325;&#25773;&#21644;&#35760;&#24518;&#26356;&#26032;&#12290;&#22312;&#35760;&#24518;&#37325;&#25773;&#38454;&#27573;&#65292;AdaER&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;(C-CMR)&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflictin
&lt;/p&gt;</description></item><item><title>G-Mix&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;Mixup&#21644;&#38160;&#24230;&#24863;&#30693;&#26497;&#23567;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;&#20108;&#36827;&#21046;G-Mix&#21644;&#20998;&#35299;G-Mix&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#20248;&#21270;DNN&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03236</link><description>&lt;p&gt;
G-Mix: &#19968;&#31181;&#36890;&#29992;&#30340;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24179;&#38754;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima. (arXiv:2308.03236v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03236
&lt;/p&gt;
&lt;p&gt;
G-Mix&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;Mixup&#21644;&#38160;&#24230;&#24863;&#30693;&#26497;&#23567;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;&#20108;&#36827;&#21046;G-Mix&#21644;&#20998;&#35299;G-Mix&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#20248;&#21270;DNN&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#24403;&#21069;&#30340;DNNs&#22312;&#36807;&#21442;&#25968;&#21270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#22686;&#24378;DNNs&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;Mixup&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#20135;&#29983;&#20102;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#25104;&#21151;&#30340;&#38160;&#24230;&#24863;&#30693;&#26497;&#23567;&#21270;(SAM)&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#35757;&#32451;&#25439;&#22833;&#24179;&#38754;&#30340;&#38160;&#24230;&#19982;&#27169;&#22411;&#27867;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026;&#36890;&#29992;&#28151;&#21512;(G-Mix)&#65292;&#23427;&#32467;&#21512;&#20102;Mixup&#21644;SAM&#30340;&#20248;&#28857;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#12290;&#25552;&#20379;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#24320;&#21457;&#30340;G-Mix&#26694;&#26550;&#22914;&#20309;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#20248;&#21270;DNN&#24615;&#33021;&#19982;G-Mix&#26694;&#26550;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65306;&#20108;&#36827;&#21046;G-Mix&#21644;&#20998;&#35299;G-Mix&#12290;&#36825;&#20123;&#31639;&#27861;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#23558;&#20854;&#20998;&#25104;&#20004;&#20010;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#30340;&#36136;&#37327;&#65292;&#21457;&#29616;&#20102;&#23433;&#20840;&#28431;&#27934;&#30340;&#37325;&#35201;&#35777;&#25454;&#65292;&#24182;&#35780;&#20272;&#20102;&#36755;&#20837;&#21442;&#25968;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.02955</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#26234;&#33021;&#21512;&#32422;&#21019;&#24314;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of AI-based Smart Contract Creation. (arXiv:2308.02955v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#30340;&#36136;&#37327;&#65292;&#21457;&#29616;&#20102;&#23433;&#20840;&#28431;&#27934;&#30340;&#37325;&#35201;&#35777;&#25454;&#65292;&#24182;&#35780;&#20272;&#20102;&#36755;&#20837;&#21442;&#25968;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;Google Palm2&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#29983;&#25104;&#20284;&#20046;&#26159;&#31532;&#19968;&#20010;&#25104;&#29087;&#30340;AI&#23545;&#31243;&#24207;&#21592;&#30340;&#23454;&#20363;&#12290;LLMs&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#24320;&#28304;&#26234;&#33021;&#21512;&#32422;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;Solidity&#20013;&#21033;&#29992;&#27604;&#20854;&#20182;&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#26356;&#22810;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;&#23545;LLMs&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#29983;&#25104;&#30340;&#26368;&#21021;&#21644;&#38750;&#27491;&#24335;&#35780;&#20272;&#20196;&#20154;&#20805;&#28385;&#24076;&#26395;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#20197;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#30340;&#38480;&#21046;&#21644;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#26088;&#22312;&#35780;&#20272;&#36755;&#20837;&#21442;&#25968;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#29983;&#25104;&#30340;&#26234;&#33021;&#21512;&#32422;&#20013;&#24341;&#20837;&#20102;&#20851;&#38190;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#21516;&#26102;&#20063;&#24433;&#21709;&#20102;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall q
&lt;/p&gt;</description></item><item><title>NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02751</link><description>&lt;p&gt;
NeRFs: &#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02751
&lt;/p&gt;
&lt;p&gt;
NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#35270;&#22270;&#21512;&#25104;&#25110;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#31561;&#38382;&#39064;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#20063;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;NeRFs&#36890;&#36807;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#35270;&#22270;&#30456;&#20851;&#36752;&#23556;&#21644;&#20307;&#31215;&#23494;&#24230;&#31561;&#20307;&#31215;&#21442;&#25968;&#65292;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#20307;&#31215;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#65292;&#27599;&#24180;&#26377;&#25968;&#21315;&#31687;&#35770;&#25991;&#22312;&#20854;&#22522;&#30784;&#19978;&#25193;&#23637;&#25110;&#30456;&#20851;&#30740;&#31350;&#65292;&#22810;&#20301;&#20316;&#32773;&#21644;&#32593;&#31449;&#25552;&#20379;&#27010;&#36848;&#21644;&#35843;&#30740;&#65292;&#24182;&#26377;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#21019;&#19994;&#20844;&#21496;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;NeRFs&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#38271;&#36798;&#19977;&#21313;&#24180;&#30340;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#26368;&#32456;&#24341;&#20986;NeRFs&#35770;&#25991;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02442</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#38468;&#21152;kNN&#22270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;kNN&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;kNN&#22270;&#23545;&#20110;k&#20540;&#30340;&#22266;&#23450;&#20381;&#36182;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#20998;&#31867;&#27169;&#22411;&#31867;&#20284;&#65292;&#20915;&#31574;&#36793;&#30028;&#19978;&#23384;&#22312;&#30340;&#27169;&#31946;&#26679;&#26412;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#38468;&#21152;k-&#26368;&#36817;&#37051;&#22270;&#65288;paNNG&#65289;&#65292;&#23427;&#23558;&#33258;&#36866;&#24212;&#30340;kNN&#19982;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#8220;&#25289;&#8221;&#23427;&#20204;&#22238;&#21040;&#21407;&#22987;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;paNNG&#30340;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;</title><link>http://arxiv.org/abs/2307.14751</link><description>&lt;p&gt;
FLARE: &#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#23545;&#25351;&#32441;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#36827;&#34892;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14751
&lt;/p&gt;
&lt;p&gt;
FLARE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#24182;&#27979;&#37327;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#26469;&#39564;&#35777;&#34987;&#30423;&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FLARE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#30097;&#20284;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31574;&#30053;&#26159;&#21542;&#26159;&#21478;&#19968;&#20010;&#65288;&#21463;&#23475;&#65289;&#31574;&#30053;&#30340;&#38750;&#27861;&#21103;&#26412;&#30340;&#25351;&#32441;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36890;&#36807;&#25214;&#21040;&#19981;&#21487;&#20256;&#36882;&#30340;&#12289;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#25513;&#30721;&#65292;&#21363;&#25200;&#21160;&#65292;&#21487;&#20197;&#29983;&#25104;&#25104;&#21151;&#22320;&#20174;&#21463;&#23475;&#31574;&#30053;&#20256;&#36882;&#21040;&#20854;&#20462;&#25913;&#29256;&#26412;&#20294;&#19981;&#33021;&#20256;&#36882;&#21040;&#29420;&#31435;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;FLARE&#21033;&#29992;&#36825;&#20123;&#25513;&#30721;&#20316;&#20026;&#25351;&#32441;&#65292;&#36890;&#36807;&#23545;&#36890;&#36807;&#25513;&#30721;&#25200;&#21160;&#30340;&#29366;&#24577;&#19978;&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#20540;&#36827;&#34892;&#27979;&#37327;&#26469;&#39564;&#35777;&#34987;&#30423;&#30340;DRL&#31574;&#30053;&#30340;&#30495;&#23454;&#25152;&#26377;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLARE&#26159;&#26377;&#25928;&#30340;&#65288;&#23545;&#20110;&#34987;&#30423;&#21103;&#26412;&#20855;&#26377;100%&#30340;&#21160;&#20316;&#19968;&#33268;&#24615;&#65289;&#65292;&#24182;&#19988;&#19981;&#20250;&#38169;&#35823;&#22320;&#25351;&#25511;&#29420;&#31435;&#31574;&#30053;&#65288;&#26080;&#35823;&#25253;&#65289;&#12290;FLARE&#36824;&#23545;&#27169;&#22411;&#20462;&#25913;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#34987;&#26356;&#26126;&#26234;&#30340;&#23545;&#25163;&#35268;&#36991;&#32780;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25513;&#30721;&#37117;&#26159;&#36866;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.12510</link><description>&lt;p&gt;
Temporal Graph Benchmark&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Temporal Graph Benchmark&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#21040;TGB&#65292;&#24182;&#20351;&#29992;&#21313;&#19968;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;DyGLib&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21160;&#24577;&#22270;&#24211;(DyGLib)&#25193;&#23637;&#21040;Temporal Graph Benchmark (TGB)&#65292;&#23545;TGB&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;TGB&#30456;&#27604;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#21313;&#19968;&#31181;&#27969;&#34892;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#19968;&#33268;&#65307;&#65288;2&#65289;&#20351;&#29992;DyGLib&#26102;&#65292;&#19968;&#20123;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#22312;TGB&#19978;&#35780;&#20272;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35797;&#22270;&#25552;&#20379;&#21487;&#30452;&#25509;&#21442;&#32771;&#30340;&#32467;&#26524;&#20379;&#21518;&#32493;&#30740;&#31350;&#20351;&#29992;&#12290;&#26412;&#39033;&#30446;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#36164;&#28304;&#22343;&#21487;&#22312;https://github.com/yule-BUAA/DyGLib_TGB&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;&#26412;&#24037;&#20316;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#27426;&#36814;&#31038;&#21306;&#25552;&#20379;&#21453;&#39304;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#30340; logits &#20998;&#24067;&#26356;&#21152;&#38598;&#20013;&#65292;&#24182;&#19988;&#35821;&#20041;&#26126;&#30830;&#20294;&#19982;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918; (MTS) &#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451; (MDT) &#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.12463</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#33976;&#39311;&#65306;&#19981;&#35201;&#24573;&#35270;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Distillation: Do Not Overlook Calibration. (arXiv:2307.12463v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#30340; logits &#20998;&#24067;&#26356;&#21152;&#38598;&#20013;&#65292;&#24182;&#19988;&#35821;&#20041;&#26126;&#30830;&#20294;&#19982;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#20250;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918; (MTS) &#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451; (MDT) &#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#36807;&#33976;&#39311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#36755;&#20986;&#65292;&#24182;&#38656;&#35201;&#36890;&#36807;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20462;&#27491;&#12290;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#22914;&#28201;&#24230;&#32553;&#25918;&#21644;&#28151;&#21512;&#35757;&#32451;&#65292;&#22312;&#21407;&#22987;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#25928;&#26524;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23545;&#20174;&#22823;&#28304;&#25968;&#25454;&#38598;&#33976;&#39311;&#20986;&#30340;&#25968;&#25454;&#36827;&#34892;&#26657;&#20934;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33976;&#39311;&#25968;&#25454;&#20250;&#23548;&#33268;&#32593;&#32476;&#26080;&#27861;&#26657;&#20934;&#65292;&#21407;&#22240;&#26159;&#65288;i&#65289;&#26368;&#22823;logit&#20998;&#24067;&#26356;&#20026;&#38598;&#20013;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#20998;&#31867;&#20219;&#21153;&#26080;&#20851;&#20294;&#35821;&#20041;&#24847;&#20041;&#26126;&#30830;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36974;&#34109;&#28201;&#24230;&#32553;&#25918;&#65288;MTS&#65289;&#21644;&#36974;&#34109;&#33976;&#39311;&#35757;&#32451;&#65288;MDT&#65289;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#33976;&#39311;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#33976;&#39311;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.11897</link><description>&lt;p&gt;
Hindsight-DICE&#65306;&#31283;&#23450;&#20449;&#29992;&#20998;&#37197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hindsight-DICE&#31639;&#27861;&#65292;&#21033;&#29992;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#25913;&#21892;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#29615;&#22659;&#24448;&#24448;&#25552;&#20379;&#24456;&#23569;&#30340;&#35780;&#20272;&#21453;&#39304;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#34892;&#20026;&#30340;&#38271;&#26102;&#38388;&#36712;&#36857;&#20165;&#20197;&#19968;&#20010;&#32456;&#27490;&#20449;&#21495;&#26631;&#35760;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#38750;&#24179;&#20961;&#22870;&#21169;&#21644;&#35302;&#21457;&#27492;&#31867;&#21453;&#39304;&#30340;&#20010;&#20307;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#26102;&#38388;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#31181;&#20449;&#29992;&#20998;&#37197;&#25361;&#25112;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#37325;&#35201;&#25277;&#26679;&#27604;&#29575;&#20272;&#35745;&#25216;&#26415;&#26469;&#26174;&#33879;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22788;&#29702;&#12290;&#34429;&#28982;&#20351;&#29992;&#25152;&#35859;&#30340;&#20107;&#21518;&#31574;&#30053;&#20026;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#36820;&#22238;&#36820;&#22238;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#24212;&#29992;&#37325;&#35201;&#25277;&#26679;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#25110;&#36807;&#24230;&#28382;&#21518;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#65292;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11629</link><description>&lt;p&gt;
&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-agent Skill Discovery based on Kronecker Graphs. (arXiv:2307.11629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Kronecker&#22270;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#65292;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#33021;&#21457;&#29616;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#32463;&#29992;&#20110;&#25913;&#21892;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19979;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#30340;&#25506;&#32034;&#65292;&#36890;&#36807;&#36830;&#25509;&#29366;&#24577;&#36716;&#25442;&#22270;&#30340;Fiedler&#21521;&#37327;&#25152;&#25552;&#20379;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#26368;&#36828;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#29616;&#26377;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#21333;&#26234;&#33021;&#20307;&#25216;&#33021;&#21457;&#29616;&#65292;&#35201;&#20040;&#21464;&#24471;&#38590;&#20197;&#23454;&#29616;&#65292;&#35201;&#20040;&#26080;&#27861;&#30452;&#25509;&#21457;&#29616;&#25913;&#21892;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36830;&#36890;&#24615;&#30340;&#32852;&#21512;&#25216;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#30452;&#25509;&#35745;&#31639;&#20855;&#26377;&#21327;&#20316;&#25506;&#32034;&#34892;&#20026;&#30340;&#22810;&#26234;&#33021;&#20307;&#25216;&#33021;&#65292;&#21516;&#26102;&#20139;&#21463;&#21040;&#20998;&#35299;&#30340;&#20415;&#21033;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32852;&#21512;&#29366;&#24577;&#31354;&#38388;&#36817;&#20284;&#20026;&#19968;&#20010;Kronecker&#22270;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#20010;&#26234;&#33021;&#20307;&#30340;&#36716;&#25442;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#30452;&#25509;&#20272;&#35745;&#20854;Fiedler&#21521;&#37327;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#32771;&#34385;&#21040;&#30452;&#25509;&#35745;&#31639;&#25289;&#26222;&#25289;&#26031;&#35889;&#22312;&#35745;&#31639;&#19978;&#30340;&#22797;&#26434;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent option discovery either become prohibitive or fail to directly discover joint options that improve the connectivity of the joint state space. In this paper, we show how to directly compute multi-agent options with collaborative exploratory behaviors while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10253</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#29992;&#20110;&#20117;&#26354;&#32447;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#26680;&#24515;&#38075;&#20117;&#36880;&#28176;&#25104;&#20026;&#22320;&#36136;&#24037;&#31243;&#20013;&#30340;&#20027;&#35201;&#21208;&#25506;&#26041;&#27861;&#65292;&#20117;&#26354;&#32447;&#20316;&#20026;&#22320;&#36136;&#20449;&#24687;&#30340;&#20027;&#35201;&#36733;&#20307;&#26085;&#30410;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22320;&#36136;&#29615;&#22659;&#12289;&#27979;&#20117;&#35774;&#22791;&#12289;&#38075;&#23380;&#36136;&#37327;&#21644;&#31361;&#21457;&#20107;&#20214;&#31561;&#22240;&#32032;&#37117;&#20250;&#24433;&#21709;&#20117;&#26354;&#32447;&#30340;&#36136;&#37327;&#12290;&#20197;&#24448;&#30340;&#37325;&#26032;&#27979;&#20117;&#25110;&#25163;&#24037;&#20462;&#27491;&#26041;&#27861;&#25104;&#26412;&#39640;&#25928;&#29575;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#39044;&#27979;&#32570;&#22833;&#20117;&#26354;&#32447;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#19978;&#21152;&#20837;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#20998;&#26512;&#25968;&#25454;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#26377;&#36873;&#25321;&#22320;&#23558;LSTM&#20013;&#30340;&#20027;&#23548;&#35745;&#31639;&#32467;&#26524;&#21253;&#25324;&#22312;&#20869;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#33267;O(nlogn)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26694;&#26550;CAF&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#26469;&#36991;&#20813;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.04937</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22270;&#21453;&#20107;&#23454;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26694;&#26550;CAF&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21453;&#20107;&#23454;&#26469;&#36991;&#20813;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#36827;&#34892;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#24314;&#27169;&#22270;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#20542;&#21521;&#20110;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21644;&#25918;&#22823;&#20559;&#35265;&#65292;&#24341;&#36215;&#20102;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#20351;&#29992;GNN&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#23454;&#29616;&#20844;&#24179;&#24863;&#30693;&#30340;GNN&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;GNN&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#20844;&#24179;&#27010;&#24565;&#26469;&#23398;&#20064;&#20844;&#24179;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#22312;&#32479;&#35745;&#24322;&#24120;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20943;&#36731;&#20559;&#35265;&#12290;&#21463;&#22240;&#26524;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#26377;&#20960;&#31181;&#26041;&#27861;&#21033;&#29992;&#22270;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#26469;&#20943;&#36731;&#19981;&#20844;&#24179;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#21463;&#21040;&#36890;&#36807;&#25200;&#21160;&#25110;&#29983;&#25104;&#33719;&#24471;&#30340;&#38750;&#29616;&#23454;&#21453;&#20107;&#23454;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#22240;&#26524;&#35270;&#35282;&#30475;&#24453;&#20844;&#24179;&#22270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#22240;&#26524;&#20998;&#26512;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;CAF&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36873;&#25321;&#21453;&#20107;&#23454;&#20197;&#36991;&#20813;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04345</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning as Computationally Constrained Reinforcement Learning. (arXiv:2307.04345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#33021;&#22815;&#22312;&#28459;&#38271;&#30340;&#29983;&#21629;&#21608;&#26399;&#20869;&#39640;&#25928;&#31215;&#32047;&#30693;&#35782;&#24182;&#21457;&#23637;&#36234;&#26469;&#36234;&#22797;&#26434;&#25216;&#33021;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#12290;&#36830;&#32493;&#23398;&#20064;&#36825;&#19968;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#27010;&#24565;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03206</link><description>&lt;p&gt;
DENCLUE&#30340;&#26368;&#20248;&#24102;&#23485;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#24037;&#19994;&#20013;&#65292;&#32858;&#31867;&#31639;&#27861;&#26159;&#31639;&#27861;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#24037;&#20316;&#12290;&#23613;&#31649;&#22312;2010&#24180;&#20043;&#21069;&#65292;&#32858;&#31867;&#31639;&#27861;&#32463;&#21382;&#20102;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#23454;&#38469;&#24037;&#19994;&#26631;&#20934;&#20043;&#21518;&#65292;&#19982;&#35813;&#30740;&#31350;&#20027;&#39064;&#30456;&#20851;&#30340;&#21019;&#26032;&#20572;&#28382;&#19981;&#21069;&#12290;2007&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DENCLUE&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#25968;&#25454;&#32467;&#26500;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;2011&#24180;&#65292;&#35813;&#31639;&#27861;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#20173;&#28982;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;Q-learning&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20048;&#35266;&#24615;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;&#31574;&#30053;&#35299;&#20915;Q-learning&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#21644;&#31639;&#27861;&#25910;&#25947;&#21152;&#36895;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02632</link><description>&lt;p&gt;
Q-Learning &#30340;&#31283;&#23450;&#24615;&#36890;&#36807;&#35774;&#35745;&#21644;&#20048;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;Q-learning&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20351;&#29992;&#20048;&#35266;&#24615;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;&#31574;&#30053;&#35299;&#20915;Q-learning&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#21644;&#31639;&#27861;&#25910;&#25947;&#21152;&#36895;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;20&#19990;&#32426;80&#24180;&#20195;Chris Watkins&#30340;&#35770;&#25991;&#20013;&#20171;&#32461;&#20197;&#26469;&#65292;Q-learning&#24050;&#25104;&#20026;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#37096;&#20998;&#26159;&#20851;&#20110;&#38543;&#26426;&#36924;&#36817;&#21644;Q-learning&#30340;&#25945;&#31243;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;INFORMS APS&#21457;&#24067;&#30340;&#31532;&#19968;&#23626;&#24212;&#29992;&#27010;&#29575;&#20449;&#25176;&#20840;&#20307;&#22823;&#20250;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#33021;&#21152;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#20854;&#20182;&#35774;&#32622;&#20013;&#30340;&#38543;&#26426;&#36924;&#36817;&#12290;&#20004;&#20010;&#36129;&#29486;&#26159;&#20840;&#26032;&#30340;&#65306;1. Q-learning&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31283;&#23450;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#24453;&#30740;&#31350;&#30340;&#35805;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20048;&#35266;&#35757;&#32451;&#21644;&#20462;&#25913;&#21518;&#30340;Gibbs&#31574;&#30053;&#65292;&#21487;&#20197;&#23384;&#22312;&#28385;&#36275;&#25237;&#24433;Bellman&#26041;&#31243;&#30340;&#35299;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#26159;&#31283;&#23450;&#30340;&#65288;&#21442;&#25968;&#20272;&#35745;&#26377;&#30028;&#65289;&#12290;&#25910;&#25947;&#24615;&#20173;&#28982;&#26159;&#20247;&#22810;&#24453;&#30740;&#31350;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;2. &#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#23567;&#25209;&#37327;&#25191;&#34892;&#20013;&#25913;&#21892;&#20102;&#36924;&#36817;&#31639;&#27861;&#30340;&#36845;&#20195;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13812</link><description>&lt;p&gt;
&#25345;&#32493;&#24615;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity in Deep Continual Learning. (arXiv:2306.13812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13812
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#19987;&#38376;&#29992;&#20110;&#19968;&#27425;&#24615;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#25345;&#32493;&#24615;&#23398;&#20064;&#65292;&#22914;&#26524;&#23558;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#24212;&#29992;&#20110;&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#21017;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#21487;&#33021;&#22312;&#35760;&#20303;&#26089;&#26399;&#30340;&#20363;&#23376;&#26041;&#38754;&#36973;&#36935;&#22833;&#36133;&#12290;&#26356;&#20026;&#22522;&#26412;&#20294;&#19981;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#21487;&#22609;&#24615;&#20007;&#22833;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#37325;&#26500;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#12290;&#22312;ImageNet&#20013;&#65292;&#20108;&#20803;&#20998;&#31867;&#30340;&#24615;&#33021;&#20174;&#19968;&#20010;&#26089;&#26399;&#20219;&#21153;&#30340;89&#65285;&#27491;&#30830;&#19979;&#38477;&#21040;77&#65285;&#65292;&#25110;&#32773;&#22823;&#32422;&#31561;&#20110;&#32447;&#24615;&#32593;&#32476;&#30340;&#27700;&#24179;&#12290;&#36825;&#31181;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#21457;&#29983;&#22312;&#21508;&#31181;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#20248;&#21270;&#22120;&#21644;&#28608;&#27963;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#19981;&#20250;&#22240;&#25209;&#37327;&#24402;&#19968;&#21270;&#25110;&#25918;&#24323;&#32780;&#24471;&#21040;&#32531;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Contrastive Plasticity&#65292;&#21487;&#20197;&#32531;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#21516;&#26102;&#20445;&#30041;&#35760;&#20303;&#26087;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;Contrastive Plasticity&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#24102;&#26469;&#38750;&#24120;&#23569;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasti
&lt;/p&gt;</description></item><item><title>TACOformer &#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#21033;&#29992; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention &#27169;&#22359;&#65292;&#21516;&#26102;&#24314;&#27169;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13592</link><description>&lt;p&gt;
TACOformer: &#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#20196;&#29260;&#36890;&#36947;&#22797;&#21512;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition. (arXiv:2306.13592v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13592
&lt;/p&gt;
&lt;p&gt;
TACOformer &#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#21033;&#29992; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention &#27169;&#22359;&#65292;&#21516;&#26102;&#24314;&#27169;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#35782;&#21035;&#25104;&#20026;&#20102;&#19968;&#20010;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#27169;&#24577;&#12289;&#22810;&#36890;&#36947;&#30340;&#29983;&#29702;&#20449;&#21495;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#24182;&#25429;&#33719;&#36328;&#27169;&#24577;&#30340;&#20381;&#36182;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#24573;&#30053;&#20102;&#22810;&#20010;&#36890;&#36947;&#20449;&#21495;&#20043;&#38388;&#30340;&#20196;&#29260;&#21040;&#20196;&#29260;&#25110;&#36890;&#36947;&#21040;&#36890;&#36947;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#35270;&#35282;&#65292;&#23427;&#25972;&#21512;&#20102;&#36890;&#36947;&#32423;&#21035;&#21644;&#20196;&#29260;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#37319;&#29992;&#22797;&#21512;&#26426;&#21046;&#36827;&#34892;&#36328;&#36890;&#36947;&#22788;&#29702;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#31216;&#20026; Token-chAnnel COmpound&#65288;TACO&#65289;Cross Attention&#65292;&#29992;&#20110;&#25191;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TACOformer &#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel correlations of multichannel signals from different modalities, which limits the classification capability of the models to some extent. In this paper, we propose a comprehensive perspective of multimodal fusion that integrates channel-level and token-level cross-modal interactions. Specifically, we introduce a unified cross attention module called Token-chAnnel COmpound (TACO) Cross Attention to perform multimodal fusion, which simultaneously models channel-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#27169;&#24577;&#20998;&#31867;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35774;&#35745;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.08306</link><description>&lt;p&gt;
&#36808;&#21521;&#24179;&#34913;&#22810;&#27169;&#24577;&#20998;&#31867;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Balanced Active Learning for Multimodal Classification. (arXiv:2306.08306v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#27169;&#24577;&#20998;&#31867;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35774;&#35745;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#36739;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#36873;&#25321;&#21482;&#23545;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#26377;&#36129;&#29486;&#30340;&#26679;&#26412;&#65292;&#26469;&#20943;&#23569;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#21333;&#27169;&#24577;&#20219;&#21153;&#35774;&#35745;&#65292;&#24403;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#20174;&#20027;&#23548;&#27169;&#24577;&#20013;&#36873;&#25321;&#26679;&#26412;&#30340;&#20559;&#35265;&#12290;&#36825;&#31181;&#19981;&#20844;&#24179;&#38459;&#30861;&#20102;&#24179;&#34913;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#35774;&#35745;&#26356;&#24179;&#34913;&#30340;&#22810;&#27169;&#24577;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20934;&#21017;&#12290;&#22312;&#36981;&#24490;&#36825;&#20123;&#20934;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#24577;&#20043;&#38388;&#35843;&#33410;&#26799;&#24230;&#23884;&#20837;&#21644;&#20027;&#23548;&#31243;&#24230;&#65292;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#36991;&#20813;&#36138;&#23146;&#22320;&#36873;&#25321;&#26679;&#26412;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training multimodal networks requires a vast amount of data due to their larger parameter space compared to unimodal networks. Active learning is a widely used technique for reducing data annotation costs by selecting only those samples that could contribute to improving model performance. However, current active learning strategies are mostly designed for unimodal tasks, and when applied to multimodal data, they often result in biased sample selection from the dominant modality. This unfairness hinders balanced multimodal learning, which is crucial for achieving optimal performance. To address this issue, we propose three guidelines for designing a more balanced multimodal active learning strategy. Following these guidelines, a novel approach is proposed to achieve more fair data selection by modulating the gradient embedding with the dominance degree among modalities. Our studies demonstrate that the proposed method achieves more balanced multimodal learning by avoiding greedy sample
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#40657;&#30418;&#20248;&#21270;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#22122;&#25193;&#25955;&#20248;&#21270;&#27169;&#22411;&#65288;DDOM&#65289;&#12290;DDOM&#36890;&#36807;&#23398;&#20064;&#40657;&#30418;&#20989;&#25968;&#20540;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#39640;&#32500;&#24230;&#19979;&#19968;&#23545;&#22810;&#26144;&#23556;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.07180</link><description>&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Black-Box Optimization. (arXiv:2306.07180v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#40657;&#30418;&#20248;&#21270;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#22122;&#25193;&#25955;&#20248;&#21270;&#27169;&#22411;&#65288;DDOM&#65289;&#12290;DDOM&#36890;&#36807;&#23398;&#20064;&#40657;&#30418;&#20989;&#25968;&#20540;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#39640;&#32500;&#24230;&#19979;&#19968;&#23545;&#22810;&#26144;&#23556;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#40657;&#30418;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22266;&#23450;&#30340;&#20989;&#25968;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#20248;&#21270;&#19968;&#20010;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#21069;&#21521;&#26041;&#27861;&#21644;&#21453;&#21521;&#26041;&#27861;&#12290;&#21069;&#21521;&#26041;&#27861;&#23398;&#20064;&#40657;&#30418;&#20989;&#25968;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#32780;&#21453;&#21521;&#26041;&#27861;&#30452;&#25509;&#23558;&#20989;&#25968;&#20540;&#26144;&#23556;&#21040;&#36755;&#20837;&#22495;&#30340;&#30456;&#24212;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22312;&#39640;&#32500;&#24230;&#20013;&#23398;&#20064;&#19968;&#23545;&#22810;&#26144;&#23556;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31163;&#32447;&#40657;&#30418;&#20248;&#21270;&#30340;&#26032;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#22122;&#25193;&#25955;&#20248;&#21270;&#27169;&#22411;&#65288;DDOM&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;DDOM&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#40657;&#30418;&#20989;&#25968;&#22495;&#19978;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#20989;&#25968;&#20540;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;DDOM&#20013;&#30340;&#20960;&#31181;&#35774;&#35745;&#36873;&#25321;&#65292;&#20363;&#22914;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#20197;&#20415;&#37325;&#28857;&#20851;&#27880;&#39640;&#20989;&#25968;&#20540;&#65292;&#20197;&#21450;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of offline black-box optimization (BBO) is to optimize an expensive black-box function using a fixed dataset of function evaluations. Prior works consider forward approaches that learn surrogates to the black-box function and inverse approaches that directly map function values to corresponding points in the input domain of the black-box function. These approaches are limited by the quality of the offline dataset and the difficulty in learning one-to-many mappings in high dimensions, respectively. We propose Denoising Diffusion Optimization Models (DDOM), a new inverse approach for offline black-box optimization based on diffusion models. Given an offline dataset, DDOM learns a conditional generative model over the domain of the black-box function conditioned on the function values. We investigate several design choices in DDOM, such as re-weighting the dataset to focus on high function values and the use of classifier-free guidance at test-time to enable generalization to fun
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2306.06236</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24322;&#26500;&#20132;&#36890;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#31639;&#27861;iPLAN
&lt;/p&gt;
&lt;p&gt;
iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06236
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;MARL&#31639;&#27861;&#30340;iPLAN&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#23494;&#24230;&#19988;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#34892;&#20026;&#25110;&#30636;&#26102;&#28608;&#21169;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#20445;&#38556;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#25110;&#24847;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36712;&#36857;&#21644;&#24847;&#22270;&#39044;&#27979;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23494;&#24230;&#21644;&#24322;&#26500;&#20132;&#36890;&#22330;&#26223;&#20013;&#36827;&#34892;&#24847;&#22270;&#24863;&#30693;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;iPLAN&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#20165;&#20174;&#20854;&#26412;&#22320;&#35266;&#27979;&#20013;&#25512;&#26029;&#38468;&#36817;&#39550;&#39542;&#32773;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#28608;&#21169;&#22240;&#32032;&#65306;&#34892;&#20026;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#38271;&#26399;&#35268;&#21010;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#39550;&#39542;&#34892;&#20026;&#25110;&#20010;&#24615;&#65307;&#30636;&#26102;&#28608;&#21169;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#30701;&#26399;&#35268;&#21010;&#65292;&#20197;&#22522;&#20110;&#24403;&#21069;&#20132;&#36890;&#29366;&#24577;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27969;&#25512;&#29702;&#27169;&#22359;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#25512;&#26029;&#23545;&#25163;&#30340;&#28608;&#21169;&#24182;&#23558;&#20854;&#25512;&#26029;&#20449;&#24687;&#32435;&#20837;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16822</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16822
&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26085;&#30410;&#34987;&#29992;&#20110;&#39537;&#21160;&#37096;&#32626;&#22312;5G&#20113;&#36793;&#32536;&#36830;&#32493;&#20307;&#19978;&#30340;&#22797;&#26434;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#36816;&#34892;&#12290;&#30456;&#24212;&#22320;&#65292;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#26356;&#20855;&#38750;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#28436;&#21270;&#38656;&#35201;&#23450;&#20041;&#26032;&#30340;&#20445;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#35748;&#35777;&#20316;&#20026;&#31995;&#32479;&#21644;&#36719;&#20214;&#39564;&#35777;&#30340;&#26368;&#27969;&#34892;&#30340;&#20445;&#35777;&#25216;&#26415;&#65292;&#19981;&#33021;&#31435;&#21363;&#36866;&#29992;&#20110;&#20854;&#34892;&#20026;&#30001;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25512;&#29702;&#20915;&#23450;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20135;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#36234;&#26469;&#36234;&#25512;&#23815;&#23450;&#20041;ML&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#65289;&#30340;&#35748;&#35777;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#35748;&#35777;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15121</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#36827;&#34892;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#36229;&#36234;&#20010;&#20307;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#65288;NPTs&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22522;&#20110;&#37325;&#26500;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;NPT&#26469;&#37325;&#26500;&#27491;&#24120;&#26679;&#26412;&#30340;&#36974;&#34109;&#29305;&#24449;&#12290;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25972;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#22312;&#29983;&#25104;&#24322;&#24120;&#24471;&#20998;&#26102;&#37325;&#26500;&#36974;&#34109;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#25104;&#21151;&#32467;&#21512;&#29305;&#24449;&#20043;&#38388;&#21644;&#26679;&#26412;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#21644;AUROC&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
&lt;/p&gt;</description></item><item><title>GraphFC&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11377</link><description>&lt;p&gt;
GraphFC: &#24102;&#26377;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphFC: Customs Fraud Detection with Label Scarcity. (arXiv:2305.11377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11377
&lt;/p&gt;
&lt;p&gt;
GraphFC&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#23427;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30340;&#28023;&#20851;&#23448;&#21592;&#27599;&#24180;&#38754;&#23545;&#30528;&#28023;&#37327;&#30340;&#20132;&#26131;&#12290;&#38543;&#30528;&#36830;&#36890;&#24615;&#21644;&#20840;&#29699;&#21270;&#30340;&#22686;&#21152;&#65292;&#28023;&#20851;&#20132;&#26131;&#25345;&#32493;&#22686;&#38271;&#12290;&#19982;&#28023;&#20851;&#20132;&#26131;&#30456;&#20851;&#30340;&#26159;&#28023;&#20851;&#27450;&#35784;&#8212;&#8212;&#21363;&#26377;&#24847;&#20462;&#25913;&#36135;&#29289;&#30003;&#25253;&#20197;&#36991;&#20813;&#31246;&#27454;&#21644;&#20851;&#31246;&#12290;&#30001;&#20110;&#20154;&#25163;&#19981;&#36275;&#65292;&#28023;&#20851;&#21150;&#20844;&#23460;&#21482;&#33021;&#23545;&#26377;&#38480;&#25968;&#37327;&#30340;&#30003;&#25253;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#33258;&#21160;&#21270;&#28023;&#20851;&#27450;&#35784;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26032;&#36827;&#30003;&#25253;&#26816;&#26597;&#30340;&#26631;&#31614;&#25968;&#25454;&#21463;&#38480;&#65292;ML&#26041;&#27861;&#24212;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;$\textbf{GraphFC}$&#65288;&#28023;&#20851;&#27450;&#35784;$\textbf{GNN}$&#65288;$\textbf{G}$raph $\textbf{N}$eural $\textbf{N}$etworks&#65289;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#12289;&#39046;&#22495;&#29305;&#23450;&#12289;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#37327;&#26631;&#31614;&#25968;&#25454;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;$\textbf{GraphFC}$&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#28023;&#20851;&#20132;&#26131;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26631;&#31614;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#28023;&#20851;&#27450;&#35784;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Custom officials across the world encounter huge volumes of transactions. With increased connectivity and globalization, the customs transactions continue to grow every year. Associated with customs transactions is the customs fraud - the intentional manipulation of goods declarations to avoid the taxes and duties. With limited manpower, the custom offices can only undertake manual inspection of a limited number of declarations. This necessitates the need for automating the customs fraud detection by machine learning (ML) techniques. Due the limited manual inspection for labeling the new-incoming declarations, the ML approach should have robust performance subject to the scarcity of labeled data. However, current approaches for customs fraud detection are not well suited and designed for this real-world setting. In this work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for $\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific, semi-supervised graph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11284</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#23433;&#20840;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35828;&#35805;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#24085;&#37329;&#26862;&#30149;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20005;&#26684;&#30340;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#38459;&#30861;&#20102;&#26426;&#26500;&#38388;&#20849;&#20139;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19981;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#31561;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.01154</link><description>&lt;p&gt;
FedAVO&#65306;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#24378;&#35843;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;FL&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21487;&#33021;&#23548;&#33268;&#36890;&#20449;&#21463;&#38480;&#24182;&#19988;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#25302;&#24310;&#65292;&#38656;&#35201;&#23545;&#23458;&#25143;-&#26381;&#21153;&#22120;&#36890;&#20449;&#25104;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#36873;&#25321;&#30340;&#23458;&#25143;&#27604;&#20363;&#21644;&#26412;&#22320;&#35757;&#32451;&#24490;&#29615;&#27425;&#25968;&#26159;&#23545;FL&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#20004;&#20010;&#36229;&#21442;&#25968;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#20559;&#22909;&#65292;&#22240;&#27492;FL&#20174;&#19994;&#32773;&#24456;&#38590;&#25163;&#21160;&#36873;&#25321;&#36825;&#20123;&#36229;&#21442;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedAVO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;FL&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#65288;AVO&#65289;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;AVO&#36827;&#34892;FL&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
&lt;/p&gt;</description></item><item><title>SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.00795</link><description>&lt;p&gt;
SelfDocSeg: &#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00795
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#25366;&#25496;&#12289;&#35782;&#21035;&#21040;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12289;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#31561;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24573;&#30053;&#20102;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36825;&#19968;&#20851;&#38190;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#20013;&#29983;&#25104;&#20266;&#24067;&#23616;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30495;&#23454;&#26631;&#31614;&#25110;&#20854;&#23548;&#20986;&#29289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.10031</link><description>&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65306;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10031
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#20013;&#20805;&#28385;&#20102;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#20854;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65306;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#21040;&#34507;&#30333;&#36136;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#12290;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21644;&#20174;&#36825;&#20123;&#31995;&#32479;&#30456;&#20851;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#22914;&#39044;&#27979;&#19968;&#20010;&#20154;&#23646;&#20110;&#21738;&#20010;&#31038;&#21306;&#25110;&#39044;&#27979;&#19968;&#20010;&#34507;&#30333;&#36136;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#21512;&#29702;&#30340;&#33647;&#29289;&#24320;&#21457;&#38774;&#28857;&#12290;TDL&#24050;&#32463;&#35777;&#26126;&#25317;&#26377;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#30340;&#20248;&#21183;&#65292;&#36825;&#20026;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;TDL&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#20063;&#23548;&#33268;&#20102;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#20307;&#31995;&#32467;&#26500;&#31526;&#21495;&#21644;&#26415;&#35821;&#19978;&#30340;&#19981;&#19968;&#33268;&#12290;&#36825;&#23545;&#20110;&#24314;&#31435;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#21644;&#23558;TNN&#37096;&#32626;&#21040;&#26032;&#30340;&#29616;&#23454;&#38382;&#39064;&#20013;&#37117;&#26159;&#19968;&#20010;&#30495;&#27491;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09987</link><description>&lt;p&gt;
Tetra-NeRF&#65306;&#20351;&#29992;&#22235;&#38754;&#20307;&#34920;&#31034;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#26500;&#38382;&#39064;&#12290;NeRF &#24120;&#29992;&#30340;&#22330;&#26223;&#34920;&#31034;&#26159;&#23558;&#22330;&#26223;&#30340;&#19968;&#33268;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#32454;&#20998;&#19982; MLP &#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#30340;&#65288;&#31232;&#30095;&#65289;&#28857;&#20113;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#32780;&#38750;&#19968;&#33268;&#30340;&#32454;&#20998;&#25110;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#34920;&#31034;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#19977;&#32500;&#20960;&#20309;&#22788;&#29702;&#12289;&#19977;&#35282;&#24418;&#28210;&#26579;&#21644;&#29616;&#20195;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#27010;&#24565;&#12290;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#22330;&#26223;&#32454;&#33410;&#12290;&#19982;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26412;&#22320;&#27493;&#39588;&#21644;&#35843;&#25972;&#21151;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05127</link><description>&lt;p&gt;
&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#31169;&#26377;&#32852;&#37030;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance of Private Federated Models in Medical Image Analysis. (arXiv:2304.05127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26412;&#22320;&#27493;&#39588;&#21644;&#35843;&#25972;&#21151;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#21307;&#23398;&#24212;&#29992;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#19982;&#21307;&#30103;&#25968;&#25454;&#30456;&#20851;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25968;&#25454;&#25152;&#26377;&#26435;&#12290;&#27492;&#22806;&#65292;FL&#21487;&#20197;&#25552;&#39640;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;ML&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#21307;&#30103;&#25968;&#25454;&#36890;&#24120;&#26159;&#22810;&#26679;&#21270;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#30149;&#20154;&#32676;&#20307;&#32780;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20934;&#30830;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;ML&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;FL&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#30340;&#21307;&#30103;&#25968;&#25454;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#20445;&#25252;&#35813;&#36807;&#31243;&#23433;&#20840;&#21644;&#31169;&#23494;&#24615;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#26412;&#22320;&#27493;&#39588;&#65288;&#19968;&#31181;&#25552;&#39640;FL&#36890;&#20449;&#25928;&#29575;&#30340;&#24120;&#29992;&#26041;&#27861;&#65289;&#21644;&#35843;&#25972;&#36890;&#20449;&#27425;&#25968;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) approach that allows data to be trained without being centralized. This approach is particularly beneficial for medical applications because it addresses some key challenges associated with medical data, such as privacy, security, and data ownership. On top of that, FL can improve the quality of ML models used in medical applications. Medical data is often diverse and can vary significantly depending on the patient population, making it challenging to develop ML models that are accurate and generalizable. FL allows medical data to be used from multiple sources, which can help to improve the quality and generalizability of ML models. Differential privacy (DP) is a go-to algorithmic tool to make this process secure and private. In this work, we show that the model performance can be further improved by employing local steps, a popular approach to improving the communication efficiency of FL, and tuning the number of communica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#65292;&#33021;&#22815;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#65292;&#33021;&#22815;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.15634</link><description>&lt;p&gt;
&#23398;&#20064;&#36895;&#29575;&#34920;&#22312;&#20998;&#24067;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Rate Schedules in the Presence of Distribution Shift. (arXiv:2303.15634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#65292;&#33021;&#22815;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#65292;&#33021;&#22815;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#39062;&#20998;&#26512;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21482;&#26377;&#24120;&#25968;&#24046;&#24322;&#30340;&#21518;&#24724;&#19978;&#19979;&#30028;&#12290;&#23545;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#22522;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#26799;&#24230;&#33539;&#25968;&#23450;&#20041;&#20102;&#19968;&#31181;&#21518;&#24724;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26102;&#38388;&#34920;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#39044;&#26399;&#21518;&#24724;&#30340;&#19978;&#38480;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#39044;&#35745;&#25439;&#22833;&#39046;&#22495;&#30340;&#21464;&#21270;&#38656;&#35201;&#26356;&#22810;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#20197;&#35828;&#26126;&#36825;&#20123;&#23398;&#20064;&#36895;&#29575;&#34920;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedule
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (DDAE) &#26159;&#21542;&#33021;&#36890;&#36807;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#35757;&#32451;&#33719;&#21462;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;DDAE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#26159;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.09769</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#26159;&#32479;&#19968;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Autoencoders are Unified Self-supervised Learners. (arXiv:2303.09769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (DDAE) &#26159;&#21542;&#33021;&#36890;&#36807;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#35757;&#32451;&#33719;&#21462;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;DDAE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#26159;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#31867;&#20284;&#20110;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#39044;&#35757;&#32451;&#33719;&#21462;&#20998;&#31867;&#30340;&#36776;&#21035;&#24615;&#34920;&#31034;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32593;&#32476;&#65292;&#21363;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;(DDAE)&#26159;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;:&#36890;&#36807;&#22312;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;DDAE&#24050;&#32463;&#22312;&#20013;&#38388;&#23618;&#23398;&#20064;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#20351;&#25193;&#25955;&#39044;&#35757;&#32451;&#25104;&#20026;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#35780;&#20272;&#12290;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22312;CIFAR-10&#21644;Tiny-ImageNet&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#39318;&#27425;&#21487;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#20174;Image&#19978;&#30340;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations at its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for self-supervised generative and discriminative learning. To verify this, we perform linear probe and fine-tuning evaluations on multi-class datasets. Our diffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked autoencoders and contrastive learning for the first time. Additionally, transfer learning from Image
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.08757</link><description>&lt;p&gt;
&#21033;&#29992;&#22235;&#32500;CT&#28748;&#27880;&#25104;&#20687;&#23545;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22235;&#32500;CTP&#20840;&#38754;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#65292;&#20197;&#20998;&#21106;&#30097;&#20284;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#24739;&#32773;&#30340;&#26775;&#27515;&#21306;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#12289;&#24555;&#36895;&#30340;&#24613;&#24615;&#32570;&#34880;&#24615;&#21330;&#20013;&#65288;AIS&#65289;&#24739;&#32773;&#32570;&#34880;&#21306;&#65288;&#26680;&#24515;&#21644;&#21322;&#24433;&#21306;&#65289;&#39044;&#27979;&#26041;&#27861;&#23545;&#20110;&#25913;&#36827;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#30097;&#20284;AIS&#24739;&#32773;&#26089;&#26399;&#35780;&#20272;&#30340;&#20027;&#35201;&#27169;&#24335;&#20043;&#19968;&#12290;CT&#28748;&#27880;&#25104;&#20687;&#65288;CTP&#65289;&#36890;&#24120;&#29992;&#20316;&#20027;&#35201;&#35780;&#20272;&#25163;&#27573;&#65292;&#20197;&#30830;&#23450;&#21330;&#20013;&#20301;&#32622;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#32570;&#34880;&#24615;&#30149;&#28790;&#20307;&#31215;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;CTP&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#37117;&#20351;&#29992;&#24050;&#32463;&#22788;&#29702;&#36807;&#30340;&#19977;&#32500;&#24425;&#33394;&#22320;&#22270;&#20316;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#24120;&#35268;&#35270;&#35273;&#35780;&#20272;&#30340;&#36755;&#20837;&#12290;&#25110;&#32773;&#65292;&#22522;&#20110;&#20999;&#29255;&#30340;&#20108;&#32500;+&#26102;&#38388;&#36755;&#20837;&#20351;&#29992;&#21407;&#22987;CTP&#25968;&#25454;&#65292;&#20854;&#20013;&#24573;&#30053;&#20102;&#22312;&#20307;&#31215;&#19978;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#20010;&#22235;&#32500;CTP&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;4D&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#25913;&#21892;AIS&#24739;&#32773;&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.05101</link><description>&lt;p&gt;
&#38750;&#23545;&#35282;&#24230;&#37327;&#20013;&#30340;&#21487;&#25193;&#23637;&#38543;&#26426;&#26799;&#24230;&#37324;&#26364;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35266;&#23519;&#21040;&#65292;&#21253;&#21547;&#24494;&#20998;&#20960;&#20309;&#27010;&#24565;&#30340;&#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#37324;&#26364;&#24230;&#37327;&#36890;&#36807;&#32771;&#34385;&#23616;&#37096;&#26354;&#29575;&#26469;&#25913;&#21892;&#21518;&#39564;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#35282;&#24230;&#37327;&#20197;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#36825;&#20250;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#25506;&#32034;&#24615;&#65292;&#22312;&#23545;&#27604;&#23545;&#35282;&#24230;&#37327;&#21482;&#26377;&#36731;&#24494;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#12290;&#23545;&#20110;&#20854;&#20182;&#19968;&#20123;&#36873;&#25321;&#65292;&#21518;&#39564;&#20998;&#24067;&#22312;&#31616;&#21333;&#24230;&#37327;&#19979;&#20063;&#36275;&#22815;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.00028</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#22238;&#24402;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#19979;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#31163;&#25955;&#21270;&#29615;&#22659;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21487;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#26696;&#65292;&#29992;&#20110;&#30417;&#27979;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#31354;&#38388;&#65288;&#25110;&#26102;&#31354;&#65289;&#30456;&#20851;&#29616;&#35937;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24050;&#30693;&#20869;&#26680;&#20989;&#25968;&#21442;&#25968;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#21040;&#29615;&#22659;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#26410;&#26631;&#35760;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#35825;&#23548;&#28857;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#38382;&#39064;&#12290;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#31435;&#26041;&#32423;&#21035;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#12290;&#22312;&#20505;&#36873;&#20256;&#24863;&#22120;&#25918;&#32622;&#28857;&#38598;&#21512;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36138;&#23146;&#39034;&#24207;&#36873;&#25321;&#31639;&#27861;&#26469;&#25214;&#21040;&#36739;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#26469;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.14640</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning with Adaptive Weighted Loss for Imbalanced Cold-Start Recommendation. (arXiv:2302.14640v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#25439;&#22833;&#26469;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#22312;&#25429;&#25417;&#29992;&#25143;&#21916;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20919;&#21551;&#21160;&#25512;&#33616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#28041;&#21450;&#26377;&#38480;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#39034;&#24207;&#25512;&#33616;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#26131;&#20110;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#31639;&#27861;&#23558;&#20919;&#21551;&#21160;&#25512;&#33616;&#25551;&#36848;&#20026;&#19968;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#37117;&#34987;&#34920;&#31034;&#20026;&#38656;&#35201;&#36866;&#24212;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26679;&#26412;&#22312;&#31867;&#21035;&#25110;&#20540;&#19978;&#22343;&#21248;&#20998;&#24067;&#65292;&#32780;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#24182;&#19981;&#31526;&#21512;&#36825;&#26679;&#30340;&#20998;&#24067;&#65288;&#20363;&#22914;&#65292;&#22810;&#27425;&#35266;&#30475;&#21916;&#27426;&#30340;&#35270;&#39057;&#65292;&#21482;&#30041;&#19979;&#27491;&#38754;&#35780;&#20998;&#32780;&#27809;&#26377;&#36127;&#38754;&#35780;&#20998;&#65289;&#12290;&#22240;&#27492;&#65292;&#21344;&#25454;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#22823;&#37096;&#20998;&#30340;&#19981;&#24179;&#34913;&#29992;&#25143;&#21453;&#39304;&#21487;&#33021;&#20027;&#23548;&#30528;&#29992;&#25143;&#30340;&#36866;&#24212;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommenders have made great strides in capturing a user's preferences. Nevertheless, the cold-start recommendation remains a fundamental challenge as they typically involve limited user-item interactions for personalization. Recently, gradient-based meta-learning approaches have emerged in the sequential recommendation field due to their fast adaptation and easy-to-integrate abilities. The meta-learning algorithms formulate the cold-start recommendation as a few-shot learning problem, where each user is represented as a task to be adapted. While meta-learning algorithms generally assume that task-wise samples are evenly distributed over classes or values, user-item interactions in real-world applications do not conform to such a distribution (e.g., watching favorite videos multiple times, leaving only positive ratings without any negative ones). Consequently, imbalanced user feedback, which accounts for the majority of task training data, may dominate the user adaptation pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21363;&#21487;&#24674;&#22797;&#30697;&#38453;$M$&#65292;&#27492;&#26041;&#27861;&#20811;&#26381;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#38656;&#35201;&#31934;&#30830;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#26356;&#31526;&#21512;&#23454;&#38469;&#23454;&#29616;&#20013;&#23545;&#25928;&#29575;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.11068</link><description>&lt;p&gt;
&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time. (arXiv:2302.11068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21363;&#21487;&#24674;&#22797;&#30697;&#38453;$M$&#65292;&#27492;&#26041;&#27861;&#20811;&#26381;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#38656;&#35201;&#31934;&#30830;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#26356;&#31526;&#21512;&#23454;&#38469;&#23454;&#29616;&#20013;&#23545;&#25928;&#29575;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;$M\in \mathbb{R}^{m\times n}$&#65292;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#35201;&#27714;&#25105;&#20204;&#36890;&#36807;&#21482;&#35266;&#23519;&#19968;&#32452;&#25351;&#23450;&#30340;&#26465;&#30446;$\Omega\subseteq [m]\times [n]$&#26469;&#25214;&#21040;$M$&#30340;&#31209;&#20026;$k$&#30340;&#36817;&#20284;$UV^\top$&#65292;&#20854;&#20013;$U\in \mathbb{R}^{m\times k}$&#65292;$V\in \mathbb{R}^{n\times k}$&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;--&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#12290;Jain&#12289;Netrapalli&#21644;Sanghavi~\cite{jns13}&#35777;&#26126;&#20102;&#22914;&#26524;$M$&#30340;&#34892;&#21644;&#21015;&#26159;&#19981;&#30456;&#24178;&#30340;&#65292;&#37027;&#20040;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21487;&#38752;&#22320;&#24674;&#22797;&#30697;&#38453;$M$&#12290;&#34429;&#28982;&#26679;&#26412;&#22797;&#26434;&#24230;&#20043;&#21518;&#34987;&#25913;&#36827;~\cite{glz17}&#65292;&#20294;&#20132;&#26367;&#26368;&#23567;&#21270;&#27493;&#39588;&#35201;&#27714;&#31934;&#30830;&#35745;&#31639;&#12290;&#36825;&#38459;&#30861;&#20102;&#26356;&#39640;&#25928;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#26410;&#25551;&#36848;&#20132;&#26367;&#26368;&#23567;&#21270;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#20854;&#20013;&#26356;&#26032;&#36890;&#24120;&#26159;&#36817;&#20284;&#25191;&#34892;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $\Omega\subseteq [m]\times [n]$. In particular, we examine an approach that is widely used in practice -- the alternating minimization framework. Jain, Netrapalli and Sanghavi~\cite{jns13} showed that if $M$ has incoherent rows and columns, then alternating minimization provably recovers the matrix $M$ by observing a nearly linear in $n$ number of entries. While the sample complexity has been subsequently improved~\cite{glz17}, alternating minimization steps are required to be computed exactly. This hinders the development of more efficient algorithms and fails to depict the practical implementation of alternating minimization, where the updates are usually performed approximately in favor of efficiency.  In this p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HLSDataset&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;FPGA&#35774;&#35745;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10977</link><description>&lt;p&gt;
HLSDataset: &#29992;&#20110;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;FPGA&#35774;&#35745;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HLSDataset: Open-Source Dataset for ML-Assisted FPGA Design using High Level Synthesis. (arXiv:2302.10977v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HLSDataset&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;FPGA&#35774;&#35745;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#36827;&#34892;&#35774;&#35745;&#25506;&#32034;&#65292;&#20197;&#22312;FPGA&#35774;&#35745;&#30340;&#26089;&#26399;&#38454;&#27573;&#25552;&#20379;&#26356;&#22909;&#12289;&#26356;&#24555;&#30340;&#24615;&#33021;&#20197;&#21450;&#36164;&#28304;&#21644;&#21151;&#32791;&#20272;&#35745;&#12290;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#21644;&#22823;&#23481;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#39640;&#32423;&#32508;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;FPGA&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;HLSDataset&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#24120;&#29992;&#30340;HLS C&#22522;&#20934;&#65288;Polybench&#12289;Machsuite&#12289;CHStone&#21644;Rossetta&#65289;&#29983;&#25104;&#30340;&#12290;&#29983;&#25104;&#30340;Verilog&#26679;&#26412;&#21253;&#21547;&#20102;&#22810;&#31181;&#25351;&#20196;&#65292;&#21253;&#25324;&#24490;&#29615;&#23637;&#24320;&#12289;&#24490;&#29615;&#27969;&#27700;&#32447;&#21644;&#25968;&#32452;&#21010;&#20998;&#65292;&#20197;&#30830;&#20445;&#35206;&#30422;&#20102;&#20248;&#21270;&#21644;&#29616;&#23454;&#35774;&#35745;&#12290;&#27599;&#31181;FPGA&#31867;&#22411;&#29983;&#25104;&#30340;Verilog&#26679;&#26412;&#24635;&#25968;&#36817;9000&#20010;&#12290;&#20026;&#20102;&#28436;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#32463;&#36807;&#25105;&#20204;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21151;&#32791;&#20272;&#35745;&#21644;&#36164;&#28304;&#20351;&#29992;&#20272;&#35745;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#37117;&#22312;&#20844;&#20849;&#24179;&#21488;&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has been widely adopted in design exploration using high level synthesis (HLS) to give a better and faster performance, and resource and power estimation at very early stages for FPGA-based design. To perform prediction accurately, high-quality and large-volume datasets are required for training ML models.This paper presents a dataset for ML-assisted FPGA design using HLS, called HLSDataset. The dataset is generated from widely used HLS C benchmarks including Polybench, Machsuite, CHStone and Rossetta. The Verilog samples are generated with a variety of directives including loop unroll, loop pipeline and array partition to make sure optimized and realistic designs are covered. The total number of generated Verilog samples is nearly 9,000 per FPGA type. To demonstrate the effectiveness of our dataset, we undertake case studies to perform power estimation and resource usage estimation with ML models trained with our dataset. All the codes and dataset are public at t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;logit&#25439;&#22833;&#21644;&#29305;&#24449;&#20146;&#21644;&#21147;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10899</link><description>&lt;p&gt;
&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;logit&#25439;&#22833;&#21644;&#29305;&#24449;&#20146;&#21644;&#21147;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20146;&#21644;&#21147;&#65288;FA&#65289;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;DNN&#30340;&#20013;&#38388;&#29305;&#24449;&#22270;&#19978;&#30340;FA&#25439;&#22833;&#36215;&#21040;&#20102;&#23558;&#20013;&#38388;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#25945;&#32473;&#23398;&#29983;&#30340;&#20316;&#29992;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#20256;&#32479;&#30340;KD&#20013;&#20316;&#29992;&#20110;&#32593;&#32476;&#36755;&#20986;&#32423;&#21035;&#30340;logits&#25439;&#22833;&#12290;&#23558;logit&#25439;&#22833;&#21644;FA&#25439;&#22833;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#24471;&#21040;&#30340;&#30417;&#30563;&#27604;&#26469;&#33258;&#26631;&#35760;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#30340;&#30417;&#30563;&#26356;&#24378;&#12290;&#25152;&#24471;&#21040;&#30340;FAQD&#33021;&#22815;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#21387;&#32553;&#27169;&#22411;&#65292;&#36825;&#24102;&#26469;&#20102;&#21363;&#26102;&#30340;&#23454;&#38469;&#25928;&#30410;&#65292;&#22240;&#20026;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#32780;&#26080;&#26631;&#31614;&#25968;&#25454;&#21448;&#26159;&#20016;&#23500;&#30340;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#26631;&#35760;&#36890;&#24120;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29305;&#24449;&#20146;&#21644;&#21147;&#65288;FFA&#65289;&#25439;&#22833;&#65292;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20934;&#30830;&#36817;&#20284;FA&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#12290;&#21327;&#26041;&#24046;&#26680;&#26159;&#39640;&#26031;&#36807;&#31243;&#30340;&#20027;&#35201;&#24341;&#25806;&#65292;&#21253;&#21547;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#26102;&#31354;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#21512;&#36866;&#30340;&#26680;&#24212;&#35813;&#24314;&#27169;&#32852;&#21512;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#21487;&#20998;&#31163;&#30340;&#26102;&#31354;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;&#31616;&#21333;&#21644;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38750;&#21487;&#20998;&#31163;&#26680;&#21253;&#21547;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#30456;&#20851;&#24615;&#30340;&#26102;&#31354;&#20132;&#20114;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#20855;&#26377;&#26174;&#24335;&#34920;&#36798;&#24335;&#30340;&#38750;&#21487;&#20998;&#31163;&#26680;&#26159;&#22522;&#20110;&#25968;&#23398;&#32771;&#34385;&#65288;&#21487;&#20801;&#35768;&#26465;&#20214;&#65289;&#32780;&#38750;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#35770;&#35777;&#30340;&#28151;&#21512;&#35889;&#26041;&#27861;&#26469;&#29983;&#25104;&#21327;&#26041;&#24046;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#25512;&#23548;&#20102;&#19968;&#31867;&#26032;&#22411;&#30340;&#29289;&#29702;&#21160;&#26426;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#23427;&#20204;&#30340;&#26681;&#28304;&#26469;&#33258;&#38543;&#26426;&#32447;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Brainomaly&#65292;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#31070;&#32463;&#30142;&#30149;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;Brainomaly&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#33041;&#37096;MR&#22270;&#20687;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#26080;&#30417;&#30563;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09200</link><description>&lt;p&gt;
Brainomaly:&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;T1&#21152;&#26435;&#33041;&#37096;MR&#22270;&#20687;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#31070;&#32463;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images. (arXiv:2302.09200v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Brainomaly&#65292;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#31070;&#32463;&#30142;&#30149;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;Brainomaly&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#33041;&#37096;MR&#22270;&#20687;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#26080;&#30417;&#30563;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#23384;&#22312;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32597;&#35265;&#30142;&#30149;&#26469;&#35828;&#65292;&#27880;&#37322;&#38656;&#27714;&#30340;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#24456;&#39640;&#12290;&#26080;&#30417;&#30563;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36825;&#20123;&#22330;&#26223;&#19979;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#34429;&#28982;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#26159;&#19987;&#27880;&#20110;&#20165;&#20174;&#20581;&#24247;&#21463;&#35797;&#32773;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#29616;&#23454;&#24773;&#20917;&#19979;&#24120;&#24120;&#23384;&#22312;&#21253;&#21547;&#21516;&#26102;&#20581;&#24247;&#21644;&#24739;&#30149;&#21463;&#35797;&#32773;&#30340;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#26410;&#26631;&#27880;&#30340;&#22270;&#20687;&#21487;&#20197;&#25913;&#21892;&#26080;&#30417;&#30563;&#30142;&#30149;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#26410;&#21033;&#29992;&#29305;&#23450;&#20110;&#33041;&#22270;&#20687;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#31070;&#32463;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Brainomaly&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#31070;&#32463;&#30142;&#30149;&#26816;&#27979;&#30340;&#22522;&#20110;GAN&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of deep neural networks in the medical imaging domain is challenging due to the difficulties in acquiring large annotated datasets, especially for rare diseases, which involve high costs, time, and effort for annotation. Unsupervised disease detection methods, such as anomaly detection, can significantly reduce human effort in these scenarios. While anomaly detection typically focuses on learning from images of healthy subjects only, real-world situations often present unannotated datasets with a mixture of healthy and diseased subjects. Recent studies have demonstrated that utilizing such unannotated images can improve unsupervised disease and anomaly detection. However, these methods do not utilize knowledge specific to registered neuroimages, resulting in a subpar performance in neurologic disease detection. To address this limitation, we propose Brainomaly, a GAN-based image-to-image translation method specifically designed for neurologic disease detection. Bra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#65292;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#24182;&#25193;&#23637;&#20026;DCEM-P&#20197;&#28385;&#36275;&#26356;&#22810;&#26041;&#31243;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.01538</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#30340;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;&#24212;&#29992;&#20110;&#22266;&#20307;&#21147;&#23398;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A deep complementary energy method for solid mechanics using minimum complementary energy principle. (arXiv:2302.01538v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#65292;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#24182;&#25193;&#23637;&#20026;DCEM-P&#20197;&#28385;&#36275;&#26356;&#22810;&#26041;&#31243;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#22266;&#20307;&#21147;&#23398;&#20013;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#65292;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#24322;&#36924;&#36817;&#33021;&#21147;&#12290;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;(PINNs)&#21644;&#28145;&#24230;&#33021;&#37327;&#26041;&#27861;(DEM)&#22312;&#35299;&#20915;PDE&#26041;&#38754;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#23567;&#21183;&#33021;&#21407;&#29702;&#21644;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#26159;&#22266;&#20307;&#21147;&#23398;&#20013;&#20004;&#20010;&#37325;&#35201;&#30340;&#21464;&#20998;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;DEM&#26159;&#22522;&#20110;&#26368;&#23567;&#21183;&#33021;&#21407;&#29702;&#65292;&#20294;&#23427;&#32570;&#20047;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#30340;&#37325;&#35201;&#24418;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#20114;&#34917;&#33021;&#37327;&#21407;&#29702;&#30340;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#26041;&#27861;(DCEM)&#12290;DCEM&#30340;&#36755;&#20986;&#20989;&#25968;&#26159;&#24212;&#21147;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;DCEM&#25193;&#23637;&#21040;DCEM-Plus (DCEM-P)&#65292;&#28155;&#21152;&#28385;&#36275;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20114;&#34917;&#33021;&#37327;&#31639;&#23376;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of deep learning has significantly impacted various fields, particularly in solving partial differential equations (PDEs) in solid mechanics, benefiting greatly from the remarkable approximation capabilities of neural networks. In solving PDEs, Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) have garnered substantial attention. The principle of minimum potential energy and complementary energy are two important variational principles in solid mechanics. However,DEM is based on the principle of minimum potential energy, but it lacks the important form of minimum complementary energy. To bridge this gap, we propose the deep complementary energy method (DCEM) based on the principle of minimum complementary energy. The output function of DCEM is the stress function. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfy partial differential equations. Furthermore, we propose a deep complementary energy operator metho
&lt;/p&gt;</description></item><item><title>NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.12667</link><description>&lt;p&gt;
NeSyFOLD: &#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12667
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;NeSyFOLD&#65292;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#21019;&#24314;&#19968;&#20010;NeSyFOLD&#27169;&#22411;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;NeSyFOLD&#30340;&#23398;&#20064;&#27969;&#31243;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#22312;&#36755;&#20837;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;CNN&#65292;&#24182;&#25552;&#21462;&#26368;&#21518;&#19968;&#23618;&#26680;&#30340;&#28608;&#27963;&#20316;&#20026;&#20108;&#36827;&#21046;&#20540;&#65307;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#33021;&#22815;&#20998;&#31867;&#22270;&#20687;&#30340;&#36923;&#36753;&#31243;&#24207;&#8212;&#8212;&#34920;&#31034;&#20026;&#27599;&#20010;&#26680;&#23545;&#24212;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#21521;&#37327;&#65292;&#21516;&#26102;&#20135;&#29983;&#36923;&#36753;&#35299;&#37322;&#12290;&#30001;FOLD-SE-M&#31639;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#20855;&#26377;&#26680;&#32534;&#21495;&#20316;&#20026;&#35859;&#35789;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#27010;&#24565;&#12290;&#36825;&#20010;&#26144;&#23556;&#34987;&#29992;&#26469;&#23558;&#35268;&#21017;&#38598;&#20013;&#30340;&#35859;&#35789;&#21517;&#65288;&#26680;&#32534;&#21495;&#65289;&#26367;&#25442;&#20026;&#23545;&#24212;&#30340;&#35821;&#20041;&#27010;&#24565;&#26631;&#31614;&#12290;&#32467;&#26524;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#21487;&#20197;&#34987;&#20154;&#31867;&#30452;&#35266;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;NeSyFOLD&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#21644;&#35299;&#37322;&#24615;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
&lt;/p&gt;</description></item><item><title>Boosting&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21542;&#21017;&#24182;&#34892;&#21270;&#30340;&#25928;&#26524;&#24182;&#19981;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2301.09627</link><description>&lt;p&gt;
Boosting&#31639;&#27861;&#30340;&#24182;&#34892;&#21270;&#19981;&#21487;&#33021;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impossibility of Parallelizing Boosting. (arXiv:2301.09627v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09627
&lt;/p&gt;
&lt;p&gt;
Boosting&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21542;&#21017;&#24182;&#34892;&#21270;&#30340;&#25928;&#26524;&#24182;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Boosting&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#23558;&#19968;&#31995;&#21015;&#24369;&#23398;&#20064;&#22120;&#38598;&#25104;&#25104;&#19968;&#20010;&#24378;&#23398;&#20064;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#39034;&#24207;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Boosting&#31639;&#27861;&#30340;&#24182;&#34892;&#21270;&#21487;&#33021;&#24615;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#24378;&#28872;&#30340;&#36127;&#38754;&#32467;&#26524;&#65292;&#21363;&#26174;&#33879;&#30340;&#24182;&#34892;&#21270;&#38656;&#35201;&#25351;&#25968;&#32423;&#21035;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of boosting is to convert a sequence of weak learners into a strong learner. At their heart, these methods are fully sequential. In this paper, we investigate the possibility of parallelizing boosting. Our main contribution is a strong negative result, implying that significant parallelization of boosting requires an exponential blow-up in the total computing resources needed for training.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.05712</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#32508;&#36848;: &#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26679;&#26412;&#26469;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36807;&#22810;&#30340;&#26679;&#26412;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#20316;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23376;&#38598;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#12290;SSL&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#30456;&#20851;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#32508;&#36848;&#30740;&#31350;&#26469;&#35299;&#37322;&#19981;&#21516;&#30340;SSL&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#28436;&#21464;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#12289;&#19977;&#20010;&#20027;&#35201;&#36235;&#21183;&#21644;&#24453;&#35299;&#38382;&#39064;&#30340;&#35270;&#35282;&#32508;&#36848;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#22823;&#22810;&#25968;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#27010;&#36848;&#20102;SSL&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#40723;&#21169;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#26469;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.01520</link><description>&lt;p&gt;
&#25506;&#32034;&#21487;&#35299;&#37322;&#30340;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#65306;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy. (arXiv:2301.01520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#40723;&#21169;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#26469;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#24037;&#20855;&#12290;&#22312;&#32473;&#23450;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#23547;&#25214;&#24182;&#21521;&#29992;&#25143;&#26174;&#31034;&#20915;&#31574;&#36793;&#30028;&#19978;&#31867;&#20284;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#20219;&#21153;&#30340;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#25239;&#29983;&#25104;&#21453;&#20107;&#23454;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#22312;&#32473;&#23450;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#23545;&#30446;&#26631;&#31867;&#21035;&#27809;&#26377;&#20808;&#39564;&#20551;&#35774;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#28789;&#27963;&#24615;&#20801;&#35768;&#21457;&#29616;&#22303;&#22320;&#35206;&#30422;&#31867;&#21035;&#20043;&#38388;&#30340;&#26377;&#36259;&#20449;&#24687;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#29305;&#28857;&#26159;&#40723;&#21169;&#21453;&#20107;&#23454;&#19982;&#21407;&#22987;&#26679;&#26412;&#20043;&#38388;&#20165;&#22312;&#19968;&#20010;&#23567;&#32780;&#32039;&#20945;&#30340;&#26102;&#38388;&#27573;&#20869;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26102;&#38388;&#36830;&#32493;&#30340;&#25200;&#21160;&#20801;&#35768;&#24471;&#21040;&#26356;&#31232;&#30095;&#19988;&#22240;&#27492;&#26356;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#21046;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;/&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are an emerging tool to enhance interpretability of deep learning models. Given a sample, these methods seek to find and display to the user similar samples across the decision boundary. In this paper, we propose a generative adversarial counterfactual approach for satellite image time series in a multi-class setting for the land cover classification task. One of the distinctive features of the proposed approach is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the discovery of interesting information on the relationship between land cover classes. The other feature consists of encouraging the counterfactual to differ from the original sample only in a small and compact temporal segment. These time-contiguous perturbations allow for a much sparser and, thus, interpretable solution. Furthermore, plausibility/realism of the generated counterfactual explanations is enforced via the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2212.13939</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;AraGPT-2&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;Euclidean&#12289;cosine&#12289;Jaccard&#21644;BLEU&#36317;&#31163;&#35780;&#20272;&#29983;&#25104;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#20805;&#36275;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20805;&#36275;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;DA&#36890;&#36807;&#23545;&#29616;&#26377;&#25968;&#25454;&#24212;&#29992;&#36716;&#25442;&#26469;&#29983;&#25104;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#21152;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#38463;&#25289;&#20271;&#35821;&#30340;DA&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#37322;&#20041;&#25110;&#22522;&#20110;&#22122;&#22768;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38463;&#25289;&#20271;&#35821;DA&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#26368;&#36817;&#24378;&#22823;&#30340;&#24314;&#27169;&#25216;&#26415;AraGPT-2&#26469;&#36827;&#34892;&#22686;&#24378;&#36807;&#31243;&#12290;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#12289;&#20313;&#24358;&#36317;&#31163;&#12289;Jaccard&#36317;&#31163;&#21644;BLEU&#36317;&#31163;&#23545;&#29983;&#25104;&#30340;&#21477;&#23376;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#12289;&#35821;&#20041;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;AraBERT transformer&#23545;&#24773;&#24863;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#22312;&#22810;&#20010;&#29992;&#25143;&#30340;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#20854;&#24863;&#30693;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#36136;&#22320;&#21644;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#65292;&#24182;&#20351;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2212.13332</link><description>&lt;p&gt;
&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#22312;&#22810;&#20010;&#29992;&#25143;&#30340;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#20854;&#24863;&#30693;&#24615;&#33021;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#36136;&#22320;&#21644;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#65292;&#24182;&#20351;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#32570;&#20047;&#20154;&#31867;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20132;&#20114;&#20013;&#32463;&#21382;&#21040;&#30340;&#20016;&#23500;&#35302;&#35273;&#20449;&#21495;&#65292;&#20363;&#22914;&#22312;&#34920;&#38754;&#19978;&#30340;&#27178;&#21521;&#31227;&#21160;&#20013;&#24863;&#21463;&#21040;&#30340;&#36136;&#22320;&#24863;&#12290;&#22312;VR&#29615;&#22659;&#20013;&#28155;&#21152;&#36924;&#30495;&#30340;&#35302;&#35273;&#36136;&#22320;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#29992;&#25143;&#20132;&#20114;&#30340;&#21464;&#21270;&#21644;&#19990;&#30028;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#36136;&#22320;&#30340;&#27169;&#22411;&#12290;&#30446;&#21069;&#23384;&#22312;&#29992;&#20110;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#30340;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#31181;&#36136;&#22320;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#20316;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#35302;&#35273;&#36136;&#22320;&#28210;&#26579;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#29992;&#25143;&#30340;&#24863;&#30693;&#24615;&#33021;&#35780;&#20272;&#26469;&#21576;&#29616;&#36924;&#30495;&#30340;&#36136;&#22320;&#25391;&#21160;&#12290;&#35813;&#27169;&#22411;&#32479;&#19968;&#36866;&#29992;&#20110;&#25152;&#26377;&#26448;&#26009;&#65292;&#20351;&#29992;&#26469;&#33258;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#65288;GelSight&#65289;&#30340;&#25968;&#25454;&#65292;&#22312;&#23454;&#26102;&#26465;&#20214;&#19979;&#21576;&#29616;&#36866;&#24403;&#30340;&#34920;&#38754;&#12290;&#22312;&#36136;&#22320;&#28210;&#26579;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#39640;&#24102;&#23485;&#30340;&#25391;&#35302;&#35273;&#20256;&#24863;&#22120;&#36830;&#25509;&#21040;&#19968;&#20010;3D&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems
&lt;/p&gt;</description></item><item><title>DALL-E &#21644; Flamingo &#36890;&#36807;&#37325;&#24314;&#22270;&#20687;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#20219;&#21153;&#26469;&#29702;&#35299;&#24444;&#27492;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#25551;&#36848;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.12249</link><description>&lt;p&gt;
DALL-E &#21644; Flamingo &#20114;&#30456;&#29702;&#35299;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do DALL-E and Flamingo Understand Each Other?. (arXiv:2212.12249v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12249
&lt;/p&gt;
&lt;p&gt;
DALL-E &#21644; Flamingo &#36890;&#36807;&#37325;&#24314;&#22270;&#20687;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#20219;&#21153;&#26469;&#29702;&#35299;&#24444;&#27492;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#25551;&#36848;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;&#21644;&#21019;&#24314;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#19968;&#36827;&#23637;&#20307;&#29616;&#22312;&#19987;&#27880;&#20110;&#35268;&#27169;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#20986;&#29616;&#19978;&#65292;&#22914;&#33879;&#21517;&#30340; Flamingo &#27169;&#22411;&#21644;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340; DALL-E &#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20540;&#24471;&#25506;&#31350;&#30340;&#26377;&#36259;&#38382;&#39064;&#26159; Flamingo &#21644; DALL-E &#26159;&#21542;&#29702;&#35299;&#24444;&#27492;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#24314;&#20219;&#21153;&#65292;&#20854;&#20013; Flamingo &#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#65292;&#28982;&#21518; DALL-E &#20351;&#29992;&#36825;&#20010;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#26469;&#21512;&#25104;&#19968;&#24352;&#26032;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#30456;&#20284;&#65292;&#37027;&#20040;&#36825;&#20123;&#27169;&#22411;&#23601;&#20114;&#30456;&#29702;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20339;&#30340;&#22270;&#20687;&#25551;&#36848;&#26159;&#37027;&#20123;&#33021;&#22815;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#20284;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of multimodal research focusing on the comprehension and creation of both images and text has witnessed significant strides. This progress is exemplified by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth exploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Specifically, we study the relationship between the quality of the image reconstruction and that of the text generation. We find that an optimal description of an image is one that gives rise to a generated image similar to the original one. Th
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#22312;&#19982;&#38750;&#31283;&#24577;&#20154;&#31867;&#20849;&#21516;&#20114;&#21160;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24418;&#24335;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#19982;&#21160;&#24577;&#20154;&#31867;&#20849;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2212.09586</link><description>&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#20197;&#19982;&#20154;&#31867;&#20849;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Latent Representations to Co-Adapt to Humans. (arXiv:2212.09586v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#22312;&#19982;&#38750;&#31283;&#24577;&#20154;&#31867;&#20849;&#21516;&#20114;&#21160;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#24418;&#24335;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#19982;&#21160;&#24577;&#20154;&#31867;&#20849;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#12289;&#36947;&#36335;&#25110;&#24037;&#21378;&#20013;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#65292;&#20154;&#31867;&#30340;&#34892;&#20026;&#36890;&#24120;&#20250;&#22240;&#20026;&#26426;&#22120;&#20154;&#32780;&#25913;&#21464;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#38750;&#31283;&#24577;&#30340;&#20154;&#31867;&#26159;&#19968;&#20010;&#25361;&#25112;&#65306;&#26426;&#22120;&#20154;&#24050;&#32463;&#23398;&#20250;&#19982;&#21407;&#22987;&#20154;&#31867;&#21327;&#35843;&#19968;&#36215;&#25191;&#34892;&#30340;&#21160;&#20316;&#21487;&#33021;&#22312;&#20154;&#31867;&#36866;&#24212;&#26426;&#22120;&#20154;&#21518;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24418;&#24335;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#65288;&#21363;&#33258;&#25105;&#20195;&#29702;&#65289;&#33021;&#22815;&#19982;&#21160;&#24577;&#20154;&#31867;&#65288;&#21363;&#20854;&#20182;&#20195;&#29702;&#65289;&#20849;&#21516;&#36866;&#24212;&#65292;&#21482;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20302;&#32423;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#12290;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#65292;&#20154;&#31867;&#19981;&#20165;&#20250;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#20570;&#20986;&#21453;&#24212;&#65292;&#32780;&#19988;&#20154;&#31867;&#30340;&#21453;&#24212;&#26041;&#24335;&#38543;&#30528;&#26102;&#38388;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#26159;&#65292;&#26426;&#22120;&#20154;&#19981;&#38656;&#35201;&#24314;&#31435;&#20154;&#31867;&#30340;&#31934;&#30830;&#27169;&#22411;&#65292;&#32780;&#26159;&#21487;&#20197;&#23398;&#20064;&#21644;&#25512;&#29702;&#20154;&#31867;&#31574;&#30053;&#21644;&#31574;&#30053;&#21160;&#24577;&#30340;&#39640;&#32423;&#34920;&#31034;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RILI&#65306;&#31283;&#20581;&#22320;&#24433;&#21709;&#28508;&#22312;&#24847;&#22270;&#12290;RILI&#39318;&#20808;&#23558;&#20302;&#32423;&#26426;&#22120;&#20154;&#35266;&#23519;&#23884;&#20837;&#21040;&#39640;&#32423;&#34920;&#31034;&#20013;...
&lt;/p&gt;
&lt;p&gt;
When robots interact with humans in homes, roads, or factories the human's behavior often changes in response to the robot. Non-stationary humans are challenging for robot learners: actions the robot has learned to coordinate with the original human may fail after the human adapts to the robot. In this paper we introduce an algorithmic formalism that enables robots (i.e., ego agents) to co-adapt alongside dynamic humans (i.e., other agents) using only the robot's low-level states, actions, and rewards. A core challenge is that humans not only react to the robot's behavior, but the way in which humans react inevitably changes both over time and between users. To deal with this challenge, our insight is that -- instead of building an exact model of the human -- robots can learn and reason over high-level representations of the human's policy and policy dynamics. Applying this insight we develop RILI: Robustly Influencing Latent Intent. RILI first embeds low-level robot observations into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#32467;&#26500;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#25239;&#33740;&#32957;&#30340;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#24207;&#21015;&#23646;&#24615;&#21644;&#20108;&#32423;&#32467;&#26500;&#30340;&#32957;&#38142;&#65292;&#24182;&#36890;&#36807;&#28287;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20013;&#20004;&#20010;&#20505;&#36873;&#32957;&#38142;&#30340;&#24378;&#22823;&#25239;&#33740;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09450</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#32467;&#26500;&#21152;&#36895;&#25239;&#33740;&#32957;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Antimicrobial Peptide Discovery with Latent Structure. (arXiv:2212.09450v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#32467;&#26500;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#25239;&#33740;&#32957;&#30340;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#24207;&#21015;&#23646;&#24615;&#21644;&#20108;&#32423;&#32467;&#26500;&#30340;&#32957;&#38142;&#65292;&#24182;&#36890;&#36807;&#28287;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20013;&#20004;&#20010;&#20505;&#36873;&#32957;&#38142;&#30340;&#24378;&#22823;&#25239;&#33740;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#33740;&#32957;&#65288;AMPs&#65289;&#26159;&#23545;&#25239;&#32784;&#33647;&#30149;&#21407;&#20307;&#30340;&#26377;&#24076;&#26395;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#34987;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;AMPs&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32957;&#24207;&#21015;&#23646;&#24615;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#37325;&#35201;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35774;&#35745;AMPs&#30340;&#28508;&#22312;&#24207;&#21015;-&#32467;&#26500;&#27169;&#22411;&#65288;LSSAMP&#65289;&#12290;LSSAMP&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#23610;&#24230;&#21521;&#37327;&#37327;&#21270;&#26469;&#34920;&#31034;&#20108;&#32423;&#32467;&#26500;&#65288;&#20363;&#22914;&#945;&#34746;&#26059;&#21644;&#946;&#25240;&#21472;&#65289;&#12290;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#26679;&#26412;&#36827;&#34892;&#37319;&#26679;&#65292;LSSAMP&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#24207;&#21015;&#23646;&#24615;&#21644;&#20108;&#32423;&#32467;&#26500;&#30340;&#32957;&#38142;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;LSSAMP&#29983;&#25104;&#30340;&#32957;&#38142;&#20855;&#26377;&#24456;&#39640;&#30340;&#25239;&#33740;&#27963;&#24615;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#28287;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20013;&#20004;&#20010;&#20505;&#36873;&#32957;&#38142;&#20855;&#26377;&#24378;&#22823;&#30340;&#25239;&#33740;&#27963;&#24615;&#12290;&#20195;&#30721;&#24050;&#22312;https://github.com/dqwang122/LSSAMP&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antimicrobial peptides (AMPs) are promising therapeutic approaches against drug-resistant pathogens. Recently, deep generative models are used to discover new AMPs. However, previous studies mainly focus on peptide sequence attributes and do not consider crucial structure information. In this paper, we propose a latent sequence-structure model for designing AMPs (LSSAMP). LSSAMP exploits multi-scale vector quantization in the latent space to represent secondary structures (e.g. alpha helix and beta sheet). By sampling in the latent space, LSSAMP can simultaneously generate peptides with ideal sequence attributes and secondary structures. Experimental results show that the peptides generated by LSSAMP have a high probability of antimicrobial activity. Our wet laboratory experiments verified that two of the 21 candidates exhibit strong antimicrobial activity. The code is released at https://github.com/dqwang122/LSSAMP.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; ShapeNet &#21512;&#25104;&#25968;&#25454;&#38598; SPARF&#65292;&#21253;&#25324;&#36229;&#36807; 100 &#19975;&#20010;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#30340;&#36752;&#23556;&#22330;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447; SuRFNet&#65292;&#36890;&#36807;&#23398;&#20064;&#23569;&#37327;&#35270;&#22270;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;</title><link>http://arxiv.org/abs/2212.09100</link><description>&lt;p&gt;
SPARF&#65306;&#20174;&#23569;&#37327;&#36755;&#20837;&#22270;&#20687;&#20013;&#23398;&#20064;&#22823;&#35268;&#27169;&#30340; 3D &#31232;&#30095;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images. (arXiv:2212.09100v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; ShapeNet &#21512;&#25104;&#25968;&#25454;&#38598; SPARF&#65292;&#21253;&#25324;&#36229;&#36807; 100 &#19975;&#20010;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#30340;&#36752;&#23556;&#22330;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447; SuRFNet&#65292;&#36890;&#36807;&#23398;&#20064;&#23569;&#37327;&#35270;&#22270;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRFs) &#30340;&#36827;&#23637;&#23558;&#26032;&#35270;&#35282;&#21512;&#25104;&#38382;&#39064;&#30475;&#20316;&#26159;&#31232;&#30095;&#36752;&#23556;&#22330; (SRF) &#20248;&#21270;&#65292;&#20351;&#29992;&#31232;&#30095;&#20307;&#32032;&#36827;&#34892;&#39640;&#25928;&#24555;&#36895;&#28210;&#26579; (plenoxels, InstantNGP)&#12290;&#20026;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#37319;&#29992; SRF &#20316;&#20026; 3D &#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPARF&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110; ShapeNet &#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#65292;&#30001; $\sim$ 17 &#30334;&#19975;&#24352;&#22270;&#20687;&#32452;&#25104;&#65292;&#20174;&#36817; 40,000 &#20010;&#39640;&#20998;&#36776;&#29575;&#24418;&#29366;&#28210;&#26579;&#32780;&#26469; (400 X 400 &#20687;&#32032;)&#12290;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#21253;&#25324;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20855;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#36807;&#30340;&#36752;&#23556;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447;&#65288;SuRFNet&#65289;&#65292;&#23427;&#20174;&#23569;&#37327;&#35270;&#22270;&#20013;&#23398;&#20064;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#25910;&#38598;&#30340; SPARF &#25968;&#25454;&#38598;&#21644; 3D &#31232;&#30095;&#21367;&#31215;&#26469;&#23454;&#29616;&#30340;&#12290;SuRFNet &#20351;&#29992;&#23569;&#37327;/&#21333;&#20010;&#22270;&#20687;&#30340;&#37096;&#20998; SRF &#21644;&#29305;&#23450;&#30340; SRF &#25439;&#22833;&#26469;&#23398;&#20064;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#65288;FDFA&#65289;&#65292;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#21644;&#21160;&#37327;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.07282</link><description>&lt;p&gt;
&#20302;&#26041;&#24046;&#21069;&#21521;&#26799;&#24230;&#31639;&#27861;&#65306;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#32467;&#21512;&#21160;&#37327;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-Variance Forward Gradients using Direct Feedback Alignment and Momentum. (arXiv:2212.07282v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#31639;&#27861;&#65288;FDFA&#65289;&#65292;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#21644;&#21160;&#37327;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#36827;&#34892;&#12290;&#20294;&#26159;&#65292;&#21453;&#21521;&#20256;&#25773;&#26399;&#38388;&#30340;&#38169;&#35823;&#39034;&#24207;&#20256;&#25773;&#21644;&#26435;&#37325;&#20256;&#36755;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#27492;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#23547;&#25214;BP&#30340;&#26412;&#22320;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#65288;FDFA&#65289;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;Activity-Perturbed&#21069;&#21521;&#26799;&#24230;&#65292;&#30452;&#25509;&#21453;&#39304;&#23545;&#40784;&#21644;&#21160;&#37327;&#27861;&#26469;&#35745;&#31639;DNN&#20013;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning in Deep Neural Networks (DNNs) is commonly performed using the error Backpropagation (BP) algorithm. The sequential propagation of errors and the transport of weights during the backward pass limits its efficiency and scalability. Therefore, there is growing interest in finding local alternatives to BP. Recently, methods based on Forward-Mode Automatic Differentiation have been proposed, such as the Forward Gradient algorithm and its variants. However, Forward Gradients suffer from high variance in large DNNs, which affects convergence. In this paper, we address the large variance of Forward Gradients and propose the Forward Direct Feedback Alignment (FDFA) algorithm that combines Activity-Perturbed Forward Gradients with Direct Feedback Alignment and momentum to compute low-variance gradient estimates in DNNs. Our results provides both theoretical proof and empirical evidence that our proposed method achieves lower variance compared to previous Forward Gradient tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24615;&#24378;&#12289;&#36866;&#29992;&#20110;&#22797;&#26434;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#26102;&#21464;&#20998;&#24067;&#20197;&#26356;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06653</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23436;&#25972;&#21327;&#26041;&#24046;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Dynamic Mixture Model with Full Covariance for Probabilistic Traffic Forecasting. (arXiv:2212.06653v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24615;&#24378;&#12289;&#36866;&#29992;&#20110;&#22797;&#26434;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#26102;&#21464;&#20998;&#24067;&#20197;&#26356;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20803;&#22810;&#27493;&#39588;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25110;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#31616;&#21333;&#22320;&#20551;&#35774;&#35823;&#24046;&#36981;&#24490;&#29420;&#31435;&#19988;&#21508;&#21521;&#21516;&#24615;&#30340;&#39640;&#26031;&#25110;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#20551;&#35774;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#26102;&#31354;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#38750;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#19978;&#23384;&#22312;&#24378;&#28872;&#30340;&#21516;&#26102;&#30456;&#20851;&#24615;&#65292;&#28041;&#21450;&#20256;&#24863;&#22120;&#21644;&#39044;&#27979;&#26102;&#38388;&#36328;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30697;&#38453;&#21464;&#37327;&#35823;&#24046;&#36807;&#31243;&#30340;&#26102;&#21464;&#20998;&#24067;&#24314;&#27169;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#30697;&#38453;&#27491;&#24577;&#20998;&#24067;&#21442;&#25968;&#21270;&#27599;&#20010;&#28151;&#21512;&#25104;&#20998;&#65292;&#24182;&#20801;&#35768;&#28151;&#21512;&#26435;&#37325;&#38543;&#26102;&#38388;&#21464;&#21270;&#21644;&#21487;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based multivariate and multistep-ahead traffic forecasting models are typically trained with the mean squared error (MSE) or mean absolute error (MAE) as the loss function in a sequence-to-sequence setting, simply assuming that the errors follow an independent and isotropic Gaussian or Laplacian distributions. However, such assumptions are often unrealistic for real-world traffic forecasting tasks, where the probabilistic distribution of spatiotemporal forecasting is very complex with strong concurrent correlations across both sensors and forecasting horizons in a time-varying manner. In this paper, we model the time-varying distribution for the matrix-variate error process as a dynamic mixture of zero-mean Gaussian distributions. To achieve efficiency, flexibility, and scalability, we parameterize each mixture component using a matrix normal distribution and allow the mixture weight to change and be predictable over time. The proposed method can be seamlessly integrated 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#31227;&#21160;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#25552;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26102;&#30340;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#35774;&#35745;&#35299;&#20915;&#20102;&#39640;&#25191;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.05974</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31227;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Few-Shot Learning for Mobile NLP. (arXiv:2212.05974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#31227;&#21160;&#23569;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#25552;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26102;&#30340;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#35774;&#35745;&#35299;&#20915;&#20102;&#39640;&#25191;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#31227;&#21160;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#32852;&#37030;&#38544;&#31169;&#20445;&#25252;&#29615;&#22659;&#20013;&#23545;&#22522;&#30784;NLP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#33267;&#23569;&#25968;&#21313;&#19975;&#20010;&#26469;&#33258;&#31227;&#21160;&#23458;&#25143;&#31471;&#30340;&#24102;&#26631;&#31614;&#35757;&#32451;&#26679;&#26412;&#65307;&#28982;&#32780;&#31227;&#21160;&#29992;&#25143;&#36890;&#24120;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#24847;&#24895;&#25110;&#30693;&#35782;&#12290;&#36825;&#31181;&#25968;&#25454;&#26631;&#31614;&#30340;&#19981;&#36275;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#22330;&#26223;&#65292;&#23427;&#25104;&#20026;&#31227;&#21160;NLP&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#31350;&#20102;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#32852;&#37030;NLP&#65288;FedFSL&#65289;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#26631;&#31614;&#21644;&#25552;&#31034;&#23398;&#20064;&#31561;&#31639;&#27861;&#36827;&#23637;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#22312;&#20165;&#26377;0.05%&#65288;&#23569;&#20110;100&#20010;&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#34987;&#26631;&#35760;&#65292;&#20854;&#20313;&#25968;&#25454;&#26410;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20855;&#20307;&#23454;&#26045;&#36825;&#20010;&#24037;&#20316;&#27969;&#31243;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FeS&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#26032;&#35774;&#35745;&#35299;&#20915;&#20102;&#39640;&#25191;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#21253;&#25324;&#35838;&#31243;&#36827;&#24230;&#25511;&#21046;&#65292;&#30446;&#26631;&#32593;&#32476;&#21644;&#39564;&#35777;&#22120;&#32593;&#32476;&#30340;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) sees rich mobile applications. To support various language understanding tasks, a foundation NLP model is often fine-tuned in a federated, privacy-preserving setting (FL). This process currently relies on at least hundreds of thousands of labeled training samples from mobile clients; yet mobile users often lack willingness or knowledge to label their data. Such an inadequacy of data labels is known as a few-shot scenario; it becomes the key blocker for mobile NLP applications.  For the first time, this work investigates federated NLP in the few-shot scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and prompt learning, we first establish a training pipeline that delivers competitive accuracy when only 0.05% (fewer than 100) of the training data is labeled and the remaining is unlabeled. To instantiate the workflow, we further present a system FeS, addressing the high execution cost with novel designs. (1) Curriculum pacing, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#24449;&#38477;&#32500;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#22270;&#26102;&#21516;&#26102;&#20351;&#29992;&#23616;&#37096;&#29305;&#24449;&#21644;&#21407;&#22987;&#29305;&#24449;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2212.05853</link><description>&lt;p&gt;
DeepCut: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering. (arXiv:2212.05853v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#24449;&#38477;&#32500;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#26500;&#24314;&#22270;&#26102;&#21516;&#26102;&#20351;&#29992;&#23616;&#37096;&#29305;&#24449;&#21644;&#21407;&#22987;&#29305;&#24449;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#30417;&#30563;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#32791;&#21147;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#28608;&#21169;&#20102;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#25552;&#21462;&#28145;&#24230;&#29305;&#24449;&#26469;&#26500;&#24314;&#22270;&#65292;&#28982;&#21518;&#20877;&#24212;&#29992;&#32463;&#20856;&#30340;&#32858;&#31867;&#26041;&#27861;&#22914;k-means&#21644;&#24402;&#19968;&#21270;&#21106;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#29305;&#24449;&#20013;&#30340;&#39640;&#32500;&#20449;&#24687;&#38477;&#32500;&#20026;&#19968;&#23545;&#19968;&#30340;&#26631;&#37327;&#20146;&#21644;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#29992;&#23427;&#26469;&#26367;&#20195;&#32463;&#20856;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#22312;&#20248;&#21270;&#30456;&#21516;&#30340;&#32858;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#21516;&#26102;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GNN&#21516;&#26102;&#25509;&#21463;&#23616;&#37096;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#23545;&#19968;&#20146;&#21644;&#21147;&#21644;&#21407;&#22987;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#12290;&#21407;&#22987;&#29305;&#24449;&#19982;&#32858;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#36830;&#25509;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#38544;&#24335;&#22320;&#23545;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#32858;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image segmentation is a fundamental task in computer vision. Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods. Current approaches often rely on extracting deep features from pre-trained networks to construct a graph, and classical clustering methods like k-means and normalized-cuts are then applied as a post-processing step. However, this approach reduces the high-dimensional information encoded in the features to pair-wise scalar affinities. To address this limitation, this study introduces a lightweight Graph Neural Network (GNN) to replace classical clustering methods while optimizing for the same clustering objective function. Unlike existing methods, our GNN takes both the pair-wise affinities between local image features and the raw features as input. This direct connection between the raw features and the clustering objective enables us to implicitly perform classification of the clusters between different graphs, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.03241</link><description>&lt;p&gt;
P{\O}DA: &#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
P{\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand.
&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#38656;&#35201;&#22312;&#35757;&#32451;&#26102;&#35775;&#38382;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#22312;&#26576;&#20123;&#19981;&#24120;&#35265;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#8221;&#20219;&#21153;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#20351;&#29992;&#30446;&#26631;&#22495;&#30340;&#21333;&#20010;&#36890;&#29992;&#25991;&#26412;&#25551;&#36848;&#65288;&#21363;&#25552;&#31034;&#65289;&#26469;&#35843;&#25972;&#22312;&#28304;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#19968;&#27425;&#24615;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#24182;&#19988;gi
&lt;/p&gt;
&lt;p&gt;
Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.02710</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#35937;&#35782;&#21035;&#65306;&#38754;&#21521;&#23545;&#35937;&#27010;&#24565;&#23398;&#20064;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35937;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#20307;&#39564;&#30340;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#26426;&#22120;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#26356;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#23545;&#35937;&#20855;&#26377;&#21738;&#20123;&#23646;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#23545;&#35937;&#20570;&#20160;&#20040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#20197;&#25512;&#21160;&#23545;&#35937;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#23427;&#35201;&#27714;&#26426;&#22120;&#25512;&#29702;&#20986;&#23545;&#35937;&#30340;&#20316;&#29992;&#65292;&#24182;&#21516;&#26102;&#32473;&#20986;&#21407;&#22240;&#65306;&#26159;&#21738;&#20123;&#23646;&#24615;&#20351;&#24471;&#19968;&#20010;&#23545;&#35937;&#20855;&#26377;&#36825;&#20123;&#20316;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345; OCL&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#65292;&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#30340;&#23545;&#35937;&#27010;&#24565;&#65288;&#31867;&#21035;&#12289;&#23646;&#24615;&#12289;&#20316;&#29992;&#65289;&#65292;&#20197;&#21450;&#19977;&#20010;&#23618;&#27425;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512; OCL &#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#65306;Object Concept Reasoning Network (OCRN)&#12290;&#23427;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#21644;&#27010;&#24565;&#23454;&#20363;&#21270;&#26469;&#25512;&#26029;&#19977;&#20010;&#23618;&#27425;&#65292;&#36981;&#24490;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
&lt;/p&gt;</description></item><item><title>PhysDiff&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29289;&#29702;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02500</link><description>&lt;p&gt;
PhysDiff: &#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhysDiff: Physics-Guided Human Motion Diffusion Model. (arXiv:2212.02500v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02500
&lt;/p&gt;
&lt;p&gt;
PhysDiff&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20154;&#20307;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29289;&#29702;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20026;&#29983;&#25104;&#22810;&#26679;&#32780;&#36924;&#30495;&#30340;&#20154;&#20307;&#21160;&#20316;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24448;&#24448;&#24573;&#35270;&#20102;&#29289;&#29702;&#23450;&#24459;&#65292;&#24182;&#19988;&#32463;&#24120;&#29983;&#25104;&#20986;&#20855;&#26377;&#26126;&#26174;&#32570;&#38519;&#30340;&#19981;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#30340;&#21160;&#20316;&#65292;&#22914;&#28014;&#31354;&#12289;&#28369;&#33050;&#21644;&#36879;&#22320;&#12290;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#29983;&#25104;&#21160;&#20316;&#30340;&#36136;&#37327;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;PhysDiff&#65289;&#65292;&#23427;&#23558;&#29289;&#29702;&#32422;&#26463;&#34701;&#20837;&#21040;&#25193;&#25955;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#36816;&#21160;&#27169;&#20223;&#30340;&#29289;&#29702;&#36816;&#21160;&#25237;&#24433;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#25193;&#25955;&#27493;&#39588;&#30340;&#21435;&#22122;&#21160;&#20316;&#25237;&#24433;&#20026;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#30340;&#21160;&#20316;&#12290;&#25237;&#24433;&#21518;&#30340;&#21160;&#20316;&#36827;&#19968;&#27493;&#29992;&#20110;&#19979;&#19968;&#20010;&#25193;&#25955;&#27493;&#39588;&#65292;&#20197;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#27169;&#22411;&#20013;&#20351;&#29992;&#29289;&#29702;&#23450;&#24459;&#20351;&#24471;&#21160;&#20316;&#36880;&#27493;&#36235;&#21521;&#20110;&#31526;&#21512;&#29289;&#29702;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#23454;&#29616;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#35201;&#27714;&#20184;&#20986;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00192</link><description>&lt;p&gt;
&#36808;&#21521;&#23454;&#29992;&#30340;&#23569;&#26679;&#26412;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Few-shot Federated NLP. (arXiv:2212.00192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#23454;&#29616;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#35201;&#27714;&#20184;&#20986;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#23454;&#38469;&#19978;&#65292;&#31169;&#26377;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#24322;&#26500;&#30340;&#31227;&#21160;&#35774;&#22791;&#19978;&#65292;&#24182;&#19988;&#21487;&#33021;&#34987;&#31105;&#27490;&#19978;&#20256;&#12290;&#27492;&#22806;&#65292;&#31934;&#24515;&#31574;&#21010;&#30340;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#24456;&#31232;&#32570;&#65292;&#36825;&#22686;&#21152;&#20102;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#27169;&#25311;&#20102;&#29616;&#23454;&#24773;&#20917;&#19979;&#31232;&#32570;&#26631;&#35760;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20559;&#26012;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUG-FedPrompt&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#19979;&#65292;AUG-FedPrompt&#33021;&#22815;&#19982;&#23436;&#25972;&#24494;&#35843;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31454;&#20105;&#24615;&#33021;&#26159;&#20197;&#26174;&#33879;&#30340;&#31995;&#32479;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models have emerged as the predominant solution for natural language processing (NLP). Fine-tuning such pre-trained models for downstream tasks often requires a considerable amount of labeled private data. In practice, private data is often distributed across heterogeneous mobile devices and may be prohibited from being uploaded. Moreover, well-curated labeled data is often scarce, presenting an additional challenge. To address these challenges, we first introduce a data generator for federated few-shot learning tasks, which encompasses the quantity and skewness of scarce labeled data in a realistic setting. Subsequently, we propose AUG-FedPrompt, a prompt-based federated learning system that exploits abundant unlabeled data for data augmentation. Our experiments indicate that AUG-FedPrompt can perform on par with full-set fine-tuning with a limited amount of labeled data. However, such competitive performance comes at a significant system cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#22312;&#32447;&#38598;&#25104;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#27809;&#26377;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#20998;&#31867;&#22120;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14963</link><description>&lt;p&gt;
&#22312;&#32447;&#38598;&#25104;&#25345;&#32493;&#23398;&#20064;&#30340;&#31070;&#32463;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture for Online Ensemble Continual Learning. (arXiv:2211.14963v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#22312;&#32447;&#38598;&#25104;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#27809;&#26377;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#20998;&#31867;&#22120;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31867;&#21035;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25345;&#32493;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#27599;&#20010;&#31034;&#20363;&#21482;&#20986;&#29616;&#19968;&#27425;&#26102;&#65292;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#36825;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#22312;&#32447;&#23398;&#20064;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24448;&#24448;&#38754;&#20020;&#22256;&#38590;&#65292;&#25110;&#32773;&#23384;&#22312;&#26080;&#27861;&#24494;&#20998;&#30340;&#32452;&#20214;&#25110;&#32773;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23436;&#20840;&#21487;&#24494;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#27809;&#26377;&#20869;&#23384;&#32531;&#20914;&#21306;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#21442;&#32771;&#26041;&#27861;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#23567;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#24615;&#33021;&#26377;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#26356;&#23569;&#30340;&#20998;&#31867;&#22120;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning with an increasing number of classes is a challenging task. The difficulty rises when each example is presented exactly once, which requires the model to learn online. Recent methods with classic parameter optimization procedures have been shown to struggle in such setups or have limitations like non-differentiable components or memory buffers. For this reason, we present the fully differentiable ensemble method that allows us to efficiently train an ensemble of neural networks in the end-to-end regime. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods. The conducted experiments have also shown a significant increase in the performance for small ensembles, which demonstrates the capability of obtaining relatively high classification accuracy with a reduced number of classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MPC&#21451;&#22909;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;MPCViT&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#26469;&#23454;&#29616;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.13955</link><description>&lt;p&gt;
MPCViT&#65306;&#20351;&#29992;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#31934;&#20934;&#39640;&#25928;&#30340;MPC&#21451;&#22909;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention. (arXiv:2211.13955v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MPC&#21451;&#22909;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;MPCViT&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#26469;&#23454;&#29616;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;(MPC)&#21487;&#20197;&#30452;&#25509;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#20445;&#25252;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;Vision Transformers(ViTs)&#65292;&#24182;&#26410;&#19987;&#20026;MPC&#35774;&#35745;&#25110;&#20248;&#21270;&#65292;&#22240;&#27492;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;Softmax&#26159;&#20027;&#35201;&#30340;&#24310;&#36831;&#29942;&#39048;&#65292;&#30001;&#20110;&#39640;&#36890;&#20449;&#22797;&#26434;&#24615;&#65292;&#21487;&#26377;&#36873;&#25321;&#22320;&#26367;&#25442;&#25110;&#32447;&#24615;&#21270;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPC&#21451;&#22909;&#30340;ViT&#65292;&#31216;&#20026;MPCViT&#65292;&#22312;MPC&#20013;&#23454;&#29616;&#20934;&#30830;&#32780;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#12290;&#22522;&#20110;&#23545;Softmax&#27880;&#24847;&#21147;&#21644;&#20854;&#20182;&#27880;&#24847;&#21147;&#21464;&#20307;&#30340;&#31995;&#32479;&#24310;&#36831;&#21644;&#31934;&#24230;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#26500;&#27880;&#24847;&#21147;&#20248;&#21270;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;MPC&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#24085;&#32047;&#25176;&#20248;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPCViT+&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;Softmax&#26367;&#25442;&#21644;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the So
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#27604;&#36739;&#20102;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#30340;&#27450;&#39575;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#31350;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21306;&#21035;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13035</link><description>&lt;p&gt;
&#35854;&#35328;&#21487;&#20197;&#34987;&#20266;&#36896;&#21527;&#65311;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#27604;&#36739;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#30340;&#27450;&#39575;&#35270;&#39057;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Can lies be faked? Comparing low-stakes and high-stakes deception video datasets from a Machine Learning perspective. (arXiv:2211.13035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#27604;&#36739;&#20102;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#30340;&#27450;&#39575;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#31350;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21306;&#21035;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35854;&#35328;&#23545;&#20154;&#31867;&#31038;&#20250;&#26377;&#30528;&#24040;&#22823;&#24433;&#21709;&#65292;&#24182;&#19988;&#27450;&#39575;&#26816;&#27979;&#30340;&#20154;&#31867;&#20934;&#30830;&#29575;&#20165;&#20026;54&#65285;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20173;&#28982;&#26080;&#27861;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#26377;&#25928;&#25191;&#34892;&#33258;&#21160;&#21270;&#30340;&#27450;&#39575;&#26816;&#27979;&#65292;&#21407;&#22240;&#22312;&#20110;&#25968;&#25454;&#31232;&#32570;&#12290;&#20844;&#24320;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#24182;&#19988;&#30001;&#20110;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#35854;&#35328;&#20043;&#38388;&#30340;&#27010;&#24565;&#21306;&#21035;&#65292;&#21019;&#36896;&#26032;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#22256;&#38590;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20004;&#31181;&#35854;&#35328;&#26159;&#22914;&#27492;&#19981;&#21516;&#65292;&#20197;&#33267;&#20110;&#19968;&#20010;&#31181;&#31867;&#30340;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#21478;&#19968;&#20010;&#31181;&#31867;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#33719;&#24471;&#20302;&#39118;&#38505;&#27450;&#39575;&#30340;&#25968;&#25454;&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#27169;&#25311;&#65288;&#20266;&#36896;&#65289;&#65292;&#20294;&#36825;&#20123;&#35854;&#35328;&#19981;&#20855;&#26377;&#19982;&#30495;&#23454;&#30340;&#39640;&#39118;&#38505;&#35854;&#35328;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#25110;&#28145;&#24230;&#65292;&#32780;&#30495;&#23454;&#30340;&#39640;&#39118;&#38505;&#35854;&#35328;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#23545;&#20110;&#33258;&#21160;&#21270;&#27450;&#39575;&#26816;&#27979;&#31995;&#32479;&#20855;&#26377;&#23454;&#38469;&#20852;&#36259;&#12290;&#20026;&#20102;&#20174;&#23454;&#36341;&#35282;&#24230;&#25506;&#31350;&#36825;&#20010;&#21306;&#21035;&#26159;&#21542;&#25104;&#31435;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#19968;&#20010;&#39640;&#39118;&#38505;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20302;&#39118;&#38505;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great impact of lies in human societies and a meager 54% human accuracy for Deception Detection (DD), Machine Learning systems that perform automated DD are still not viable for proper application in real-life settings due to data scarcity. Few publicly available DD datasets exist and the creation of new datasets is hindered by the conceptual distinction between low-stakes and high-stakes lies. Theoretically, the two kinds of lies are so distinct that a dataset of one kind could not be used for applications for the other kind. Even though it is easier to acquire data on low-stakes deception since it can be simulated (faked) in controlled settings, these lies do not hold the same significance or depth as genuine high-stakes lies, which are much harder to obtain and hold the practical interest of automated DD systems. To investigate whether this distinction holds true from a practical perspective, we design several experiments comparing a high-stakes DD dataset and a low-stak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10773</link><description>&lt;p&gt;
&#19968;&#31181; $k$ &#26368;&#36817;&#37051;&#30340;&#20004;&#38454;&#27573;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#33258;&#21160;&#36866;&#24212;&#20998;&#24067;&#27604;&#20363;&#21464;&#21270;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#38024;&#23545;&#33258;&#28982;&#20445;&#30041;&#36825;&#20123;&#20248;&#31168;&#29305;&#24615;&#30340;&#26412;&#22320;&#25237;&#31080;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#65292;&#35774;&#35745;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#27492; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#20026;&#36890;&#36807;&#25105;&#20204;&#26041;&#26696;&#33719;&#21462;&#30340;&#26679;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#24403;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968; $\mathbb{P}(Y=y|X=x)$ &#36275;&#22815;&#24179;&#28369;&#24182;&#19988; Tsybakov &#22122;&#22768;&#26465;&#20214;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
$k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#20197;&#22686;&#21152;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.01160</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#24191;&#21578;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Profit-Maximizing Strategy for Advertising on the e-Commerce Platforms. (arXiv:2211.01160v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#32452;&#21512;&#20197;&#22686;&#21152;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#31649;&#29702;&#24179;&#21488;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#30005;&#23376;&#21830;&#21153;&#21334;&#23478;/&#24191;&#21578;&#21830;&#30340;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#26041;&#27861;&#26469;&#21560;&#24341;&#30446;&#26631;&#23458;&#25143;&#12290;&#23613;&#31649;&#26377;&#20854;&#20248;&#21183;&#65292;&#20294;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#22312;&#32447;&#21334;&#23478;&#26469;&#35828;&#65292;&#27491;&#30830;&#37197;&#32622;&#24191;&#21578;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26080;&#25928;&#30340;&#31574;&#30053;&#24448;&#24448;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#26080;&#25928;&#28857;&#20987;&#65292;&#20174;&#32780;&#23548;&#33268;&#24191;&#21578;&#36153;&#29992;&#19982;&#38144;&#21806;&#22686;&#38271;&#19981;&#25104;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#24191;&#21578;&#23450;&#21521;&#36873;&#39033;&#30340;&#26032;&#39062;&#30340;&#26368;&#22823;&#21270;&#21033;&#28070;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#23558;&#23450;&#21521;&#21463;&#20247;&#36716;&#21270;&#20026;&#23454;&#38469;&#36141;&#20080;&#32773;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20248;&#21270;&#25361;&#25112;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#36873;&#32972;&#21253;&#38382;&#39064;&#65288;MCKP&#65289;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;&#26469;&#33258;&#22825;&#29483;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#24191;&#21578;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online advertising management platform has become increasingly popular among e-commerce vendors/advertisers, offering a streamlined approach to reach target customers. Despite its advantages, configuring advertising strategies correctly remains a challenge for online vendors, particularly those with limited resources. Ineffective strategies often result in a surge of unproductive ``just looking'' clicks, leading to disproportionately high advertising expenses comparing to the growth of sales. In this paper, we present a novel profit-maximing strategy for targeting options of online advertising. The proposed model aims to find the optimal set of features to maximize the probability of converting targeted audiences into actual buyers. We address the optimization challenge by reformulating it as a multiple-choice knapsack problem (MCKP). We conduct an empirical study featuring real-world data from Tmall to show that our proposed method can effectively optimize the advertising strategy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;11&#31181;&#33080;&#37096;&#21311;&#21517;&#21270;&#26041;&#27861;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.10651</link><description>&lt;p&gt;
&#29702;&#35299;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#65306;Fant\^omas&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fant\^omas: Understanding Face Anonymization Reversibility. (arXiv:2210.10651v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;11&#31181;&#33080;&#37096;&#21311;&#21517;&#21270;&#26041;&#27861;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33080;&#37096;&#22270;&#20687;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#20449;&#24687;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35782;&#21035;&#20010;&#20154;&#24182;&#25512;&#26029;&#20182;&#20204;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#38544;&#31169;&#39118;&#38505;&#65292;&#21311;&#21517;&#21270;&#26041;&#27861;&#20351;&#29992;&#23545;&#28165;&#26224;&#22270;&#20687;&#36827;&#34892;&#36716;&#25442;&#20197;&#28151;&#28102;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#23450;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23427;&#20204;&#20197;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22768;&#26126;&#21457;&#34920;&#65292;&#20294;&#26377;&#26102;&#24182;&#26410;&#32463;&#36807;&#20196;&#20154;&#20449;&#26381;&#30340;&#26041;&#27861;&#35780;&#20272;&#12290;&#23558;&#21311;&#21517;&#21270;&#22270;&#20687;&#36870;&#36716;&#22238;&#20223;&#30495;&#20854;&#30495;&#23454;&#36755;&#20837;&#30340;&#31243;&#24230;&#65292;&#29978;&#33267;&#33021;&#34987;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#35782;&#21035;&#65292;&#36825;&#23545;&#20110;&#21311;&#21517;&#21270;&#30340;&#32570;&#38519;&#26159;&#26368;&#24378;&#26377;&#21147;&#30340;&#25351;&#26631;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#32467;&#26524;&#30340;&#30830;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#27861;&#32780;&#35328;&#36825;&#26159;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#21738;&#20123;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#21487;&#36870;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#33080;&#37096;&#21311;&#21517;&#36870;&#36716;&#29616;&#35937;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32463;&#36807;&#27979;&#35797;&#30340;15&#31181;&#26041;&#27861;&#20013;&#65292;&#26377;11&#31181;&#33267;&#23569;&#37096;&#20998;&#21487;&#36870;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#26500;&#21644;&#21453;&#28436;&#26159;&#22914;&#20309;&#23454;&#29616;&#21487;&#36870;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face images are a rich source of information that can be used to identify individuals and infer private information about them. To mitigate this privacy risk, anonymizations employ transformations on clear images to obfuscate sensitive information, all while retaining some utility. Albeit published with impressive claims, they sometimes are not evaluated with convincing methodology.  Reversing anonymized images to resemble their real input -- and even be identified by face recognition approaches -- represents the strongest indicator for flawed anonymization. Some recent results indeed indicate that this is possible for some approaches. It is, however, not well understood, which approaches are reversible, and why. In this paper, we provide an exhaustive investigation in the phenomenon of face anonymization reversibility. Among other things, we find that 11 out of 15 tested face anonymizations are at least partially reversible and highlight how both reconstruction and inversion are the u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.02390</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#39640;&#25928;&#30340;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#23398;&#20064;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25552;&#31034;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#35270;&#20026;&#21487;&#35757;&#32451;&#30340;&#65292;&#21516;&#26102;&#20923;&#32467;&#20854;&#20313;&#37096;&#20998;&#65292;&#24182;&#20248;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24050;&#30693;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#24433;&#21709;&#20102;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#21046;&#23450;&#20026;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20943;&#23569;&#23545;&#24050;&#35265;&#25552;&#31034;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#25552;&#31034;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20197;&#27010;&#29575;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20316;&#20026;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#25105;&#20204;&#30340;&#25552;&#35758;&#19982;&#22522;&#20110;&#22270;&#20687;&#26080;&#26465;&#20214;&#25110;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26412;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#20559;&#24046;&#21644;&#22806;&#25512;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24658;&#23450;&#27493;&#38271;&#21644;Markov&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;LSA&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#26497;&#38480;&#21644;&#31283;&#23450;&#20998;&#24067;&#65292;&#24182;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#30340;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#20010;&#26497;&#38480;&#30340;&#20559;&#24046;&#19982;&#27493;&#38271;&#25104;&#27604;&#20363;&#65292;&#30452;&#33267;&#26356;&#39640;&#38454;&#39033;&#12290;&#22312;&#21487;&#36870;&#38142;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20559;&#24046;&#19982;Markov&#25968;&#25454;&#30340;&#28151;&#21512;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.00953</link><description>&lt;p&gt;
Markov&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#20559;&#24046;&#21644;&#22806;&#25512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes. (arXiv:2210.00953v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#20013;&#30340;&#20559;&#24046;&#21644;&#22806;&#25512;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24658;&#23450;&#27493;&#38271;&#21644;Markov&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;LSA&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#26497;&#38480;&#21644;&#31283;&#23450;&#20998;&#24067;&#65292;&#24182;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#30340;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#20010;&#26497;&#38480;&#30340;&#20559;&#24046;&#19982;&#27493;&#38271;&#25104;&#27604;&#20363;&#65292;&#30452;&#33267;&#26356;&#39640;&#38454;&#39033;&#12290;&#22312;&#21487;&#36870;&#38142;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20559;&#24046;&#19982;Markov&#25968;&#25454;&#30340;&#28151;&#21512;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#24658;&#23450;&#27493;&#38271;&#21644;Markov&#25968;&#25454;&#30340;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#65288;LSA&#65289;&#12290;&#23558;&#25968;&#25454;&#21644;LSA&#36845;&#20195;&#30340;&#32852;&#21512;&#36807;&#31243;&#35270;&#20026;&#26102;&#38388;&#40784;&#27425;Markov&#38142;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#22312;Wasserstein&#36317;&#31163;&#19979;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#26497;&#38480;&#21644;&#31283;&#23450;&#20998;&#24067;&#65292;&#24182;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#30340;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26497;&#38480;&#30340;&#20559;&#24046;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#27493;&#38271;&#23637;&#24320;&#20026;&#26080;&#38480;&#32423;&#25968;&#12290;&#22240;&#27492;&#65292;&#20559;&#24046;&#19982;&#27493;&#38271;&#25104;&#27604;&#20363;&#65292;&#30452;&#33267;&#26356;&#39640;&#38454;&#39033;&#12290;&#36825;&#20010;&#32467;&#26524;&#19982;i.i.d.&#25968;&#25454;&#19979;&#30340;LSA&#24418;&#25104;&#23545;&#27604;&#65292;&#20854;&#20013;&#20559;&#24046;&#20026;&#38646;&#12290;&#22312;&#21487;&#36870;&#38142;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20559;&#24046;&#19982;Markov&#25968;&#25454;&#30340;&#28151;&#21512;&#26102;&#38388;&#20043;&#38388;&#20851;&#31995;&#30340;&#19968;&#33324;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#23427;&#20204;&#22823;&#33268;&#25104;&#27491;&#27604;&#30340;&#32467;&#35770;&#12290;&#34429;&#28982;Polyak-Ruppert&#23614;&#24179;&#22343;&#20943;&#23569;&#20102;LSA&#36845;&#20195;&#30340;&#26041;&#24046;&#65292;&#20294;&#24182;&#19981;&#24433;&#21709;&#20559;&#24046;&#12290;&#20197;&#19978;&#29305;&#24449;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
We consider Linear Stochastic Approximation (LSA) with a constant stepsize and Markovian data. Viewing the joint process of the data and LSA iterate as a time-homogeneous Markov chain, we prove its convergence to a unique limiting and stationary distribution in Wasserstein distance and establish non-asymptotic, geometric convergence rates. Furthermore, we show that the bias vector of this limit admits an infinite series expansion with respect to the stepsize. Consequently, the bias is proportional to the stepsize up to higher order terms. This result stands in contrast with LSA under i.i.d. data, for which the bias vanishes. In the reversible chain setting, we provide a general characterization of the relationship between the bias and the mixing time of the Markovian data, establishing that they are roughly proportional to each other.  While Polyak-Ruppert tail-averaging reduces the variance of the LSA iterates, it does not affect the bias. The above characterization allows us to show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21160;&#21147;&#23398;&#24615;&#36136;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#65292;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.07171</link><description>&lt;p&gt;
&#23398;&#20064;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#36827;&#34892;&#22235;&#36275;&#21160;&#29289;&#30340;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning to Exploit Elastic Actuators for Quadruped Locomotion. (arXiv:2209.07171v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#30340;&#21160;&#21147;&#23398;&#24615;&#36136;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#65292;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22235;&#36275;&#36816;&#21160;&#20013;&#65292;&#22522;&#20110;&#24377;&#31783;&#30340;&#25191;&#34892;&#22120;&#21487;&#20197;&#25552;&#39640;&#33021;&#25928;&#21644;&#24615;&#33021;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#38590;&#24230;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#36890;&#36807;&#24314;&#27169;&#19982;&#20223;&#30495;&#26469;&#23547;&#25214;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30452;&#25509;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#30001;&#20013;&#22830;&#27169;&#24335;&#21457;&#29983;&#22120;&#65288;CPGs&#65289;&#21512;&#25104;&#27493;&#24577;&#65292;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#21442;&#25968;&#24471;&#21040;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#36816;&#21160;&#30340;&#24320;&#29615;&#25511;&#21046;&#22120;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20351;&#25511;&#21046;&#22120;&#26356;&#21152;&#31283;&#20581;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#26412;&#25991;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#38381;&#29615;&#25511;&#21046;&#65292;&#23398;&#20064;&#22312;CPGs&#20043;&#19978;&#30340;&#30699;&#27491;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;DLR&#24377;&#24615;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#21040;&#30340;&#24930;&#36305;&#21644;&#36339;&#36291;&#27493;&#24577;&#34920;&#26126;&#65292;&#22312;&#20248;&#21270;&#21160;&#24577;&#36816;&#21160;&#26102;&#21033;&#29992;&#24377;&#24615;&#25191;&#34892;&#22120;&#21160;&#21147;&#23398;&#26159;&#33258;&#28982;&#21457;&#29983;&#30340;&#65292;&#23613;&#31649;&#26080;&#27169;&#22411;&#25511;&#21046;&#22120;&#65292;&#20294;&#20173;&#21487;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#36816;&#21160;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spring-based actuators in legged locomotion provide energy-efficiency and improved performance, but increase the difficulty of controller design. While previous work has focused on extensive modeling and simulation to find optimal controllers for such systems, we propose to learn model-free controllers directly on the real robot. In our approach, gaits are first synthesized by central pattern generators (CPGs), whose parameters are optimized to quickly obtain an open-loop controller that achieves efficient locomotion. Then, to make this controller more robust and further improve the performance, we use reinforcement learning to close the loop, to learn corrective actions on top of the CPGs. We evaluate the proposed approach on the DLR elastic quadruped bert. Our results in learning trotting and pronking gaits show that exploitation of the spring actuator dynamics emerges naturally from optimizing for dynamic motions, yielding high-performing locomotion despite being model-free. The who
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.04587</link><description>&lt;p&gt;
&#22810;&#28857;-BAX: &#19968;&#31181;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#21457;&#23556;&#24230;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v4 [physics.acc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26463;&#21457;&#23556;&#24230;&#23545;&#20110;&#39640;&#20142;&#24230;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20248;&#21270;&#36890;&#24120;&#20250;&#21463;&#21040;&#26102;&#38388;&#38480;&#21046;&#65292;&#22240;&#20026;&#21457;&#23556;&#24230;&#35745;&#31639;&#36890;&#24120;&#26159;&#36890;&#36807;&#22235;&#26497;&#25195;&#25551;&#23436;&#25104;&#30340;&#65292;&#32780;&#22235;&#26497;&#25195;&#25551;&#36890;&#24120;&#36739;&#24930;&#12290;&#36825;&#31181;&#35745;&#31639;&#26159;&#19968;&#31181;&#22810;&#28857;&#26597;&#35810;&#65292;&#21363;&#27599;&#20010;&#26597;&#35810;&#37117;&#38656;&#35201;&#22810;&#20010;&#36741;&#21161;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#22788;&#29702;&#36825;&#26679;&#30340;&#30446;&#26631;&#26102;&#36895;&#24230;&#24930;&#19988;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#33719;&#21462;&#23436;&#25972;&#30340;&#27979;&#37327;&#24207;&#21015;&#65292;&#20294;&#27599;&#20010;&#26597;&#35810;&#20165;&#36820;&#22238;&#21457;&#23556;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36125;&#21494;&#26031;&#31639;&#27861;&#25191;&#34892;(BAX)&#24212;&#29992;&#20110;&#26597;&#35810;&#21644;&#24314;&#27169;&#21333;&#20010;&#26463;&#27969;&#23610;&#23544;&#27979;&#37327;&#12290;BAX&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#21152;&#36895;&#22120;&#20013;&#33719;&#21462;&#21457;&#23556;&#24230;&#25351;&#26631;&#26469;&#36991;&#20813;&#22312;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#32531;&#24930;&#30340;&#22810;&#28857;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;BAX&#26469;&#26368;&#23567;&#21270;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#30340;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of $\textit{multi-point query}$, i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose applying Bayesian Algorithm Execution (BAX) to instead query and model individual beam-size measurements. BAX avoids the slow multi-point query on the accelerator by acquiring points through a $\textit{virtual objective}$, i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. Here, we use BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Adv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.01566</link><description>&lt;p&gt;
&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#65306;&#22312;&#26377;&#38480;&#33539;&#22260;&#20869;&#23454;&#29616;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#20174;&#21487;&#34920;&#36798;&#24615;&#21040;&#21487;&#25191;&#34892;&#24615;&#23454;&#29616;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#30721;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35821;&#20041;&#37329;&#23383;&#22612;&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20195;&#30721;&#29983;&#25104;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#39064;&#65292;&#37319;&#29992;&#31070;&#32463;&#27169;&#22411;&#20026;&#39044;&#26399;&#21151;&#33021;&#29983;&#25104;&#20195;&#30721;&#12290;&#30001;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#36719;&#20214;&#23618;&#27425;&#24847;&#35782;&#65292;&#23427;&#20204;&#22312;&#39033;&#30446;&#32423;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;&#20195;&#30721;&#29983;&#25104;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25105;&#20204;&#35753;&#20854;&#21442;&#19982;&#20174;&#8220;&#21487;&#34920;&#36798;&#24615;&#8221;&#21040;&#8220;&#21487;&#25191;&#34892;&#24615;&#8221;&#30340;&#33258;&#39030;&#21521;&#19979;&#24320;&#21457;&#65292;&#36825;&#22312;&#26377;&#38480;&#30340;&#33539;&#22260;&#20869;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#23427;&#20174;&#22823;&#37327;&#30340;&#26679;&#26412;&#12289;&#29305;&#24449;&#21644;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#20316;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20195;&#30721;&#25968;&#25454;&#19978;&#24314;&#31435;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#21363;&#20195;&#30721;&#20998;&#31867;&#27861;&#65292;&#21033;&#29992;&#20195;&#30721;&#20449;&#24687;&#30340;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#23618;&#35821;&#20041;&#37329;&#23383;&#22612;(SP)&#26469;&#20851;&#32852;&#25991;&#26412;&#25968;&#25454;&#21644;&#20195;&#30721;&#25968;&#25454;&#12290;&#23427;&#35782;&#21035;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20851;&#20110;&#24320;&#21457;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20214;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;</title><link>http://arxiv.org/abs/2208.10533</link><description>&lt;p&gt;
&#19968;&#20123;&#30417;&#30563;&#26159;&#24517;&#39035;&#30340;&#65306;&#36890;&#36807;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#31070;&#35861;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22266;&#26377;&#38382;&#39064;&#26159;&#36890;&#36807;&#38543;&#26426;&#34892;&#21160;&#25506;&#32034;&#29615;&#22659;&#65292;&#20854;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#12290;&#30456;&#21453;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#65288;&#20808;&#21069;&#23398;&#20064;&#30340;&#25110;&#30828;&#32534;&#30721;&#30340;&#65289;&#31070;&#35861;&#31574;&#30053;&#12289;&#31163;&#32447;&#25968;&#25454;&#25110;&#28436;&#31034;&#26469;&#25913;&#21892;&#25506;&#32034;&#12290;&#20294;&#22312;&#20351;&#29992;&#31070;&#35861;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22823;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#22320;&#23558;&#31070;&#35861;&#32463;&#39564;&#34701;&#20837;&#21040;&#23398;&#20064;&#31574;&#30053;&#20013;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#65288;Critic Confidence Guided Exploration&#65292;CCGE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#36825;&#26679;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;CCGE&#20197;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20026;&#24314;&#35758;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#24403;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;CCGE&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#21152;&#21306;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2207.13243</link><description>&lt;p&gt;
&#36208;&#21521;&#36879;&#26126;AI: &#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#30340;&#35299;&#37322;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#20869;&#37096;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#24110;&#21161;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20998;&#26512;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#22312;&#19981;&#24443;&#24213;&#29702;&#35299;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#20204;&#30340;&#25285;&#24551;&#12290;&#35299;&#37322;&#23427;&#20204;&#30340;&#26377;&#25928;&#24037;&#20855;&#23558;&#23545;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;AI&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#36807;&#24110;&#21161;&#35782;&#21035;&#38382;&#39064;&#12289;&#20462;&#22797;&#38169;&#35823;&#21644;&#22686;&#36827;&#22522;&#26412;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;"&#20869;&#37096;"&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#23427;&#20204;&#19987;&#27880;&#20110;&#35299;&#37322;DNNs&#30340;&#20869;&#37096;&#32452;&#20214;&#65292;&#38750;&#24120;&#36866;&#21512;&#20110;&#24320;&#21457;&#26426;&#26800;&#29702;&#35299;&#12289;&#25351;&#23548;&#25163;&#21160;&#20462;&#25913;&#21644;&#36870;&#21521;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;DNN&#21487;&#35299;&#37322;&#24615;&#19978;&#65292;&#36805;&#36895;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#23545;&#26041;&#27861;&#36827;&#34892;&#24443;&#24213;&#31995;&#32479;&#21270;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;300&#22810;&#31687;&#20316;&#21697;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#26041;&#27861;&#25353;&#32593;&#32476;&#30340;&#21738;&#20010;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.  Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22270;&#23398;&#20064;&#20013;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;&#22270;&#36793;&#19978;&#30340;&#24046;&#20998;&#38544;&#31169;&#24341;&#20837;&#30340;&#22122;&#22768;&#25200;&#20081;&#20102;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#38480;&#21046;&#20102;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.11836</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#23545;&#27604;&#23398;&#20064;&#20943;&#36731;DP&#28385;&#36275;&#30340;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Performance Sacrifice in DP-Satisfied Federated Settings through Graph Contrastive Learning. (arXiv:2207.11836v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22270;&#23398;&#20064;&#20013;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;&#22270;&#36793;&#19978;&#30340;&#24046;&#20998;&#38544;&#31169;&#24341;&#20837;&#30340;&#22122;&#22768;&#25200;&#20081;&#20102;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#38480;&#21046;&#20102;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22270;&#23398;&#20064;&#27169;&#22411;&#26159;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#12290;&#22312;&#23398;&#26415;&#30028;&#65292;&#20351;&#29992;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#20248;&#21270;&#22270;&#27169;&#22411;&#26159;&#35757;&#32451;&#24378;&#22823;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36825;&#26679;&#20570;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65288;&#20363;&#22914;&#22522;&#20110;&#22270;&#36793;&#30340;&#24046;&#20998;&#38544;&#31169;&#65289;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32852;&#37030;&#22270;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#33021;&#30830;&#20445;&#34920;&#31034;&#22312;&#22270;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#30340;&#23433;&#20840;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22270;&#36793;&#19978;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22270;&#36793;&#19978;&#30340;&#24046;&#20998;&#38544;&#31169;&#24341;&#20837;&#22122;&#22768;&#25200;&#20081;&#20102;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#26159;&#22270;&#23545;&#27604;&#22686;&#24378;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, graph learning models are indispensable tools to help researchers explore graph-structured data. In academia, using sufficient training data to optimize a graph model on a single device is a typical approach for training a capable graph learning model. Due to privacy concerns, however, it is infeasible to do so in real-world scenarios. Federated learning provides a practical means of addressing this limitation by introducing various privacy-preserving mechanisms, such as differential privacy (DP) on the graph edges. However, although DP in federated graph learning can ensure the security of sensitive information represented in graphs, it usually causes the performance of graph learning models to degrade. In this paper, we investigate how DP can be implemented on graph edges and observe a performance decrease in our experiments. In addition, we note that DP on graph edges introduces noise that perturbs graph proximity, which is one of the graph augmentations in graph contrast
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#36716;&#25442;&#34920;&#31034;&#20026;&#21521;&#37327;&#24182;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;</title><link>http://arxiv.org/abs/2207.11437</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks. (arXiv:2207.11437v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36923;&#36753;&#32508;&#21512;&#32467;&#26524;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#36716;&#25442;&#34920;&#31034;&#20026;&#21521;&#37327;&#24182;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36923;&#36753;&#32508;&#21512;&#38454;&#27573;&#65292;&#32508;&#21512;&#24037;&#20855;&#20013;&#30340;&#32467;&#26500;&#36716;&#25442;&#38656;&#35201;&#19982;&#20248;&#21270;&#24207;&#21015;&#32467;&#21512;&#65292;&#24182;&#20316;&#29992;&#20110;&#30005;&#36335;&#65292;&#20197;&#28385;&#36275;&#25351;&#23450;&#30340;&#30005;&#36335;&#38754;&#31215;&#21644;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#32508;&#21512;&#20248;&#21270;&#24207;&#21015;&#30340;&#36816;&#34892;&#26102;&#38388;&#36739;&#38271;&#65292;&#20026;&#30005;&#36335;&#23545;&#32508;&#21512;&#20248;&#21270;&#24207;&#21015;&#30340;&#32467;&#26524;&#36136;&#37327;&#65288;QoR&#65289;&#36827;&#34892;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#24037;&#31243;&#24072;&#26356;&#24555;&#22320;&#25214;&#21040;&#26356;&#22909;&#30340;&#20248;&#21270;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#30005;&#36335;-&#20248;&#21270;&#24207;&#21015;&#23545;&#30340;QoR&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23884;&#20837;&#26041;&#27861;&#23558;&#32467;&#26500;&#36716;&#25442;&#36716;&#21270;&#20026;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65288;Transformer&#65289;&#25552;&#21462;&#20248;&#21270;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#36807;&#31243;&#33021;&#22815;&#20174;&#30005;&#36335;&#27867;&#21270;&#21040;&#30005;&#36335;&#65292;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#34987;&#34920;&#31034;&#20026;&#37051;&#25509;&#30697;&#38453;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20110;&#23398;&#20064;&#30005;&#36335;&#30340;&#22270;&#34920;&#31034;&#21644;&#39044;&#27979;QoR&#12290;
&lt;/p&gt;
&lt;p&gt;
In the logic synthesis stage, structure transformations in the synthesis tool need to be combined into optimization sequences and act on the circuit to meet the specified circuit area and delay. However, logic synthesis optimization sequences are time-consuming to run, and predicting the quality of the results (QoR) against the synthesis optimization sequence for a circuit can help engineers find a better optimization sequence faster. In this work, we propose a deep learning method to predict the QoR of unseen circuit-optimization sequences pairs. Specifically, the structure transformations are translated into vectors by embedding methods and advanced natural language processing (NLP) technology (Transformer) is used to extract the features of the optimization sequences. In addition, to enable the prediction process of the model to be generalized from circuit to circuit, the graph representation of the circuit is represented as an adjacency matrix and a feature matrix. Graph neural net
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.10786</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#28041;&#21450;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#40657;&#30418;&#20248;&#21270; (BBO) &#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#24120;&#20551;&#35774;&#22312;&#32447;&#20989;&#25968;&#35780;&#20272;&#30340;&#39044;&#31639;&#24456;&#23567;&#65292;&#20294;&#24448;&#24448;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22266;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#36924;&#36817;&#20989;&#25968;&#25110;&#20854;&#21453;&#20989;&#25968;&#65292;&#20294;&#22312;&#31163;&#25968;&#25454;&#20998;&#24067;&#36739;&#36828;&#26102;&#19981;&#22815;&#31934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BONET&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#22312;BONET&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#23450;&#38271;&#36712;&#36857;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#21040;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#30340;&#21333;&#35843;&#36716;&#25442;&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26469;&#21512;&#25104;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#36712;&#36857;&#12290;&#22312;Design-Bench&#19978;&#20351;&#29992;&#34987;&#22240;&#26524;&#25513;&#34109;&#30340;Transformer&#23454;&#20363;&#21270;BONET&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#24179;&#22343;&#25490;&#21517;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
&lt;/p&gt;</description></item><item><title>StudioGAN&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#21644;&#22522;&#20934;&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;GAN&#26550;&#26500;&#12289;&#26465;&#20214;&#26041;&#27861;&#12289;&#23545;&#25239;&#25439;&#22833;&#12289;&#27491;&#21017;&#21270;&#27169;&#22359;&#12289;&#21487;&#24494;&#22686;&#24378;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#35780;&#20272;&#39592;&#26550;&#12290;&#36825;&#20010;&#22522;&#20934;&#24211;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#21508;&#31181;GAN&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.09479</link><description>&lt;p&gt;
StudioGAN: GAN&#22270;&#20687;&#21512;&#25104;&#30340;&#20998;&#31867;&#21644;&#22522;&#20934;&#24211;
&lt;/p&gt;
&lt;p&gt;
StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09479
&lt;/p&gt;
&lt;p&gt;
StudioGAN&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#21644;&#22522;&#20934;&#24211;&#65292;&#25903;&#25345;&#22810;&#31181;GAN&#26550;&#26500;&#12289;&#26465;&#20214;&#26041;&#27861;&#12289;&#23545;&#25239;&#25439;&#22833;&#12289;&#27491;&#21017;&#21270;&#27169;&#22359;&#12289;&#21487;&#24494;&#22686;&#24378;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#35780;&#20272;&#39592;&#26550;&#12290;&#36825;&#20010;&#22522;&#20934;&#24211;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#21508;&#31181;GAN&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26159;&#23454;&#29616;&#36924;&#30495;&#22270;&#20687;&#21512;&#25104;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GAN&#30740;&#31350;&#39046;&#22495;&#24182;&#27809;&#26377;&#25552;&#20379;&#21487;&#38752;&#30340;&#22522;&#20934;&#24211;&#65292;&#20174;&#32780;&#26080;&#27861;&#36827;&#34892;&#19968;&#33268;&#19988;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#39564;&#35777;&#30340;GAN&#23454;&#29616;&#65292;&#30740;&#31350;&#32773;&#20204;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22797;&#29616;&#22522;&#20934;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;GAN&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;StudioGAN&#30340;&#26032;&#30340;&#24320;&#28304;&#24211;&#12290;StudioGAN&#25903;&#25345;7&#31181;GAN&#26550;&#26500;&#12289;9&#31181;&#26465;&#20214;&#26041;&#27861;&#12289;4&#31181;&#23545;&#25239;&#25439;&#22833;&#12289;12&#31181;&#27491;&#21017;&#21270;&#27169;&#22359;&#12289;3&#31181;&#21487;&#24494;&#22686;&#24378;&#26041;&#27861;&#12289;7&#31181;&#35780;&#20272;&#25351;&#26631;&#21644;5&#31181;&#35780;&#20272;&#39592;&#26550;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#38598;(CIFAR10&#12289;ImageNet&#12289;AFHQv2&#12289;FFHQ&#21644;Baby/Papa/Granpa-ImageNet)&#20197;&#21450;3&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#39592;&#26550;(InceptionV3&#12289;SwAV&#21644;Swin Transformer)&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 12 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25991;&#31456;&#23558;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#24314;&#27169;&#20026;&#29366;&#24577;&#31354;&#38388;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#32463;&#20856;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#20256;&#36882;&#20989;&#25968;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Adam&#21464;&#20307;&#65292;&#31216;&#20026;AdamSSM&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#20934;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.02034</link><description>&lt;p&gt;
&#19968;&#20010;&#25511;&#21046;&#29702;&#35770;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning. (arXiv:2206.02034v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25991;&#31456;&#23558;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#24314;&#27169;&#20026;&#29366;&#24577;&#31354;&#38388;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#32463;&#20856;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#20256;&#36882;&#20989;&#25968;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Adam&#21464;&#20307;&#65292;&#31216;&#20026;AdamSSM&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#20934;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#24050;&#32463;&#22312;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#26368;&#36817;&#30340;&#20363;&#23376;&#21253;&#25324;AdaGrad&#21644;Adam&#12290;&#34429;&#28982;Adam&#36890;&#24120;&#25910;&#25947;&#26356;&#24555;&#65292;&#20294;&#26159;Adam&#30340;&#19968;&#20123;&#21464;&#20307;&#65292;&#27604;&#22914;AdaBelief&#31639;&#27861;&#65292;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;Adam&#19982;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30456;&#27604;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#29366;&#24577;&#31354;&#38388;&#26694;&#26550;&#20013;&#24314;&#27169;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#31616;&#21270;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65288;&#22914;AdaGrad&#12289;Adam&#21644;AdaBelief&#65289;&#30340;&#25910;&#25947;&#35777;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#20856;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#20256;&#36882;&#20989;&#25968;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Adam&#21464;&#20307;&#65292;&#31216;&#20026;AdamSSM&#12290;&#25105;&#20204;&#22312;&#20256;&#36882;&#20989;&#25968;&#20013;&#20174;&#24179;&#26041;&#26799;&#24230;&#21040;&#20108;&#38454;&#30697;&#20272;&#35745;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#21512;&#36866;&#30340;&#26497;&#28857;-&#38646;&#28857;&#23545;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AdamSSM&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#22522;&#20934;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient methods have become popular in optimizing deep neural networks; recent examples include AdaGrad and Adam. Although Adam usually converges faster, variations of Adam, for instance, the AdaBelief algorithm, have been proposed to enhance Adam's poor generalization ability compared to the classical stochastic gradient method. This paper develops a generic framework for adaptive gradient methods that solve non-convex optimization problems. We first model the adaptive gradient methods in a state-space framework, which allows us to present simpler convergence proofs of adaptive optimizers such as AdaGrad, Adam, and AdaBelief. We then utilize the transfer function paradigm from classical control theory to propose a new variant of Adam, coined AdamSSM. We add an appropriate pole-zero pair in the transfer function from squared gradients to the second moment estimate. We prove the convergence of the proposed AdamSSM algorithm. Applications on benchmark machine learning tasks of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#21644;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#37117;&#20855;&#26377;&#21019;&#26032;&#20043;&#22788;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01409</link><description>&lt;p&gt;
&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid Models for Mixed Variables in Bayesian Optimization. (arXiv:2206.01409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#21644;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#37117;&#20855;&#26377;&#21019;&#26032;&#20043;&#22788;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23450;&#37327;&#65288;&#36830;&#32493;&#21644;&#25972;&#25968;&#65289;&#21644;&#23450;&#24615;&#65288;&#20998;&#31867;&#65289;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#23558;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#32467;&#26500;&#65288;MCTS&#65289;&#29992;&#20110;&#20998;&#31867;&#21464;&#37327;&#65292;&#24182;&#23558;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#12290;&#22312;&#25628;&#32034;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#23558;&#39057;&#29575;&#27966;&#30340;&#19978;&#32622;&#20449;&#24230;&#26641;&#25628;&#32034;&#65288;UCTS&#65289;&#21644;&#36125;&#21494;&#26031;&#29380;&#21033;&#20811;&#38647;&#25628;&#32034;&#31574;&#30053;&#36827;&#34892;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#26641;&#32467;&#26500;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#34701;&#21512;&#12290;&#22312;&#20195;&#29702;&#27169;&#22411;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#38024;&#23545;&#28151;&#21512;&#21464;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#22312;&#32447;&#26680;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#65292;&#21253;&#25324;&#21160;&#24577;&#26680;&#36873;&#25321;&#12289;&#29420;&#29305;&#30340;UCTS&#65288;hybridM&#65289;&#21644;&#36125;&#21494;&#26031;&#26356;&#26032;&#31574;&#30053;&#65288;hybridD&#65289;&#65292;&#23558;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#23450;&#20301;&#20026;&#28151;&#21512;&#21464;&#37327;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#27493;&#12290;&#25968;&#20540;&#23454;&#39564;&#20984;&#26174;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#65292;&#20984;&#26174;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new type of hybrid models for Bayesian optimization (BO) adept at managing mixed variables, encompassing both quantitative (continuous and integer) and qualitative (categorical) types. Our proposed new hybrid models merge Monte Carlo Tree Search structure (MCTS) for categorical variables with Gaussian Processes (GP) for continuous ones. Addressing efficiency in searching phase, we juxtapose the original (frequentist) upper confidence bound tree search (UCTS) and the Bayesian Dirichlet search strategies, showcasing the tree architecture's integration into Bayesian optimization. Central to our innovation in surrogate modeling phase is online kernel selection for mixed-variable BO. Our innovations, including dynamic kernel selection, unique UCTS (hybridM) and Bayesian update strategies (hybridD), position our hybrid models as an advancement in mixed-variable surrogate models. Numerical experiments underscore the hybrid models' superiority, highlighting their potentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.02654</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#22312;&#35745;&#25968;&#21644;&#37319;&#26679;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications. (arXiv:2205.02654v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#22240;&#26524;&#20998;&#26512;&#20013;&#65292;&#20174;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#20013;&#35745;&#25968;&#21644;&#37319;&#26679;&#26377;&#21521;&#26080;&#29615;&#22270;&#26159;&#22522;&#26412;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#25104;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#38271;&#26399;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26377;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#20123;&#31361;&#30772;&#20351;&#24471;&#22312;&#27963;&#36291;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#21644;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#26041;&#38754;&#65292;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65292;&#21407;&#26412;&#35748;&#20026;&#19981;&#21487;&#34892;&#30340;&#31574;&#30053;&#23454;&#38469;&#21487;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. As we show in experiments, these breakthroughs make thought-to-be-infeasible strategies in active learning of causal structures and causal effect identification with regard to a Markov equivalence class practically applicable.
&lt;/p&gt;</description></item><item><title>ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.16475</link><description>&lt;p&gt;
ConceptEvo&#65306;&#35299;&#35835;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#27010;&#24565;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16475
&lt;/p&gt;
&lt;p&gt;
ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ConceptEvo&#65292;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#32479;&#19968;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#25581;&#31034;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;DNN&#35299;&#37322;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#31354;&#30333;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#35757;&#32451;&#21518;&#30340;&#35299;&#37322;&#12290;ConceptEvo&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;&#65288;1&#65289;&#19968;&#31181;&#29983;&#25104;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#19981;&#21516;&#27169;&#22411;&#30340;&#24182;&#34892;&#27604;&#36739;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21457;&#29616;&#21644;&#37327;&#21270;&#31867;&#21035;&#39044;&#27979;&#20013;&#37325;&#35201;&#27010;&#24565;&#28436;&#21464;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;260&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20154;&#26426;&#35780;&#20272;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConceptEvo&#21487;&#20197;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#26377;&#24847;&#20041;&#19988;&#23545;&#39044;&#27979;&#37325;&#35201;&#30340;&#28436;&#21464;&#12290;ConceptEvo&#36866;&#29992;&#20110;&#29616;&#20195;&#65288;ConvNeXt&#65289;&#21644;&#32463;&#20856;&#30340;DNNs&#65288;&#20363;&#22914;VGGs&#65292;InceptionV3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25968;&#25454;&#21387;&#32553;&#39046;&#22495;&#65292;&#20854;&#20013;&#24212;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#26631;&#20934;&#21270;&#27969;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#21387;&#32553;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24517;&#35201;&#32972;&#26223;&#30693;&#35782;&#65288;&#22914;&#20449;&#24687;&#35770;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#30340;&#22238;&#39038;&#65292;&#24182;&#23545;&#30446;&#21069;&#25991;&#29486;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31934;&#36873;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2202.06533</link><description>&lt;p&gt;
&#31070;&#32463;&#25968;&#25454;&#21387;&#32553;&#23548;&#35770;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Neural Data Compression. (arXiv:2202.06533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25968;&#25454;&#21387;&#32553;&#39046;&#22495;&#65292;&#20854;&#20013;&#24212;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#25968;&#25454;&#21387;&#32553;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#26631;&#20934;&#21270;&#27969;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#21387;&#32553;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24517;&#35201;&#32972;&#26223;&#30693;&#35782;&#65288;&#22914;&#20449;&#24687;&#35770;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#30340;&#22238;&#39038;&#65292;&#24182;&#23545;&#30446;&#21069;&#25991;&#29486;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31934;&#36873;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25968;&#25454;&#21387;&#32553;&#26159;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25968;&#25454;&#21387;&#32553;&#30340;&#39046;&#22495;&#12290;&#26368;&#36817;&#22312;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#25968;&#25454;&#21387;&#32553;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#26631;&#20934;&#21270;&#27969;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#21387;&#32553;&#31639;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22238;&#39038;&#20449;&#24687;&#35770;&#65288;&#20363;&#22914;&#29109;&#32534;&#30721;&#12289;&#29575;&#22833;&#30495;&#29702;&#35770;&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;&#20363;&#22914;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#12289;&#24863;&#30693;&#24230;&#37327;&#65289;&#31561;&#24517;&#35201;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#23545;&#30446;&#21069;&#25991;&#29486;&#20013;&#22522;&#26412;&#24605;&#24819;&#21644;&#26041;&#27861;&#30340;&#31934;&#36873;&#25351;&#21335;&#65292;&#23558;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20171;&#32461;&#32473;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#21463;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. The present article aims to introduce this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#38544;&#24335;&#26354;&#38754;&#21160;&#24577;&#21464;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23558;&#20854;&#25193;&#23637;&#21040;&#26102;&#31354;&#32500;&#24230;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#25311;&#21512;&#21644;&#27700;&#24179;&#38598;&#26041;&#31243;&#30340;&#32422;&#26463;&#65292;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#24182;&#36924;&#36817;&#24213;&#23618;&#20960;&#20309;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2201.09636</link><description>&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Surface Evolution. (arXiv:2201.09636v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#38544;&#24335;&#26354;&#38754;&#21160;&#24577;&#21464;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23558;&#20854;&#25193;&#23637;&#21040;&#26102;&#31354;&#32500;&#24230;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#25311;&#21512;&#21644;&#27700;&#24179;&#38598;&#26041;&#31243;&#30340;&#32422;&#26463;&#65292;&#32593;&#32476;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#24182;&#36924;&#36817;&#24213;&#23618;&#20960;&#20309;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#38544;&#24335;&#26354;&#38754;&#22312;&#27700;&#24179;&#38598;&#26041;&#31243;&#19979;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#23427;&#23558;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#30340;&#34920;&#31034;&#25193;&#23637;&#21040;&#26102;&#31354; $\mathbb{R}^3\times \mathbb{R}$&#65292;&#20174;&#32780;&#20026;&#36830;&#32493;&#20960;&#20309;&#21464;&#25442;&#25552;&#20379;&#20102;&#26426;&#21046;&#12290;&#20363;&#23376;&#21253;&#25324;&#23558;&#21021;&#22987;&#26354;&#38754;&#28436;&#21270;&#20026;&#19968;&#33324;&#30340;&#21521;&#37327;&#22330;&#65292;&#20351;&#29992;&#24179;&#22343;&#26354;&#29575;&#26041;&#31243;&#36827;&#34892;&#24179;&#28369;&#21644;&#23574;&#38160;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#25554;&#20540;&#12290;&#32593;&#32476;&#35757;&#32451;&#32771;&#34385;&#20004;&#20010;&#32422;&#26463;&#12290;&#25968;&#25454;&#39033;&#36127;&#36131;&#23558;&#21021;&#22987;&#26465;&#20214;&#36866;&#37197;&#21040;&#30456;&#24212;&#30340;&#26102;&#38388;&#28857;&#65292;&#36890;&#24120;&#20026; $\mathbb{R}^3 \times \{0\}$&#12290;&#28982;&#21518;&#65292;&#27700;&#24179;&#38598;&#26041;&#31243;&#39033;&#36843;&#20351;&#32593;&#32476;&#36924;&#36817;&#27700;&#24179;&#38598;&#26041;&#31243;&#32473;&#20986;&#30340;&#24213;&#23618;&#20960;&#20309;&#28436;&#21270;&#65292;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#12290;&#32593;&#32476;&#36824;&#21487;&#20197;&#22522;&#20110;&#20808;&#21069;&#35757;&#32451;&#36807;&#30340;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the use of smooth neural networks for modeling dynamic variations of implicit surfaces under the level set equation (LSE). For this, it extends the representation of neural implicit surfaces to the space-time $\mathbb{R}^3\times \mathbb{R}$, which opens up mechanisms for continuous geometric transformations. Examples include evolving an initial surface towards general vector fields, smoothing and sharpening using the mean curvature equation, and interpolations of initial conditions.  The network training considers two constraints. A data term is responsible for fitting the initial condition to the corresponding time instant, usually $\mathbb{R}^3 \times \{0\}$. Then, a LSE term forces the network to approximate the underlying geometric evolution given by the LSE, without any supervision. The network can also be initialized based on previously trained initial conditions, resulting in faster convergence compared to the standard approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#31354;&#38388;&#35843;&#25972;&#26469;&#21457;&#29616;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#36716;&#21270;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#37325;&#35201;&#23646;&#24615;&#24182;&#35299;&#26512;&#20854;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2111.08794</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#31354;&#38388;&#35843;&#25972;&#30740;&#31350;&#20174;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#21040;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Investigating Conversion from Mild Cognitive Impairment to Alzheimer's Disease using Latent Space Manipulation. (arXiv:2111.08794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#31354;&#38388;&#35843;&#25972;&#26469;&#21457;&#29616;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#36716;&#21270;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#37325;&#35201;&#23646;&#24615;&#24182;&#35299;&#26512;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#20840;&#29699;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#30340;&#26368;&#24120;&#35265;&#30340;&#30196;&#21574;&#30149;&#21407;&#22240;&#12290;&#30740;&#31350;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#28508;&#22312;&#21407;&#22240;&#21644;&#39118;&#38505;&#22240;&#32032;&#23545;&#20110;&#39044;&#38450;&#20854;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#34987;&#35748;&#20026;&#26159;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20043;&#21069;&#30340;&#19968;&#20010;&#20013;&#38388;&#38454;&#27573;&#12290;&#26089;&#26399;&#39044;&#27979;&#20174;MCI&#36716;&#21270;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#23545;&#20110;&#37319;&#21462;&#24517;&#35201;&#30340;&#39044;&#38450;&#25514;&#26045;&#20197;&#20943;&#32531;&#30149;&#24773;&#36827;&#23637;&#21644;&#24320;&#21457;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;&#20316;&#20026;MCI&#36716;&#21270;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26631;&#24535;&#24615;&#21464;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#25805;&#32437;MCI&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#35757;&#32451;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#65292;&#33719;&#21462;&#36825;&#20123;&#37325;&#35201;&#23646;&#24615;&#24182;&#35299;&#35835;&#23427;&#20204;&#23548;&#33268;MCI&#36716;&#21270;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#29983;&#25104;&#35299;&#30721;&#22120;&#21644;&#23548;&#33268;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#29983;&#25104;...
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease is the most common cause of dementia that affects millions of lives worldwide. Investigating the underlying causes and risk factors of Alzheimer's disease is essential to prevent its progression. Mild Cognitive Impairment (MCI) is considered an intermediate stage before Alzheimer's disease. Early prediction of the conversion from the MCI to Alzheimer's is crucial to take necessary precautions for decelerating the progression and developing suitable treatments. In this study, we propose a deep learning framework to discover the variables which are identifiers of the conversion from MCI to Alzheimer's disease. In particular, the latent space of a variational auto-encoder network trained with the MCI and Alzheimer's patients is manipulated to obtain the significant attributes and decipher their behavior that leads to the conversion from MCI to Alzheimer's disease. By utilizing a generative decoder and the dimensions that lead to the Alzheimer's diagnosis, we generate s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#20248;&#24863;&#30693;&#30697;&#38453;&#23545;&#24191;&#20041;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#37325;&#26500;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24863;&#30693;&#30697;&#38453;&#30340;&#39057;&#35889;&#23574;&#38160;&#24230;&#23545;&#24674;&#22797;&#24615;&#33021;&#24456;&#37325;&#35201;&#65292;&#24182;&#19988;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;$f$&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24863;&#30693;&#30697;&#38453;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.03237</link><description>&lt;p&gt;
&#38754;&#21521;&#24191;&#20041;&#32447;&#24615;&#36870;&#38382;&#39064;&#35774;&#35745;&#26368;&#20248;&#24863;&#30693;&#30697;&#38453;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Designing Optimal Sensing Matrices for Generalized Linear Inverse Problems. (arXiv:2111.03237v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#26368;&#20248;&#24863;&#30693;&#30697;&#38453;&#23545;&#24191;&#20041;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#37325;&#26500;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24863;&#30693;&#30697;&#38453;&#30340;&#39057;&#35889;&#23574;&#38160;&#24230;&#23545;&#24674;&#22797;&#24615;&#33021;&#24456;&#37325;&#35201;&#65292;&#24182;&#19988;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;$f$&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24863;&#30693;&#30697;&#38453;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#36870;&#38382;&#39064;$\mathbf{y}= f(\mathbf{Ax})$&#65292;&#20854;&#20013;$\mathbf{x}\in\mathbb{R}^n$&#26159;&#24863;&#20852;&#36259;&#20449;&#21495;&#65292;$\mathbf{A}$&#26159;&#24863;&#30693;&#30697;&#38453;&#65292;$f$&#26159;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;$\mathbf{y} \in \mathbb{R}^m$&#26159;&#27979;&#37327;&#21521;&#37327;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#33258;&#30001;&#35774;&#35745;&#24863;&#30693;&#30697;&#38453;$\mathbf{A}$&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;$\mathbf{A}$&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#37325;&#26500;&#24615;&#33021;&#12290;&#20316;&#20026;&#20248;&#21270;&#35774;&#35745;&#30340;&#31532;&#19968;&#27493;&#65292;&#20102;&#35299;&#24863;&#30693;&#30697;&#38453;&#23545;&#20174;$\mathbf{y}$&#24674;&#22797;$\mathbf{x}$&#30340;&#38590;&#24230;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#19968;&#31181;&#26368;&#25104;&#21151;&#30340;&#24674;&#22797;&#26041;&#27861;&#65292;&#21363;&#26399;&#26395;&#20256;&#25773;(EP)&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#29992;&#20110;$\mathbf{A}$&#39057;&#35889;&#30340;&#23574;&#38160;&#24230;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#27492;&#24230;&#37327;&#23545;EP&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#23574;&#38160;&#30340;&#39057;&#35889;&#21487;&#33021;&#20250;&#25439;&#23475;&#25110;&#25913;&#21892;&#24674;&#22797;&#24615;&#33021;&#65292;&#36825;&#21462;&#20915;&#20110;$f$&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24863;&#30693;&#30697;&#38453;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26368;&#22823;&#21270;&#20102;EP&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an inverse problem $\mathbf{y}= f(\mathbf{Ax})$, where $\mathbf{x}\in\mathbb{R}^n$ is the signal of interest, $\mathbf{A}$ is the sensing matrix, $f$ is a nonlinear function and $\mathbf{y} \in \mathbb{R}^m$ is the measurement vector. In many applications, we have some level of freedom to design the sensing matrix $\mathbf{A}$, and in such circumstances we could optimize $\mathbf{A}$ to achieve better reconstruction performance. As a first step towards optimal design, it is important to understand the impact of the sensing matrix on the difficulty of recovering $\mathbf{x}$ from $\mathbf{y}$.  In this paper, we study the performance of one of the most successful recovery methods, i.e., the expectation propagation (EP) algorithm. We define a notion of spikiness for the spectrum of $\bmmathbfA}$ and show the importance of this measure for the performance of EP. We show that whether a spikier spectrum can hurt or help the recovery performance depends on $f$. Based on our frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#65292;&#20197;&#35299;&#20915;&#24179;&#28369;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.15073</link><description>&lt;p&gt;
MMD&#32858;&#21512;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
MMD Aggregated Two-Sample Test. (arXiv:2110.15073v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#65292;&#20197;&#35299;&#20915;&#24179;&#28369;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26680;&#26816;&#39564;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#26680;&#65292;&#25105;&#20204;&#20351;&#29992;&#25490;&#21015;&#25110;&#37326;&#34542;&#33258;&#20030;&#65288;wild bootstrap&#65289;&#26500;&#36896;&#20102;&#19968;&#20010;MMD&#26816;&#39564;&#65292;&#36825;&#20004;&#31181;&#27969;&#34892;&#30340;&#25968;&#20540;&#31243;&#24207;&#21487;&#30830;&#23450;&#27979;&#35797;&#38408;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27979;&#35797;&#21487;&#20197;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#25511;&#21046;I&#22411;&#38169;&#35823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#23427;&#20173;&#28982;&#20445;&#25345;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#65292;&#36825;&#19982;&#20197;&#21069;&#30340;MMD&#27979;&#35797;&#19981;&#21516;&#65292;&#21069;&#32773;&#21482;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#20445;&#35777;&#27491;&#30830;&#30340;&#27979;&#35797;&#27700;&#24179;&#12290;&#24403;&#23494;&#24230;&#24046;&#24322;&#22312;Sobolev&#29699;&#20013;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MMD&#26816;&#39564;&#22312;&#29305;&#23450;&#30340;&#26680;&#20989;&#25968;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#35813;&#26680;&#20989;&#25968;&#20381;&#36182;&#20110;Sobolev&#29699;&#30340;&#24179;&#28369;&#21442;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20010;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#20351;&#29992;&#20855;&#26377;&#29305;&#23450;&#26680;&#30340;&#26368;&#20248;MMD&#26816;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#24179;&#22343;&#27979;&#35797;&#65292;&#31216;&#20026;MMDAgg&#12290;&#27979;&#35797;&#21151;&#29575;&#22312;Sobolev&#29699;&#30340;&#24179;&#28369;&#21442;&#25968;&#19978;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised ove
&lt;/p&gt;</description></item><item><title>CausalAF&#26159;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#20174;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2110.13939</link><description>&lt;p&gt;
CausalAF: &#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#22240;&#26524;&#33258;&#22238;&#24402;&#27969;
&lt;/p&gt;
&lt;p&gt;
CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13939
&lt;/p&gt;
&lt;p&gt;
CausalAF&#26159;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#20174;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20851;&#38190;&#23433;&#20840;&#22330;&#26223;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#20294;&#26159;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#21644;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#29575;&#21463;&#38480;&#20110;&#20851;&#38190;&#23433;&#20840;&#22330;&#26223;&#30340;&#31232;&#32570;&#24615;&#21644;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#20165;&#20174;&#35266;&#27979;&#25968;&#25454;&#20272;&#35745;&#20998;&#24067;&#30340;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#20851;&#31995;&#20316;&#20026;&#20808;&#39564;&#34701;&#20837;&#21040;&#22330;&#26223;&#29983;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;&#33258;&#22238;&#24402;&#27969;&#65288;CausalAF&#65289;&#12290;CausalAF&#36890;&#36807;&#26032;&#39062;&#30340;&#22240;&#26524;&#25513;&#34109;&#25805;&#20316;&#65292;&#40723;&#21169;&#29983;&#25104;&#27169;&#22411;&#25581;&#31034;&#21644;&#36981;&#24490;&#29983;&#25104;&#23545;&#35937;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#38543;&#26426;&#37319;&#26679;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#22914;&#20309;&#24341;&#36215;&#39118;&#38505;&#24773;&#20917;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;CausalAF&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating safety-critical scenarios, which are crucial yet difficult to collect, provides an effective way to evaluate the robustness of autonomous driving systems. However, the diversity of scenarios and efficiency of generation methods are heavily restricted by the rareness and structure of safety-critical scenarios. Therefore, existing generative models that only estimate distributions from observational data are not satisfying to solve this problem. In this paper, we integrate causality as a prior into the scenario generation and propose a flow-based generative framework, Causal Autoregressive Flow (CausalAF). CausalAF encourages the generative model to uncover and follow the causal relationship among generated objects via novel causal masking operations instead of searching the sample only from observational data. By learning the cause-and-effect mechanism of how the generated scenario causes risk situations rather than just learning correlations from data, CausalAF significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#19982;&#32852;&#21512;&#31354;&#38388;&#30340;Wasserstein&#36317;&#31163;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.04923</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization. (arXiv:2106.04923v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#19982;&#32852;&#21512;&#31354;&#38388;&#30340;Wasserstein&#36317;&#31163;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#39046;&#22495;&#20559;&#31227;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#32771;&#34385;&#36825;&#20123;&#39046;&#22495;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20363;&#22914;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#30340;&#19968;&#33268;&#34920;&#29616;&#27809;&#26377;&#24378;&#26377;&#21147;&#30340;&#20445;&#38556;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#22312;&#26576;&#20010;&#39046;&#22495;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#26159;&#20197;&#25439;&#23475;&#20854;&#20182;&#39046;&#22495;&#34920;&#29616;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#36890;&#36807;&#25552;&#20986;&#32463;&#20856;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#21644;&#32852;&#21512;&#31354;&#38388;&#65288;&#34920;&#31034;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#65289;&#20013;&#30340;Wasserstein&#36317;&#31163;&#20043;&#38388;&#30340;&#19968;&#32452;&#25968;&#23398;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20998;&#31867;&#25110;&#22238;&#24402;&#25439;&#22833;&#19982;GAN&#31867;&#22411;&#30340;&#39046;&#22495;&#21028;&#21035;&#22120;&#32467;&#21512;&#26102;&#24418;&#25104;&#20102;&#30495;&#23454;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#19981;&#21464;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shifts in the training data are common in practical applications of machine learning; they occur for instance when the data is coming from different sources. Ideally, a ML model should work well independently of these shifts, for example, by learning a domain-invariant representation. However, common ML losses do not give strong guarantees on how consistently the ML model performs for different domains, in particular, whether the model performs well on a domain at the expense of its performance on another domain. In this paper, we build new theoretical foundations for this problem, by contributing a set of mathematical relations between classical losses for supervised ML and the Wasserstein distance in joint space (i.e. representation and output space). We show that classification or regression losses, when combined with a GAN-type discriminator between domains, form an upper-bound to the true Wasserstein distance between domains. This implies a more invariant representation and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#21035;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426;&#29275;&#39039;&#27861;&#22312;&#26368;&#23567;&#21270;&#23545;&#25968;&#20984;&#20989;&#25968;&#20013;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#24418;&#25104;&#26356;&#26032;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36845;&#20195;&#24320;&#22987;&#26102;&#20943;&#24369;&#26087;&#35266;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2104.12949</link><description>&lt;p&gt;
&#21028;&#21035;&#36125;&#21494;&#26031;&#28388;&#27874;&#20026;&#38543;&#26426;&#29275;&#39039;&#27861;&#22312;&#26368;&#23567;&#21270;&#23545;&#25968;&#20984;&#20989;&#25968;&#20013;&#25552;&#20379;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Discriminative Bayesian filtering lends momentum to the stochastic Newton method for minimizing log-convex functions. (arXiv:2104.12949v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#21035;&#36125;&#21494;&#26031;&#28388;&#27874;&#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426;&#29275;&#39039;&#27861;&#22312;&#26368;&#23567;&#21270;&#23545;&#25968;&#20984;&#20989;&#25968;&#20013;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#24418;&#25104;&#26356;&#26032;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36845;&#20195;&#24320;&#22987;&#26102;&#20943;&#24369;&#26087;&#35266;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26368;&#23567;&#21270;&#19968;&#32452;&#23545;&#25968;&#20984;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#38543;&#26426;&#29275;&#39039;&#27861;&#36890;&#36807;&#23545;&#23436;&#25972;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#21644;&#28023;&#26862;&#30697;&#38453;&#36827;&#34892;&#23376;&#37319;&#26679;&#29256;&#26412;&#30340;&#36845;&#20195;&#26356;&#26032;&#20854;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#32622;&#20110;&#19968;&#31181;&#20855;&#26377;&#21028;&#21035;&#24615;&#35266;&#27979;&#36807;&#31243;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#32972;&#26223;&#20013;&#12290;&#24212;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#21487;&#20197;&#24471;&#21040;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#24418;&#25104;&#26356;&#26032;&#26102;&#32771;&#34385;&#20102;&#26799;&#24230;&#21644;&#28023;&#26862;&#30697;&#38453;&#30340;&#25972;&#20010;&#21382;&#21490;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#30697;&#38453;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#26087;&#35266;&#27979;&#30340;&#24433;&#21709;&#38543;&#26102;&#38388;&#20943;&#24369;&#65292;&#31867;&#20284;&#20110;Polyak&#30340;&#37325;&#29699;&#21160;&#21147;&#12290;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#22238;&#39038;&#20102;&#38543;&#26426;&#29275;&#39039;&#27861;&#30340;&#20854;&#20182;&#30456;&#20851;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
To minimize the average of a set of log-convex functions, the stochastic Newton method iteratively updates its estimate using subsampled versions of the full objective's gradient and Hessian. We contextualize this optimization problem as sequential Bayesian inference on a latent state-space model with a discriminatively-specified observation process. Applying Bayesian filtering then yields a novel optimization algorithm that considers the entire history of gradients and Hessians when forming an update. We establish matrix-based conditions under which the effect of older observations diminishes over time, in a manner analogous to Polyak's heavy ball momentum. We illustrate various aspects of our approach with an example and review other relevant innovations for the stochastic Newton method.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRaWl&#30340;&#22270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#28216;&#36208;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#23376;&#22270;&#26469;&#25552;&#21462;&#21644;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26816;&#27979;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#24182;&#35745;&#31639;&#38750;&#23616;&#37096;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CRaWl&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;Weisfeiler Leman&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#26550;&#26500;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2102.08786</link><description>&lt;p&gt;
&#36208;&#20986;Weisfeiler Leman&#23618;&#27425;&#32467;&#26500;&#65306;&#36229;&#36234;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing. (arXiv:2102.08786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.08786
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRaWl&#30340;&#22270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#28216;&#36208;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#23376;&#22270;&#26469;&#25552;&#21462;&#21644;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26816;&#27979;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#24182;&#35745;&#31639;&#38750;&#23616;&#37096;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CRaWl&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;Weisfeiler Leman&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#26550;&#26500;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;CRaWl&#12290;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#20284;&#65292;CRaWl&#23618;&#22312;&#22270;&#19978;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#65292;&#22240;&#27492;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;GNN&#23618;&#36827;&#34892;&#32452;&#21512;&#25110;&#20132;&#38169;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;CRaWl&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26412;&#36136;&#19978;&#26377;&#30528;&#19981;&#21516;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;CRaWl&#23618;&#21033;&#29992;&#19968;&#32500;&#21367;&#31215;&#20174;&#36890;&#36807;&#22270;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#20986;&#29616;&#30340;&#23376;&#22270;&#20013;&#25552;&#21462;&#21644;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26816;&#27979;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#24182;&#35745;&#31639;&#38750;&#23616;&#37096;&#29305;&#24449;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;CRaWl&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;Weisfeiler Leman&#31639;&#27861;&#20197;&#21450;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#26080;&#27861;&#27604;&#36739;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CRaWl&#33021;&#22815;&#34920;&#36798;&#30340;&#20989;&#25968;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#32467;&#26524;&#21487;&#20197;&#25193;&#23637;&#21040;Weisfeiler Leman&#23618;&#27425;&#30340;&#26356;&#39640;&#32423;&#21035;&#65292;&#20174;&#32780;&#20063;&#36866;&#29992;&#20110;&#39640;&#38454;GNN&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CRaWl&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#26550;&#26500;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CRaWl, a novel neural network architecture for graph learning. Like graph neural networks, CRaWl layers update node features on a graph and thus can freely be combined or interleaved with GNN layers. Yet CRaWl operates fundamentally different from message passing graph neural networks. CRaWl layers extract and aggregate information on subgraphs appearing along random walks through a graph using 1D Convolutions. Thereby it detects long range interactions and computes non-local features. As the theoretical basis for our approach, we prove a theorem stating that the expressiveness of CRaWl is incomparable with that of the Weisfeiler Leman algorithm and hence with graph neural networks. That is, there are functions expressible by CRaWl, but not by GNNs and vice versa. This result extends to higher levels of the Weisfeiler Leman hierarchy and thus to higher-order GNNs. Empirically, we show that CRaWl matches state-of-the-art GNN architectures across a multitude of benchmark datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#36890;&#36807;&#35757;&#32451;DRL&#20195;&#29702;&#26469;&#20132;&#26131;&#32929;&#31080;&#65292;&#24182;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2101.07107</link><description>&lt;p&gt;
&#39640;&#39057;&#20132;&#26131;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Active High Frequency Trading. (arXiv:2101.07107v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#36890;&#36807;&#35757;&#32451;DRL&#20195;&#29702;&#26469;&#20132;&#26131;&#32929;&#31080;&#65292;&#24182;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#36827;&#34892;&#27963;&#36291;&#30340;&#39640;&#39057;&#20132;&#26131;&#12290;&#25105;&#20204;&#35757;&#32451;DRL&#20195;&#29702;&#20351;&#29992;Proximal Policy Optimization&#31639;&#27861;&#26469;&#20132;&#26131;&#19968;&#21333;&#20301;&#30340;&#33521;&#29305;&#23572;&#20844;&#21496;&#32929;&#31080;&#12290;&#35757;&#32451;&#26159;&#22312;&#36830;&#32493;&#19977;&#20010;&#26376;&#30340;&#39640;&#39057;&#38480;&#20215;&#22996;&#25176;&#31807;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#26368;&#21518;&#19968;&#20010;&#26376;&#26159;&#39564;&#35777;&#25968;&#25454;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#22122;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20215;&#26684;&#21464;&#21160;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#32452;&#25104;&#21518;&#32773;&#12290;&#28982;&#21518;&#22312;&#25509;&#19979;&#26469;&#30340;&#19968;&#20010;&#26376;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#20351;&#29992;&#39034;&#24207;&#27169;&#22411;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29366;&#24577;&#29305;&#24449;&#21270;&#26041;&#24335;&#65292;&#23427;&#20204;&#22312;&#22522;&#20110;LOB&#30340;&#20803;&#29305;&#24449;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35748;&#20026;&#20195;&#29702;&#33021;&#22815;&#21019;&#24314;&#23545;&#24213;&#23618;&#29615;&#22659;&#30340;&#21160;&#24577;&#34920;&#31034;&#12290;&#23427;&#20204;&#33021;&#22815;&#35782;&#21035;&#20598;&#23572;&#20986;&#29616;&#30340;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading in the stock market. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2012.14331</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#21644;&#20998;&#31867;&#27491;&#24577;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23545;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#30340;&#20219;&#24847;&#22495;&#20869;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#30456;&#20851;&#27010;&#29575;&#23494;&#24230;&#21644;&#32479;&#35745;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23545;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#21644;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#27491;&#24577;&#27010;&#29575;&#20998;&#24067;&#22312;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#38656;&#35201;&#22312;&#29305;&#23450;&#21306;&#22495;&#20869;&#23545;&#36825;&#20123;&#20998;&#24067;&#36827;&#34892;&#31215;&#20998;&#65292;&#36825;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#36890;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12289;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#25110;&#36719;&#20214;&#26469;&#35745;&#31639;&#36825;&#20123;&#31215;&#20998;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25968;&#23398;&#32467;&#26524;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#20197;&#19979;&#20869;&#23481;&#65306;&#65288;i&#65289;&#20219;&#24847;&#21442;&#25968;&#32500;&#24230;&#19979;&#20219;&#24847;&#22495;&#20869;&#27861;&#21521;&#30340;&#27010;&#29575;&#65292;&#65288;ii&#65289;&#27861;&#21521;&#21521;&#37327;&#20989;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#12289;&#32047;&#31215;&#20998;&#24067;&#21644;&#36870;&#32047;&#31215;&#20998;&#24067;&#65292;&#65288;iii&#65289;&#20219;&#24847;&#25968;&#37327;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#31867;&#35823;&#24046;&#12289;&#36125;&#21494;&#26031;&#26368;&#20248;&#36776;&#21035;&#25351;&#25968;&#20197;&#21450;&#20854;&#19982;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#65288;iv&#65289;&#27492;&#31867;&#38382;&#39064;&#30340;&#32500;&#24230;&#38477;&#20302;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#65288;v&#65289;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#29983;&#29289;&#21644;&#24515;&#29702;&#23398;&#26469;&#28436;&#31034;&#36825;&#20123;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#23402;&#29983;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#36827;&#34892;&#20302;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.04841</link><description>&lt;p&gt;
&#19968;&#31080;&#21542;&#20915;&#26435;&#65306;&#29992;&#20110;&#20302;&#26679;&#26412;&#38738;&#20809;&#30524;&#35786;&#26029;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis. (arXiv:2012.04841v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.04841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#23402;&#29983;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#36827;&#34892;&#20302;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#38738;&#20809;&#30524;&#65292;&#36825;&#20123;&#22270;&#20687;&#36890;&#24120;&#22312;&#30524;&#31185;&#26816;&#26597;&#20013;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;CNN&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#33391;&#22909;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#22312;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36275;&#22815;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#30142;&#30149;&#25110;&#19987;&#23478;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23427;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#23402;&#29983;&#32593;&#32476;&#65292;&#24341;&#20837;&#19968;&#31181;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#19988;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65307;&#65288;2&#65289;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#39069;&#22806;&#30340;&#26410;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#20219;&#21153;&#23402;&#29983;&#32593;&#32476;&#65288;MTSN&#65289;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#39592;&#24178;CNN&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#39592;&#24178;CNN&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20854;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#32463;&#36807;&#35757;&#32451;&#30340;&#39592;&#24178;CNN&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are a promising technique for automated glaucoma diagnosis from images of the fundus, and these images are routinely acquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a large amount of well-labeled data for training, which may not be available in many biomedical image classification applications, especially when diseases are rare and where labeling by experts is costly. This article makes two contributions to address this issue: (1) It extends the conventional Siamese network and introduces a training method for low-shot learning when labeled data are limited and imbalanced, and (2) it introduces a novel semi-supervised learning strategy that uses additional unlabeled training data to achieve greater accuracy. Our proposed multi-task Siamese network (MTSN) can employ any backbone CNN, and we demonstrate with four backbone CNNs that its accuracy with limited training data approaches the accuracy of backbone CNNs trained wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#32593;&#32476;&#21644;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38544;&#21547;&#20559;&#22909;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2011.13772</link><description>&lt;p&gt;
&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#30340;&#26799;&#24230;&#19979;&#38477;&#65306;&#21160;&#21147;&#23398;&#21644;&#23545;&#20302;&#31209;&#30340;&#38544;&#21547;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank. (arXiv:2011.13772v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.13772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#32593;&#32476;&#21644;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38544;&#21547;&#20559;&#22909;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#27604;&#35757;&#32451;&#25968;&#25454;&#28857;&#26356;&#22810;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#26377;&#22810;&#20010;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#38646;&#35757;&#32451;&#35823;&#24046;&#65292;&#22240;&#27492;&#35757;&#32451;&#31639;&#27861;&#23545;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#38544;&#21547;&#20559;&#22909;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#24448;&#24448;&#26356;&#21916;&#27426;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#32447;&#24615;&#32593;&#32476;&#21644;&#20272;&#35745;&#38382;&#39064;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#25105;&#20204;&#19981;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#20173;&#28982;&#25581;&#31034;&#20102;&#38544;&#21547;&#20559;&#22909;&#30340;&#29616;&#35937;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#23545;&#39321;&#33609;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#25910;&#25947;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#34920;&#24449;&#20102;&#39057;&#35889;&#30340;&#21160;&#24577;&#25910;&#25947;&#12290;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#30830;&#23450;&#36845;&#20195;&#30340;&#26377;&#25928;&#31209;&#25509;&#36817;&#20110;&#20302;&#31209;&#25237;&#24433;&#30340;&#26377;&#25928;&#31209;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, it is common to use more network parameters than training points. In such scenarioof over-parameterization, there are usually multiple networks that achieve zero training error so that thetraining algorithm induces an implicit bias on the computed solution. In practice, (stochastic) gradientdescent tends to prefer solutions which generalize well, which provides a possible explanation of thesuccess of deep learning. In this paper we analyze the dynamics of gradient descent in the simplifiedsetting of linear networks and of an estimation problem. Although we are not in an overparameterizedscenario, our analysis nevertheless provides insights into the phenomenon of implicit bias. In fact, wederive a rigorous analysis of the dynamics of vanilla gradient descent, and characterize the dynamicalconvergence of the spectrum. We are able to accurately locate time intervals where the effective rankof the iterates is close to the effective rank of a low-rank projection of the gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#30340;&#21040;&#36798;&#26159;&#36830;&#32493;&#30340;&#65292;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#30340;&#25928;&#29992;&#20540;&#21644;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#30340;&#24211;&#23384;&#37327;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21021;&#22987;&#24211;&#23384;&#37327;&#19982;&#39038;&#23458;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#38543;&#30528;&#39038;&#23458;&#25968;&#30340;&#22686;&#21152;&#65292;&#39044;&#26399;&#30340;&#36951;&#25022;&#23558;&#20197;&#23545;&#25968;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/1912.08917</link><description>&lt;p&gt;
&#22312;&#22810;&#31192;&#20070;&#38382;&#39064;&#21644;&#36830;&#32493;&#20272;&#20540;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations. (arXiv:1912.08917v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#30340;&#21040;&#36798;&#26159;&#36830;&#32493;&#30340;&#65292;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#30340;&#25928;&#29992;&#20540;&#21644;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#30340;&#24211;&#23384;&#37327;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21021;&#22987;&#24211;&#23384;&#37327;&#19982;&#39038;&#23458;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#38543;&#30528;&#39038;&#23458;&#25968;&#30340;&#22686;&#21152;&#65292;&#39044;&#26399;&#30340;&#36951;&#25022;&#23558;&#20197;&#23545;&#25968;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;$n$&#20010;&#39038;&#23458;&#22312;$n$&#20010;&#26102;&#26399;&#20869;&#39034;&#24207;&#21040;&#36798;&#65292;&#24744;&#24517;&#39035;&#21160;&#24577;&#20915;&#23450;&#35201;&#28385;&#36275;&#21738;&#20010;&#39038;&#23458;&#12290;&#28385;&#36275;&#31532;$t$&#20010;&#26102;&#26399;&#30340;&#39038;&#23458;&#21487;&#20197;&#33719;&#24471;&#25928;&#29992;$u_{t}\in \mathbb{R}_{+}$&#65292;&#24182;&#20943;&#23569;&#24744;&#30340;&#24211;&#23384;&#37327;$A_{t}\in \mathbb{R}_{+}^{M}$&#12290;&#39038;&#23458;&#21521;&#37327;$(u_{t}, A_{t}')'$&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20854;&#20013;$u_{t}$&#26159;&#20174;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#65292;$A_{t}$&#26159;&#20174;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#30740;&#31350;&#20102;&#35813;&#31995;&#32479;&#30340;&#36951;&#25022;&#65292;&#21363;&#22914;&#26524;&#24744;&#19981;&#38656;&#35201;&#21363;&#26102;&#20915;&#31574;&#65292;&#24744;&#21487;&#20197;&#33719;&#24471;&#30340;&#39069;&#22806;&#25928;&#29992;&#12290;&#25105;&#23637;&#31034;&#20102;&#22914;&#26524;&#24744;&#30340;&#21021;&#22987;&#24211;&#23384;&#36164;&#20135;&#19982;$n$&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#37027;&#20040;&#24403;$n \rightarrow \infty$&#26102;&#65292;&#24744;&#30340;&#39044;&#26399;&#36951;&#25022;&#26159;$ \Theta(\log(n)) $&#12290;&#25105;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$ \Theta(\log(n)) $&#30340;&#36951;&#25022;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#23558;&#36825;&#20010;&#32467;&#26524;&#25193;&#23637;&#21040;Arlotto&#21644;Gurich&#65288;2019&#65289;&#30340;&#22810;&#31192;&#20070;&#38382;&#39064;&#65292;&#20854;&#20013;&#31192;&#20070;&#20272;&#20540;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
I study a general revenue management problem in which $ n $ customers arrive sequentially over $ n $ periods, and you must dynamically decide which to satisfy. Satisfying the period-$ t $ customer yields utility $ u_{t} \in \mathbb{R}_{+} $ and decreases your inventory holdings by $ A_{t} \in \mathbb{R}_{+}^{M} $. The customer vectors, $ (u_{t}, A_{t}')' $, are i.i.d., with $ u_{t} $ drawn from a finite-mean continuous distribution and $ A_{t} $ drawn from a bounded discrete or continuous distribution. I study this system's regret, which is the additional utility you could get if you didn't have to make decisions on the fly. I show that if your initial inventory endowment scales linearly with $ n $ then your expected regret is $ \Theta(\log(n)) $ as $ n \rightarrow \infty $. I provide a simple policy that achieves this $ \Theta(\log(n)) $ regret rate. Finally, I extend this result to Arlotto and Gurich's (2019) multisecretary problem with uniformly distributed secretary valuations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/1710.05468</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#22238;&#24212;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#38750;&#34394;&#31354;&#27867;&#21270;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#29702;&#35770;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
&lt;/p&gt;</description></item></channel></rss>