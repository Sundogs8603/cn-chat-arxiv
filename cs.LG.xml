<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#65292;&#20351;&#20854;&#38646;-shot&#34920;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24182;&#21457;&#24067;&#20102;Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.02643</link><description>&lt;p&gt;
&#23436;&#25104;&#20998;&#21106;&#20219;&#21153;&#30340;&#19975;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segment Anything. (arXiv:2304.02643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#65292;&#20351;&#20854;&#38646;-shot&#34920;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24182;&#21457;&#24067;&#20102;Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;Segment Anything&#65288;SA&#65289;&#8221;&#39033;&#30446;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#25910;&#38598;&#24490;&#29615;&#20013;&#26500;&#24314;&#20102;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;1100&#19975;&#20010;&#35768;&#21487;&#21644;&#38544;&#31169;&#23562;&#37325;&#30340;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#21547;10&#20159;&#20010;&#25513;&#33180;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#21709;&#24212;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#12290;&#25105;&#20204;&#23545;&#20854;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#38646;-shot &#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051; -- &#36890;&#24120;&#26159;&#31454;&#20105;&#23545;&#25163;&#25110;&#29978;&#33267;&#36229;&#36807;&#20043;&#21069;&#30340;&#23436;&#20840;&#30417;&#30563;&#32467;&#26524;&#12290;&#25105;&#20204;&#27491;&#22312; https://segment-anything.com &#19978;&#21457;&#24067; Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#20998;&#31867;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#21644;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#65292;&#20998;&#26512;&#21518;&#21457;&#29616;&#20854;&#19982;&#20869;&#26680;&#23725;&#22238;&#24402;&#33258;&#25105;&#33976;&#39311;&#21644;&#26222;&#36890;GPR&#23545;&#24212;&#12290;&#20854;&#20013;GPC&#30340;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#36817;&#20284;&#23545;&#24212;&#20110;&#25968;&#25454;&#22797;&#21046;&#21644;&#21327;&#26041;&#24046;&#30340;&#29305;&#23450;&#32553;&#25918;&#12290;</title><link>http://arxiv.org/abs/2304.02641</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#33258;&#25105;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation for Gaussian Process Regression and Classification. (arXiv:2304.02641v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21644;&#20998;&#31867;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#21644;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#65292;&#20998;&#26512;&#21518;&#21457;&#29616;&#20854;&#19982;&#20869;&#26680;&#23725;&#22238;&#24402;&#33258;&#25105;&#33976;&#39311;&#21644;&#26222;&#36890;GPR&#23545;&#24212;&#12290;&#20854;&#20013;GPC&#30340;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#36817;&#20284;&#23545;&#24212;&#20110;&#25968;&#25454;&#22797;&#21046;&#21644;&#21327;&#26041;&#24046;&#30340;&#29305;&#23450;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;&#30693;&#35782;&#33976;&#39311;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#21644;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#65288;GPC&#65289;&#20013;&#65307;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#21644;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#26368;&#20687;&#30446;&#21069;&#26426;&#22120;&#23398;&#20064;&#30340;&#22823;&#22810;&#25968;&#33976;&#39311;&#25216;&#26415;&#65292;&#23427;&#22312;&#25945;&#24072;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#19978;&#37325;&#26032;&#36866;&#21512;&#19968;&#20010;&#27169;&#22411;&#65292;&#32780;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#21017;&#37325;&#26032;&#20351;&#29992;&#23436;&#25972;&#27010;&#29575;&#21518;&#39564;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#34920;&#26126;GPR&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#20869;&#26680;&#23725;&#22238;&#24402;&#33258;&#25105;&#33976;&#39311;&#32467;&#26524;&#23494;&#20999;&#30456;&#20851;&#65292;&#32780;GPR&#30340;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#19982;&#20855;&#26377;&#29305;&#23450;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#26222;&#36890;GPR&#30456;&#23545;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;GPC&#30340;&#20998;&#24067;&#20013;&#24515;&#26041;&#27861;&#36817;&#20284;&#23545;&#24212;&#20110;&#25968;&#25454;&#22797;&#21046;&#21644;&#21327;&#26041;&#24046;&#30340;&#29305;&#23450;&#32553;&#25918;&#65292;&#32780;GPC&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two approaches to extend the notion of knowledge distillation to Gaussian Process Regression (GPR) and Gaussian Process Classification (GPC); data-centric and distribution-centric. The data-centric approach resembles most current distillation techniques for machine learning, and refits a model on deterministic predictions from the teacher, while the distribution-centric approach, re-uses the full probabilistic posterior for the next iteration. By analyzing the properties of these approaches, we show that the data-centric approach for GPR closely relates to known results for self-distillation of kernel ridge regression and that the distribution-centric approach for GPR corresponds to ordinary GPR with a very particular choice of hyperparameters. Furthermore, we demonstrate that the distribution-centric approach for GPC approximately corresponds to data duplication and a particular scaling of the covariance and that the data-centric approach for GPC requires redefining the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65306;GenPhys&#65292;&#23427;&#23558;&#20174;&#29289;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.02637</link><description>&lt;p&gt;
GenPhys&#65306;&#20174;&#29289;&#29702;&#36807;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GenPhys: From Physical Processes to Generative Models. (arXiv:2304.02637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65306;GenPhys&#65292;&#23427;&#23558;&#20174;&#29289;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;(DM)&#21644;&#26368;&#36817;&#30340;&#27850;&#26494;&#27969;&#29983;&#25104;&#27169;&#22411;(PFGM)&#37117;&#21463;&#21040;&#29289;&#29702;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#38382;&#19968;&#19979;&#65306;&#29289;&#29702;&#36807;&#31243;&#33021;&#21542;&#25552;&#20379;&#26356;&#22810;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23478;&#26063;&#65292;&#21363;&#20174;&#29289;&#29702;&#36807;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;(GenPhys)&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#25551;&#36848;&#29289;&#29702;&#36807;&#31243;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;s-&#29983;&#25104;&#30340;PDEs (s&#20195;&#34920;&#24179;&#28369;)&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#12290;GenPhys&#21253;&#21547;&#20102;&#20004;&#20010;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;(DM&#21644;PFGM)&#65292;&#24182;&#29978;&#33267;&#24341;&#20986;&#20102;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#20363;&#22914;&#65292;&#21463;&#24369;&#30456;&#20114;&#20316;&#29992;&#21551;&#21457;&#30340;&#8220;Yukawa&#29983;&#25104;&#27169;&#22411;&#8221;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26576;&#20123;&#29289;&#29702;&#36807;&#31243;&#40664;&#35748;&#19981;&#23646;&#20110;GenPhys&#23478;&#26063;&#65292;&#20363;&#22914;&#27874;&#21160;&#26041;&#31243;&#21644;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#20462;&#25913;&#21152;&#20837;&#21040;GenPhys&#23478;&#26063;&#20013;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;GenPhys&#26469;&#25506;&#32034;&#21644;&#25193;&#23637;&#29983;&#25104;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since diffusion models (DM) and the more recent Poisson flow generative models (PFGM) are inspired by physical processes, it is reasonable to ask: Can physical processes offer additional new generative models? We show that the answer is yes. We introduce a general family, Generative Models from Physical Processes (GenPhys), where we translate partial differential equations (PDEs) describing physical processes to generative models. We show that generative models can be constructed from s-generative PDEs (s for smooth). GenPhys subsume the two existing generative models (DM and PFGM) and even give rise to new families of generative models, e.g., "Yukawa Generative Models" inspired from weak interactions. On the other hand, some physical processes by default do not belong to the GenPhys family, e.g., the wave equation and the Schr\"{o}dinger equation, but could be made into the GenPhys family with some modifications. Our goal with GenPhys is to explore and expand the design space of gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#22522;&#26412;&#27493;&#39588;&#65292;&#21363;&#21033;&#29992;Landsat&#24433;&#20687;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;FIA&#25968;&#25454;&#65292;&#22312;&#25972;&#20010;&#32445;&#32422;&#24030;&#65288;1990&#24180;&#33267;2019&#24180;&#65289;&#20197;&#36739;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#21046;&#20316;&#21382;&#21490;&#26862;&#26519;&#29983;&#29289;&#37327;&#22320;&#22270;&#65292;&#20197;&#20415;&#23454;&#29616;&#20010;&#20307;&#21644;&#26223;&#35266;&#23610;&#24230;&#30340;&#24211;&#23384;&#21464;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.02632</link><description>&lt;p&gt;
&#21382;&#21490;&#26862;&#26519;&#29983;&#29289;&#37327;&#30340;&#22320;&#22270;&#21046;&#20316;&#65292;&#29992;&#20110;&#31934;&#30830;&#35780;&#20272;&#20010;&#20307;&#21644;&#26223;&#35266;&#23610;&#24230;&#30340;&#24211;&#23384;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mapping historical forest biomass for stock-change assessments at parcel to landscape scales. (arXiv:2304.02632v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#22522;&#26412;&#27493;&#39588;&#65292;&#21363;&#21033;&#29992;Landsat&#24433;&#20687;&#12289;&#28608;&#20809;&#38647;&#36798;&#21644;FIA&#25968;&#25454;&#65292;&#22312;&#25972;&#20010;&#32445;&#32422;&#24030;&#65288;1990&#24180;&#33267;2019&#24180;&#65289;&#20197;&#36739;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#21046;&#20316;&#21382;&#21490;&#26862;&#26519;&#29983;&#29289;&#37327;&#22320;&#22270;&#65292;&#20197;&#20415;&#23454;&#29616;&#20010;&#20307;&#21644;&#26223;&#35266;&#23610;&#24230;&#30340;&#24211;&#23384;&#21464;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21382;&#21490;&#26862;&#26519;&#21160;&#24577;&#65292;&#29305;&#21035;&#26159;&#26862;&#26519;&#29983;&#29289;&#37327;&#21644;&#30899;&#20648;&#37327;&#30340;&#21464;&#21270;&#65292;&#24050;&#25104;&#20026;&#35780;&#20272;&#30446;&#21069;&#26862;&#26519;&#27668;&#20505;&#25928;&#30410;&#24182;&#22312;&#21508;&#31181;&#25919;&#31574;&#12289;&#30417;&#31649;&#21644;&#31649;&#29702;&#26041;&#26696;&#19979;&#39044;&#27979;&#26410;&#26469;&#25928;&#30410;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#21033;&#29992;Landsat&#24433;&#20687;&#12289;&#32654;&#22269;&#26862;&#26519;&#23616;&#26862;&#26519;&#24211;&#23384;&#19982;&#20998;&#26512;&#65288;FIA&#65289;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#65292;&#20197;&#24180;&#24230;30&#31859;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#65288;1990&#24180;&#33267;2019&#24180;&#65292;&#21253;&#25324;&#32445;&#32422;&#24030;&#30340;&#25152;&#26377;&#22320;&#21306;&#65289;&#65292;&#20351;&#29992;&#20813;&#36153;&#30340;&#25968;&#25454;&#21644;&#24320;&#28304;&#24037;&#20855;&#65292;&#25551;&#36848;&#20102;&#24314;&#31435;&#22522;&#20110;&#22320;&#22270;&#30340;&#24211;&#23384;&#21464;&#21270;&#26694;&#26550;&#30340;&#22522;&#26412;&#27493;&#39588;&#65306;&#21046;&#20316;&#21382;&#21490;&#26862;&#26519;&#29983;&#29289;&#37327;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding historical forest dynamics, specifically changes in forest biomass and carbon stocks, has become critical for assessing current forest climate benefits and projecting future benefits under various policy, regulatory, and stewardship scenarios. Carbon accounting frameworks based exclusively on national forest inventories are limited to broad-scale estimates, but model-based approaches that combine these inventories with remotely sensed data can yield contiguous fine-resolution maps of forest biomass and carbon stocks across landscapes over time. Here we describe a fundamental step in building a map-based stock-change framework: mapping historical forest biomass at fine temporal and spatial resolution (annual, 30m) across all of New York State (USA) from 1990 to 2019, using freely available data and open-source tools.  Using Landsat imagery, US Forest Service Forest Inventory and Analysis (FIA) data, and off-the-shelf LiDAR collections we developed three modeling approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02621</link><description>&lt;p&gt;
&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#32423;&#21035;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#20219;&#21153;&#22240;&#20026;&#20854;&#21487;&#20943;&#23569;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;WSSS&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#22312;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#19978;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#31867;&#21035;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20272;&#35745;&#23545;&#35937;&#20301;&#32622;&#65292;CAMs&#35782;&#21035;&#22270;&#20687;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;CAMs&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#24418;&#24335;&#21270;&#30340;&#20998;&#21106;&#25513;&#30721;&#30340;&#26041;&#24335;&#22312;&#32570;&#20047;&#20687;&#32032;&#32423;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#12290;&#22312;SEAM&#22522;&#32447;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25552;&#39640;CAM&#23398;&#20064;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#23427;&#26159;GAP&#30340;&#26367;&#20195;&#26041;&#27861;&#65307;&#65288;2&#65289;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21363;&#23545;&#35937;&#36718;&#24275;&#20960;&#20046;&#20165;&#19982;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#36793;&#32536;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#35299;&#37322;CAM&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23558;&#23616;&#37096;&#31354;&#38388;&#19968;&#33268;&#24615;&#32422;&#26463;&#34701;&#20837;CAM&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20132;&#26367;&#26356;&#26032;CAM&#21644;&#20998;&#21106;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of image-level weakly-supervised semantic segmentation (WSSS) has gained popularity in recent years, as it reduces the vast data annotation cost for training segmentation models. The typical approach for WSSS involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. In case of the SEAM baseline, a previous work proposed to improve CAM learning in two ways: (1) Importance sampling, which is a substitute for GAP, and (2) the feature similarity loss, which utilizes a heuristic that object contours almost exclusively align with color edges in images. In this work, we propose a different probabilistic interpretation of CAMs for thes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;&#65292;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#22312;&#37319;&#26679;&#39640;&#26031;&#20998;&#24067;&#20013;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2304.02599</link><description>&lt;p&gt;
&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Query lower bounds for log-concave sampling. (arXiv:2304.02599v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25968;&#20985;&#37319;&#26679;&#30340;&#26597;&#35810;&#19979;&#30028;&#65292;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#22312;&#37319;&#26679;&#39640;&#26031;&#20998;&#24067;&#20013;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#23545;&#25968;&#20985;&#37319;&#26679;&#22312;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#30456;&#24212;&#30340;&#35777;&#26126;&#27492;&#20219;&#21153;&#30340;&#19979;&#30028;&#30340;&#38382;&#39064;&#20173;&#28982;&#24456;&#38590;&#65292;&#20197;&#21069;&#21482;&#30693;&#36947;&#22312;&#19968;&#32500;&#20013;&#23384;&#22312;&#36739;&#23567;&#30340;&#19979;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;&#19979;&#26597;&#35810;&#19979;&#30028;&#65306;&#65288;1&#65289;&#22312;&#32500;&#24230; $d\ge 2$&#20013;&#20174;&#24378;&#23545;&#25968;&#20985;&#21644;&#23545;&#25968;&#20809;&#28369;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\Omega(\log \kappa)$ &#26597;&#35810;&#65292;&#36825;&#22312;&#20219;&#20309;&#22266;&#23450;&#32500;&#24230;&#19978;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#65288;2&#65289;&#20174;&#39640;&#26031;&#20998;&#24067;&#20013;&#37319;&#26679;&#38656;&#35201; $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ &#26597;&#35810;&#65288;&#22240;&#27492;&#20063;&#36866;&#29992;&#20110;&#22312;&#32500;&#25968; $d$ &#20013;&#37319;&#26679;&#19968;&#33324;&#30340;&#23545;&#25968;&#20985;&#21644;&#20809;&#28369;&#20998;&#24067;&#65289;&#65292;&#36825;&#23545;&#20110;&#39640;&#26031;&#31867;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#37324; $\kappa$ &#26159;&#30446;&#26631;&#20998;&#24067;&#30340;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#65288;1&#65289;&#19968;&#31181;&#22810;&#23610;&#24230;&#26500;&#36896;&#65292;&#21463;&#21040;&#20102;&#20851;&#20110;&#35856;&#25391;&#20998;&#26512;&#20013;&#30340;Kakeya&#29468;&#24819;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#32422;&#31616;&#65292;&#35777;&#26126;&#20102;&#22359;Krylov&#31639;&#27861;&#22312;&#27492;&#38382;&#39064;&#20013;&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log-concave sampling has witnessed remarkable algorithmic advances in recent years, but the corresponding problem of proving lower bounds for this task has remained elusive, with lower bounds previously known only in dimension one. In this work, we establish the following query lower bounds: (1) sampling from strongly log-concave and log-smooth distributions in dimension $d\ge 2$ requires $\Omega(\log \kappa)$ queries, which is sharp in any constant dimension, and (2) sampling from Gaussians in dimension $d$ (hence also from general log-concave and log-smooth distributions in dimension $d$) requires $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ queries, which is nearly sharp for the class of Gaussians. Here $\kappa$ denotes the condition number of the target distribution. Our proofs rely upon (1) a multiscale construction inspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel reduction that demonstrates that block Krylov algorithms are optimal for this probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#20083;&#31361;&#20999;&#38500;&#26415;&#30340;&#21147;&#20256;&#24863;&#22120;&#25163;&#26415;&#38075;&#65292;&#33021;&#22815;&#36890;&#36807;&#20934;&#30830;&#27979;&#37327;&#24037;&#20855;-&#32452;&#32455;&#30456;&#20114;&#20316;&#29992;&#21147;&#23454;&#29616;&#21147;&#25511;&#21046;&#21644;&#21453;&#39304;&#65292;&#20026;&#25163;&#26415;&#21307;&#29983;&#25552;&#20379;&#23454;&#26102;&#21147;&#21453;&#39304;&#65292;&#25552;&#39640;&#25163;&#26415;&#23433;&#20840;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.02583</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#20083;&#31361;&#20999;&#38500;&#26415;&#23454;&#26102;&#21147;&#21453;&#39304;&#30340;&#21147;&#20256;&#24863;&#22120;&#25163;&#26415;&#38075;
&lt;/p&gt;
&lt;p&gt;
A force-sensing surgical drill for real-time force feedback in robotic mastoidectomy. (arXiv:2304.02583v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#20083;&#31361;&#20999;&#38500;&#26415;&#30340;&#21147;&#20256;&#24863;&#22120;&#25163;&#26415;&#38075;&#65292;&#33021;&#22815;&#36890;&#36807;&#20934;&#30830;&#27979;&#37327;&#24037;&#20855;-&#32452;&#32455;&#30456;&#20114;&#20316;&#29992;&#21147;&#23454;&#29616;&#21147;&#25511;&#21046;&#21644;&#21453;&#39304;&#65292;&#20026;&#25163;&#26415;&#21307;&#29983;&#25552;&#20379;&#23454;&#26102;&#21147;&#21453;&#39304;&#65292;&#25552;&#39640;&#25163;&#26415;&#23433;&#20840;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#32819;&#31185;&#25163;&#26415;&#20013;&#20351;&#29992;&#26426;&#22120;&#20154;&#21487;&#20197;&#20943;&#36731;&#25163;&#26415;&#21307;&#29983;&#22312;&#20391;&#39045;&#24213;&#20851;&#38190;&#32467;&#26500;&#21608;&#22260;&#39592;&#32452;&#32455;&#28165;&#38500;&#36807;&#31243;&#20013;&#30340;&#20219;&#21153;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#22320;&#36827;&#20837;&#35299;&#21078;&#36890;&#36947;&#38656;&#35201;&#24320;&#21457;&#20808;&#36827;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20197;&#20027;&#21160;&#38480;&#21046;&#25163;&#26415;&#24037;&#20855;&#19982;&#37325;&#35201;&#35299;&#21078;&#32467;&#26500;&#20043;&#38388;&#30340;&#20132;&#20114;&#21147;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25163;&#26415;&#38075;&#20855;&#22791;&#30340;&#21147;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#27979;&#37327;&#20934;&#30830;&#30340;&#24037;&#20855;-&#32452;&#32455;&#30456;&#20114;&#20316;&#29992;&#21147;&#20197;&#23454;&#29616;&#21521;&#25163;&#26415;&#21307;&#29983;&#30340;&#21147;&#25511;&#21046;&#21644;&#21453;&#39304;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23433;&#35013;&#22312;&#21327;&#20316;&#25511;&#21046;&#25163;&#26415;&#26426;&#22120;&#20154;&#19978;&#30340;&#21147;&#20256;&#24863;&#22120;&#25163;&#26415;&#38075;&#30340;&#35774;&#35745;&#12289;&#26657;&#20934;&#21644;&#39564;&#35777;&#12290;&#32467;&#26524;&#65306;&#25163;&#26415;&#38075;&#23574;&#31471;&#30340;&#21147;&#27979;&#37327;&#32467;&#26524;&#36890;&#36807;&#29983;&#40481;&#34507;&#38075;&#23380;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#22312;&#35813;&#23454;&#39564;&#20013;&#65292;&#23433;&#35013;&#22312;&#34507;&#19979;&#26041;&#30340;&#21147;&#20256;&#24863;&#22120;&#20316;&#20026;&#22320;&#38754;&#30495;&#20540;&#12290;&#28857;&#21644;&#36335;&#24452;&#38075;&#23380;&#23454;&#39564;&#30340;&#24179;&#22343;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Purpose: Robotic assistance in otologic surgery can reduce the task load of operating surgeons during the removal of bone around the critical structures in the lateral skull base. However, safe deployment into the anatomical passageways necessitates the development of advanced sensing capabilities to actively limit the interaction forces between the surgical tools and critical anatomy.  Methods: We introduce a surgical drill equipped with a force sensor that is capable of measuring accurate tool-tissue interaction forces to enable force control and feedback to surgeons. The design, calibration and validation of the force-sensing surgical drill mounted on a cooperatively controlled surgical robot are described in this work.  Results: The force measurements on the tip of the surgical drill are validated with raw-egg drilling experiments, where a force sensor mounted below the egg serves as ground truth. The average root mean square error (RMSE) for points and path drilling experiments ar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26816;&#39564;&#20102;&#22312;&#24515;&#33039;&#30149;&#23398;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#23581;&#35797;&#21306;&#20998;&#19977;&#31181;&#29305;&#23450;&#30149;&#29702;&#12290;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.02577</link><description>&lt;p&gt;
ECG&#29305;&#24449;&#37325;&#35201;&#24615;&#25490;&#21517;&#65306;&#24515;&#33039;&#30149;&#19987;&#23478;&#19982;&#31639;&#27861;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
ECG Feature Importance Rankings: Cardiologists vs. Algorithms. (arXiv:2304.02577v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02577
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26816;&#39564;&#20102;&#22312;&#24515;&#33039;&#30149;&#23398;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#23581;&#35797;&#21306;&#20998;&#19977;&#31181;&#29305;&#23450;&#30149;&#29702;&#12290;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#37325;&#35201;&#24615;&#26041;&#27861;&#25215;&#35834;&#25552;&#20379;&#26681;&#25454;&#32473;&#23450;&#20998;&#31867;&#20219;&#21153;&#23545;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#24207;&#12290;&#23384;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#25490;&#24207;&#32463;&#24120;&#19981;&#19968;&#33268;&#65292;&#30001;&#20110;&#32570;&#20047;&#38500;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#22806;&#30340;&#22522;&#20934;&#30495;&#30456;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#24456;&#38590;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24515;&#33039;&#30149;&#23398;&#39046;&#22495;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#35797;&#22270;&#26681;&#25454;&#19982;&#24515;&#33039;&#30149;&#19987;&#23478;&#20915;&#31574;&#35268;&#21017;&#20013;&#20351;&#29992;&#30340;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#26469;&#21306;&#20998;&#20581;&#24247;&#21463;&#35797;&#32773;&#20013;&#30340;&#19977;&#31181;&#29305;&#23450;&#30149;&#29702;&#12290;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#21478;&#19968;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#26377;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#30340;&#38382;&#39064;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature importance methods promise to provide a ranking of features according to importance for a given classification task. A wide range of methods exist but their rankings often disagree and they are inherently difficult to evaluate due to a lack of ground truth beyond synthetic datasets. In this work, we put feature importance methods to the test on real-world data in the domain of cardiology, where we try to distinguish three specific pathologies from healthy subjects based on ECG features comparing to features used in cardiologists' decision rules as ground truth. Some methods generally performed well and others performed poorly, while some methods did well on some but not all of the problems considered.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02574</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21512;&#35268;&#24322;&#31574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#19981;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20063;&#19981;&#33021;&#20197;&#22312;&#32447;&#26041;&#24335;&#33719;&#21462;&#25968;&#25454;&#65288;&#22312;&#23454;&#39564;&#36153;&#29992;&#39640;&#26114;&#12289;&#39118;&#38505;&#39640;&#25110;&#19981;&#36947;&#24503;&#30340;&#24773;&#20917;&#19979;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#65289;&#12290;&#38024;&#23545;&#36825;&#31181;&#24212;&#29992;&#65292;&#24517;&#39035;&#20351;&#29992;&#22312;&#19981;&#21516;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#65288;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;&#65288;&#30446;&#26631;&#31574;&#30053;&#65289;&#30340;&#22870;&#21169;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#24322;&#31574;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#37117;&#27809;&#26377;&#20934;&#30830;&#24615;&#21644;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#26032;&#22411;OPE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;OPE&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#30446;&#26631;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#22788;&#29702;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#20272;&#35745;&#30340;&#22870;&#21169;&#21306;&#38388;&#30340;&#21512;&#35268;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02549</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#23402;&#29983;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Siamese Autoencoders. (arXiv:2304.02549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24448;&#24448;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#24471;&#30340;&#12290;&#30456;&#21453;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20943;&#23569;&#20102;&#23454;&#29616;&#30456;&#21516;&#25110;&#26356;&#39640;&#19979;&#28216;&#24615;&#33021;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#30446;&#26631;&#26159;&#22312;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#39044;&#20808;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20415;&#32593;&#32476;&#33021;&#22815;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#20013;&#30340;&#36755;&#20837;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#21644;&#23402;&#29983;&#32593;&#32476;&#65288;&#22914;SimSiam&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#29305;&#24449;&#30340;&#29305;&#24615;&#65288;&#20363;&#22914;&#65292;&#32454;&#33410;&#32423;&#21035;&#65289;&#19982;&#32473;&#23450;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#20248;&#21183;&#30340;&#26032;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21517;&#20026;SidAE&#65288;&#23402;&#29983;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#26368;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully supervised models often require large amounts of labeled training data, which tends to be costly and hard to acquire. In contrast, self-supervised representation learning reduces the amount of labeled data needed for achieving the same or even higher downstream performance. The goal is to pre-train deep neural networks on a self-supervised task such that afterwards the networks are able to extract meaningful features from raw input data. These features are then used as inputs in downstream tasks, such as image classification. Previously, autoencoders and Siamese networks such as SimSiam have been successfully employed in those tasks. Yet, challenges remain, such as matching characteristics of the features (e.g., level of detail) to the given task and data set. In this paper, we present a new self-supervised method that combines the benefits of Siamese architectures and denoising autoencoders. We show that our model, called SidAE (Siamese denoising autoencoder), outperforms two se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#26377;&#22122;&#38899;&#31867;&#21035;&#26631;&#31614;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#34920;&#29616;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02539</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65306;&#20998;&#31867;&#38382;&#39064;&#30340;&#27010;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-annotator Deep Learning: A Probabilistic Framework for Classification. (arXiv:2304.02539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02539
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#26377;&#22122;&#38899;&#31867;&#21035;&#26631;&#31614;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#34920;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#22797;&#26434;&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#28982;&#32780;&#30001;&#26131;&#38169;&#27880;&#37322;&#32773;&#65288;&#22914;&#20247;&#21253;&#24037;&#20154;&#65289;&#25552;&#20379;&#30340;&#23545;&#24212;&#31867;&#21035;&#26631;&#31614;&#26377;&#22122;&#38899;&#12290;&#22312;&#36825;&#26679;&#30340;&#22810;&#27880;&#37322;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#35757;&#32451;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#27425;&#20248;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;&#22810;&#27880;&#37322;&#28145;&#24230;&#23398;&#20064;&#65288;MaDL&#65289;&#30340;&#27010;&#29575;&#35757;&#32451;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#22320;&#38754;&#30495;&#30456;&#27169;&#22411;&#21644;&#19968;&#20010;&#27880;&#37322;&#32773;&#34920;&#29616;&#27169;&#22411;&#12290;&#22320;&#38754;&#30495;&#30456;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#23454;&#20363;&#30340;&#30495;&#23454;&#31867;&#21035;&#65292;&#32780;&#27880;&#37322;&#32773;&#34920;&#29616;&#27169;&#22411;&#25512;&#26029;&#27880;&#37322;&#32773;&#34920;&#29616;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;&#27169;&#22359;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#27880;&#37322;&#32773;&#30340;&#34920;&#29616;&#20570;&#20986;&#19981;&#21516;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#32771;&#34385;&#31867;&#21035;&#25110;&#23454;&#20363;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23398;&#20064;&#27880;&#37322;&#32773;&#23884;&#20837;&#20197;&#20272;&#35745;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#37322;&#32773;&#30340;&#23494;&#24230;&#20316;&#20026;&#36801;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#20272;&#35745;&#30495;&#23454;&#30340;&#31867;&#21035;&#26631;&#31614;&#65292;&#32780;&#19988;&#33021;&#22815;&#20272;&#35745;&#25152;&#20998;&#37197;&#26631;&#31614;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27880;&#37322;&#32773;&#24615;&#33021;&#20551;&#35774;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaIRNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#24102;&#26377;&#25110;&#19981;&#24102;&#26377;&#39044;&#22788;&#29702;&#30340;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#20197;&#26816;&#27979;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#19982;&#30740;&#31350;&#38382;&#39064;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02531</link><description>&lt;p&gt;
&#23398;&#20064;&#27604;&#36739;&#32437;&#21521;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Learning to Compare Longitudinal Images. (arXiv:2304.02531v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaIRNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#24102;&#26377;&#25110;&#19981;&#24102;&#26377;&#39044;&#22788;&#29702;&#30340;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#20197;&#26816;&#27979;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#19982;&#30740;&#31350;&#38382;&#39064;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#30740;&#31350;&#26159;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30740;&#31350;&#21644;&#25551;&#36848;&#26102;&#38388;&#21160;&#24577;&#30340;&#24120;&#29992;&#25216;&#26415;&#65292;&#20854;&#20013;&#20174;&#30456;&#21516;&#30340;&#20154;&#32676;&#20013;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#33719;&#21462;&#19968;&#31995;&#21015;&#22270;&#20687;&#12290;&#20256;&#32479;&#30340;&#32437;&#21521;&#27604;&#36739;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#22788;&#29702;&#26469;&#26631;&#20934;&#21270;&#22122;&#22768;&#21464;&#21270;&#65292;&#20363;&#22914;&#22270;&#20687;&#26041;&#21521;&#25110;&#23545;&#27604;&#24230;&#24046;&#24322;&#65292;&#28982;&#21518;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#20197;&#26816;&#27979;&#24863;&#20852;&#36259;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861; PaIRNet&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#27604;&#36739;&#32437;&#21521;&#22270;&#20687;&#23545;&#65292;&#26080;&#35770;&#26159;&#21542;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;PaIRNet&#23398;&#20064;&#35782;&#21035;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#30340;&#26368;&#20855;&#36776;&#21035;&#24615;&#30340;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#30456;&#20284;&#24230;&#24471;&#20998;&#20197;&#21453;&#26144;&#23427;&#20204;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal studies, where a series of images from the same set of individuals are acquired at different time-points, represent a popular technique for studying and characterizing temporal dynamics in biomedical applications. The classical approach for longitudinal comparison involves normalizing for nuisance variations, such as image orientation or contrast differences, via pre-processing. Statistical analysis is, in turn, conducted to detect changes of interest, either at the individual or population level. This classical approach can suffer from pre-processing issues and limitations of the statistical modeling. For example, normalizing for nuisance variation might be hard in settings where there are a lot of idiosyncratic changes. In this paper, we present a simple machine learning-based approach that can alleviate these issues. In our approach, we train a deep learning model (called PaIRNet, for Pairwise Image Ranking Network) to compare pairs of longitudinal images, with or witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.02497</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Tuning for Adversarially Robust Models. (arXiv:2304.02497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26126;&#30830;&#23545;&#25239;&#29615;&#22659;&#19979;&#38656;&#35201;&#39069;&#22806;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#24265;&#20215;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#26696;&#38477;&#20302;&#35843;&#33410;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#65292;&#26088;&#22312;&#30830;&#23450;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#21738;&#20123;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26159;&#38656;&#35201;&#35843;&#33410;&#30340;&#65292;&#21516;&#26102;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#30340;&#35843;&#33410;&#25104;&#26412;&#12290;&#36890;&#36807;&#23545;3&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#20808;&#21069;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#35813;&#38382;&#39064;&#22312;&#23545;&#25239;&#29615;&#22659;&#19979;&#30340;&#22797;&#26434;&#24615;&#20027;&#35201;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#38656;&#35201;&#35843;&#25972;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#24179;&#34913;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#65307;&#38656;&#35201;&#29420;&#31435;&#35843;&#25972;&#26631;&#20934;&#35757;&#32451;&#21644;&#23545;&#25239;&#35757;&#32451;&#38454;&#27573;&#30340;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#21033;&#29992;&#24265;&#20215;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26469;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#33410;&#25104;&#26412;&#30340;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the problem of hyper-parameter tuning (HPT) for robust (i.e., adversarially trained) models, with the twofold goal of i) establishing which additional HPs are relevant to tune in adversarial settings, and ii) reducing the cost of HPT for robust models. We pursue the first goal via an extensive experimental study based on 3 recent models widely adopted in the prior literature on adversarial robustness. Our findings show that the complexity of the HPT problem, already notoriously expensive, is exacerbated in adversarial settings due to two main reasons: i) the need of tuning additional HPs which balance standard and adversarial training; ii) the need of tuning the HPs of the standard and adversarial training phases independently. Fortunately, we also identify new opportunities to reduce the cost of HPT for robust models. Specifically, we propose to leverage cheap adversarial training methods to obtain inexpensive, yet highly correlated, estimations of the quality ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2304.02492</link><description>&lt;p&gt;
&#37327;&#21270;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22797;&#26434;&#24230;&#22312;&#21160;&#35789;&#20064;&#24471;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#23398;&#20064;&#21517;&#35789;&#30340;&#24847;&#20041;&#36890;&#24120;&#27604;&#23398;&#20064;&#21160;&#35789;&#30340;&#24847;&#20041;&#26089;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#26159;&#30001;&#20110;&#35821;&#35328;&#25152;&#25351;&#30340;&#19990;&#30028;&#20013;&#31867;&#21035;&#30340;&#35270;&#35273;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#35821;&#35328;&#26412;&#36523;&#30340;&#32467;&#26500;&#65292;&#36824;&#26159;&#20004;&#31181;&#20449;&#24687;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25152;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#28304;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23450;&#37327;&#22320;&#27979;&#35797;&#20102;&#20851;&#20110;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#36825;&#19977;&#20010;&#20551;&#35828;&#12290;&#36890;&#36807;&#26816;&#26597;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#65292;&#19982;&#21517;&#35789;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#21160;&#35789;&#30340;&#34920;&#31034;&#22312;&#22495;&#20869;&#36890;&#24120;&#26356;&#21152;&#21464;&#21270;&#21644;&#19981;&#21487;&#36776;&#35782;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#23398;&#20064;&#23454;&#20363;&#65292;&#37027;&#20040;&#21160;&#35789;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#27604;&#21517;&#35789;&#31995;&#32479;&#20013;&#30340;&#19981;&#22826;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#35821;&#35328;&#21457;&#23637;&#30340;&#36807;&#31243;&#31867;&#20284;&#65292;&#22914;&#26524;&#22312;&#23398;&#20064;&#26399;&#38388;&#26377;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#65292;&#37027;&#20040;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#37117;&#21464;&#24471;&#26356;&#21152;&#21563;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#19981;&#23545;&#31216;&#24615;&#33267;&#23569;&#37096;&#20998;&#24402;&#22240;&#20110;&#26356;&#22797;&#26434;&#30340;&#21160;&#35789;&#21547;&#20041;&#65292;&#36825;&#38656;&#35201;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35270;&#35273;&#22797;&#26434;&#24230;&#25110;&#35821;&#35328;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn the meanings of nouns earlier than the meanings of verbs. However, it is unclear whether this asymmetry is a result of complexity in the visual structure of categories in the world to which language refers, the structure of language itself, or the interplay between the two sources of information. We quantitatively test these three hypotheses regarding early verb learning by employing visual and linguistic representations of words sourced from large-scale pre-trained artificial neural networks. Examining the structure of both visual and linguistic embedding spaces, we find, first, that the representation of verbs is generally more variable and less discriminable within domain than the representation of nouns. Second, we find that if only one learning instance per category is available, visual and linguistic representations are less well aligned in the verb system than in the noun system. However, in parallel with the course of human language development, if mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292; MIR&#21644;MFI&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#21487;&#29983;&#25104;p&#20540;&#36873;&#25321;&#30456;&#20851;&#21644;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02490</link><description>&lt;p&gt;
&#20998;&#26512;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#25581;&#24320;&#38543;&#26426;&#26862;&#26519;&#30340;&#40657;&#21283;&#23376;
&lt;/p&gt;
&lt;p&gt;
Opening the random forest black box by the analysis of the mutual impact of features. (arXiv:2304.02490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292; MIR&#21644;MFI&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#21487;&#29983;&#25104;p&#20540;&#36873;&#25321;&#30456;&#20851;&#21644;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28789;&#27963;&#24615;&#24182;&#21487;&#25552;&#20379;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#20197;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36890;&#24120;&#19981;&#32771;&#34385;&#36873;&#25321;&#65292;&#22240;&#27492;&#20063;&#34987;&#24573;&#30053;&#20102;&#23545;&#20998;&#26512;&#26679;&#26412;&#30340;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#38543;&#26426;&#26862;&#26519;&#20013;&#29305;&#24449;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290; Mutual forest impact&#65288;MFI&#65289;&#26159;&#19968;&#20010;&#20851;&#31995;&#21442;&#25968;&#65292;&#35780;&#20272;&#29305;&#24449;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#24615;&#65292;&#22240;&#27492;&#36229;&#36234;&#20102;&#30456;&#20851;&#31995;&#25968;&#20998;&#26512;&#12290; Mutual impurity reduction&#65288;MIR&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#65292;&#23427;&#23558;&#36825;&#31181;&#20851;&#31995;&#21442;&#25968;&#19982;&#21508;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#30456;&#32467;&#21512;&#12290; MIR&#21644;MFI&#19982;&#27979;&#35797;&#31243;&#24207;&#19968;&#36215;&#23454;&#29616;&#65292;&#29983;&#25104;p&#20540;&#20197;&#36873;&#25321;&#30456;&#20851;&#21644;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#25311;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forest is a popular machine learning approach for the analysis of high-dimensional data because it is flexible and provides variable importance measures for the selection of relevant features. However, the complex relationships between the features are usually not considered for the selection and thus also neglected for the characterization of the analysed samples. Here we propose two novel approaches that focus on the mutual impact of features in random forests. Mutual forest impact (MFI) is a relation parameter that evaluates the mutual association of the featurs to the outcome and, hence, goes beyond the analysis of correlation coefficients. Mutual impurity reduction (MIR) is an importance measure that combines this relation parameter with the importance of the individual features. MIR and MFI are implemented together with testing procedures that generate p-values for the selection of related and important features. Applications to various simulated data sets and the comparis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#31215;&#26497;&#25512;&#33616;&#20154;&#26426;&#20132;&#20114;&#23454;&#39564;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23454;&#26102;&#22609;&#36896;&#30446;&#26631;&#65292;&#24182;&#20197;&#38081;&#30005;&#34180;&#33180;&#30340;&#21387;&#30005;&#21709;&#24212;&#21147;&#35889;&#20026;&#20363;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.02484</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#31215;&#26497;&#25512;&#33616;&#20154;&#26426;&#20132;&#20114;&#23454;&#39564;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A dynamic Bayesian optimized active recommender system for curiosity-driven Human-in-the-loop automated experiments. (arXiv:2304.02484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#31215;&#26497;&#25512;&#33616;&#20154;&#26426;&#20132;&#20114;&#23454;&#39564;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#23454;&#26102;&#22609;&#36896;&#30446;&#26631;&#65292;&#24182;&#20197;&#38081;&#30005;&#34180;&#33180;&#30340;&#21387;&#30005;&#21709;&#24212;&#21147;&#35889;&#20026;&#20363;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#21033;&#29992;&#31215;&#26497;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#23454;&#39564;&#26448;&#26009;&#30340;&#21512;&#25104;&#21644;&#34920;&#24449;&#24050;&#32463;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20363;&#22914;&#36890;&#36807;&#22312;&#21516;&#27493;&#36752;&#23556;&#19979;&#23545;&#32452;&#21512;&#21512;&#37329;&#36827;&#34892;&#34893;&#23556;&#27979;&#37327;&#65292;&#25110;&#32773;&#21033;&#29992;&#33258;&#21160;&#21270;&#21512;&#25104;&#26426;&#22120;&#20154;&#22312;&#21270;&#23398;&#31354;&#38388;&#20013;&#25628;&#32034;&#38041;&#38043;&#30719;&#12290;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#30340;&#30446;&#26631;&#29289;&#24615;&#26159;&#20107;&#20808;&#23450;&#20041;&#22909;&#30340;&#65292;&#25805;&#20316;&#26399;&#38388;&#30340;&#20154;&#31867;&#21453;&#39304;&#26377;&#38480;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#23454;&#39564;&#27969;&#31243;&#65292;&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#31215;&#26497;&#25512;&#33616;&#31995;&#32479;&#65288;BOARS&#65289;&#23454;&#26102;&#22609;&#36896;&#30446;&#26631;&#65292;&#24182;&#19988;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26694;&#26550;&#22312;&#38081;&#30005;&#34180;&#33180;&#30340;&#21387;&#30005;&#21709;&#24212;&#21147;&#35889;&#39044;&#20808;&#33719;&#21462;&#25968;&#25454;&#21644;&#21407;&#23376;&#21147;&#26174;&#24494;&#38236;&#23454;&#26102;&#20248;&#21270;&#23545;&#31216;&#21387;&#30005;&#21709;&#24212;&#24133;&#24230;&#28382;&#22238;&#29305;&#24449;&#23547;&#25214;&#20013;&#30340;&#24212;&#29992;&#23454;&#20363;&#12290;&#21457;&#29616;&#36825;&#26679;&#30340;&#29305;&#24449;&#20986;&#29616;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of experimental materials synthesis and characterization through active learning methods has been growing over the last decade, with examples ranging from measurements of diffraction on combinatorial alloys at synchrotrons, to searches through chemical space with automated synthesis robots for perovskites. In virtually all cases, the target property of interest for optimization is defined apriori with limited human feedback during operation. In contrast, here we present the development of a new type of human in the loop experimental workflow, via a Bayesian optimized active recommender system (BOARS), to shape targets on the fly, employing human feedback. We showcase examples of this framework applied to pre-acquired piezoresponse force spectroscopy of a ferroelectric thin film, and then implement this in real time on an atomic force microscope, where the optimization proceeds to find symmetric piezoresponse amplitude hysteresis loops. It is found that such features appear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#27169;&#20223;&#23398;&#20064;&#65288;QIL&#65289;&#20197;&#21152;&#36895;&#23398;&#20064;&#65292;&#20854;&#20013;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#20195;&#26367;DNN&#26469;&#34920;&#31034;&#31574;&#30053;&#65292;&#20998;&#21035;&#24320;&#21457;&#20102;&#37327;&#23376;&#34892;&#20026;&#20811;&#38534;&#65288;Q-BC&#65289;&#21644;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;Q-GAIL&#65289;&#20004;&#31181;QIL&#31639;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;QIL&#21487;&#20197;&#23454;&#29616;&#33267;&#23569;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22522;&#20110;VQC&#30340;&#31574;&#30053;&#34920;&#31034;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#37327;&#23376;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02480</link><description>&lt;p&gt;
&#37327;&#23376;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum Imitation Learning. (arXiv:2304.02480v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#27169;&#20223;&#23398;&#20064;&#65288;QIL&#65289;&#20197;&#21152;&#36895;&#23398;&#20064;&#65292;&#20854;&#20013;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#20195;&#26367;DNN&#26469;&#34920;&#31034;&#31574;&#30053;&#65292;&#20998;&#21035;&#24320;&#21457;&#20102;&#37327;&#23376;&#34892;&#20026;&#20811;&#38534;&#65288;Q-BC&#65289;&#21644;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;Q-GAIL&#65289;&#20004;&#31181;QIL&#31639;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;QIL&#21487;&#20197;&#23454;&#29616;&#33267;&#23569;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22522;&#20110;VQC&#30340;&#31574;&#30053;&#34920;&#31034;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#37327;&#23376;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35299;&#20915;&#21508;&#31181;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26469;&#35757;&#32451;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#31639;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#39640;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24076;&#26395;&#21033;&#29992;&#37327;&#23376;&#20248;&#21183;&#21152;&#36895;IL&#30340;&#37327;&#23376;&#27169;&#20223;&#23398;&#20064;&#65288;QIL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;QIL&#31639;&#27861;&#65292;&#37327;&#23376;&#34892;&#20026;&#20811;&#38534;&#65288;Q-BC&#65289;&#21644;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;Q-GAIL&#65289;&#12290;Q-BC&#37319;&#29992;&#36127;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#19987;&#23478;&#25968;&#25454;&#24773;&#20917;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#65292;&#32780;Q-GAIL&#37319;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#22312;&#32447;&#21644;&#22312;&#32447;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#26377;&#38480;&#30340;&#19987;&#23478;&#25968;&#25454;&#24773;&#20917;&#12290;&#23545;&#20110;&#20004;&#31181;QIL&#31639;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#20195;&#26367;DNN&#26469;&#34920;&#31034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#21644;&#32553;&#25918;&#21442;&#25968;&#36827;&#34892;&#20462;&#25913;&#20197;&#22686;&#24378;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#32463;&#20856;&#25968;&#25454;&#32534;&#30721;&#20026;&#37327;&#23376;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#27169;&#25311;&#22120;&#30340;VQC&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;QIL&#21487;&#20197;&#23454;&#29616;&#33267;&#23569;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22522;&#20110;VQC&#30340;&#31574;&#30053;&#34920;&#31034;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#37327;&#23376;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable successes in solving various complex decision-making tasks, training an imitation learning (IL) algorithm with deep neural networks (DNNs) suffers from the high computation burden. In this work, we propose quantum imitation learning (QIL) with a hope to utilize quantum advantage to speed up IL. Concretely, we develop two QIL algorithms, quantum behavioural cloning (Q-BC) and quantum generative adversarial imitation learning (Q-GAIL). Q-BC is trained with a negative log-likelihood loss in an off-line manner that suits extensive expert data cases, whereas Q-GAIL works in an inverse reinforcement learning scheme, which is on-line and on-policy that is suitable for limited expert data cases. For both QIL algorithms, we adopt variational quantum circuits (VQCs) in place of DNNs for representing policies, which are modified with data re-uploading and scaling parameters to enhance the expressivity. We first encode classical data into quantum states as inputs, then perform V
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21464;&#20998;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#20854;&#20013;&#19968;&#20010;&#29305;&#20363;&#65292;&#21487;&#20197;&#23558;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#26679;&#26412;&#20998;&#31163;&#24320;&#26469;&#12290;</title><link>http://arxiv.org/abs/2304.02473</link><description>&lt;p&gt;
&#20840;&#21464;&#20998;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fully Variational Noise-Contrastive Estimation. (arXiv:2304.02473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21464;&#20998;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#20854;&#20013;&#19968;&#20010;&#29305;&#20363;&#65292;&#21487;&#20197;&#23558;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#26679;&#26412;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#22522;&#26412;&#29702;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#26063;&#21487;&#29992;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#22122;&#22768;&#21644;&#25968;&#25454;&#26679;&#26412;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#20998;&#21035;&#19979;&#30028;&#20026;&#21464;&#20998;&#36125;&#21494;&#26031;&#20013;&#30340;&#30456;&#24212;&#39033;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#26063;&#20026;&#20840;&#21464;&#20998;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#20363;&#26159;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#22240;&#27492;&#20063;&#21487;&#20197;&#29702;&#35299;&#20026;&#36890;&#36807;&#36866;&#24403;&#30340;&#20998;&#31867;&#25439;&#22833;&#23558;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#26679;&#26412;&#20998;&#31163;&#24320;&#26469;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20840;&#21464;&#20998;NCE&#30446;&#26631;&#20989;&#25968;&#26063;&#20013;&#30340;&#20854;&#20182;&#20363;&#23376;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#22312;&#23454;&#35777;&#34892;&#20026;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
By using the underlying theory of proper scoring rules, we design a family of noise-contrastive estimation (NCE) methods that are tractable for latent variable models. Both terms in the underlying NCE loss, the one using data samples and the one using noise samples, can be lower-bounded as in variational Bayes, therefore we call this family of losses fully variational noise-contrastive estimation. Variational autoencoders are a particular example in this family and therefore can be also understood as separating real data from synthetic samples using an appropriate classification loss. We further discuss other instances in this family of fully variational NCE objectives and indicate differences in their empirical behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#20026;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02472</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35746;&#21333;&#27969;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#27874;&#21160;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Volatility Prediction Using Deep CNNs Trained on Order Flow. (arXiv:2304.02472v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#20026;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26032;&#20852;&#30340;&#36164;&#20135;&#31867;&#21035;&#65292;&#21152;&#23494;&#36135;&#24065;&#30456;&#27604;&#20256;&#32479;&#30340;&#32929;&#31080;&#24066;&#22330;&#26126;&#26174;&#26356;&#20855;&#27874;&#21160;&#24615;&#12290;&#30001;&#20110;&#20854;&#22823;&#22810;&#25968;&#26102;&#20505;&#26159;&#26080;&#30417;&#31649;&#30340;&#65292;&#27969;&#21160;&#24615;&#36890;&#24120;&#36739;&#20302;&#65292;&#21152;&#23494;&#36164;&#20135;&#30340;&#20215;&#26684;&#22312;&#20960;&#20998;&#38047;&#20869;&#23601;&#21487;&#20986;&#29616;&#26174;&#33879;&#21464;&#21160;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#24040;&#39069;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#23558;&#24066;&#22330;&#20449;&#24687;&#32534;&#30721;&#25104;&#22270;&#20687;&#24182;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#23454;&#29616;&#27874;&#21160;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#32534;&#30721;&#21644;&#30456;&#24212;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#30340;&#24066;&#22330;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#26377;&#28508;&#21147;&#26356;&#22909;&#22320;&#25429;&#25417;&#24066;&#22330;&#21160;&#24577;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27874;&#21160;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a newly emerged asset class, cryptocurrency is evidently more volatile compared to the traditional equity markets. Due to its mostly unregulated nature, and often low liquidity, the price of crypto assets can sustain a significant change within minutes that in turn might result in considerable losses. In this paper, we employ an approach for encoding market information into images and making predictions of short-term realized volatility by employing Convolutional Neural Networks. We then compare the performance of the proposed encoding and corresponding model with other benchmark models. The experimental results demonstrate that this representation of market data with a Convolutional Neural Network as a predictive model has the potential to better capture the market dynamics and a better volatility prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#32467;&#26500;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#19979;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#37327;&#19978;&#37117;&#21253;&#21547;&#20984;&#38750;&#20809;&#28369;&#39033;&#30340;&#22797;&#21512;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65307;&#29702;&#35770;&#35777;&#26126;&#20102;&#26412;&#31639;&#27861;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02441</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#26368;&#22823;&#21270;&#27861;&#35299;&#20915;&#22797;&#21512;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decentralized gradient descent maximization method for composite nonconvex strongly-concave minimax problems. (arXiv:2304.02441v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#32467;&#26500;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#19979;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#37327;&#19978;&#37117;&#21253;&#21547;&#20984;&#38750;&#20809;&#28369;&#39033;&#30340;&#22797;&#21512;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65307;&#29702;&#35770;&#35777;&#26126;&#20102;&#26412;&#31639;&#27861;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#24335;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#32467;&#26500;&#20248;&#21270;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#20204;&#23581;&#35797;&#20102;&#19968;&#20123;&#21162;&#21147;&#65292;&#20294;&#26159;&#23427;&#20204;&#37117;&#23616;&#38480;&#20110;&#26368;&#22810;&#23545;&#26497;&#22823;&#21464;&#37327;&#26045;&#21152;&#19968;&#20010;&#32422;&#26463;&#30340;&#20809;&#28369;&#38382;&#39064;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#23581;&#35797;&#35299;&#20915;&#21487;&#20197;&#22312;&#26368;&#23567;&#21270;&#21644;&#26368;&#22823;&#21270;&#21464;&#37327;&#19978;&#37117;&#21253;&#21547;&#20984;&#38750;&#20809;&#28369;&#39033;&#30340;&#22797;&#21512;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#24341;&#20837;&#19968;&#20010;&#20056;&#23376;&#26469;&#21560;&#25910;&#21452;&#37325;&#20849;&#35782;&#32422;&#26463;&#12290;&#28040;&#38500;&#21452;&#37325;&#20849;&#35782;&#32422;&#26463;&#20351;&#24471;&#26368;&#20855;&#25915;&#20987;&#24615;&#30340;&#65288;&#21363;&#26412;&#22320;&#26368;&#22823;&#21270;&#32780;&#19981;&#26159;&#26799;&#24230;&#19978;&#21319;&#27493;&#39588;&#65289;&#21452;&#37325;&#26356;&#26032;&#21464;&#24471;&#21487;&#33021;&#65292;&#26377;&#21161;&#20110;&#37319;&#29992;&#26356;&#22823;&#30340;&#21407;&#22987;&#27493;&#38271;&#21644;&#26356;&#22909;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23558;&#38750;&#20809;&#28369;&#24615;&#21644;&#23545;&#21452;&#21464;&#37327;&#19978;&#30340;&#19968;&#33268;&#24615;&#36827;&#34892;&#20998;&#31163;&#65292;&#26377;&#21161;&#20110;&#20998;&#26512;&#20998;&#24067;&#24335;&#31639;&#27861;&#65307;&#22240;&#27492;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38750;&#20984;&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;&#26576;&#20123;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39640;&#27010;&#29575;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#20223;&#30495;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#12289;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimax problems have recently attracted a lot of research interests. A few efforts have been made to solve decentralized nonconvex strongly-concave (NCSC) minimax-structured optimization; however, all of them focus on smooth problems with at most a constraint on the maximization variable. In this paper, we make the first attempt on solving composite NCSC minimax problems that can have convex nonsmooth terms on both minimization and maximization variables. Our algorithm is designed based on a novel reformulation of the decentralized minimax problem that introduces a multiplier to absorb the dual consensus constraint. The removal of dual consensus constraint enables the most aggressive (i.e., local maximization instead of a gradient ascent step) dual update that leads to the benefit of taking a larger primal stepsize and better complexity results. In addition, the decoupling of the nonsmoothness and consensus on the dual variable eases the analysis of a decentralized algorithm; thus our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02407</link><description>&lt;p&gt;
&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65306;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20013;&#30340;&#36974;&#25377;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness Mapping. (arXiv:2304.02407v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#22312;&#24456;&#20037;&#20197;&#21069;&#20154;&#20204;&#23601;&#21457;&#29616;&#65292;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20849;&#21516;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#30340;&#20114;&#34917;&#29305;&#24449;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#27169;&#24577;&#23545;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#20173;&#28982;&#20196;&#20154;&#22256;&#24785;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#27169;&#24577;&#32423;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36974;&#25377;&#25935;&#24863;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26089;&#26399;&#34701;&#21512;&#24773;&#20917;&#19979;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#36825;&#19968;&#20219;&#21153;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#30410;&#20110;&#35832;&#22914;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#31561;&#36741;&#21161;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly harnessing complementary features of multi-modal input data in a common latent space has been found to be beneficial long ago. However, the influence of each modality on the models decision remains a puzzle. This study proposes a deep learning framework for the modality-level interpretation of multimodal earth observation data in an end-to-end fashion. While leveraging an explainable machine learning method, namely Occlusion Sensitivity, the proposed framework investigates the influence of modalities under an early-fusion scenario in which the modalities are fused before the learning process. We show that the task of wilderness mapping largely benefits from auxiliary data such as land cover and night time light data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>DRAC&#26159;&#19968;&#20010;&#29992;&#20110;&#36229;&#24191;&#35282;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#26512;&#25361;&#25112;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#19977;&#20010;&#20219;&#21153;&#65306;DR&#25439;&#20260;&#20998;&#21106;&#12289;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#21644;DR&#20998;&#32423;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#26469;&#33258;&#22320;&#29702;&#19978;&#22810;&#26679;&#21270;&#30340;11&#20010;&#12289;12&#20010;&#21644;13&#20010;&#22242;&#38431;&#30340;&#31215;&#26497;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.02389</link><description>&lt;p&gt;
DRAC: &#20351;&#29992;&#36229;&#24191;&#35282;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#26512;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
DRAC: Diabetic Retinopathy Analysis Challenge with Ultra-Wide Optical Coherence Tomography Angiography Images. (arXiv:2304.02389v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02389
&lt;/p&gt;
&lt;p&gt;
DRAC&#26159;&#19968;&#20010;&#29992;&#20110;&#36229;&#24191;&#35282;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#26512;&#25361;&#25112;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#19977;&#20010;&#20219;&#21153;&#65306;DR&#25439;&#20260;&#20998;&#21106;&#12289;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#21644;DR&#20998;&#32423;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#26469;&#33258;&#22320;&#29702;&#19978;&#22810;&#26679;&#21270;&#30340;11&#20010;&#12289;12&#20010;&#21644;13&#20010;&#22242;&#38431;&#30340;&#31215;&#26497;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#33258;&#21160;&#20998;&#26512;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#23545;&#20943;&#23569;&#35270;&#21147;&#20007;&#22833;&#21644;&#22833;&#26126;&#30340;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#22522;&#20934;&#12290;&#36229;&#24191;&#35282;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;UW-OCTA&#65289;&#26159;DR&#35786;&#26029;&#31995;&#32479;&#20013;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#21644;&#23433;&#20840;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#20026;&#20102;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#20351;&#29992;UW-OCTA&#22270;&#20687;&#36827;&#34892;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#20998;&#26512;&#30340;&#31185;&#23398;&#22522;&#20934;&#21046;&#23450;&#65292;&#25105;&#20204;&#19982;&#31532;25&#23626;&#21307;&#23398;&#22270;&#20687;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#22269;&#38469;&#20250;&#35758;&#65288;MICCAI 2022&#65289;&#32852;&#21512;&#32452;&#32455;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DRAC-Diabetic Retinopathy Analysis Challenge&#8221;&#30340;&#25361;&#25112;&#27604;&#36187;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#19977;&#20010;&#20219;&#21153;&#65306;DR&#25439;&#20260;&#20998;&#21106;&#12289;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#21644;DR&#20998;&#32423;&#12290;&#31185;&#23398;&#30028;&#31215;&#26497;&#21709;&#24212;&#35813;&#25361;&#25112;&#65292;&#24182;&#20174;&#22320;&#29702;&#19978;&#19981;&#21516;&#30340;&#26426;&#26500;&#25552;&#20132;&#20102;11&#20010;&#12289;12&#20010;&#21644;13&#20010;&#22242;&#38431;&#25552;&#20379;&#30340;&#36825;&#19977;&#20010;&#20219;&#21153;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-assisted automatic analysis of diabetic retinopathy (DR) is of great importance in reducing the risks of vision loss and even blindness. Ultra-wide optical coherence tomography angiography (UW-OCTA) is a non-invasive and safe imaging modality in DR diagnosis system, but there is a lack of publicly available benchmarks for model development and evaluation. To promote further research and scientific benchmarking for diabetic retinopathy analysis using UW-OCTA images, we organized a challenge named "DRAC - Diabetic Retinopathy Analysis Challenge" in conjunction with the 25th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022). The challenge consists of three tasks: segmentation of DR lesions, image quality assessment and DR grading. The scientific community responded positively to the challenge, with 11, 12, and 13 teams from geographically diverse institutes submitting different solutions in these three tasks, respectively. This p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23545;&#22522;&#20110;&#35270;&#35273;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#30340;&#20551;&#38451;&#24615;&#20559;&#35265;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#38750;&#32447;&#24615;&#21487;&#20998;&#31163;&#31867;&#21035;&#21644;&#36880;&#28176;&#22686;&#21152;&#30340;&#20266;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#20026;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#37327;&#21270;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.02383</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#26377;&#22810;&#22909;&#65311;&#19968;&#20010;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
How good Neural Networks interpretation methods really are? A quantitative benchmark. (arXiv:2304.02383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23545;&#22522;&#20110;&#35270;&#35273;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#30340;&#20551;&#38451;&#24615;&#20559;&#35265;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#38750;&#32447;&#24615;&#21487;&#20998;&#31163;&#31867;&#21035;&#21644;&#36880;&#28176;&#22686;&#21152;&#30340;&#20266;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#20026;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#37327;&#21270;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21807;&#26377;&#36890;&#36807;&#20984;&#26174;&#27169;&#22411;&#35748;&#20026;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#30340;&#35266;&#28857;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#20123; Saliency Maps&#65288;SMs&#65289;&#29992;&#20110;&#39640;&#24230;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#36229;&#36234;&#20102;&#32447;&#24615;&#29305;&#24449;&#36873;&#25321; (FS) &#26041;&#27861;&#30340;&#23616;&#38480;&#12290;&#28982;&#32780;&#65292;SM &#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20027;&#35201;&#26159;&#36890;&#36807;&#23450;&#24615; (&#35270;&#35273;&#19978;) &#35780;&#20272;&#30340;&#65292;&#32570;&#20047;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#23450;&#20041;&#24615;&#26631;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476; (NNs) &#35299;&#37322;&#26041;&#27861;&#30340;&#21512;&#25104;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23601;&#27492;&#30446;&#30340;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#21487;&#20998;&#31163;&#31867;&#21035;&#21644;&#36880;&#28176;&#22686;&#21152;&#30340;&#20266;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20063;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340; FS &#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency Maps (SMs) have been extensively used to interpret deep learning models decision by highlighting the features deemed relevant by the model. They are used on highly nonlinear problems, where linear feature selection (FS) methods fail at highlighting relevant explanatory variables. However, the reliability of gradient-based feature attribution methods such as SM has mostly been only qualitatively (visually) assessed, and quantitative benchmarks are currently missing, partially due to the lack of a definite ground truth on image data. Concerned about the apophenic biases introduced by visual assessment of these methods, in this paper we propose a synthetic quantitative benchmark for Neural Networks (NNs) interpretation methods. For this purpose, we built synthetic datasets with nonlinearly separable classes and increasing number of decoy (random) features, illustrating the challenge of FS in high-dimensional settings. We also compare these methods to conventional approaches such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#35299;&#37322;&#33021;&#21147;&#19968;&#30452;&#26159;&#38480;&#21046;&#20854;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#22914;&#21307;&#23398;&#12289;&#32593;&#32476;&#23433;&#20840;&#25110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21463;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#33021;&#37327;&#26223;&#35266;&#30740;&#31350;&#26041;&#27861;&#21551;&#21457;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#20419;&#36827;&#27169;&#22411;&#20915;&#31574;&#12290;&#36890;&#36807;&#35782;&#21035;&#25439;&#22833;&#26223;&#35266;&#23616;&#37096;&#26497;&#23567;&#20540;&#32452;&#20013;&#30340;&#23432;&#24658;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#23384;&#22312;&#31867;&#20284;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22352;&#26631;&#19981;&#21464;&#37327;&#25110;&#26377;&#24207;&#21442;&#25968;&#26469;&#30830;&#23450;&#20998;&#23376;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#26223;&#35266;&#20013;&#27809;&#26377;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24212;&#29992;&#22522;&#20110;&#19981;&#21464;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;MARL&#25152;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#19981;&#20165;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.02370</link><description>&lt;p&gt;
&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#65306;&#20165;&#38656;&#19981;&#21464;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective control of two-dimensional Rayleigh--B\'enard convection: invariant multi-agent reinforcement learning is all you need. (arXiv:2304.02370v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24212;&#29992;&#22522;&#20110;&#19981;&#21464;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20108;&#32500;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;MARL&#25152;&#33719;&#24471;&#30340;&#25511;&#21046;&#27861;&#21017;&#19981;&#20165;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#26159;&#20960;&#31181;&#24037;&#19994;&#21644;&#22320;&#29699;&#31185;&#23398;&#27969;&#21160;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#20174;&#22522;&#26412;&#27969;&#20307;&#21147;&#23398;&#30340;&#35282;&#24230;&#20063;&#26159;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#30340;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#25511;&#21046;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#65292;&#20363;&#22914;&#36890;&#36807;&#35843;&#33410;&#35268;&#33539;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#24418;&#24577;&#19979;&#24213;&#26495;&#21152;&#28909;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#25511;&#21046;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#20110;&#23485;&#36890;&#36947;&#20869;&#30340;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#23616;&#37096;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#19981;&#21464;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26469;&#33719;&#21462;&#26377;&#25928;&#30340;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#25511;&#21046;&#12290;&#24212;&#29992;&#20110;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#30340;MARL&#26694;&#26550;&#20801;&#35768;&#22686;&#21152;&#25511;&#21046;&#27573;&#30340;&#25968;&#37327;&#65292;&#32780;&#19981;&#20250;&#36935;&#21040;&#22240;naive DRL&#21160;&#20316;&#23610;&#23544;&#32500;&#25968;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#36825;&#24471;&#30410;&#20110;MARL&#33021;&#22815;&#37325;&#22797;&#20351;&#29992;&#22312;&#19981;&#21516;&#21306;&#22495;&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#24418;&#25104;&#26377;&#25928;&#25910;&#25947;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MARL&#26041;&#27861;&#22312;&#29790;&#21033;-&#36125;&#32435;&#24503;&#23545;&#27969;&#25511;&#21046;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#22312;&#24037;&#19994;&#21644;&#22320;&#29699;&#31185;&#23398;&#27969;&#21160;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rayleigh-B\'enard convection (RBC) is a recurrent phenomenon in several industrial and geoscience flows and a well-studied system from a fundamental fluid-mechanics viewpoint. However, controlling RBC, for example by modulating the spatial distribution of the bottom-plate heating in the canonical RBC configuration, remains a challenging topic for classical control-theory methods. In the present work, we apply deep reinforcement learning (DRL) for controlling RBC. We show that effective RBC control can be obtained by leveraging invariant multi-agent reinforcement learning (MARL), which takes advantage of the locality and translational invariance inherent to RBC flows inside wide channels. The MARL framework applied to RBC allows for an increase in the number of control segments without encountering the curse of dimensionality that would result from a naive increase in the DRL action-size dimension. This is made possible by the MARL ability for re-using the knowledge generated in differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#27835;&#30103;&#35745;&#21010;&#38774;&#20307;&#31215;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#36718;&#24275;&#26041;&#27861;&#65292;&#20351;&#31934;&#30830;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02353</link><description>&lt;p&gt;
&#20351;&#29992;U-Net&#22312;CT&#24207;&#21015;&#20013;&#20998;&#21106;&#20840;&#39592;&#39635;&#29031;&#23556;&#30340;&#35745;&#21010;&#38774;&#20307;&#31215;
&lt;/p&gt;
&lt;p&gt;
Segmentation of Planning Target Volume in CT Series for Total Marrow Irradiation Using U-Net. (arXiv:2304.02353v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;U-Net&#26550;&#26500;&#36827;&#34892;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#27835;&#30103;&#35745;&#21010;&#38774;&#20307;&#31215;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#36718;&#24275;&#26041;&#27861;&#65292;&#20351;&#31934;&#30830;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#27835;&#30103;&#65288;RT&#65289;&#26159;&#27835;&#30103;&#21508;&#31181;&#30284;&#30151;&#65292;&#21253;&#25324;&#24613;&#24615;&#28107;&#24052;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;ALL&#65289;&#21644;&#24613;&#24615;&#39635;&#24615;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31934;&#30830;&#23450;&#20301;&#21361;&#21450;&#22120;&#23448;&#65288;OARs&#65289;&#21644;&#27835;&#30103;&#21306;&#22495;&#23545;&#20110;&#26377;&#25928;&#30340;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#35843;&#24378;&#25918;&#23556;&#27835;&#30103;&#65288;IMRT&#65289;&#25216;&#26415;&#65292;&#22914;&#20840;&#39592;&#39635;&#29031;&#23556;&#65288;TMI&#65289;&#21644;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#65288;TMLI&#65289;&#65292;&#19982;&#20840;&#36523;&#29031;&#23556;&#65288;TBI&#65289;&#30456;&#27604;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25918;&#23556;&#27835;&#30103;&#36882;&#36865;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#25918;&#23556;&#32959;&#30244;&#23398;&#23478;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#20013;&#32791;&#26102;&#25163;&#21160;&#20998;&#21106;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#36718;&#24275;&#26041;&#27861;&#65292;&#20351;&#29992;U-Net&#26550;&#26500;&#23545;TMLI&#27835;&#30103;&#35745;&#21010;&#38774;&#20307;&#31215;&#65288;PTV&#65289;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#22312;2011&#24180;&#33267;2021&#24180;&#26399;&#38388;&#22312;Humanitas&#30740;&#31350;&#21307;&#38498;&#25509;&#21463;TMLI&#27835;&#30103;&#30340;100&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiotherapy (RT) is a key component in the treatment of various cancers, including Acute Lymphocytic Leukemia (ALL) and Acute Myelogenous Leukemia (AML). Precise delineation of organs at risk (OARs) and target areas is essential for effective treatment planning. Intensity Modulated Radiotherapy (IMRT) techniques, such as Total Marrow Irradiation (TMI) and Total Marrow and Lymph node Irradiation (TMLI), provide more precise radiation delivery compared to Total Body Irradiation (TBI). However, these techniques require time-consuming manual segmentation of structures in Computerized Tomography (CT) scans by the Radiation Oncologist (RO). In this paper, we present a deep learning-based auto-contouring method for segmenting Planning Target Volume (PTV) for TMLI treatment using the U-Net architecture. We trained and compared two segmentation models with two different loss functions on a dataset of 100 patients treated with TMLI at the Humanitas Research Hospital between 2011 and 2021. Despi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#35299;&#32544;&#25351;&#26631;&#65292;&#29992;&#20110;&#20462;&#27491;&#29616;&#26377;&#25351;&#26631;&#30340;&#20004;&#20010;&#32570;&#38519;&#65292;&#24182;&#36890;&#36807;&#23558;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#34913;&#37327;&#32534;&#30721;&#22120;&#30340;&#35299;&#32544;&#33021;&#21147;&#12290;&#26032;&#25351;&#26631;&#19982;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26368;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.02335</link><description>&lt;p&gt;
&#20462;&#27491;&#26222;&#36941;&#35299;&#32544;&#24230;&#37327;&#20013;&#30340;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Correcting Flaws in Common Disentanglement Metrics. (arXiv:2304.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#35299;&#32544;&#25351;&#26631;&#65292;&#29992;&#20110;&#20462;&#27491;&#29616;&#26377;&#25351;&#26631;&#30340;&#20004;&#20010;&#32570;&#38519;&#65292;&#24182;&#36890;&#36807;&#23558;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#34913;&#37327;&#32534;&#30721;&#22120;&#30340;&#35299;&#32544;&#33021;&#21147;&#12290;&#26032;&#25351;&#26631;&#19982;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26368;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#22914;&#22823;&#23567;&#25110;&#24418;&#29366;&#65292;&#30001;&#19981;&#21516;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#12290;&#37327;&#21270;&#29305;&#23450;&#34920;&#31034;&#35299;&#32544;&#30340;&#31243;&#24230;&#24182;&#38750;&#26131;&#20107;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#25351;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#29616;&#26377;&#25351;&#26631;&#30340;&#20004;&#20010;&#32570;&#38519;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#23558;&#19968;&#20010;&#20173;&#28982;&#32416;&#32544;&#30340;&#27169;&#22411;&#35780;&#20998;&#39640;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#25351;&#26631;&#65292;&#20197;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#25105;&#20204;&#32771;&#34385;&#20102;&#32452;&#21512;&#27867;&#21270;&#20219;&#21153;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#29992;&#23427;&#26469;&#34913;&#37327;&#32534;&#30721;&#22120;&#30340;&#35299;&#32544;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#30340;&#34920;&#29616;&#36890;&#24120;&#30456;&#24403;&#24046;&#65292;&#19982;&#22823;&#22810;&#25968;&#35299;&#32544;&#24230;&#37327;&#30456;&#20851;&#65292;&#26368;&#24378;&#28872;&#22320;&#19982;&#25105;&#20204;&#26032;&#25552;&#20986;&#30340;&#24230;&#37327;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen growing interest in learning disentangled representations, in which distinct features, such as size or shape, are represented by distinct neurons. Quantifying the extent to which a given representation is disentangled is not straightforward; multiple metrics have been proposed. In this paper, we identify two failings of existing metrics, which mean they can assign a high score to a model which is still entangled, and we propose two new metrics, which redress these problems. We then consider the task of compositional generalization. Unlike prior works, we treat this as a classification problem, which allows us to use it to measure the disentanglement ability of the encoder, without depending on the decoder. We show that performance on this task is (a) generally quite poor, (b) correlated with most disentanglement metrics, and (c) most strongly correlated with our newly proposed metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#26469;&#36798;&#21040;&#39640;&#25928;CNN&#30340;&#30446;&#30340;&#65292;&#35299;&#20915;&#20102;&#39640;&#21098;&#26525;&#29575;&#19979;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02319</link><description>&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#23454;&#29616;&#39640;&#25928;CNN
&lt;/p&gt;
&lt;p&gt;
Efficient CNNs via Passive Filter Pruning. (arXiv:2304.02319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#26469;&#36798;&#21040;&#39640;&#25928;CNN&#30340;&#30446;&#30340;&#65292;&#35299;&#20915;&#20102;&#39640;&#21098;&#26525;&#29575;&#19979;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#23545;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#22120;&#30340;&#35201;&#27714;&#65292;CNN&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#26368;&#36817;&#23454;&#29616;CNN&#35745;&#31639;&#25928;&#29575;&#30340;&#21162;&#21147;&#21253;&#25324;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#65292;&#20854;&#26681;&#25454;&#28388;&#27874;&#22120;&#30340;&#8220;&#37325;&#35201;&#24615;&#8221;&#28040;&#38500;CNN&#20013;&#30340;&#26576;&#20123;&#28388;&#27874;&#22120;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#35201;&#20040;&#26159;&#8220;&#20027;&#21160;&#22411;&#8221;&#65292;&#21363;&#20351;&#29992;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#29305;&#24449;&#26144;&#23556;&#26469;&#37327;&#21270;&#28388;&#27874;&#22120;&#30340;&#37325;&#35201;&#24615;&#65307;&#35201;&#20040;&#26159;&#8220;&#34987;&#21160;&#22411;&#8221;&#65292;&#21363;&#20351;&#29992;&#28388;&#27874;&#22120;&#30340;&#36880;&#20803;&#32032;&#33539;&#25968;&#35745;&#31639;&#28388;&#27874;&#22120;&#30340;&#37325;&#35201;&#24615;&#32780;&#19981;&#28041;&#21450;&#25968;&#25454;&#12290;&#22312;&#39640;&#21098;&#26525;&#29575;&#19979;&#65292;&#21363;&#38656;&#35201;&#20174;&#32593;&#32476;&#20013;&#21098;&#26525;&#22823;&#37327;&#30340;&#28388;&#27874;&#22120;&#26102;&#65292;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#28040;&#38500;&#30456;&#23545;&#36739;&#23567;&#30340;&#33539;&#25968;&#28388;&#27874;&#22120;&#32780;&#19981;&#32771;&#34385;&#28388;&#27874;&#22120;&#22312;&#20135;&#29983;&#33410;&#28857;&#36755;&#20986;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have shown state-of-the-art performance in various applications. However, CNNs are resource-hungry due to their requirement of high computational complexity and memory storage. Recent efforts toward achieving computational efficiency in CNNs involve filter pruning methods that eliminate some of the filters in CNNs based on the \enquote{importance} of the filters. The majority of existing filter pruning methods are either "active", which use a dataset and generate feature maps to quantify filter importance, or "passive", which compute filter importance using entry-wise norm of the filters without involving data. Under a high pruning ratio where large number of filters are to be pruned from the network, the entry-wise norm methods eliminate relatively smaller norm filters without considering the significance of the filters in producing the node output, resulting in degradation in the performance. To address this, we present a passive filter pruning me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#19981;&#21464;&#23398;&#20064;&#31639;&#27861;&#22312;&#35266;&#27979;&#25968;&#25454;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#22810;&#29615;&#22659;&#27979;&#35797;&#65292;&#20351;&#29992;&#22810;&#20010;&#35757;&#32451;&#29615;&#22659;&#26469;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#22240;&#26524;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.02286</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21464;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
A step towards the applicability of algorithms based on invariant causal learning on observational data. (arXiv:2304.02286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#19981;&#21464;&#23398;&#20064;&#31639;&#27861;&#22312;&#35266;&#27979;&#25968;&#25454;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#22810;&#29615;&#22659;&#27979;&#35797;&#65292;&#20351;&#29992;&#22810;&#20010;&#35757;&#32451;&#29615;&#22659;&#26469;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#22240;&#26524;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20174;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#20013;&#33719;&#30410;&#65292;&#20197;&#20415;&#36827;&#34892;&#35299;&#37322;&#65292;&#24182;&#20174;&#22240;&#26524;&#25512;&#26029;&#20013;&#33719;&#30410;&#20197;&#36827;&#34892;&#25512;&#24191;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#20010;&#35757;&#32451;&#29615;&#22659;&#65292;&#20026;&#19981;&#21464;&#23416;&#32722;&#31639;&#27861;&#25552;&#20379;&#26381;&#21153;&#65292;&#35813;&#31639;&#27861;&#30340;&#19968;&#20123;&#26159;&#19987;&#27880;&#20110;&#22240;&#26524;&#21457;&#29616;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#30452;&#25509;&#25552;&#20379;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#22240;&#26524;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning can benefit from causal discovery for interpretation and from causal inference for generalization. In this line of research, a few invariant learning algorithms for out-of-distribution (OOD) generalization have been proposed by using multiple training environments to find invariant relationships. Some of them are focused on causal discovery as Invariant Causal Prediction (ICP), which finds causal parents of a variable of interest, and some directly provide a causal optimal predictor that generalizes well in OOD environments as Invariant Risk Minimization (IRM). This group of algorithms works under the assumption of multiple environments that represent different interventions in the causal inference context. Those environments are not normally available when working with observational data and real-world applications. Here we propose a method to generate them in an efficient way. We assess the performance of this unsupervised learning problem by implementing ICP on simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#33609;&#22270;&#30028;&#38480;&#65292;&#34920;&#26126;&#31232;&#30095;&#24674;&#22797;&#27604;&#31232;&#30095;&#22238;&#24402;&#26356;&#23481;&#26131;&#33609;&#22270;&#65292;&#23545;&#20110;&#31232;&#30095;l_p&#22238;&#24402;&#65292;&#20854;&#19978;&#30028;&#21253;&#25324;l_2&#30340;&#39069;&#22806;&#28155;&#21152;&#39033;&#12290;</title><link>http://arxiv.org/abs/2304.02261</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#33609;&#22270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Sketching Bounds for Sparse Linear Regression. (arXiv:2304.02261v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#33609;&#22270;&#30028;&#38480;&#65292;&#34920;&#26126;&#31232;&#30095;&#24674;&#22797;&#27604;&#31232;&#30095;&#22238;&#24402;&#26356;&#23481;&#26131;&#33609;&#22270;&#65292;&#23545;&#20110;&#31232;&#30095;l_p&#22238;&#24402;&#65292;&#20854;&#19978;&#30028;&#21253;&#25324;l_2&#30340;&#39069;&#22806;&#28155;&#21152;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#19979;k-&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#30340;&#36951;&#24536;&#33609;&#22270;&#65292;&#22914;l_p&#33539;&#25968;&#25110;&#24191;&#27867;&#30340;hinge-like&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;logistic&#21644;ReLU&#25439;&#22833;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#31232;&#30095;l_2&#33539;&#25968;&#22238;&#24402;&#65292;&#23384;&#22312;&#19968;&#20010;&#36951;&#24536;&#33609;&#22270;&#20998;&#24067;&#65292;&#20855;&#26377;&#920;(klog(d/k)/&#949;^2)&#25490;&#65292;&#36825;&#26159;&#32039;&#30340;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#36825;&#25193;&#23637;&#21040;l_p&#25439;&#22833;&#65292;&#19978;&#30028;&#36824;&#26377;&#19968;&#20010;&#38468;&#21152;&#30340;O(klog(k/&#949;)/&#949;^2)&#39033;&#12290;&#36825;&#24314;&#31435;&#20102;&#19982;&#30456;&#20851;&#30340;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#30340;&#20986;&#20154;&#24847;&#26009;&#30340;&#20998;&#31163;&#65292;&#36825;&#26159;&#31232;&#30095;&#22238;&#24402;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;l_2&#33539;&#25968;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;O(klog(d)/&#949;+klog(k/&#949;)/&#949;^2)&#34892;&#30340;&#19978;&#30028;&#65292;&#34920;&#26126;&#31232;&#30095;&#24674;&#22797;&#27604;&#31232;&#30095;&#22238;&#24402;&#26356;&#23481;&#26131;&#33609;&#22270;&#12290;&#23545;&#20110;&#21253;&#25324;&#31232;&#30095;logistic&#21644;&#31232;&#30095;ReLU&#22238;&#24402;&#22312;&#20869;&#30340;hinge-like&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#31232;&#30095;&#22238;&#24402;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19982;&#20043;&#23545;&#24212;&#30340;&#26368;&#20248;&#33609;&#22270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study oblivious sketching for $k$-sparse linear regression under various loss functions such as an $\ell_p$ norm, or from a broad class of hinge-like loss functions, which includes the logistic and ReLU losses. We show that for sparse $\ell_2$ norm regression, there is a distribution over oblivious sketches with $\Theta(k\log(d/k)/\varepsilon^2)$ rows, which is tight up to a constant factor. This extends to $\ell_p$ loss with an additional additive $O(k\log(k/\varepsilon)/\varepsilon^2)$ term in the upper bound. This establishes a surprising separation from the related sparse recovery problem, which is an important special case of sparse regression. For this problem, under the $\ell_2$ norm, we observe an upper bound of $O(k \log (d)/\varepsilon + k\log(k/\varepsilon)/\varepsilon^2)$ rows, showing that sparse recovery is strictly easier to sketch than sparse regression. For sparse regression under hinge-like loss functions including sparse logistic and sparse ReLU regression, we giv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02247</link><description>&lt;p&gt;
&#12298;&#35299;&#24320;&#32467;&#26500;&#19982;&#39118;&#26684;&#30340;&#32445;&#24102;&#65306;&#36890;&#36807;&#35825;&#23548;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12299;
&lt;/p&gt;
&lt;p&gt;
Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#25919;&#27835;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24046;&#36317;&#36827;&#34892;&#30740;&#31350;&#12290;&#20808;&#21069;&#36827;&#34892;&#30417;&#30563;&#24335;&#25991;&#26723;&#20998;&#31867;&#30340;&#24037;&#20316;&#21487;&#33021;&#20250;&#20559;&#21521;&#21508;&#32593;&#31449;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32423;&#35821;&#20041;&#21644;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#26356;&#24378;&#22823;&#21644;&#19981;&#21463;&#39118;&#26684;&#24433;&#21709;&#30340;&#26816;&#27979;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#27880;&#24847;&#21147;&#22836;&#30340;&#19981;&#21516;&#38598;&#21512;&#26377;&#25928;&#22320;&#32534;&#30721;&#38271;&#25991;&#26723;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#36825;&#31181;&#22495;&#20381;&#36182;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26032;&#38395;&#20013;&#24120;&#29992;&#30340;&#35805;&#35821;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22797;&#21046;&#23398;&#20064;&#31639;&#27861;&#30340;&#21015;&#34920;&#21644;&#35777;&#26126;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#21015;&#34920;&#22797;&#21046;&#21644;&#35777;&#20070;&#22797;&#21046;&#30340;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#19978;&#24314;&#31435;&#20102;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.02240</link><description>&lt;p&gt;
&#22797;&#21046;&#23398;&#20064;&#20013;&#30340;&#21015;&#34920;&#21644;&#35777;&#26126;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
List and Certificate Complexities in Replicable Learning. (arXiv:2304.02240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02240
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22797;&#21046;&#23398;&#20064;&#31639;&#27861;&#30340;&#21015;&#34920;&#21644;&#35777;&#26126;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#21015;&#34920;&#22797;&#21046;&#21644;&#35777;&#20070;&#22797;&#21046;&#30340;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#19978;&#24314;&#31435;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#22797;&#21046;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#31639;&#27861;&#65292;&#21363;&#20351;&#19981;&#21516;&#30340;&#36816;&#34892;&#35266;&#23519;&#21040;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#21516;&#26679;&#26412;&#38598;&#65292;&#20063;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#36755;&#20986;&#30456;&#21516;&#30340;&#35268;&#33539;&#27169;&#22411;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#24378;&#30340;&#22797;&#21046;&#24615;&#24182;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#21487;&#34892;&#30340;&#22797;&#21046;&#27010;&#24565;&#65292;&#31216;&#20026;&#21015;&#34920;&#22797;&#21046;&#21644;&#35777;&#20070;&#22797;&#21046;&#12290;&#30452;&#35266;&#22320;&#65292;&#36825;&#20123;&#27010;&#24565;&#25429;&#25417;&#20102;&#65288;&#26080;&#65289;&#22797;&#21046;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#21015;&#34920;&#21644;&#35777;&#20070;&#22797;&#26434;&#24230;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21305;&#37197;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate replicable learning algorithms. Ideally, we would like to design algorithms that output the same canonical model over multiple runs, even when different runs observe a different set of samples from the unknown data distribution. In general, such a strong notion of replicability is not achievable. Thus we consider two feasible notions of replicability called list replicability and certificate replicability. Intuitively, these notions capture the degree of (non) replicability. We design algorithms for certain learning problems that are optimal in list and certificate complexity. We establish matching impossibility results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25351;&#20986;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#22270;&#20687;&#25200;&#21160;&#26041;&#27861;&#24182;&#19981;&#33021;&#25269;&#24481;JPEG&#21387;&#32553;&#65292;&#22240;&#27492;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#32534;&#36753;&#21644;&#28145;&#24230;&#20266;&#36896;&#25915;&#20987;&#65292;&#24314;&#35758;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2304.02234</link><description>&lt;p&gt;
JPEG&#21387;&#32553;&#22270;&#20687;&#21487;&#20197;&#32469;&#36807;&#23545;&#25239;AI&#32534;&#36753;&#30340;&#20445;&#25252;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
JPEG Compressed Images Can Bypass Protections Against AI Editing. (arXiv:2304.02234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25351;&#20986;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#22270;&#20687;&#25200;&#21160;&#26041;&#27861;&#24182;&#19981;&#33021;&#25269;&#24481;JPEG&#21387;&#32553;&#65292;&#22240;&#27492;&#19981;&#33021;&#26377;&#25928;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#32534;&#36753;&#21644;&#28145;&#24230;&#20266;&#36896;&#25915;&#20987;&#65292;&#24314;&#35758;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20351;&#24471;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#32534;&#36753;&#25110;&#29983;&#25104;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#20854;&#26131;&#29992;&#24615;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#32534;&#36753;&#25110;&#28145;&#24230;&#20266;&#36896;&#30340;&#25285;&#24551;&#12290;&#20026;&#38450;&#27490;&#20256;&#25773;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24494;&#19981;&#21487;&#35265;&#30340;&#25200;&#21160;&#20316;&#20026;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25200;&#21160;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#19981;&#33021;&#32463;&#21463;JPEG&#21387;&#32553;&#30340;&#32771;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24369;&#28857;&#65292;&#22240;&#20026;JPEG&#20855;&#26377;&#26222;&#36941;&#30340;&#20351;&#29992;&#21644;&#21487;&#33719;&#21462;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21152;&#24615;&#24494;&#19981;&#21487;&#35265;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#40723;&#21169;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#26469;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02229</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#28151;&#21512;&#22238;&#24402;&#65288;Mixed Regression via Approximate Message Passing&#65289;
&lt;/p&gt;
&lt;p&gt;
Mixed Regression via Approximate Message Passing. (arXiv:2304.02229v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#20013;&#20855;&#26377;&#22810;&#20010;&#20449;&#21495;&#21644;&#28508;&#21464;&#37327;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#34987;&#31216;&#20026;&#30697;&#38453;GLM&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#28151;&#21512;&#32447;&#24615;&#22238;&#24402;&#12289;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#31639;&#27861;&#26469;&#20272;&#35745;&#30697;&#38453;GLM&#20013;&#30340;&#20449;&#21495;&#21644;&#28508;&#21464;&#37327;&#65292;&#24182;&#22312;&#39640;&#32500;&#26497;&#38480;&#20013;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#34920;&#24449;&#12290;&#35813;&#34920;&#24449;&#26159;&#36890;&#36807;&#29366;&#24577;&#28436;&#21270;&#36882;&#24402;&#26469;&#35745;&#31639;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#28176;&#36817;&#24615;&#33021;&#24230;&#37327;&#65292;&#20363;&#22914;&#20449;&#22122;&#27604;&#19979;&#38477;&#38408;&#20540;&#65288;threshold&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables. This model, which we call a matrix GLM, covers many widely studied problems in statistical learning, including mixed linear regression, max-affine regression, and mixture-of-experts. In mixed linear regression, each observation comes from one of $L$ signal vectors (regressors), but we do not know which one; in max-affine regression, each observation comes from the maximum of $L$ affine functions, each defined via a different signal vector. The goal in all these problems is to estimate the signals, and possibly some of the latent variables, from the observations. We propose a novel approximate message passing (AMP) algorithm for estimation in a matrix GLM and rigorously characterize its performance in the high-dimensional limit. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02223</link><description>&lt;p&gt;
&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Intrinsic Dimensional Entropy. (arXiv:2304.02223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29109;&#27979;&#37327;&#20381;&#36182;&#20110;&#27010;&#29575;&#20998;&#24067;&#22312;&#26679;&#26412;&#31354;&#38388;X&#19978;&#30340;&#23637;&#24067;&#24773;&#20917;&#65292;&#26368;&#22823;&#21487;&#23454;&#29616;&#29109;&#19982;&#26679;&#26412;&#31354;&#38388;&#22522;&#25968;|X|&#25104;&#27604;&#20363;&#12290;&#23545;&#20110;&#26377;&#38480;|X|&#65292;&#36825;&#20135;&#29983;&#20102;&#28385;&#36275;&#35768;&#22810;&#37325;&#35201;&#23646;&#24615;&#65288;&#22914;&#23545;&#21452;&#23556;&#30340;&#19981;&#21464;&#24615;&#65289;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#32780;&#21516;&#26679;&#19981;&#33021;&#28385;&#36275;&#36830;&#32493;&#31354;&#38388;&#30340;&#35201;&#27714;&#65288;&#20854;&#20013;|X|=&#26080;&#31351;&#22823;&#65289;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;R&#21644;R^d&#65288;d&#22312;Z+&#20013;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#22522;&#25968;&#65288;&#26469;&#33258;Cantor&#30340;&#23545;&#24212;&#35770;&#35777;&#65289;&#65292;&#22522;&#25968;&#20381;&#36182;&#24615;&#29109;&#27979;&#37327;&#26080;&#27861;&#32534;&#30721;&#25968;&#25454;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#23545;&#36830;&#32493;&#31354;&#38388;&#23450;&#20041;&#29109;&#27979;&#37327;&#20013;&#22522;&#25968;&#21644;&#20998;&#24067;&#23637;&#24067;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#36830;&#32493;&#31354;&#38388;&#21487;&#20197;&#36827;&#34892;&#22810;&#36718;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#20363;&#22914;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;&#29992;&#20998;&#24067;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#30340;&#24179;&#22343;&#20540;&#26469;&#34920;&#31034;&#27979;&#37327;&#29109;&#65292;&#34987;&#31216;&#20026;ID-Entropy&#65292;&#37027;&#20040;&#21487;&#20197;&#20316;&#20026;&#36830;&#32493;&#31354;&#38388;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#21516;&#26102;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most entropy measures depend on the spread of the probability distribution over the sample space X, and the maximum entropy achievable scales proportionately with the sample space cardinality |X|. For a finite |X|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;SSAD&#30340;&#38646;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20854;&#36866;&#24212;&#30446;&#26631;&#22495;&#20013;&#32570;&#23569;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22495;&#23545;&#25239;&#32593;&#32476;&#21644;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.02221</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot domain adaptation of anomalous samples for semi-supervised anomaly detection. (arXiv:2304.02221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;SSAD&#30340;&#38646;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20854;&#36866;&#24212;&#30446;&#26631;&#22495;&#20013;&#32570;&#23569;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22495;&#23545;&#25239;&#32593;&#32476;&#21644;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#26159;&#19968;&#31181;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#24456;&#38590;&#33719;&#24471;&#30446;&#26631;&#22495;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#22240;&#27492;SSAD&#26041;&#27861;&#19981;&#23481;&#26131;&#36866;&#24212;&#22495;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;SSAD&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#22495;&#27809;&#26377;&#21487;&#29992;&#30340;&#24322;&#24120;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21521;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;SSAD&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#22495;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#33719;&#24471;&#22495;&#19981;&#21464;&#28508;&#21464;&#37327;&#12290;&#30001;&#20110;&#35299;&#30721;&#22120;&#19981;&#33021;&#20165;&#20174;&#22495;&#19981;&#21464;&#28508;&#21464;&#37327;&#37325;&#26500;&#21407;&#22987;&#25968;&#25454;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#35299;&#30721;&#22120;&#24314;&#31435;&#22312;&#22495;&#26631;&#31614;&#20043;&#19978;&#12290;&#20026;&#20102;&#24357;&#34917;&#30446;&#26631;&#22495;&#32570;&#23569;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#20351;SSAD&#27169;&#22411;&#36866;&#24212;&#30446;&#26631;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection~(SSAD) is a task where normal data and a limited number of anomalous data are available for training. In practical situations, SSAD methods suffer adapting to domain shifts, since anomalous data are unlikely to be available for the target domain in the training phase. To solve this problem, we propose a domain adaptation method for SSAD where no anomalous data are available for the target domain. First, we introduce a domain-adversarial network to a variational auto-encoder-based SSAD model to obtain domain-invariant latent variables. Since the decoder cannot reconstruct the original data solely from domain-invariant latent variables, we conditioned the decoder on the domain label. To compensate for the missing anomalous data of the target domain, we introduce an importance sampling-based weighted loss function that approximates the ideal loss function. Experimental results indicate that the proposed method helps adapt SSAD models to the target domain 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#65292;&#36824;&#35752;&#35770;&#20102;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.02220</link><description>&lt;p&gt;
&#20851;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the universal approximation property of radial basis function neural networks. (arXiv:2304.02220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#65292;&#36824;&#35752;&#35770;&#20102;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#30340;&#36924;&#36817;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#20854;&#20013;&#24179;&#28369;&#22240;&#23376;&#34987;&#26367;&#25442;&#20026;&#20301;&#31227;&#12290;&#25105;&#20204;&#22312;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#23450;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;d&#32500;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#22810;&#20803;&#20989;&#25968;&#12290;&#23545;&#20110;&#26377;&#38480;&#20010;&#22266;&#23450;&#20013;&#24515;&#30340;RBF&#32593;&#32476;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20445;&#35777;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider a new class of RBF (Radial Basis Function) neural networks, in which smoothing factors are replaced with shifts. We prove under certain conditions on the activation function that these networks are capable of approximating any continuous multivariate function on any compact subset of the $d$-dimensional Euclidean space. For RBF networks with finitely many fixed centroids we describe conditions guaranteeing approximation with arbitrary precision.
&lt;/p&gt;</description></item><item><title>PIKS&#26159;&#19968;&#31181;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20013;&#24322;&#24120;&#20540;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24555;&#36895;&#21457;&#29616;&#36235;&#21183;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.02208</link><description>&lt;p&gt;
PIKS&#65306;&#19968;&#31181;&#36890;&#36807;&#24320;&#25918;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#35782;&#21035;&#21487;&#25805;&#20316;&#36235;&#21183;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PIKS: A Technique to Identify Actionable Trends for Policy-Makers Through Open Healthcare Data. (arXiv:2304.02208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02208
&lt;/p&gt;
&lt;p&gt;
PIKS&#26159;&#19968;&#31181;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20013;&#24322;&#24120;&#20540;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24555;&#36895;&#21457;&#29616;&#36235;&#21183;&#21644;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#37325;&#35201;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35201;&#27714;&#25552;&#39640;&#36879;&#26126;&#24230;&#65292;&#25919;&#24220;&#22312;&#22810;&#20010;&#39046;&#22495;&#21253;&#25324;&#36130;&#25919;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#21457;&#24067;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#26377;&#25928;&#22320;&#25506;&#32034;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20844;&#20849;&#21355;&#29983;&#30340;&#20851;&#38190;&#38382;&#39064;&#21253;&#25324;&#24555;&#36895;&#35782;&#21035;&#21644;&#20998;&#26512;&#36235;&#21183;&#20197;&#21450;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#36825;&#21487;&#20197;&#24555;&#36895;&#35843;&#25972;&#25919;&#31574;&#20197;&#24212;&#23545;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#25216;&#26415;&#65292;&#31216;&#20026; PIKS&#65288;&#22522;&#20110;&#20462;&#21098;&#30340;&#36845;&#20195; K &#22343;&#20540;&#20142;&#28857;&#25628;&#32034;&#65289;&#65292;&#23427;&#23558;&#36845;&#20195; k &#22343;&#20540;&#31639;&#27861;&#19982;&#20462;&#21098;&#25628;&#32034;&#20142;&#28857;&#25195;&#25551;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#27492;&#25216;&#26415;&#24212;&#29992;&#20110;&#35782;&#21035;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#65306;&#32445;&#32422;&#24030;&#20840;&#38754;&#35268;&#21010;&#21644;&#30740;&#31350;&#21512;&#20316;&#31995;&#32479;&#21644;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#20840;&#38754;&#21355;&#29983;&#35268;&#21010;&#21644;&#21457;&#23637;&#21381;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#19977;&#31181;&#29616;&#26377;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#25216;&#26415;&#65288;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#12289;&#23396;&#31435;&#26862;&#26519;&#21644;&#28145;&#24230;&#33258;&#22238;&#24402;&#32593;&#32476;&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;PIKS&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#32445;&#32422;&#24030;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;PIKS&#35782;&#21035;&#21487;&#25805;&#20316;&#36235;&#21183;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With calls for increasing transparency, governments are releasing greater amounts of data in multiple domains including finance, education and healthcare. The efficient exploratory analysis of healthcare data constitutes a significant challenge. Key concerns in public health include the quick identification and analysis of trends, and the detection of outliers. This allows policies to be rapidly adapted to changing circumstances. We present an efficient outlier detection technique, termed PIKS (Pruned iterative-k means searchlight), which combines an iterative k-means algorithm with a pruned searchlight based scan. We apply this technique to identify outliers in two publicly available healthcare datasets from the New York Statewide Planning and Research Cooperative System, and California's Office of Statewide Health Planning and Development. We provide a comparison of our technique with three other existing outlier detection techniques, consisting of auto-encoders, isolation forests an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#27169;&#26495;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#35299;&#37322;&#24615;&#65292;&#22635;&#34917;&#20102;&#22522;&#20110;&#28909;&#21147;&#22270;&#30340;&#21487;&#35299;&#37322;AI&#33258;&#21160;&#21270;&#12289;&#20132;&#20114;&#24335;&#12289;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.02202</link><description>&lt;p&gt;
&#29992;&#28909;&#21147;&#22270;&#23383;&#24149;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#35299;&#37322;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models. (arXiv:2304.02202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#27169;&#26495;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#35299;&#37322;&#24615;&#65292;&#22635;&#34917;&#20102;&#22522;&#20110;&#28909;&#21147;&#22270;&#30340;&#21487;&#35299;&#37322;AI&#33258;&#21160;&#21270;&#12289;&#20132;&#20114;&#24335;&#12289;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#21147;&#22270;&#22312;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#28909;&#21147;&#22270;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#26159;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#25552;&#39640;&#29983;&#25104;&#28909;&#21147;&#22270;&#30340;&#36136;&#37327;&#25110;&#21457;&#29616;&#26367;&#20195;&#28909;&#21147;&#22270;&#29983;&#25104;&#25216;&#26415;&#65292;&#32780;&#24456;&#23569;&#26377;&#20154;&#20184;&#20986;&#21162;&#21147;&#20351;&#22522;&#20110;&#28909;&#21147;&#22270;&#30340;&#21487;&#35299;&#37322;AI&#33258;&#21160;&#21270;&#12289;&#20132;&#20114;&#24335;&#12289;&#21487;&#25193;&#23637;&#12289;&#26131;&#20110;&#35775;&#38382;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#30340;&#26694;&#26550;&#65306;(1) &#19978;&#19979;&#25991;&#24314;&#27169;&#21644;(2) &#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#26495;&#30340;&#22270;&#20687;&#23383;&#24149;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#20174;&#28909;&#21147;&#22270;&#21644;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25512;&#29702;&#27169;&#22359;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#19987;&#19994;&#30693;&#35782;&#25552;&#20379;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#21644;&#28909;&#21147;&#22270;&#23383;&#24149;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#28909;&#21147;&#22270;&#23383;&#24149;&#26041;&#27861;&#30340;&#20195;&#30721;&#24050;&#22312;&#32593;&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heatmaps are widely used to interpret deep neural networks, particularly for computer vision tasks, and the heatmap-based explainable AI (XAI) techniques are a well-researched topic. However, most studies concentrate on enhancing the quality of the generated heatmap or discovering alternate heatmap generation techniques, and little effort has been devoted to making heatmap-based XAI automatic, interactive, scalable, and accessible. To address this gap, we propose a framework that includes two modules: (1) context modelling and (2) reasoning. We proposed a template-based image captioning approach for context modelling to create text-based contextual information from the heatmap and input data. The reasoning module leverages a large language model to provide explanations in combination with specialised knowledge. Our qualitative experiments demonstrate the effectiveness of our framework and heatmap captioning approach. The code for the proposed template-based heatmap captioning approach 
&lt;/p&gt;</description></item><item><title>EigenFold&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#37319;&#26679;&#32467;&#26500;&#20998;&#24067;&#38598;&#21512;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;TMScore&#24182;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#33021;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#25240;&#21472;&#36716;&#25442;&#34507;&#30333;&#21644;&#37197;&#20307;&#35825;&#23548;&#26500;&#35937;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02198</link><description>&lt;p&gt;
EigenFold&#65306;&#25193;&#25955;&#27169;&#22411;&#19979;&#30340;&#29983;&#25104;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EigenFold: Generative Protein Structure Prediction with Diffusion Models. (arXiv:2304.02198v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02198
&lt;/p&gt;
&lt;p&gt;
EigenFold&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20174;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#37319;&#26679;&#32467;&#26500;&#20998;&#24067;&#38598;&#21512;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;TMScore&#24182;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#33021;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#25240;&#21472;&#36716;&#25442;&#34507;&#30333;&#21644;&#37197;&#20307;&#35825;&#23548;&#26500;&#35937;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20010;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#24050;&#32463;&#36798;&#21040;&#20102;&#38761;&#21629;&#24615;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#38656;&#35201;&#20998;&#24067;&#24335;&#24314;&#27169;&#33539;&#24335;&#20197;&#25429;&#25417;&#25903;&#37197;&#29983;&#29289;&#21151;&#33021;&#30340;&#26500;&#35937;&#38598;&#21512;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;EigenFold&#65292;&#19968;&#31181;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#37319;&#26679;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25193;&#25955;&#36807;&#31243;&#65292;&#23558;&#32467;&#26500;&#24314;&#27169;&#20026;&#35856;&#25391;&#23376;&#31995;&#32479;&#65292;&#24182;&#33258;&#28982;&#22320;&#27839;&#30528;&#31995;&#32479;&#30340;&#26412;&#24449;&#27169;&#24335;&#35825;&#23548;&#20986;&#19968;&#31181;&#32423;&#32852;&#20998;&#36776;&#29575;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#26368;&#36817;&#30340;CAMEO&#30446;&#26631;&#20013;&#65292;EigenFold&#36798;&#21040;&#20102;0.84&#30340;&#20013;&#20301;TMScore&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#32467;&#26500;&#38598;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35780;&#20272;&#20102;EigenFold&#23545;&#20110;&#25240;&#21472;&#36716;&#25442;&#34507;&#30333;&#21644;&#37197;&#20307;&#35825;&#23548;&#26500;&#35937;&#21464;&#21270;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/bjing2016/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein structure prediction has reached revolutionary levels of accuracy on single structures, yet distributional modeling paradigms are needed to capture the conformational ensembles and flexibility that underlie biological function. Towards this goal, we develop EigenFold, a diffusion generative modeling framework for sampling a distribution of structures from a given protein sequence. We define a diffusion process that models the structure as a system of harmonic oscillators and which naturally induces a cascading-resolution generative process along the eigenmodes of the system. On recent CAMEO targets, EigenFold achieves a median TMScore of 0.84, while providing a more comprehensive picture of model uncertainty via the ensemble of sampled structures relative to existing methods. We then assess EigenFold's ability to model and predict conformational heterogeneity for fold-switching proteins and ligand-induced conformational change. Code is available at https://github.com/bjing2016/
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#32445;&#32422;&#24030; SPARCS &#30340;&#21311;&#21517;&#24739;&#32773;&#25968;&#25454;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#30149;&#20154;&#30340;&#21307;&#30103;&#25104;&#26412;&#65292;&#26368;&#22909;&#34920;&#29616;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#20915;&#31574;&#26641;&#65292;&#33719;&#24471;&#20102; 0.76 &#30340; R-square &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.02191</link><description>&lt;p&gt;
&#21033;&#29992;&#24320;&#25918;&#24335;&#21307;&#30103;&#25968;&#25454;&#26500;&#24314;&#21307;&#30103;&#25104;&#26412;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building predictive models of healthcare costs with open healthcare data. (arXiv:2304.02191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02191
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#32445;&#32422;&#24030; SPARCS &#30340;&#21311;&#21517;&#24739;&#32773;&#25968;&#25454;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#30149;&#20154;&#30340;&#21307;&#30103;&#25104;&#26412;&#65292;&#26368;&#22909;&#34920;&#29616;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#20915;&#31574;&#26641;&#65292;&#33719;&#24471;&#20102; 0.76 &#30340; R-square &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20840;&#29699;&#21307;&#30103;&#25104;&#26412;&#30340;&#24555;&#36895;&#19978;&#28072;&#65292;&#25511;&#21046;&#21307;&#30103;&#25104;&#26412;&#24050;&#25104;&#26174;&#33879;&#30340;&#20851;&#27880;&#28857;&#65292;&#20854;&#20851;&#38190;&#20043;&#19968;&#22312;&#20110;&#20215;&#26684;&#36879;&#26126;&#24230;&#65292;&#30149;&#20154;&#20250;&#36873;&#20215;&#26684;&#36739;&#20302;&#30340;&#21307;&#21153;&#26381;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#36825;&#38656;&#35201;&#25968;&#25454;&#20844;&#24320;&#65292;&#20197;&#21450;&#33021;&#39044;&#27979;&#21508;&#31181;&#30149;&#20154;&#29305;&#24449;&#21644;&#29366;&#20917;&#19979;&#21307;&#30103;&#25104;&#26412;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32445;&#32422;&#24030;SPARCS&#65288;&#24030;&#32423;&#35268;&#21010;&#21644;&#30740;&#31350;&#21512;&#20316;&#31995;&#32479;&#65289;&#30340;2.3&#30334;&#19975;&#35760;&#24405;&#30340;&#21435;&#36523;&#20221;&#21270;&#30149;&#20154;&#25968;&#25454;&#65292;&#20174;&#30149;&#20154;&#35786;&#26029;&#21644;&#29305;&#24449;&#20013;&#26500;&#24314;&#25104;&#26412;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#31232;&#30095;&#22238;&#24402;&#21644;&#20915;&#31574;&#26641;&#20004;&#31867;&#27169;&#22411;&#26500;&#25104;&#30340;&#27169;&#22411;&#65292;&#26368;&#32456;&#20351;&#29992;&#28145;&#24230;&#20026;10&#30340;&#20915;&#31574;&#26641;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;0.76&#30340;R-square&#20540;&#65292;&#20248;&#20110;&#25991;&#29486;&#25253;&#36947;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to rapidly rising healthcare costs worldwide, there is significant interest in controlling them. An important aspect concerns price transparency, as preliminary efforts have demonstrated that patients will shop for lower costs, driving efficiency. This requires the data to be made available, and models that can predict healthcare costs for a wide range of patient demographics and conditions. We present an approach to this problem by developing a predictive model using machine-learning techniques. We analyzed de-identified patient data from New York State SPARCS (statewide planning and research cooperative system), consisting of 2.3 million records in 2016. We built models to predict costs from patient diagnoses and demographics. We investigated two model classes consisting of sparse regression and decision trees. We obtained the best performance by using a decision tree with depth 10. We obtained an R-square value of 0.76 which is better than the values reported in the literature f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20197;&#38750;&#27954;&#21355;&#29983;&#39046;&#22495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#30340;&#20307;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#21307;&#30103;&#27169;&#24335;&#65292;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;</title><link>http://arxiv.org/abs/2304.02190</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#65306;&#20197;&#38750;&#27954;&#20581;&#24247;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Globalizing Fairness Attributes in Machine Learning: A Case Study on Health in Africa. (arXiv:2304.02190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20197;&#38750;&#27954;&#21355;&#29983;&#39046;&#22495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#30340;&#20307;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#21307;&#30103;&#27169;&#24335;&#65292;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24212;&#29992;&#26085;&#30410;&#22686;&#21152;&#65292;&#26377;&#20154;&#21628;&#21505;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#23454;&#29616;&#20844;&#24179;&#65292;&#20197;&#20102;&#35299;&#21644;&#32531;&#35299;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#20844;&#24179;&#23545;&#38750;&#27954;&#30340;&#20840;&#29699;&#21355;&#29983;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#20840;&#29699;&#21335;&#21271;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#26435;&#21147;&#22833;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#38750;&#27954;&#20581;&#24247;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#27954;&#35821;&#22659;&#19979;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#23646;&#24615;&#65292;&#24182;&#21246;&#30011;&#20102;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#21307;&#23398;&#27169;&#24335;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#33539;&#22260;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing machine learning (ML) applications in healthcare, there have been calls for fairness in ML to understand and mitigate ethical concerns these systems may pose. Fairness has implications for global health in Africa, which already has inequitable power imbalances between the Global North and South. This paper seeks to explore fairness for global health, with Africa as a case study. We propose fairness attributes for consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. This work serves as a basis and call for action for furthering research into fairness in global health.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#22823;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#25628;&#32034;&#20809;&#26631;&#25216;&#26415;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#31181;&#21464;&#37327;&#32452;&#21512;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#21487;&#24212;&#29992;&#20110;&#24320;&#25918;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#19968;&#20123;&#21307;&#38498;&#30340;&#32489;&#25928;&#22987;&#32456;&#20302;&#20110;&#21516;&#34892;&#65292;&#24182;&#20934;&#30830;&#22320;&#25351;&#20986;&#20855;&#20307;&#30340;&#20020;&#24202;&#29366;&#20917;&#19982;&#39640;&#20110;&#39044;&#26399;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.02189</link><description>&lt;p&gt;
&#19968;&#31181;&#25506;&#32034;&#22823;&#25968;&#25454;&#30340;&#31995;&#32479;&#65306;&#22522;&#20110;&#36845;&#20195;k-means&#25628;&#32034;&#20809;&#26631;&#30340;&#24320;&#25918;&#21307;&#30103;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A system for exploring big data: an iterative k-means searchlight for outlier detection on open health data. (arXiv:2304.02189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#22823;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#25628;&#32034;&#20809;&#26631;&#25216;&#26415;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#31181;&#21464;&#37327;&#32452;&#21512;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#21487;&#24212;&#29992;&#20110;&#24320;&#25918;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#19968;&#20123;&#21307;&#38498;&#30340;&#32489;&#25928;&#22987;&#32456;&#20302;&#20110;&#21516;&#34892;&#65292;&#24182;&#20934;&#30830;&#22320;&#25351;&#20986;&#20855;&#20307;&#30340;&#20020;&#24202;&#29366;&#20917;&#19982;&#39640;&#20110;&#39044;&#26399;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#20132;&#20114;&#24335;&#25506;&#32034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24213;&#23618;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#12290;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#20998;&#26512;&#30340;&#38544;&#34255;&#36235;&#21183;&#21644;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#25628;&#32034;&#20809;&#26631;&#25216;&#26415;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#31181;&#21464;&#37327;&#32452;&#21512;&#24182;&#35782;&#21035;&#24322;&#24120;&#20540;&#12290;&#22312;&#25968;&#25454;&#24211;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;-&#24212;&#29992;-&#32452;&#21512;&#33539;&#24335;&#20013;&#65292;&#24212;&#29992;&#20102;&#36845;&#20195;&#30340;k-means&#32858;&#31867;&#31639;&#27861;&#26469;&#29983;&#25104;&#29305;&#24449;&#12290;&#24322;&#24120;&#20540;&#34987;&#35782;&#21035;&#20026;&#21333;&#20010;&#20540;&#25110;&#23567;&#31751;&#12290;&#36825;&#20010;&#31639;&#27861;&#20197;&#25628;&#32034;&#20809;&#26631;&#30340;&#26041;&#24335;&#25195;&#36807;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#23376;&#38598;&#25195;&#25551;&#25216;&#26415;&#65292;&#23558;&#21253;&#21547;&#24322;&#24120;&#20540;&#30340;&#32500;&#24230;&#19982;&#20854;&#20182;&#32500;&#24230;&#25104;&#23545;&#32452;&#21512;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#32445;&#32422;&#24030;&#21457;&#24067;&#30340;&#24320;&#25918;&#21355;&#29983;&#20445;&#20581;&#25968;&#25454;&#26469;&#35828;&#26126;&#36825;&#20010;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#30340;k-means&#25628;&#32034;&#20809;&#26631;&#21518;&#36319;&#23376;&#38598;&#25195;&#25551;&#12290;&#21487;&#20197;&#35782;&#21035;&#20986;&#20960;&#20010;&#24322;&#24120;&#36235;&#21183;&#65292;&#20363;&#22914;&#35782;&#21035;&#20986;&#19968;&#20123;&#21307;&#38498;&#30340;&#32489;&#25928;&#22987;&#32456;&#20302;&#20110;&#21516;&#34892;&#65292;&#24182;&#20934;&#30830;&#22320;&#25351;&#20986;&#20855;&#20307;&#30340;&#20020;&#24202;&#29366;&#20917;&#19982;&#39640;&#20110;&#39044;&#26399;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactive exploration of large and evolving datasets is challenging as relationships between underlying variables may not be fully understood. There may be hidden trends and patterns in the data that are worthy of further exploration and analysis. We present a system that methodically explores multiple combinations of variables using a searchlight technique and identifies outliers. An iterative k-means clustering algorithm is applied to features derived through a split-apply-combine paradigm used in the database literature. Outliers are identified as singleton or small clusters. This algorithm is swept across the dataset in a searchlight manner. The dimensions that contain outliers are combined in pairs with other dimensions using a susbset scan technique to gain further insight into the outliers. We illustrate this system by anaylzing open health care data released by New York State. We apply our iterative k-means searchlight followed by subset scanning. Several anomalous trends
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20998;&#26512;&#20102;&#19981;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#20013;&#30340;&#38750;&#20984;&#24615;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#23558;&#26356;&#22810;&#22320;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#21644;&#24050;&#30693;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02146</link><description>&lt;p&gt;
&#24102;&#36830;&#32493;&#20248;&#21270;&#30340;&#32467;&#26500;&#23398;&#20064;&#65306;&#23457;&#24910;&#35266;&#23519;&#21450;&#20854;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Structure Learning with Continuous Optimization: A Sober Look and Beyond. (arXiv:2304.02146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20998;&#26512;&#20102;&#19981;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#20013;&#30340;&#38750;&#20984;&#24615;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#23558;&#26356;&#22810;&#22320;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#21644;&#24050;&#30693;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#22909;&#22351;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#20351;&#25628;&#32034;&#36807;&#31243;&#26356;&#21487;&#38752;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36830;&#32493;&#26041;&#27861;&#22312;&#20551;&#35774;&#22122;&#22768;&#26041;&#24046;&#30456;&#31561;&#21644;&#19981;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#21453;&#20363;&#12289;&#29702;&#35770;&#35777;&#26126;&#21644;&#21487;&#33021;&#30340;&#26367;&#20195;&#35299;&#37322;&#26469;&#34920;&#26126;&#36825;&#31181;&#38472;&#36848;&#22312;&#20219;&#19968;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#65292;&#38750;&#20984;&#24615;&#21487;&#33021;&#26159;&#20027;&#35201;&#38382;&#39064;&#65292;&#32780;&#36830;&#32493;&#32467;&#26500;&#23398;&#20064;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21017;&#26080;&#27861;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#23454;&#29616;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#36138;&#24515;&#25628;&#32034;&#65292;&#24182;&#24314;&#35758;&#34701;&#21512;&#20808;&#39564;&#30693;&#35782;&#25110;&#24050;&#30693;&#32467;&#26500;&#30340;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates in which cases continuous optimization for directed acyclic graph (DAG) structure learning can and cannot perform well and why this happens, and suggests possible directions to make the search procedure more reliable. Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order, and demonstrated that these approaches do not perform well after data standardization. We analyze this phenomenon for continuous approaches assuming equal and non-equal noise variances, and show that the statement may not hold in either case by providing counterexamples, justifications, and possible alternative explanations. We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#23545;&#25968;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#21464;&#37327;&#23398;&#20064;&#27169;&#22411;&#22312;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#19979;&#24471;&#20998;&#36755;&#20986;&#30340;&#26368;&#20248;&#21333;&#23792;&#36716;&#25442;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02141</link><description>&lt;p&gt;
&#32447;&#24615;&#25439;&#22833;&#19979;&#30340;&#26368;&#20248;&#21333;&#23792;&#25311;&#21512;&#38382;&#39064;&#30340;&#32447;&#24615;&#23545;&#25968;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sequential Linearithmic Time Optimal Unimodal Fitting When Minimizing Univariate Linear Losses. (arXiv:2304.02141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#23545;&#25968;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21333;&#21464;&#37327;&#23398;&#20064;&#27169;&#22411;&#22312;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#19979;&#24471;&#20998;&#36755;&#20986;&#30340;&#26368;&#20248;&#21333;&#23792;&#36716;&#25442;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#21333;&#21464;&#37327;&#23398;&#20064;&#27169;&#22411;&#22312;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#19979;&#24471;&#20998;&#36755;&#20986;&#30340;&#26368;&#20248;&#21333;&#23792;&#36716;&#25442;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#25968;&#38754;&#20540;&#21644;&#30446;&#26631;&#21306;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#26144;&#23556;&#26159;&#19968;&#20010;&#30697;&#24418;&#20989;&#25968;&#12290;&#20026;&#20102;&#33719;&#24471;&#35266;&#27979;&#26679;&#26412;&#30340;&#26368;&#20248;&#30697;&#24418;&#25311;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#26032;&#26679;&#26412;&#21040;&#26469;&#26102;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27599;&#27425;&#36845;&#20195;&#20855;&#26377;&#23545;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20248;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on optimal unimodal transformation of the score outputs of a univariate learning model under linear loss functions. We demonstrate that the optimal mapping between score values and the target region is a rectangular function. To produce this optimal rectangular fit for the observed samples, we propose a sequential approach that can its estimation with each incoming new sample. Our approach has logarithmic time complexity per iteration and is optimally efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#20010;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02119</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Initialization Approach for Nonlinear State-Space Identification via the Subspace Encoder Approach. (arXiv:2304.02119v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#20010;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUBNET&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#34987;&#24320;&#21457;&#29992;&#20110;&#20174;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23427;&#23558;&#23637;&#24320;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#26041;&#31243;&#21644;&#29366;&#24577;&#32534;&#30721;&#22120;&#20989;&#25968;&#32452;&#21512;&#36215;&#26469;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#24341;&#20837;&#32534;&#30721;&#22120;&#20989;&#25968;&#26469;&#20174;&#36807;&#21435;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#37325;&#26500;&#24403;&#21069;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#23637;&#24320;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24471;&#20197;&#21069;&#21521;&#27169;&#25311;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#21644;&#19968;&#33268;&#30340;&#27169;&#22411;&#20272;&#35745;&#65292;&#20294;&#26159;&#36890;&#36807;&#26377;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#21021;&#22987;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#12290;&#20351;&#29992;BLA&#25552;&#20379;&#30340;&#29366;&#24577;&#31354;&#38388;&#30697;&#38453;&#21450;&#20854;&#30456;&#20851;&#30340;&#21487;&#37325;&#26500;&#26144;&#23556;&#26469;&#21021;&#22987;&#21270;&#32593;&#32476;&#30340;&#29366;&#24577;&#36716;&#31227;&#37096;&#20998;&#21644;&#32534;&#30721;&#22120;&#12290;&#25913;&#36827;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#34920;&#29616;&#22312;Wiener-Hamme&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SUBNET neural network architecture has been developed to identify nonlinear state-space models from input-output data. To achieve this, it combines the rolled-out nonlinear state-space equations and a state encoder function, both parameterised as a neural network. The encoder function is introduced to reconstruct the current state from past input-output data. Hence it enables the forward simulation of the rolled-out state-space model. While this approach has shown to provide high-accuracy and consistent model estimation, its convergence can be significantly improved by efficient initialization of the training process. This paper focuses on such an initialisation of the subspace encoder approach using the Best Linear Approximation (BLA). Using the BLA provided state-space matrices and its associated reconstructability map both the state-transition part of the network and the encoder are initialized. The performance of the improved initialisation scheme is evaluated on a Wiener-Hamme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#21253;&#25324;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;U-Net&#26550;&#26500;&#25104;&#21151;&#37325;&#26500;&#20102;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.02104</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Deep learning for diffusion in porous media. (arXiv:2304.02104v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#21253;&#25324;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;U-Net&#26550;&#26500;&#25104;&#21151;&#37325;&#26500;&#20102;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#22810;&#23380;&#20171;&#36136;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20171;&#36136;&#31867;&#22411;&#65306;&#19968;&#31181;&#27169;&#25311;&#30722;&#23721;&#65292;&#21478;&#19968;&#31181;&#27169;&#25311;&#29983;&#29289;&#32452;&#32455;&#30340;&#32454;&#32990;&#22806;&#31354;&#38388;&#34893;&#29983;&#31995;&#32479;&#12290;&#37319;&#29992;Lattice Boltzmann&#26041;&#27861;&#33719;&#24471;&#24517;&#35201;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#31995;&#32479;&#20960;&#20309;&#20998;&#26512;&#30340;&#32593;&#32476;&#39044;&#27979;&#23380;&#38553;&#29575;&#21644;&#26377;&#25928;&#25193;&#25955;&#31995;&#25968;&#65292;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#65292;&#32593;&#32476;&#37325;&#26500;&#20102;&#31995;&#32479;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#12290;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CNN&#27169;&#22411;&#65306;C-Net&#21644;U-Net&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#12290;&#20004;&#20010;&#32593;&#32476;&#22343;&#28155;&#21152;&#20102;&#33258;&#24402;&#19968;&#21270;&#27169;&#22359;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20165;&#22312;&#20854;&#35757;&#32451;&#30340;&#25968;&#25454;&#31867;&#22411;&#20869;&#26377;&#21512;&#29702;&#20934;&#30830;&#24615;&#12290;&#22312;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;ResNet&#32534;&#30721;&#22120;&#30340;U-Net&#26550;&#26500;&#37325;&#26500;&#22810;&#23380;&#20171;&#36136;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#27987;&#24230;&#20998;&#24067;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26550;&#26500;&#21487;&#20197;&#25104;&#21151;&#37325;&#26500;&#20004;&#31181;&#20171;&#36136;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adopt convolutional neural networks (CNN) to predict the basic properties of the porous media. Two different media types are considered: one mimics the sandstone, and the other mimics the systems derived from the extracellular space of biological tissues. The Lattice Boltzmann Method is used to obtain the labeled data necessary for performing supervised learning. We distinguish two tasks. In the first, networks based on the analysis of the system's geometry predict porosity and effective diffusion coefficient. In the second, networks reconstruct the system's geometry and concentration map. In the first task, we propose two types of CNN models: the C-Net and the encoder part of the U-Net. Both networks are modified by adding a self-normalization module. The models predict with reasonable accuracy but only within the data type, they are trained on. For instance, the model trained on sandstone-like samples overshoots or undershoots for biological-like samples. In the second task, we pr
&lt;/p&gt;</description></item><item><title>CAMELS&#39033;&#30446;&#25193;&#23637;&#20102;&#26143;&#31995;&#24418;&#25104;&#30340;&#27169;&#22411;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;CAMELS-ASTRID&#27169;&#25311;&#22871;&#20214;&#36890;&#36807;&#22810;&#26679;&#30340;&#27169;&#25311;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26143;&#31995;&#32676;&#20307;&#21644;&#37325;&#35201;&#24433;&#21709;, &#35757;&#32451;&#38598;&#35206;&#30422;&#19981;&#21516;&#23431;&#23449;&#23398;&#21442;&#25968;&#21644;&#26143;&#31995;&#21453;&#39304;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292; &#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#26143;&#31995;&#24418;&#25104;&#30340;&#29702;&#35299;&#24182;&#20026;&#23431;&#23449;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2304.02096</link><description>&lt;p&gt;
CAMELS&#39033;&#30446;&#65306;&#20351;&#29992;&#26032;&#30340;ASTRID&#21644;28&#20010;&#21442;&#25968;&#30340;TNG&#21644;SIMBA&#22871;&#20214;&#25193;&#23637;&#26143;&#31995;&#24418;&#25104;&#27169;&#22411;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The CAMELS project: Expanding the galaxy formation model space with new ASTRID and 28-parameter TNG and SIMBA suites. (arXiv:2304.02096v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02096
&lt;/p&gt;
&lt;p&gt;
CAMELS&#39033;&#30446;&#25193;&#23637;&#20102;&#26143;&#31995;&#24418;&#25104;&#30340;&#27169;&#22411;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;CAMELS-ASTRID&#27169;&#25311;&#22871;&#20214;&#36890;&#36807;&#22810;&#26679;&#30340;&#27169;&#25311;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26143;&#31995;&#32676;&#20307;&#21644;&#37325;&#35201;&#24433;&#21709;, &#35757;&#32451;&#38598;&#35206;&#30422;&#19981;&#21516;&#23431;&#23449;&#23398;&#21442;&#25968;&#21644;&#26143;&#31995;&#21453;&#39304;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292; &#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#26143;&#31995;&#24418;&#25104;&#30340;&#29702;&#35299;&#24182;&#20026;&#23431;&#23449;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CAMELS-ASTRID&#65292;&#36825;&#26159;Cosmology and Astrophysics with MachinE Learning&#65288;CAMELS&#65289;&#39033;&#30446;&#20013;&#30340;&#31532;&#19977;&#20010;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#22871;&#20214;&#65292;&#20197;&#21450;&#22522;&#20110;&#20043;&#21069;&#30340;CAMELS-TNG&#21644;CAMELS-SIMBA&#26694;&#26550;&#25193;&#23637;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#30340;&#26032;&#27169;&#25311;&#22871;&#20214;&#65292;&#20026;&#35774;&#35745;&#29992;&#20110;&#23431;&#23449;&#23398;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#12290;&#19982;CAMELS&#20013;&#29616;&#26377;&#30340;TNG&#21644;SIMBA&#27169;&#25311;&#22871;&#20214;&#30456;&#27604;&#65292;ASTRID&#30340;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26368;&#28201;&#21644;&#30340;AGN&#21453;&#39304;&#65292;&#24182;&#19988;&#39044;&#27979;&#23545;&#29289;&#36136;&#21151;&#29575;&#35889;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;ASTRID&#30340;&#35757;&#32451;&#38598;&#28085;&#30422;&#20102;&#26356;&#24191;&#27867;&#30340;&#26143;&#31995;&#32676;&#20307;&#21644;&#23545;&#29289;&#36136;&#21151;&#29575;&#35889;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CAMELS-ASTRID, the third suite of hydrodynamical simulations in the Cosmology and Astrophysics with MachinE Learning (CAMELS) project, along with new simulation sets that extend the model parameter space based on the previous frameworks of CAMELS-TNG and CAMELS-SIMBA, to provide broader training sets and testing grounds for machine-learning algorithms designed for cosmological studies. CAMELS-ASTRID employs the galaxy formation model following the ASTRID simulation and contains 2,124 hydrodynamic simulation runs that vary 3 cosmological parameters ($\Omega_m$, $\sigma_8$, $\Omega_b$) and 4 parameters controlling stellar and AGN feedback. Compared to the existing TNG and SIMBA simulation suites in CAMELS, the fiducial model of ASTRID features the mildest AGN feedback and predicts the least baryonic effect on the matter power spectrum. The training set of ASTRID covers a broader variation in the galaxy populations and the baryonic impact on the matter power spectrum compared t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#25193;&#23637;&#65292;&#19988;&#20445;&#35777;&#28385;&#36275;&#20013;&#38388;&#21830;&#27599;&#26085;&#33021;&#28304;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.02086</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#23398;&#20064;&#36817;&#20284;Stackelberg&#35299;&#20915;&#26041;&#26696;&#22312;&#24102;&#26377;&#38656;&#27714;&#21709;&#24212;&#32858;&#21512;&#22120;&#30340;&#33021;&#28304;&#20132;&#26131;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scalable Online Learning of Approximate Stackelberg Solutions in Energy Trading Games with Demand Response Aggregators. (arXiv:2304.02086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#25193;&#23637;&#65292;&#19988;&#20445;&#35777;&#28385;&#36275;&#20013;&#38388;&#21830;&#27599;&#26085;&#33021;&#28304;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Stackelberg&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#32858;&#21512;&#22120;&#21644;&#20013;&#38388;&#21830;&#20043;&#38388;&#21452;&#21521;&#20132;&#26131;&#33021;&#28304;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#28789;&#27963;&#30340;&#33021;&#28304;&#22871;&#21033;&#21644;&#39069;&#22806;&#30340;&#36135;&#24065;&#22870;&#21169;&#65292;&#21516;&#26102;&#30830;&#20445;&#28385;&#36275;&#20013;&#38388;&#21830;&#25152;&#38656;&#30340;&#27599;&#26085;&#33021;&#28304;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#65288;&#38543;&#30528;&#20013;&#38388;&#21830;&#25968;&#37327;&#22686;&#21152;&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#32447;&#37319;&#26679;&#21644;&#23398;&#20064;&#20013;&#38388;&#21830;&#30340;&#32047;&#31215;&#26368;&#20339;&#21709;&#24212;&#65292;&#23547;&#25214;&#36817;&#20284;&#30340;&#22343;&#34913;&#35299;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#36817;&#20284;&#22343;&#34913;&#35299;&#36136;&#37327;&#30340;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#26469;&#33258;&#21152;&#21033;&#31119;&#23612;&#20122;&#26085;&#21069;&#33021;&#28304;&#24066;&#22330;&#21644;&#21152;&#24030;&#22823;&#23398;&#25140;&#32500;&#26031;&#20998;&#26657;&#24314;&#31569;&#33021;&#28304;&#38656;&#27714;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#22312;&#32447;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a Stackelberg game theoretic framework is proposed for trading energy bidirectionally between the demand-response (DR) aggregator and the prosumers. This formulation allows for flexible energy arbitrage and additional monetary rewards while ensuring that the prosumers' desired daily energy demand is met. Then, a scalable (with the number of prosumers) approach is proposed to find approximate equilibria based on online sampling and learning of the prosumers' cumulative best response. Moreover, bounds are provided on the quality of the approximate equilibrium solution. Last, real-world data from the California day-ahead energy market and the University of California at Davis building energy demands are utilized to demonstrate the efficacy of the proposed framework and the online scalable solution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02084</link><description>&lt;p&gt;
&#12298;EduceLab-Scrolls&#65306;&#21033;&#29992;X&#23556;&#32447;CT&#20174;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#20013;&#21487;&#39564;&#35777;&#22320;&#24674;&#22797;&#25991;&#26412;&#12299;
&lt;/p&gt;
&lt;p&gt;
EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT. (arXiv:2304.02084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;X&#23556;&#32447;CT&#22270;&#20687;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#36719;&#20214;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#12290;&#20182;&#20204;&#36816;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20960;&#20309;&#26694;&#26550;&#30340;&#26041;&#27861;&#26816;&#27979;&#8220;&#19981;&#21487;&#35265;&#8221;&#30340;&#30899;&#22696;&#65292;&#36798;&#21040;&#20102;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#38590;&#20197;&#36798;&#21040;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#36203;&#24211;&#20848;&#23612;&#22982;&#32440;&#33609;&#21367;&#38544;&#34255;&#25991;&#26412;&#30340;&#23436;&#25972;&#36719;&#20214;&#31649;&#36947;&#12290;&#36825;&#20010;&#22686;&#24378;&#30340;&#34394;&#25311;&#23637;&#24320;&#27969;&#27700;&#32447;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23558;&#19977;&#32500;&#21644;&#20108;&#32500;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;EduceLab-Scrolls&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#20108;&#21313;&#24180;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;EduceLab-Scrolls&#21253;&#21547;&#20102;&#19968;&#32452;&#23567;&#30862;&#29255;&#21644;&#23436;&#25972;&#21367;&#36724;&#30340;&#20307;&#31215;X&#23556;&#32447;CT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#29992;&#20110;&#30417;&#30563;&#35757;&#32451;&#27833;&#22696;&#26816;&#27979;&#27169;&#22411;&#30340;&#20108;&#32500;&#22270;&#20687;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#21367;&#36724;&#30862;&#29255;&#30340;&#39057;&#35889;&#29031;&#29255;&#19982;&#30456;&#21516;&#30862;&#29255;&#30340;X&#23556;&#32447;CT&#22270;&#20687;&#23545;&#20934;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#20687;&#31354;&#38388;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#23545;&#20934;&#20801;&#35768;&#26377;&#30417;&#30563;&#22320;&#23398;&#20064;&#26816;&#27979;X&#23556;&#32447;CT&#20013;&#8220;&#38544;&#24418;&#8221;&#30899;&#22696;&#30340;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26631;&#35760;&#32773;&#26469;&#35828;&#20063;&#26159;&#8220;&#19981;&#21487;&#33021;&#8221;&#30340;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a complete software pipeline for revealing the hidden texts of the Herculaneum papyri using X-ray CT images. This enhanced virtual unwrapping pipeline combines machine learning with a novel geometric framework linking 3D and 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset representing two decades of research effort on this problem. EduceLab-Scrolls contains a set of volumetric X-ray CT images of both small fragments and intact, rolled scrolls. The dataset also contains 2D image labels that are used in the supervised training of an ink detection model. Labeling is enabled by aligning spectral photography of scroll fragments with X-ray CT images of the same fragments, thus creating a machine-learnable mapping between image spaces and modalities. This alignment permits supervised learning for the detection of "invisible" carbon ink in X-ray CT, a task that is "impossible" even for human expert labelers. To our knowledge, this is the first aligned datas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#24037;&#20855;&#30340;&#20351;&#29992;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02064</link><description>&lt;p&gt;
&#31639;&#27861;&#30456;&#20851;&#30340;&#30028;&#38480;&#23545;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Algorithm-Dependent Bounds for Representation Learning of Multi-Source Domain Adaptation. (arXiv:2304.02064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#24037;&#20855;&#30340;&#20351;&#29992;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#35770;&#24037;&#20855;&#20174;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#65292;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#23569;&#37327;&#30446;&#26631;&#26631;&#31614;&#30340;&#30417;&#30563; MDA &#21644;&#24102;&#26377;&#20266;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563; MDA &#30340;&#32852;&#21512;&#20998;&#24067;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38024;&#23545;&#36825;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#31639;&#27861;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#20854;&#20013;&#27867;&#21270;&#30001;&#21442;&#25968;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230; MDA &#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#40784;&#38544;&#21547;&#22320;&#35299;&#20915;&#20102;&#30446;&#26631;&#20559;&#31227;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;&#20114;&#20449;&#24687;&#30028;&#38480;&#25193;&#23637;&#21040;&#27492;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#38750;&#24179;&#24248;&#30340;&#26799;&#24230;&#33539;&#25968;&#20272;&#35745;&#12290;&#35813;&#31639;&#27861;&#22312;&#25913;&#36827;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#22312;&#30446;&#26631;&#20559;&#31227; MDA &#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use information-theoretic tools to derive a novel analysis of Multi-source Domain Adaptation (MDA) from the representation learning perspective. Concretely, we study joint distribution alignment for supervised MDA with few target labels and unsupervised MDA with pseudo labels, where the latter is relatively hard and less commonly studied. We further provide algorithm-dependent generalization bounds for these two settings, where the generalization is characterized by the mutual information between the parameters and the data. Then we propose a novel deep MDA algorithm, implicitly addressing the target shift through joint alignment. Finally, the mutual information bounds are extended to this algorithm providing a non-vacuous gradient-norm estimation. The proposed algorithm has comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.02049</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Explainable Unlearning for Image Classification via Weight Filtering. (arXiv:2304.02049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21368;&#36733;&#26159;&#26368;&#36817;&#28014;&#29616;&#30340;&#19968;&#31181;&#36873;&#25321;&#24615;&#22320;&#23558;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#38598;&#20013;&#22312;&#21368;&#36733;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#25110;&#21333;&#20010;&#31867;&#21035;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20869;&#37096;&#32452;&#20214;&#30340;&#35760;&#24518;&#30697;&#38453;&#26469;&#35843;&#33410;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#21518;&#65292;&#21516;&#19968;&#32593;&#32476;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#23637;&#31034;&#20219;&#20309;&#31867;&#21035;&#30340;&#26410;&#23398;&#20064;&#34892;&#20026;&#12290;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#31867;&#21035;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#35774;&#35745;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#26469;&#24674;&#22797;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#21644;&#20013;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;CNN&#21644;Transformer-based&#39592;&#26550;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning has recently been emerging as a paradigm for selectively removing the impact of training datapoints from a network. While existing approaches have focused on unlearning either a small subset of the training data or a single class, in this paper we take a different path and devise a framework that can unlearn all classes of an image classification network in a single untraining round. Our proposed technique learns to modulate the inner components of an image classification network through memory matrices so that, after training, the same network can selectively exhibit an unlearning behavior over any of the classes. By discovering weights which are specific to each of the classes, our approach also recovers a representation of the classes which is explainable by-design. We test the proposed framework, which we name Weight Filtering network (WF-Net), on small-scale and medium-scale image classification datasets, with both CNN and Transformer-based backbones. Our work p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#25195;&#25551;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#23454;&#29616;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#20197;&#21450;&#38656;&#35201;&#24320;&#21457;&#30340;&#38754;&#21521;&#26174;&#24494;&#38236;&#24037;&#20316;&#27969;&#31243;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#38656;&#35201;&#30830;&#23450;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#30340;&#26500;&#24605;&#12289;&#32534;&#25490;&#21644;&#25191;&#34892;&#20013;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#24182;&#24320;&#21457;&#21487;&#36328;&#22810;&#20010;&#24179;&#21488;&#24212;&#29992;&#30340;&#36890;&#29992;&#36229;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.02048</link><description>&lt;p&gt;
&#25195;&#25551;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#21160;&#23454;&#39564;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Automated Experimentation in Scanning Transmission Electron Microscopy. (arXiv:2304.02048v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#25195;&#25551;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#23454;&#29616;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#20197;&#21450;&#38656;&#35201;&#24320;&#21457;&#30340;&#38754;&#21521;&#26174;&#24494;&#38236;&#24037;&#20316;&#27969;&#31243;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#38656;&#35201;&#30830;&#23450;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#30340;&#26500;&#24605;&#12289;&#32534;&#25490;&#21644;&#25191;&#34892;&#20013;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#24182;&#24320;&#21457;&#21487;&#36328;&#22810;&#20010;&#24179;&#21488;&#24212;&#29992;&#30340;&#36890;&#29992;&#36229;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#65288;&#25195;&#25551;&#65289;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#25104;&#20687;&#21644;&#20809;&#35889;&#23398;&#30340;&#21518;&#37319;&#38598;&#25968;&#25454;&#20998;&#26512;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#26159;&#23454;&#26102;&#20998;&#26512;&#21644;&#38381;&#29615;&#26174;&#24494;&#38236;&#25805;&#20316;&#12290;&#26377;&#25928;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#29616;&#22312;&#38656;&#35201;&#24320;&#21457;&#38754;&#21521;&#26174;&#24494;&#38236;&#24037;&#20316;&#27969;&#31243;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36716;&#21521;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24207;&#36143;&#25968;&#25454;&#20998;&#26512;&#21644;&#36229;&#20986;&#20998;&#24067;&#28418;&#31227;&#25928;&#24212;&#65292;&#36793;&#32536;&#25805;&#20316;&#12289;&#26412;&#22320;&#21644;&#20113;&#25968;&#25454;&#23384;&#20648;&#30340;&#35201;&#27714;&#20197;&#21450;&#29702;&#35770;&#22312;&#29615;&#25805;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#31867;&#31185;&#23398;&#23478;&#21644;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#30340;&#26500;&#24605;&#12289;&#32534;&#25490;&#21644;&#25191;&#34892;&#20013;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#24182;&#38656;&#35201;&#24320;&#21457;&#21487;&#36328;&#22810;&#20010;&#24179;&#21488;&#24212;&#29992;&#30340;&#36890;&#29992;&#36229;&#35821;&#35328;&#12290;&#36825;&#20123;&#32771;&#34385;&#22240;&#32032;&#23558;&#20849;&#21516;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#22312;&#25195;&#25551;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#25805;&#20316;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has become critical for post-acquisition data analysis in (scanning) transmission electron microscopy, (S)TEM, imaging and spectroscopy. An emerging trend is the transition to real-time analysis and closed-loop microscope operation. The effective use of ML in electron microscopy now requires the development of strategies for microscopy-centered experiment workflow design and optimization. Here, we discuss the associated challenges with the transition to active ML, including sequential data analysis and out-of-distribution drift effects, the requirements for the edge operation, local and cloud data storage, and theory in the loop operations. Specifically, we discuss the relative contributions of human scientists and ML agents in the ideation, orchestration, and execution of experimental workflows and the need to develop universal hyper languages that can apply across multiple platforms. These considerations will collectively inform the operationalization of ML in n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#19988;&#28145;&#30340;Transformer&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#65292;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23485;&#24230;&#32553;&#25918;&#24314;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.02034</link><description>&lt;p&gt;
&#21021;&#22987;&#21270;&#26102;Transformer&#30340;&#26377;&#25928;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Effective Theory of Transformers at Initialization. (arXiv:2304.02034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#19988;&#28145;&#30340;Transformer&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#65292;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23485;&#24230;&#32553;&#25918;&#24314;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;&#36825;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23485;&#19988;&#28145;&#30340;Transformer&#65288;&#21363;&#20351;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#22359;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20449;&#21495;&#20256;&#25773;&#36827;&#34892;&#20102;&#26377;&#25928;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#24314;&#35758;&#36825;&#20123;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#37319;&#29992;&#29305;&#23450;&#30340;&#23485;&#24230;&#32553;&#25918;&#12290;&#25105;&#20204;&#38543;&#21518;&#37319;&#29992;&#36825;&#20123;&#24314;&#35758;&#65292;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;Transformer&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, i.e., residual neural networks with multi-head self-attention blocks and multilayer perceptron blocks. This analysis suggests particular width scalings of initialization and training hyperparameters for these models. We then take up such suggestions, training Vision and Language Transformers in practical setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.02022</link><description>&lt;p&gt;
&#22522;&#20110;MNL&#36873;&#25321;&#27169;&#22411;&#30340;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Joint Assortment-Inventory Optimization under MNL Choices. (arXiv:2304.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#39038;&#23458;&#30340;&#36873;&#25321;&#34892;&#20026;&#37117;&#36981;&#24490;Multinomial Logit&#65288;MNL&#65289;&#36873;&#25321;&#27169;&#22411;&#65292;&#21560;&#24341;&#21147;&#21442;&#25968;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#38646;&#21806;&#21830;&#36827;&#34892;&#21608;&#26399;&#24615;&#32452;&#21512;&#21644;&#24211;&#23384;&#20915;&#31574;&#65292;&#20197;&#21160;&#24577;&#22320;&#20174;&#23454;&#29616;&#30340;&#38656;&#27714;&#20013;&#23398;&#20064;&#21560;&#24341;&#21147;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#19978;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#24635;&#21033;&#28070;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#34913;&#32452;&#21512;&#21644;&#24211;&#23384;&#22312;&#32447;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;MNL&#21560;&#24341;&#21147;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#26576;&#20123;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#26469;&#28608;&#21169;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#38745;&#24577;&#21333;&#21608;&#26399;&#32452;&#21512;&#24211;&#23384;&#35268;&#21010;&#38382;&#39064;&#30340;&#20248;&#21270;oracle&#22522;&#30784;&#20043;&#19978;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#65292;&#20197;&#21450;&#20851;&#20110;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the realized demands about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance the exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, a novel approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21452;&#21521;LSTM&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#25968;&#23383;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#36739;&#39640;&#30340;ROC AUC&#24471;&#20998;&#21644;&#20934;&#30830;&#24615;&#35782;&#21035;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#22312;&#32447;&#25307;&#32856;&#24066;&#22330;&#65292;&#24110;&#21161;&#35299;&#20915;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#30340;&#38382;&#39064;&#21644;&#25552;&#39640;&#27714;&#32844;&#27969;&#31243;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02019</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;LSTM&#26816;&#27979;&#34394;&#20551;&#32844;&#20301;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Detecting Fake Job Postings Using Bidirectional LSTM. (arXiv:2304.02019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21452;&#21521;LSTM&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#25968;&#23383;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#36739;&#39640;&#30340;ROC AUC&#24471;&#20998;&#21644;&#20934;&#30830;&#24615;&#35782;&#21035;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#22312;&#32447;&#25307;&#32856;&#24066;&#22330;&#65292;&#24110;&#21161;&#35299;&#20915;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#30340;&#38382;&#39064;&#21644;&#25552;&#39640;&#27714;&#32844;&#27969;&#31243;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;&#19978;&#65292;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#30340;&#20986;&#29616;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#32473;&#27714;&#32844;&#32773;&#21644;&#25307;&#32856;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#24517;&#35201;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#27450;&#35784;&#24615;&#25307;&#32856;&#24191;&#21578;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;Bi-LSTM&#65289;&#27169;&#22411;&#26469;&#35782;&#21035;&#34394;&#20551;&#32844;&#19994;&#24191;&#21578;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#25968;&#23383;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;0.91 ROC AUC&#24471;&#20998;&#21644;98.71&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#22312;&#32447;&#25307;&#32856;&#24066;&#22330;&#19978;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#24320;&#21457;&#24378;&#22823;&#30340;&#33258;&#21160;&#24037;&#20855;&#65292;&#24110;&#21161;&#25171;&#20987;&#34394;&#20551;&#32844;&#20301;&#24191;&#21578;&#30340;&#28363;&#29983;&#65292;&#25552;&#39640;&#27714;&#32844;&#27969;&#31243;&#30340;&#25972;&#20307;&#23436;&#25972;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25361;&#25112;,
&lt;/p&gt;
&lt;p&gt;
Fake job postings have become prevalent in the online job market, posing significant challenges to job seekers and employers. Despite the growing need to address this problem, there is limited research that leverages deep learning techniques for the detection of fraudulent job advertisements. This study aims to fill the gap by employing a Bidirectional Long Short-Term Memory (Bi-LSTM) model to identify fake job advertisements. Our approach considers both numeric and text features, effectively capturing the underlying patterns and relationships within the data. The proposed model demonstrates a superior performance, achieving a 0.91 ROC AUC score and a 98.71% accuracy rate, indicating its potential for practical applications in the online job market. The findings of this research contribute to the development of robust, automated tools that can help combat the proliferation of fake job postings and improve the overall integrity of the job search process. Moreover, we discuss challenges,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;API&#23558;&#23545;&#35937;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#20174;&#32780;&#29983;&#25104;&#36866;&#21512;&#20110;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#26032;&#39062;&#30340;&#33756;&#35889;&#21345;&#12290;</title><link>http://arxiv.org/abs/2304.02016</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#22810;&#27169;&#22359;&#30340;AI&#22823;&#21416;&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#22797;&#26434;&#33756;&#35889;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery. (arXiv:2304.02016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;API&#23558;&#23545;&#35937;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#20174;&#32780;&#29983;&#25104;&#36866;&#21512;&#20110;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#26032;&#39062;&#30340;&#33756;&#35889;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31038;&#21306;&#37319;&#29992;&#22810;&#24863;&#23448;&#25110;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#25512;&#36827;&#36825;&#19968;&#20195;AI&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#23558;&#35821;&#35328;&#21644;&#22270;&#20687;&#30456;&#32467;&#21512;&#20195;&#34920;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#29087;&#24713;&#26041;&#27861;&#65292;&#20363;&#22914;&#20174;&#25551;&#36848;&#20013;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#25110;&#22270;&#20687;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#21333;&#29255;&#24335;&#26041;&#27861;&#19982;&#22522;&#20110;&#37319;&#29992;&#22270;&#20687;&#27169;&#22411;&#26631;&#35760;&#23545;&#35937;&#30340;&#36731;&#37327;&#32423;&#21644;&#19987;&#19994;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#28982;&#21518;&#20018;&#34892;&#25552;&#20132;&#27492;&#32467;&#26524;&#23545;&#35937;&#21015;&#34920;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#12290;&#22810;&#20010;API&#30340;&#20351;&#29992;&#20351;&#24471;&#27491;&#30830;&#23545;&#35937;&#21015;&#34920;&#30340;&#24179;&#22343;&#31934;&#24230;&#36798;&#21040;95%&#20197;&#19978;&#65292;&#36825;&#20123;&#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#26368;&#26032;&#30340;Open AI&#25991;&#26412;&#29983;&#25104;&#22120;(GPT-4)&#12290;&#20026;&#20102;&#28436;&#31034;API&#20316;&#20026;&#27169;&#22359;&#21270;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#29992;&#25143;&#25293;&#19979;&#20912;&#31665;&#20013;&#26377;&#21738;&#20123;&#39135;&#26448;&#30340;&#29031;&#29255;&#65292;&#28982;&#21518;&#29983;&#25104;&#36866;&#21512;&#20110;&#25104;&#26412;&#12289;&#20934;&#22791;&#26102;&#38388;&#12289;&#39278;&#39135;&#38480;&#21046;&#12289;&#20998;&#37327;&#22823;&#23567;&#21644;&#22810;&#20010;&#22240;&#32032;&#30340;&#26032;&#39062;&#33756;&#35889;&#21345;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AI community has embraced multi-sensory or multi-modal approaches to advance this generation of AI models to resemble expected intelligent understanding. Combining language and imagery represents a familiar method for specific tasks like image captioning or generation from descriptions. This paper compares these monolithic approaches to a lightweight and specialized method based on employing image models to label objects, then serially submitting this resulting object list to a large language model (LLM). This use of multiple Application Programming Interfaces (APIs) enables better than 95% mean average precision for correct object lists, which serve as input to the latest Open AI text generator (GPT-4). To demonstrate the API as a modular alternative, we solve the problem of a user taking a picture of ingredients available in a refrigerator, then generating novel recipe cards tailored to complex constraints on cost, preparation time, dietary restrictions, portion sizes, and multip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01994</link><description>&lt;p&gt;
DWA&#65306;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;DWA&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#24182;&#25233;&#21046;&#22122;&#22768;&#12290;&#22312;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#38598;&#25104;DWA&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;(DWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;(SR)&#27169;&#22359;&#12290;DWA&#20026;&#26368;&#36817;&#25910;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#28151;&#21512;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;(DWT)&#26041;&#27861;&#27880;&#20837;&#27963;&#21147;&#12290;DWT&#33021;&#22815;&#26377;&#25928;&#22320;&#20026;SR&#25552;&#20379;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#31354;&#38388;&#38754;&#31215;&#20943;&#23569;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#27169;&#22411;&#24635;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25104;&#20026;&#21487;&#25345;&#32493;ML&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DWA&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#23567;&#27874;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#25552;&#39640;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#24120;&#35265;&#22122;&#22768;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#65292;&#22914;DWSR&#21644;MWCNN&#65292;&#21487;&#20197;&#26174;&#31034;&#20986;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SR&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;DWA&#20351;DWSR&#21644;MWCNN&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#30465;&#30053;&#20102;DWT&#34920;&#31034;&#30340;&#36890;&#36947;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.01905</link><description>&lt;p&gt;
&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#26102;&#30340;&#39640;&#25928;&#21796;&#37266;&#35789;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition. (arXiv:2304.01905v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21452;&#20851;&#27880;&#31070;&#32463;&#21464;&#25442;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#35745;&#31639;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#39640;&#21796;&#37266;&#35789;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;90&#65285;&#32780;&#20165;&#22686;&#21152;1&#65285;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20013;&#22823;&#26377;&#35048;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#39640;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#29575;&#24182;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#21033;&#29992;&#21796;&#37266;&#35789;&#26816;&#27979;&#26469;&#36873;&#25321;&#21738;&#20010;&#27880;&#24847;&#21147;&#32593;&#32476;&#25191;&#34892;&#36755;&#20837;&#38899;&#39057;&#24103;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#36335;&#24452;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#20316;&#32773;&#26377;&#25928;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#30340;&#36816;&#34892;&#26102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20351;&#29992;&#20316;&#32773;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#26102;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21452;&#20851;&#27880;&#32593;&#32476;&#21487;&#20197;&#23558;&#21796;&#37266;&#35789;&#38899;&#39057;&#24103;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;$90\%$&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#20165;&#22686;&#21152;$1\%$&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#25552;&#39640;&#20102;&#21796;&#37266;&#35789;F1&#24471;&#20998;$16\%$&#65292;&#24182;&#23558;&#19968;&#33324;&#30340;&#32597;&#35265;&#35789;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;$3\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
We present dual-attention neural biasing, an architecture designed to boost Wake Words (WW) recognition and improve inference time latency on speech recognition tasks. This architecture enables a dynamic switch for its runtime compute paths by exploiting WW spotting to select which branch of its attention networks to execute for an input audio frame. With this approach, we effectively improve WW spotting accuracy while saving runtime compute cost as defined by floating point operations (FLOPs). Using an in-house de-identified dataset, we demonstrate that the proposed dual-attention network can reduce the compute cost by $90\%$ for WW audio frames, with only $1\%$ increase in the number of parameters. This architecture improves WW F1 score by $16\%$ relative and improves generic rare word error rate by $3\%$ relative compared to the baselines.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01240</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;
&lt;/p&gt;
&lt;p&gt;
Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#65292;&#25552;&#39640;&#20102;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#35775;&#38382;&#21307;&#30103;&#36164;&#28304;&#30340;&#24120;&#35265;&#21407;&#22240;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#24515;&#29702;&#20581;&#24247;&#30340;&#37325;&#21472;&#26041;&#38754;&#12290;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#30740;&#31350;&#27492;&#37325;&#21472;&#30340;&#33391;&#22909;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30140;&#30171;&#30340;&#22823;&#37327;&#20449;&#24687;&#20445;&#23384;&#22312;&#36825;&#20123;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#20013;&#65292;&#30001;&#20110;&#20854;&#27495;&#20041;&#24615;&#65292;&#30140;&#30171;&#30340;&#25552;&#21450;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#12290;&#26412;&#39033;&#30446;&#20351;&#29992;&#21311;&#21517;&#30340;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#20026;&#35752;&#35770;&#24739;&#32773;&#30140;&#30171;&#25110;&#19981;&#35752;&#35770;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30140;&#30171;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#36755;&#20986;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#12290;&#20849;&#25163;&#21160;&#19977;&#37325;&#27880;&#37322;&#20102;1,985&#20221;&#25991;&#20214;&#65292;&#20197;&#21019;&#24314;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#19977;&#31181;&#24120;&#29992;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20026;0.787&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#30140;&#30171;&#25552;&#21450;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#23545;&#30140;&#30171;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21333;&#20803;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00971</link><description>&lt;p&gt;
&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;3D&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection, Segmentation, and Depth Estimation. (arXiv:2304.00971v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Cityscapes-3D&#30340;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#23454;&#29616;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#21333;&#20803;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#26159;TaskPrompter&#22312;&#22522;&#20110;Cityscapes-3D&#30340;&#26032;&#32852;&#21512;2D-3D&#22810;&#20219;&#21153;&#23398;&#20064;&#26631;&#20934;&#19978;&#30340;&#23454;&#29616;&#30340;&#34917;&#20805;&#25991;&#26723;&#12290;TaskPrompter&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26694;&#26550;&#65292;&#23558;&#65288;i&#65289;&#20219;&#21153;&#36890;&#29992;&#34920;&#31034;&#12289;&#65288;ii&#65289;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#21644;&#65288;iii&#65289;&#36328;&#20219;&#21153;&#20132;&#20114;&#30340;&#23398;&#20064;&#32479;&#19968;&#36215;&#26469;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#23558;&#36825;&#20123;&#23398;&#20064;&#30446;&#26631;&#20998;&#21035;&#23384;&#25918;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#27169;&#22359;&#20013;&#30456;&#21453;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#19981;&#20165;&#20943;&#23569;&#20102;&#23545;&#32467;&#26500;&#35774;&#35745;&#30340;&#32454;&#33268;&#32463;&#39564;&#38656;&#27714;&#65292;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#22240;&#20026;&#25972;&#20010;&#27169;&#22411;&#23481;&#37327;&#37117;&#33268;&#21147;&#20110;&#21516;&#26102;&#20248;&#21270;&#36825;&#19977;&#20010;&#30446;&#26631;&#12290;TaskPrompter&#22312;Cityscapes-3D&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#35201;&#27714;&#22810;&#20219;&#21153;&#27169;&#22411;&#21516;&#26102;&#20026;&#21333;&#30524;3D&#36710;&#36742;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#29983;&#25104;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report serves as a supplementary document for TaskPrompter, detailing its implementation on a new joint 2D-3D multi-task learning benchmark based on Cityscapes-3D. TaskPrompter presents an innovative multi-task prompting framework that unifies the learning of (i) task-generic representations, (ii) task-specific representations, and (iii) cross-task interactions, as opposed to previous approaches that separate these learning objectives into different network modules. This unified approach not only reduces the need for meticulous empirical structure design but also significantly enhances the multi-task network's representation learning capability, as the entire model capacity is devoted to optimizing the three objectives simultaneously. TaskPrompter introduces a new multi-task benchmark based on Cityscapes-3D dataset, which requires the multi-task model to concurrently generate predictions for monocular 3D vehicle detection, semantic segmentation, and monocular depth estimation. The
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00830</link><description>&lt;p&gt;
AUDIT&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00830
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#32534;&#36753;&#21487;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#65292;&#20363;&#22914;&#28155;&#21152;&#32972;&#26223;&#38899;&#25928;&#12289;&#26367;&#25442;&#20048;&#22120;&#21644;&#20462;&#22797;&#25439;&#22351;&#30340;&#38899;&#39057;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20197;&#36755;&#20986;&#38899;&#39057;&#30340;&#25991;&#26412;&#35828;&#26126;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#21644;&#21435;&#22122;&#36807;&#31243;&#26469;&#23454;&#29616;&#38646;-shot&#38899;&#39057;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;1&#65289;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#35757;&#32451;&#29992;&#20110;&#32534;&#36753;&#20219;&#21153;&#65292;&#19981;&#33021;&#20445;&#35777;&#33391;&#22909;&#30340;&#32534;&#36753;&#25928;&#26524;&#65307;2&#65289;&#23427;&#20204;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#20462;&#25913;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#29255;&#27573;&#65307;3&#65289;&#20182;&#20204;&#38656;&#35201;&#36755;&#20986;&#38899;&#39057;&#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#25110;&#24517;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUDIT&#65292;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;AUDIT&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#35774;&#35745;&#29305;&#28857;&#65306;1&#65289;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#26500;&#24314;&#19977;&#20803;&#35757;&#32451;&#25968;&#25454;&#65288;&#25351;&#20196;&#65292;&#36755;&#20837;&#38899;&#39057;&#65292;&#36755;&#20986;&#38899;&#39057;&#65289;&#24182;&#20351;&#29992;&#25351;&#20196;&#21644;&#36755;&#20837;&#65288;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#65289;&#38899;&#39057;&#23545;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65307;2&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#20351;&#29992;&#24456;&#23569;&#37327;&#30340;&#32534;&#36753;&#20219;&#21153;&#38899;&#39057;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65307;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#65292;&#33258;&#21160;&#23558;&#36755;&#20837;&#25351;&#20196;&#19982;&#35201;&#32534;&#36753;&#30340;&#30456;&#24212;&#38899;&#39057;&#29255;&#27573;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUDIT&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edite
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00668</link><description>&lt;p&gt;
&#22312;SAR ATR&#20013;&#21457;&#29616;&#21644;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR. (arXiv:2304.00668v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#22312;MSTAR&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#38480;&#30340;&#25104;&#20687;&#26465;&#20214;&#65292;MSTAR&#23384;&#22312;&#32972;&#26223;&#30456;&#20851;&#31561;&#25968;&#25454;&#20559;&#35265;&#65292;&#21363;&#32972;&#26223;&#26434;&#27874;&#29305;&#24615;&#19982;&#30446;&#26631;&#31867;&#21035;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36807;&#24230;&#25311;&#21512;&#26434;&#27874;&#20197;&#20943;&#23569;&#35757;&#32451;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26434;&#27874;&#30340;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#21453;&#26144;&#20102;SAR ATR&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#23450;&#24615;&#20998;&#26512;&#27492;&#29616;&#35937;&#12290;&#26412;&#25991;&#22522;&#20110;Shapley&#20540;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#12290;&#26434;&#27874;&#30340;Shapley&#20540;&#21487;&#20197;&#34913;&#37327;&#20854;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#12290;&#31616;&#35328;&#20043;&#65292;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#20449;&#26434;&#27604;&#21644;&#26434;&#27874;&#32441;&#29702;&#21487;&#27604;&#65292;&#32780;&#21508;&#31181;&#27169;&#22411;&#32467;&#26500;&#23545;&#36825;&#20123;&#20559;&#24046;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#19981;&#21516;&#12290;&#38750;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#20026;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has been widely used in SAR ATR and achieved excellent performance on the MSTAR dataset. However, due to constrained imaging conditions, MSTAR has data biases such as background correlation, i.e., background clutter properties have a spurious correlation with target classes. Deep learning can overfit clutter to reduce training errors. Therefore, the degree of overfitting for clutter reflects the non-causality of deep learning in SAR ATR. Existing methods only qualitatively analyze this phenomenon. In this paper, we quantify the contributions of different regions to target recognition based on the Shapley value. The Shapley value of clutter measures the degree of overfitting. Moreover, we explain how data bias and model bias contribute to non-causality. Concisely, data bias leads to comparable signal-to-clutter ratios and clutter textures in training and test sets. And various model structures have different degrees of overfitting for these biases. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#20004;&#20010;&#23376;&#31354;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17802</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces. (arXiv:2303.17802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20449;&#21495;&#23376;&#31354;&#38388;&#24046;&#24322;&#23376;&#31354;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#20004;&#20010;&#23376;&#31354;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#35889;&#20998;&#26512;(SSA)&#21644;&#24046;&#24322;&#23376;&#31354;&#38388;&#27010;&#24565;&#30340;&#26032;&#22411;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#30417;&#25511;&#36807;&#21435;&#21644;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25152;&#23545;&#24212;&#30340;&#20004;&#20010;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#38388;&#36731;&#24494;&#30340;&#26102;&#38388;&#21464;&#21270;&#26469;&#20316;&#20026;&#24322;&#24120;&#35780;&#20998;&#12290;&#36825;&#26159;&#20256;&#32479;SSA&#26041;&#27861;&#30340;&#33258;&#28982;&#25512;&#24191;&#65292;&#20256;&#32479;&#26041;&#27861;&#27979;&#37327;&#30340;&#26159;&#20004;&#20010;&#20449;&#21495;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#26368;&#23567;&#35282;&#24230;&#26469;&#34920;&#24449;&#21464;&#21270;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#29992;&#24046;&#24322;&#23376;&#31354;&#38388;&#26367;&#20195;&#26368;&#23567;&#35282;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;SSA&#26694;&#26550;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#23436;&#25972;&#32467;&#26500;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new method for anomaly detection in time-series data by incorporating the concept of difference subspace into the singular spectrum analysis (SSA). The key idea is to monitor slight temporal variations of the difference subspace between two signal subspaces corresponding to the past and present time-series data, as anomaly score. It is a natural generalization of the conventional SSA-based method which measures the minimum angle between the two signal subspaces as the degree of changes. By replacing the minimum angle with the difference subspace, our method boosts the performance while using the SSA-based framework as it can capture the whole structural difference between the two subspaces in its magnitude and direction. We demonstrate our method's effectiveness through performance evaluations on public time-series datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2303.17354</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#32570;&#38519;&#26816;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#23545;Masked Autoencoder &#65288;MAE&#65289;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#29983;&#25104;&#24050;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20687;&#32032;&#26631;&#31614;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20462;&#22797;&#25439;&#22351;&#30340;&#21306;&#22495;&#21644;&#20998;&#31867;&#27599;&#20010;&#20687;&#32032;&#30340;&#29366;&#24577;&#12290;&#26368;&#32456;&#65292;&#35813;&#27169;&#22411;&#20135;&#29983;&#19968;&#20010;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#19968;&#20010;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#65292;&#36825;&#20004;&#20010;&#30697;&#38453;&#32508;&#21512;&#25104;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#65292;&#26377;&#25928;&#22320;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2303.17218</link><description>&lt;p&gt;
HARFLOW3D&#65306;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;FPGA&#35774;&#22791;&#30340;&#22522;&#20110;&#24310;&#36831;&#30340;3D-CNN&#21152;&#36895;&#22120;&#24037;&#20855;&#38142;HARFLOW3D&#65292;&#23427;&#20197;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;FPGA&#30340;&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;HARFLOW3D&#30456;&#27604;&#20854;&#20182;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#24335;&#26550;&#26500;&#30340;&#24037;&#20855;&#38142;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#26144;&#23556;&#21040;FPGA&#19978;&#65292;&#32771;&#34385;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#21644;&#30446;&#26631;FPGA&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;HARFLOW3D&#24037;&#20855;&#38142;&#20197;ONNX&#26684;&#24335;&#30340;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;FPGA&#29305;&#24615;&#25551;&#36848;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#26368;&#23567;&#21270;&#35745;&#31639;&#24310;&#36831;&#30340;&#35774;&#35745;&#12290;&#35813;&#24037;&#20855;&#38142;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21253;&#25324;i) 3D CNN&#35299;&#26512;&#22120;&#65292;ii) &#24615;&#33021;&#21644;&#36164;&#28304;&#27169;&#22411;&#65292;iii) &#29992;&#20110;&#22312;&#29983;&#25104;&#30340;&#30828;&#20214;&#19978;&#25191;&#34892;3D&#27169;&#22411;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;iv) &#38024;&#23545;3D&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#36164;&#28304;&#24863;&#30693;&#20248;&#21270;&#24341;&#25806;&#65292;v) &#33258;&#21160;&#26144;&#23556;&#21040;&#21487;&#21512;&#25104;&#30340;FPGA&#20195;&#30721;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;3D CNN&#21644;FPGA&#31995;&#32479;&#37197;&#23545;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#24037;&#20855;&#38142;&#25903;&#25345;&#24191;&#27867;&#27169;&#22411;&#21644;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;3D CNN&#21152;&#36895;&#22120;&#35774;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#24037;&#20855;&#38142;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21152;&#36895;&#25913;&#36827;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;IPU&#29420;&#29305;&#30340;&#30828;&#20214;&#29305;&#24615;&#21644;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#8212;&#8212;PopSparse&#12290;</title><link>http://arxiv.org/abs/2303.16999</link><description>&lt;p&gt;
PopSparse&#65306;&#22312;IPU&#19978;&#21152;&#36895;&#30340;&#22359;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
PopSparse: Accelerated block sparse matrix multiplication on IPU. (arXiv:2303.16999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16999
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21152;&#36895;&#25913;&#36827;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;IPU&#29420;&#29305;&#30340;&#30828;&#20214;&#29305;&#24615;&#21644;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#8212;&#8212;PopSparse&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#65292;&#20351;&#29992;&#31232;&#30095;&#24615;&#38477;&#20302;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#30340;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;FLOP&#21644;&#21442;&#25968;&#25968;&#37327;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#33719;&#24471;&#21152;&#36895;&#25913;&#36827;&#36890;&#24120;&#26356;&#21152;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#29992;&#21152;&#36895;&#22120;&#65288;GPA&#65289;&#19978;&#65292;&#22914;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#23383;&#26684;&#24335;&#30340;NVIDIA GPU&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PopSparse&#65292;&#19968;&#31181;&#21033;&#29992;IPU&#30340;&#29420;&#29305;&#30828;&#20214;&#29305;&#24615;&#20197;&#21450;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#20219;&#20309;&#22359;&#32467;&#26500;&#23454;&#29616;&#22312;Graphcore IPUs&#19978;&#24555;&#36895;&#31232;&#30095;&#25805;&#20316;&#30340;&#24211;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31232;&#30095;&#24615;&#65306;&#38745;&#24577;&#31232;&#30095;&#24615;&#65292;&#20854;&#20013;&#31232;&#30095;&#27169;&#24335;&#22312;&#32534;&#35793;&#26102;&#22266;&#23450;&#65307;&#21160;&#24577;&#31232;&#30095;&#24615;&#65292;&#20854;&#20013;&#27599;&#27425;&#36816;&#34892;&#27169;&#22411;&#26102;&#37117;&#21487;&#20197;&#25913;&#21464;&#12290;&#25105;&#20204;&#38024;&#23545;&#21508;&#31181;&#22359;&#22823;&#23567;&#12289;&#30697;&#38453;&#22823;&#23567;&#21644;&#23494;&#24230;&#22312;IPU&#19978;&#36827;&#34892;&#30697;&#38453;&#20056;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#35777;&#36924;&#36817;&#31934;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16251</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#21442;&#32771;&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control. (arXiv:2303.16251v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#35777;&#36924;&#36817;&#31934;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#29702;&#35770;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28608;&#27963;&#20989;&#25968;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#23618;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#29702;&#35770;&#24182;&#27809;&#26377;&#32473;&#20986;&#19968;&#31181;&#26500;&#36896;&#24615;&#26041;&#27861;&#26469;&#29983;&#25104;&#32593;&#32476;&#21442;&#25968;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#19987;&#38376;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#20989;&#25968;&#21644;&#26576;&#20123;&#31867;&#30340;&#35299;&#26512;&#20989;&#25968;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#23454;&#29616;&#39640;&#31934;&#24230;&#36924;&#36817;&#12290;&#36825;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#29305;&#27530;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#20381;&#36182;&#20110;&#25152;&#20351;&#29992;&#30340;&#20855;&#20307;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#24179;&#28369;&#30340;&#31215;&#20998;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#28608;&#27963;&#24418;&#25104;&#30446;&#26631;&#20989;&#25968;&#31215;&#20998;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#32780;&#23545;&#20110;&#36825;&#20123;&#28608;&#27963;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30452;&#25509;&#30340;&#31215;&#20998;&#34920;&#31034;&#12290;&#26032;&#30340;&#26500;&#36896;&#20351;&#24471;&#21487;&#20197;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#25552;&#20379;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.15739</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#36125;&#21494;&#26031;&#33258;&#30001;&#33021;
&lt;/p&gt;
&lt;p&gt;
Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#29992;&#20110;&#20272;&#35745;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#23578;&#26410;&#20174;&#29702;&#35770;&#35282;&#24230;&#23436;&#20840;&#28548;&#28165;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#21644;&#22855;&#24322;&#30340;&#23398;&#20064;&#26426;&#22120;&#12290;&#27492;&#22806;&#65292;ReLU&#20989;&#25968;&#19981;&#21487;&#24494;&#65292;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20195;&#25968;&#25110;&#35299;&#26512;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#23427;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20102;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#21363;&#20351;&#23618;&#25968;&#27604;&#20272;&#35745;&#26410;&#30693;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#25152;&#24517;&#38656;&#30340;&#23618;&#25968;&#26356;&#22810;&#12290;&#30001;&#20110;Bayesian&#24191;&#20041;&#35823;&#24046;&#31561;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#33258;&#30001;&#33021;&#22686;&#21152;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#34920;&#26126;&#65292;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#26102;&#38388;&#25552;&#31034;&#26469;&#24341;&#20837;&#21160;&#24577;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#27169;&#22411;&#21040;&#35270;&#39057;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12252</link><description>&lt;p&gt;
&#21160;&#24577;&#25552;&#31034;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting Adversarial Transferability using Dynamic Cues. (arXiv:2302.12252v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#26102;&#38388;&#25552;&#31034;&#26469;&#24341;&#20837;&#21160;&#24577;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#27169;&#22411;&#21040;&#35270;&#39057;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#22312;&#22270;&#20687;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#33021;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20174;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#25915;&#20987;&#19981;&#33021;&#25429;&#25417;&#21040;&#31227;&#21160;&#29289;&#20307;&#25110;&#21464;&#21270;&#22330;&#26223;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#22240;&#27492;&#23545;&#20110;&#34920;&#31034;&#20016;&#23500;&#30340;&#22270;&#20687;&#27169;&#22411;&#65288;&#22914;Supervised Vision Transformers&#12289;Self-supervised ViTs&#21644;Vision-language&#27169;&#22411;&#65289;&#21040;&#40657;&#30418;&#35270;&#39057;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#36716;&#31227;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#20026;&#22270;&#20687;&#27169;&#22411;&#24341;&#20837;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#36890;&#36807;&#20923;&#32467;&#22270;&#20687;&#27169;&#22411;&#26469;&#20248;&#21270;&#26102;&#38388;&#25552;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#21160;&#24577;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.12126</link><description>&lt;p&gt;
KHAN&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20934;&#30830;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#20943;&#36731;&#22238;&#22768;&#23460;&#25928;&#24212;&#65292;&#21363;&#20154;&#20204;&#33853;&#20837;&#20854;&#24605;&#24819;&#65292;&#24378;&#21270;&#20854;&#29616;&#26377;&#20449;&#24565;&#12290;&#20197;&#24448;&#20851;&#20110;&#25919;&#27835;&#31435;&#22330;&#38382;&#39064;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#65288;1&#65289;&#35782;&#21035;&#21487;&#20197;&#21453;&#26144;&#26032;&#38395;&#25991;&#31456;&#25919;&#27835;&#31435;&#22330;&#30340;&#25919;&#27835;&#22240;&#32032;&#21644;&#65288;2&#65289;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#22240;&#32032;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32463;&#39564;&#19978;&#25104;&#21151;&#20102;&#65292;&#20294;&#22312;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#20854;&#35782;&#21035;&#30340;&#22240;&#32032;&#30340;&#26377;&#25928;&#24615;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#35843;&#26597;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#35266;&#23519;&#21040;&#26032;&#38395;&#25991;&#31456;&#30340;&#29615;&#22659;&#21644;&#35821;&#35843;&#65288;&#38544;&#21547;&#65289;&#20197;&#21450;&#25991;&#31456;&#20013;&#28041;&#21450;&#30340;&#29616;&#23454;&#23454;&#20307;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#26174;&#24335;&#65289;&#22312;&#30830;&#23450;&#20854;&#25919;&#27835;&#31435;&#22330;&#26041;&#38754;&#26159;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65288;KHAN&#65289;&#65292;&#37319;&#29992;&#65288;1&#65289;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#65288;2&#65289;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#20379;&#20851;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#20449;&#24687;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#35770;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23041;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24358;&#22270;&#65288;&#19977;&#35282;&#26368;&#22823;&#36807;&#28388;&#22270;&#65289;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#29305;&#24449;&#22312;&#32593;&#32476;&#20869;&#30340;&#30456;&#23545;&#20301;&#32622;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26377;&#19977;&#20010;&#29305;&#28857;&#65306;&#65288;i&#65289;&#39640;&#24230;&#21487;&#35843;&#65292;&#26131;&#20110;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#65307;&#65288;ii&#65289;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26174;&#33879;&#30340;&#31616;&#21333;&#24615;&#65307;&#65288;iii&#65289;&#35745;&#31639;&#25104;&#26412;&#27604;&#20854;&#26367;&#20195;&#26041;&#26696;&#26356;&#21152;&#20415;&#23452;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#24322;&#26500;&#35780;&#20272;&#26465;&#20214;&#19979;&#65292;&#23427;&#20248;&#20110;&#25110;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26494;&#24347;&#30340;&#36817;&#31471;&#21435;&#22122;&#22120;&#21644;&#19968;&#20010;&#26494;&#24347;&#30340;PGD&#31639;&#27861;&#65292;&#33021;&#22815;&#25910;&#25947;&#20110;&#26356;&#24191;&#27867;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#33539;&#22260;&#20869;&#12290;</title><link>http://arxiv.org/abs/2301.13731</link><description>&lt;p&gt;
&#19968;&#31181;&#26494;&#24347;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#24102;&#26377;&#36817;&#31471;&#21435;&#22122;&#22120;&#30340;&#25910;&#25947;&#25554;&#20837;-&#25773;&#25918;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A relaxed proximal gradient descent algorithm for convergent plug-and-play with proximal denoiser. (arXiv:2301.13731v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#26494;&#24347;&#30340;&#36817;&#31471;&#21435;&#22122;&#22120;&#21644;&#19968;&#20010;&#26494;&#24347;&#30340;PGD&#31639;&#27861;&#65292;&#33021;&#22815;&#25910;&#25947;&#20110;&#26356;&#24191;&#27867;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#12290;&#25554;&#20837;&#25773;&#25918;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#34987;&#34920;&#36848;&#20026;&#25968;&#25454;&#36866;&#24212;&#39033;&#21644;&#27491;&#21017;&#21270;&#39033;&#20043;&#21644;&#30340;&#22270;&#20687;&#21453;&#38382;&#39064;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25554;&#20837;&#25773;&#25918;&#26041;&#27861;&#36890;&#36807;&#22312;&#36817;&#31471;&#31639;&#27861;&#65288;&#22914;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65289;&#20013;&#25554;&#20837;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#21435;&#22122;&#22120;&#26469;&#25191;&#34892;&#27491;&#21017;&#21270;&#12290;&#20026;&#30830;&#20445;PnP&#26041;&#26696;&#30340;&#25910;&#25947;&#65292;&#35768;&#22810;&#24037;&#20316;&#30740;&#31350;&#28145;&#24230;&#21435;&#22122;&#22120;&#30340;&#29305;&#23450;&#21442;&#25968;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#35201;&#20040;&#38656;&#35201;&#26080;&#27861;&#39564;&#35777;&#30340;&#20551;&#35774;&#25110;&#27425;&#20248;&#20551;&#35774;&#65292;&#35201;&#20040;&#22312;&#36870;&#38382;&#39064;&#30340;&#21442;&#25968;&#19978;&#20551;&#35774;&#38480;&#21046;&#24615;&#26465;&#20214;&#12290;&#35266;&#23519;&#21040;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#26159;&#30001;&#20110;&#20351;&#29992;&#30340;&#36817;&#31471;&#31639;&#27861;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26494;&#24347;&#29256;&#26412;&#30340;PGD&#31639;&#27861;&#65288;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#20989;&#25968;&#21644;&#24369;&#20984;&#20989;&#25968;&#20043;&#21644;&#65289;&#12290;&#24403;&#19982;&#19968;&#20010;&#26494;&#24347;&#30340;&#36817;&#31471;&#21435;&#22122;&#22120;&#25554;&#20837;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PnP-$\alpha$PGD&#31639;&#27861;&#33021;&#22815;&#25910;&#25947;&#20110;&#26356;&#24191;&#27867;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP methods are efficient iterative algorithms for solving image inverse problems formulated as the minimization of the sum of a data-fidelity term and a regularization term. PnP methods perform regularization by plugging a pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD). To ensure convergence of PnP schemes, many works study specific parametrizations of deep denoisers. However, existing results require either unverifiable or suboptimal hypotheses on the denoiser, or assume restrictive conditions on the parameters of the inverse problem. Observing that these limitations can be due to the proximal algorithm in use, we study a relaxed version of the PGD algorithm for minimizing the sum of a convex function and a weakly convex one. When plugged with a relaxed proximal denoiser, we show that the proposed PnP-$\alpha$PGD algorithm converges for a wider range of regularization parameters
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#39321;&#33609;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.13622</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Data Representations with Joint Diffusion Models. (arXiv:2301.13622v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#39321;&#33609;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20801;&#35768;&#21512;&#25104;&#21644;&#20998;&#31867;&#25968;&#25454;&#65292;&#20294;&#24120;&#24120;&#22312;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#19981;&#24179;&#34913;&#65292;&#25110;&#32773;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#39321;&#33609;&#25193;&#25955;&#27169;&#22411;&#20197;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#31283;&#23450;&#30340;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#20013;&#22312;&#20998;&#31867;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#32852;&#21512;&#35757;&#32451;&#26041;&#27861;&#19978;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#30410;&#22788;&#20849;&#20139;&#29983;&#25104;&#21644;&#37492;&#21035;&#34920;&#31034;&#65292;&#20197;&#25552;&#20379;&#35270;&#35273;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint machine learning models that allow synthesizing and classifying data often offer uneven performance between those tasks or are unstable to train. In this work, we depart from a set of empirical observations that indicate the usefulness of internal representations built by contemporary deep diffusion-based generative models not only for generating but also predicting. We then propose to extend the vanilla diffusion model with a classifier that allows for stable joint end-to-end training with shared parameterization between those objectives. The resulting joint diffusion model outperforms recent state-of-the-art hybrid methods in terms of both classification and generation quality on all evaluated benchmarks. On top of our joint training approach, we present how we can directly benefit from shared generative and discriminative representations by introducing a method for visual counterfactual explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#30340;&#29305;&#24449;&#21270;&#21028;&#21035;&#26631;&#20934;&#65292;&#21363;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#21487;&#23398;&#20064;&#26102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#21487;&#23398;&#20064;&#65292;&#22312;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2301.02729</link><description>&lt;p&gt;
&#22810;&#36755;&#20986;&#21487;&#23398;&#20064;&#24615;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Characterization of Multioutput Learnability. (arXiv:2301.02729v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#30340;&#29305;&#24449;&#21270;&#21028;&#21035;&#26631;&#20934;&#65292;&#21363;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#21487;&#23398;&#20064;&#26102;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#21487;&#23398;&#20064;&#65292;&#22312;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#19988;&#20165;&#24403;&#20989;&#25968;&#31867;&#30340;&#27599;&#20010;&#21333;&#36755;&#20986;&#23376;&#31867;&#37117;&#21487;&#23398;&#20064;&#26102;&#65292;&#22810;&#36755;&#20986;&#20989;&#25968;&#31867;&#25165;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;&#36825;&#25552;&#20379;&#20102;&#22810;&#26631;&#35760;&#20998;&#31867;&#21644;&#22810;&#36755;&#20986;&#22238;&#24402;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#23398;&#20064;&#20013;&#21487;&#23398;&#20064;&#24615;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#22312;&#36172;&#21338;&#21453;&#39304;&#29615;&#22659;&#19979;&#30340;&#22810;&#26631;&#35760;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#21453;&#39304;&#29615;&#22659;&#19979;&#31867;&#20284;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning multioutput function classes in batch and online settings. In both settings, we show that a multioutput function class is learnable if and only if each single-output restriction of the function class is learnable. This provides a complete characterization of the learnability of multilabel classification and multioutput regression in both batch and online settings. As an extension, we also consider multilabel learnability in the bandit feedback setting and show a similar characterization as in the full-feedback setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#21644;&#32452;&#21512;&#22270;&#20687;&#29983;&#25104;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#21644;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.00366</link><description>&lt;p&gt;
&#33258;&#30417;&#30563; Cut-and-Paste GAN &#36827;&#34892;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Object Segmentation with a Cut-and-Pasting GAN. (arXiv:2301.00366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#21644;&#32452;&#21512;&#22270;&#20687;&#29983;&#25104;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#21644;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340; Cut-and-Paste GAN&#65292;&#29992;&#20110;&#36827;&#34892;&#21069;&#26223;&#23545;&#35937;&#20998;&#21106;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#32452;&#21512;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#12290;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110; U-Net &#30340;&#37492;&#21035;&#22120;&#65292;&#23436;&#25104;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#26631;&#20934;&#37492;&#21035;&#22120;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#36890;&#36807;&#20998;&#31867;&#65288;&#30495;/&#20551;&#65289;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#21019;&#24314;&#30340;&#20266;&#26631;&#31614;&#23398;&#20064;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#20351;&#29983;&#25104;&#22120;&#33021;&#22815;&#36890;&#36807;&#24378;&#21046;&#23427;&#20174;&#36776;&#21035;&#22120;&#23398;&#20064;&#27599;&#20687;&#32032;&#30340;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#21644;&#20840;&#23616;&#22270;&#20687;&#21453;&#39304;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#36974;&#32617;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel self-supervised based Cut-and-Paste GAN to perform foreground object segmentation and generate realistic composite images without manual annotations. We accomplish this goal by a simple yet effective self-supervised approach coupled with the U-Net based discriminator. The proposed method extends the ability of the standard discriminators to learn not only the global data representations via classification (real/fake) but also learn semantic and structural information through pseudo labels created using the self-supervised task. The proposed method empowers the generator to create meaningful masks by forcing it to learn informative per-pixel as well as global image feedback from the discriminator. Our experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on the standard benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;LAVISH&#30340;&#36866;&#37197;&#22120;&#21644;&#23569;&#25968;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#22312;&#20351;&#29992;&#36739;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07983</link><description>&lt;p&gt;
&#35270;&#35273;Transformer&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#38899;&#20687;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers are Parameter-Efficient Audio-Visual Learners. (arXiv:2212.07983v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;LAVISH&#30340;&#36866;&#37197;&#22120;&#21644;&#23569;&#25968;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26377;&#25928;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#22312;&#20351;&#29992;&#36739;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20165;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;ViTs&#22312;&#27809;&#26377;&#24494;&#35843;&#20219;&#20309;&#21407;&#22987;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#20854;&#25512;&#24191;&#21040;&#38899;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;latent audio-visual hybrid&#65288;LAVISH&#65289;&#30340;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#20923;&#32467;ViT&#23618;&#27880;&#20837;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#35843;&#36866;&#29992;&#20110;&#38899;&#20687;&#20219;&#21153;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#35270;&#35273;&#21644;&#38899;&#39057;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;LAVISH&#36866;&#37197;&#22120;&#20351;&#29992;&#19968;&#23567;&#32452;&#28508;&#22312;&#20196;&#29260;&#65292;&#24418;&#25104;&#19968;&#20010;&#27880;&#24847;&#29942;&#39048;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#26631;&#20934;&#20132;&#21449;&#20851;&#27880;&#30340;&#20108;&#27425;&#25104;&#26412;&#12290;&#19982;&#29616;&#26377;&#30340;&#27169;&#24577;&#29305;&#23450;&#38899;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#21487;&#35843;&#21442;&#25968;&#24182;&#19988;&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#38899;&#39057;&#39044;&#35757;&#32451;&#25110;&#22806;&#37096;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#38899;&#20687;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://genj&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved impressive results on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without finetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efficiently fuse visual and audio cues, our LAVISH adapter uses a small set of latent tokens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-specific audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https://genj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31639;&#27861;RAVEn&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#35270;&#35273;&#21644;&#21548;&#35273;&#35821;&#38899;&#34920;&#31034;&#65292;&#26080;&#38656;&#20934;&#22791;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22312;&#20302;&#36164;&#28304;&#21644;&#39640;&#36164;&#28304;&#25968;&#25454;&#35774;&#32622;&#19979;&#24471;&#21040;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#32467;&#21512;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#38750;&#20844;&#20849;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.06246</link><description>&lt;p&gt;
&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#20849;&#21516;&#23398;&#20064;&#35270;&#35273;&#21644;&#21548;&#35273;&#35821;&#38899;&#34920;&#31034;&#30340;RAVEn&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Jointly Learning Visual and Auditory Speech Representations from Raw Data. (arXiv:2212.06246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31639;&#27861;RAVEn&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#35270;&#35273;&#21644;&#21548;&#35273;&#35821;&#38899;&#34920;&#31034;&#65292;&#26080;&#38656;&#20934;&#22791;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#33021;&#22312;&#20302;&#36164;&#28304;&#21644;&#39640;&#36164;&#28304;&#25968;&#25454;&#35774;&#32622;&#19979;&#24471;&#21040;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#22312;&#32467;&#21512;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#38750;&#20844;&#20849;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#26041;&#27861;RAVEn&#65292;&#29992;&#20110;&#20849;&#21516;&#23398;&#20064;&#35270;&#35273;&#21644;&#21548;&#35273;&#35821;&#38899;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21253;&#25324;&#32534;&#30721;&#25513;&#34109;&#36755;&#20837;&#65292;&#28982;&#21518;&#39044;&#27979;&#30001;&#32531;&#24930;&#28436;&#21270;&#30340;&#21160;&#37327;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#30446;&#26631;&#12290;&#30001;&#20110;&#35270;&#39057;&#21644;&#38899;&#39057;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#23545;&#20110;&#20004;&#31181;&#27169;&#24577;&#30340;&#20808;&#21069;&#20219;&#21153;&#26159;&#19981;&#23545;&#31216;&#30340;&#65306;&#21548;&#35273;&#27969;&#39044;&#27979;&#35270;&#35273;&#21644;&#21548;&#35273;&#30446;&#26631;&#65292;&#32780;&#35270;&#35273;&#27969;&#21482;&#39044;&#27979;&#21548;&#35273;&#30446;&#26631;&#12290;&#24403;&#24494;&#35843;&#26469;&#33258;&#21333;&#20010;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#32534;&#30721;&#22120;&#26102;&#65292;&#26080;&#35770;&#26159;&#20302;&#36164;&#28304;&#36824;&#26159;&#39640;&#36164;&#28304;&#26631;&#35760;&#25968;&#25454;&#35774;&#32622;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;LRS3&#19978;RAVEn&#36229;&#36234;&#20102;&#25152;&#26377;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#65292;&#24182;&#19988;&#23558;RAVEn&#19982;&#20165;&#20351;&#29992;30&#23567;&#26102;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#25105;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#29978;&#33267;&#32988;&#36807;&#19968;&#20010;&#20165;&#22312;90,000&#23567;&#26102;&#38750;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptimNeuralTS&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#25628;&#32034;&#26377;&#23475;&#22810;&#33647;&#30103;&#27861;&#65292;&#22522;&#20110;&#31070;&#32463;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#24046;&#20998;&#36827;&#21270;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25366;&#25496;&#32034;&#36180;&#25968;&#25454;&#24211;&#24182;&#24314;&#31435;&#33647;&#29289;&#32452;&#21512;&#19982;&#20581;&#24247;&#32467;&#26524;&#20043;&#38388;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.05190</link><description>&lt;p&gt;
&#31070;&#32463;Bandits&#29992;&#20110;&#25968;&#25454;&#25366;&#25496;&#65306;&#25628;&#32034;&#26377;&#23475;&#22810;&#33647;&#30103;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Bandits for Data Mining: Searching for Dangerous Polypharmacy. (arXiv:2212.05190v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptimNeuralTS&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#25628;&#32034;&#26377;&#23475;&#22810;&#33647;&#30103;&#27861;&#65292;&#22522;&#20110;&#31070;&#32463;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#24046;&#20998;&#36827;&#21270;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25366;&#25496;&#32034;&#36180;&#25968;&#25454;&#24211;&#24182;&#24314;&#31435;&#33647;&#29289;&#32452;&#21512;&#19982;&#20581;&#24247;&#32467;&#26524;&#20043;&#38388;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33647;&#30103;&#27861;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#21516;&#26102;&#26381;&#29992;&#20116;&#31181;&#25110;&#26356;&#22810;&#33647;&#29289;&#65292;&#26159;&#32769;&#24180;&#20154;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20123;&#19981;&#24403;&#30340;&#22810;&#33647;&#30103;&#27861;&#21487;&#33021;&#19982;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#65292;&#22914;&#27515;&#20129;&#25110;&#20303;&#38498;&#12290;&#32771;&#34385;&#21040;&#38382;&#39064;&#30340;&#32452;&#21512;&#24615;&#36136;&#20197;&#21450;&#32034;&#36180;&#25968;&#25454;&#24211;&#30340;&#22823;&#23567;&#21644;&#35745;&#31639;&#32473;&#23450;&#33647;&#29289;&#32452;&#21512;&#30340;&#30830;&#20999;&#20851;&#32852;&#24230;&#25152;&#38656;&#30340;&#25104;&#26412;&#65292;&#19981;&#21487;&#33021;&#35843;&#26597;&#27599;&#31181;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20248;&#21270;&#25628;&#32034;&#28508;&#22312;&#19981;&#24403;&#30340;&#22810;&#33647;&#30103;&#27861;&#65288;PIPs&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#27748;&#26222;&#26862;&#37319;&#26679;&#21644;&#24046;&#20998;&#36827;&#21270;&#30340;OptimNeuralTS&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#22320;&#25366;&#25496;&#32034;&#36180;&#25968;&#25454;&#38598;&#24182;&#24314;&#31435;&#33647;&#29289;&#32452;&#21512;&#19982;&#20581;&#24247;&#32467;&#26524;&#20043;&#38388;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#20869;&#37096;&#24320;&#21457;&#30340;&#22810;&#33647;&#30103;&#27861;&#25968;&#25454;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#26469;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#25311;&#22120;&#21253;&#21547;500&#31181;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polypharmacy, most often defined as the simultaneous consumption of five or more drugs at once, is a prevalent phenomenon in the older population. Some of these polypharmacies, deemed inappropriate, may be associated with adverse health outcomes such as death or hospitalization. Considering the combinatorial nature of the problem as well as the size of claims database and the cost to compute an exact association measure for a given drug combination, it is impossible to investigate every possible combination of drugs. Therefore, we propose to optimize the search for potentially inappropriate polypharmacies (PIPs). To this end, we propose the OptimNeuralTS strategy, based on Neural Thompson Sampling and differential evolution, to efficiently mine claims datasets and build a predictive model of the association between drug combinations and health outcomes. We benchmark our method using two datasets generated by an internally developed simulator of polypharmacy data containing 500 drugs an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#24182;&#38750;&#21442;&#25968;&#22320;&#24314;&#27169;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#36991;&#20813;&#27169;&#22411;&#24536;&#35760;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#34920;&#31034;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.05102</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A soft nearest-neighbor framework for continual semi-supervised learning. (arXiv:2212.05102v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#24182;&#38750;&#21442;&#25968;&#22320;&#24314;&#27169;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#36991;&#20813;&#27169;&#22411;&#24536;&#35760;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#34920;&#31034;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#20173;&#28982;&#20381;&#36182;&#20110;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#19981;&#29616;&#23454;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;&#19968;&#31181;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26679;&#26412;&#19981;&#20840;&#37096;&#26631;&#35760;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#27169;&#22411;&#20250;&#24536;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#23041;&#21147;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#30001;&#20110;&#20854;&#38750;&#21442;&#25968;&#24615;&#36136;&#32780;&#28789;&#27963;&#22320;&#23545;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20026;&#24403;&#21069;&#20219;&#21153;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#20174;&#20197;&#21069;&#30340;&#20219;&#21153;&#20013;&#25552;&#28860;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#36739;&#22823;&#30340;&#20248;&#21183;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22312;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19978;&#26641;&#31435;&#20102;&#22362;&#23454;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25552;&#20986;&#19977;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#19968;&#31181;&#19982;&#21407;&#27169;&#22411;&#30340;ELBO&#36924;&#36817;&#30456;&#27604;&#20135;&#29983;&#20102;&#35777;&#25454;&#19978;&#38480;&#65288;EUBO&#65289;&#65292;&#21487;&#29992;&#26469;&#26597;&#35810;&#27169;&#22411;&#25910;&#25947;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.04451</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#19977;&#31181;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Three Variations on Variational Autoencoders. (arXiv:2212.04451v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25552;&#20986;&#19977;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#19968;&#31181;&#19982;&#21407;&#27169;&#22411;&#30340;ELBO&#36924;&#36817;&#30456;&#27604;&#20135;&#29983;&#20102;&#35777;&#25454;&#19978;&#38480;&#65288;EUBO&#65289;&#65292;&#21487;&#29992;&#26469;&#26597;&#35810;&#27169;&#22411;&#25910;&#25947;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#24050;&#30693;&#25968;&#25454;&#25512;&#26029;&#30340;&#29983;&#25104;&#27010;&#29575;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#20108;&#20010;&#21442;&#25968;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#23545;VAE&#36827;&#34892;&#20102;&#19977;&#31181;&#21464;&#20307;&#30340;&#24320;&#21457;&#65292;&#23545;&#20110;&#20854;&#20013;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#22266;&#23450;&#32534;&#30721;&#22120;&#12290;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#30340;&#21442;&#25968;&#23558;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#12290;&#22266;&#23450;&#32534;&#30721;&#22120;&#21017;&#30001;&#27010;&#29575;&#24615;PCA&#33719;&#24471;&#12290;&#36825;&#20123;&#21464;&#20307;&#19982;&#21407;&#22987;VAE&#30340;&#19979;&#38480;&#35777;&#25454;&#65288;ELBO&#65289;&#36924;&#36817;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#20135;&#29983;&#20102;&#35777;&#25454;&#19978;&#38480;&#65288;EUBO&#65289;&#65292;&#21487;&#19982;&#21407;&#22987;ELBO&#19968;&#36215;&#29992;&#26469;&#26597;&#35810;VAE&#30340;&#25910;&#25947;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are one class of generative probabilistic latent-variable models designed for inference based on known data. We develop three variations on VAEs by introducing a second parameterized encoder/decoder pair and, for one variation, an additional fixed encoder. The parameters of the encoders/decoders are to be learned with a neural network. The fixed encoder is obtained by probabilistic-PCA. The variations are compared to the Evidence Lower Bound (ELBO) approximation to the original VAE. One variation leads to an Evidence Upper Bound (EUBO) that can be used in conjunction with the original ELBO to interrogate the convergence of the VAE.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#33639;&#20809;&#26395;&#36828;&#38236;&#25968;&#25454;&#20013;&#31579;&#36873;&#20986;&#36712;&#36857;&#20107;&#20214;&#65292;&#32467;&#21512;&#23454;&#39564;&#27979;&#35797;&#20102;&#27880;&#20876;&#26497;&#39640;&#33021;&#23431;&#23449;&#23556;&#32447;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03787</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#31579;&#36873;&#33639;&#20809;&#26395;&#36828;&#38236;&#25968;&#25454;&#20013;&#36712;&#36857;&#20107;&#20214;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural Network Approach for Selecting Track-like Events in Fluorescence Telescope Data. (arXiv:2212.03787v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#33639;&#20809;&#26395;&#36828;&#38236;&#25968;&#25454;&#20013;&#31579;&#36873;&#20986;&#36712;&#36857;&#20107;&#20214;&#65292;&#32467;&#21512;&#23454;&#39564;&#27979;&#35797;&#20102;&#27880;&#20876;&#26497;&#39640;&#33021;&#23431;&#23449;&#23556;&#32447;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016-2017&#24180;&#65292;TUS&#36827;&#34892;&#20102;&#39318;&#27425;&#27979;&#35797;&#36890;&#36807;&#35266;&#27979;&#22320;&#29699;&#22812;&#38388;&#22823;&#27668;&#20013;&#30340;&#32043;&#22806;&#32447;&#33639;&#20809;&#36752;&#23556;&#26469;&#27880;&#20876;&#26497;&#39640;&#33021;&#23431;&#23449;&#23556;&#32447;(UHECRs)&#30340;&#21487;&#33021;&#24615;&#12290;&#33258;2019&#24180;&#20197;&#26469;&#65292;&#20420;&#32599;&#26031;-&#24847;&#22823;&#21033;&#33639;&#20809;&#26395;&#36828;&#38236;(Mini-EUSO)&#19968;&#30452;&#22312;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#36816;&#34892;&#12290;&#35745;&#21010;&#20110;2023&#24180;&#36827;&#34892;&#24179;&#27969;&#23618;&#23454;&#39564;EUSO-SPB2&#65292;&#35813;&#23454;&#39564;&#23558;&#20351;&#29992;&#33639;&#20809;&#26395;&#36828;&#38236;&#26469;&#27880;&#20876;UHECRs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#20202;&#22120;&#33719;&#24471;&#30340;&#22810;&#26679;&#25968;&#25454;&#20013;&#26377;&#25928;&#22320;&#25214;&#21040;&#36712;&#36857;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016-2017, TUS, the world's first experiment for testing the possibility of registering ultra-high energy cosmic rays (UHECRs) by their fluorescent radiation in the night atmosphere of Earth was carried out. Since 2019, the Russian-Italian fluorescence telescope (FT) Mini-EUSO ("UV Atmosphere") has been operating on the ISS. The stratospheric experiment EUSO-SPB2, which will employ an FT for registering UHECRs, is planned for 2023. We show how a simple convolutional neural network can be effectively used to find track-like events in the variety of data obtained with such instruments.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#21628;&#21505;&#23545;&#22270;&#20687;&#20998;&#31867;&#20013;&#22833;&#36133;&#26816;&#27979;&#30340;&#35780;&#20272;&#23454;&#36341;&#36827;&#34892;&#21453;&#24605;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#22312;&#23581;&#35797;&#36890;&#36807;&#32622;&#20449;&#24230;&#26816;&#27979;&#38169;&#35823;&#39044;&#27979;&#65292;&#20294;&#26159;&#23427;&#20204;&#30446;&#21069;&#26500;&#25104;&#20102;&#29420;&#31435;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23384;&#22312;&#35780;&#20272;&#26631;&#20934;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#21644;&#29616;&#23454;&#30340;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.15259</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#22833;&#36133;&#26816;&#27979;&#35780;&#20272;&#23454;&#36341;&#19978;&#36827;&#34892;&#21453;&#24605;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification. (arXiv:2211.15259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#21628;&#21505;&#23545;&#22270;&#20687;&#20998;&#31867;&#20013;&#22833;&#36133;&#26816;&#27979;&#30340;&#35780;&#20272;&#23454;&#36341;&#36827;&#34892;&#21453;&#24605;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#22312;&#23581;&#35797;&#36890;&#36807;&#32622;&#20449;&#24230;&#26816;&#27979;&#38169;&#35823;&#39044;&#27979;&#65292;&#20294;&#26159;&#23427;&#20204;&#30446;&#21069;&#26500;&#25104;&#20102;&#29420;&#31435;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23384;&#22312;&#35780;&#20272;&#26631;&#20934;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#21644;&#29616;&#23454;&#30340;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#21487;&#38752;&#22320;&#24212;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#26159;&#24403;&#21069;&#39046;&#22495;&#27491;&#22312;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20998;&#37197;&#32622;&#20449;&#24230;&#24471;&#21040;&#26816;&#27979;&#21040;&#38169;&#35823;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#32622;&#20449;&#24230;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12289;&#23398;&#20064;&#26126;&#30830;&#30340;&#35780;&#20998;&#20989;&#25968;&#25110;&#35780;&#20272;&#36755;&#20837;&#26159;&#21542;&#31526;&#21512;&#35757;&#32451;&#20998;&#24067;&#26469;&#33719;&#24471;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#37117;&#22768;&#26126;&#35201;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26816;&#27979;&#20998;&#31867;&#22120;&#22833;&#36133;&#30340;&#30456;&#21516;&#26368;&#32456;&#30446;&#26631;&#65292;&#20294;&#23427;&#20204;&#30446;&#21069;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26500;&#25104;&#20102;&#21508;&#33258;&#29420;&#31435;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#21333;&#29420;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36825;&#20123;&#21327;&#35758;&#21487;&#33021;&#25490;&#38500;&#20102;&#22823;&#37327;&#30456;&#20851;&#26041;&#27861;&#25110;&#24573;&#30053;&#20102;&#22823;&#37327;&#30456;&#20851;&#30340;&#22833;&#36133;&#26469;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#36825;&#20123;&#19981;&#19968;&#33268;&#24615;&#24102;&#26469;&#30340;&#24403;&#21069;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#20840;&#38754;&#21644;&#29616;&#23454;&#30340;&#22833;&#36133;&#26816;&#27979;&#35780;&#20272;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable application of machine learning-based decision systems in the wild is one of the major challenges currently investigated by the field. A large portion of established approaches aims to detect erroneous predictions by means of assigning confidence scores. This confidence may be obtained by either quantifying the model's predictive uncertainty, learning explicit scoring functions, or assessing whether the input is in line with the training distribution. Curiously, while these approaches all state to address the same eventual goal of detecting failures of a classifier upon real-life application, they currently constitute largely separated research fields with individual evaluation protocols, which either exclude a substantial part of relevant methods or ignore large parts of relevant failure sources. In this work, we systematically reveal current pitfalls caused by these inconsistencies and derive requirements for a holistic and realistic evaluation of failure detection. To demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26032;&#36235;&#21183;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;ML&#21644;CFD&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#36824;&#35780;&#20272;&#20102;&#20173;&#22312;&#24320;&#21457;&#20013;&#30340;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#24179;&#34913;&#35880;&#24910;&#20048;&#35266;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2211.15145</link><description>&lt;p&gt;
&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Emerging trends in machine learning for computational fluid dynamics. (arXiv:2211.15145v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26032;&#36235;&#21183;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;ML&#21644;CFD&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#36824;&#35780;&#20272;&#20102;&#20173;&#22312;&#24320;&#21457;&#20013;&#30340;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#24179;&#34913;&#35880;&#24910;&#20048;&#35266;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30028;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#20851;&#27880;&#27491;&#22312;&#24320;&#36767;&#35768;&#22810;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26032;&#36235;&#21183;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;ML&#21644;CFD&#20043;&#38388;&#24050;&#32463;&#20135;&#29983;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#35780;&#20272;&#20102;&#30446;&#21069;&#20173;&#22312;&#24320;&#21457;&#20013;&#19988;&#21487;&#33021;&#22312;&#26410;&#26469;&#20960;&#24180;&#20135;&#29983;&#37325;&#35201;&#25928;&#30410;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24378;&#35843;&#35880;&#24910;&#20048;&#35266;&#30340;&#24179;&#34913;&#35270;&#35282;&#23545;&#20110;&#36825;&#20123;&#26032;&#20852;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The renewed interest from the scientific community in machine learning (ML) is opening many new areas of research. Here we focus on how novel trends in ML are providing opportunities to improve the field of computational fluid dynamics (CFD). In particular, we discuss synergies between ML and CFD that have already shown benefits, and we also assess areas that are under development and may produce important benefits in the coming years. We believe that it is also important to emphasize a balanced perspective of cautious optimism for these emerging approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MPC&#21451;&#22909;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;MPCViT&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#26469;&#23454;&#29616;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.13955</link><description>&lt;p&gt;
MPCViT&#65306;&#20351;&#29992;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#31934;&#20934;&#39640;&#25928;&#30340;MPC&#21451;&#22909;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention. (arXiv:2211.13955v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MPC&#21451;&#22909;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;MPCViT&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24322;&#26500;&#27880;&#24847;&#21147;&#25628;&#32034;&#26469;&#23454;&#29616;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;(MPC)&#21487;&#20197;&#30452;&#25509;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#20445;&#25252;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;Vision Transformers(ViTs)&#65292;&#24182;&#26410;&#19987;&#20026;MPC&#35774;&#35745;&#25110;&#20248;&#21270;&#65292;&#22240;&#27492;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;Softmax&#26159;&#20027;&#35201;&#30340;&#24310;&#36831;&#29942;&#39048;&#65292;&#30001;&#20110;&#39640;&#36890;&#20449;&#22797;&#26434;&#24615;&#65292;&#21487;&#26377;&#36873;&#25321;&#22320;&#26367;&#25442;&#25110;&#32447;&#24615;&#21270;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPC&#21451;&#22909;&#30340;ViT&#65292;&#31216;&#20026;MPCViT&#65292;&#22312;MPC&#20013;&#23454;&#29616;&#20934;&#30830;&#32780;&#39640;&#25928;&#30340;ViT&#25512;&#29702;&#12290;&#22522;&#20110;&#23545;Softmax&#27880;&#24847;&#21147;&#21644;&#20854;&#20182;&#27880;&#24847;&#21147;&#21464;&#20307;&#30340;&#31995;&#32479;&#24310;&#36831;&#21644;&#31934;&#24230;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#26500;&#27880;&#24847;&#21147;&#20248;&#21270;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;MPC&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#24085;&#32047;&#25176;&#20248;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPCViT+&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;Softmax&#26367;&#25442;&#21644;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the So
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#22235;&#20803;&#25968;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#26469;&#20998;&#26512;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#20197;&#36873;&#25321;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#25551;&#36848;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#26679;&#26412;&#26377;&#25928;&#30340;&#31215;&#20998;&#25216;&#26415;&#20943;&#23569;&#20102;&#30005;&#27744;&#27169;&#22411;&#35780;&#20272;&#30340;&#25968;&#37327;&#65292;&#24182;&#25512;&#26029;&#20986;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2210.17299</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#22235;&#20803;&#25968;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Bayesian Model Selection of Lithium-Ion Battery Models via Bayesian Quadrature. (arXiv:2210.17299v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#22235;&#20803;&#25968;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#26469;&#20998;&#26512;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#20197;&#36873;&#25321;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#25551;&#36848;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#26679;&#26412;&#26377;&#25928;&#30340;&#31215;&#20998;&#25216;&#26415;&#20943;&#23569;&#20102;&#30005;&#27744;&#27169;&#22411;&#35780;&#20272;&#30340;&#25968;&#37327;&#65292;&#24182;&#25512;&#26029;&#20986;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#22823;&#37327;&#30005;&#27744;&#27169;&#22411;&#65292;&#19981;&#24635;&#26159;&#26126;&#26174;&#21738;&#20010;&#27169;&#22411;&#26368;&#22909;&#22320;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#22235;&#20803;&#25968;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#37319;&#29992;&#27169;&#22411;&#35777;&#25454;&#20316;&#20026;&#36873;&#25321;&#26631;&#20934;&#65292;&#36873;&#25321;&#26368;&#31616;&#21333;&#30340;&#25551;&#36848;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#31526;&#21512;&#22885;&#21345;&#22982;&#21059;&#20992;&#31934;&#31070;&#12290;&#20294;&#26159;&#65292;&#20272;&#35745;&#36825;&#20010;&#38656;&#35201;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#31215;&#20998;&#35745;&#31639;&#65292;&#36825;&#36890;&#24120;&#26159;&#26080;&#27861;&#25215;&#21463;&#30340;&#12290;&#36125;&#21494;&#26031;&#22235;&#20803;&#25968;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#30005;&#27744;&#27169;&#22411;&#35780;&#20272;&#30340;&#25968;&#37327;&#12290;&#36824;&#21487;&#20197;&#20316;&#20026;&#21103;&#20135;&#21697;&#25512;&#26029;&#20986;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35745;&#31639;&#12290;&#26412;&#25991;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#31561;&#25928;&#30005;&#36335;&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#32473;&#23450;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#36873;&#25321;&#26631;&#20934;&#30340;&#28789;&#25935;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27969;&#34892;&#30340;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#65292;&#20363;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
A wide variety of battery models are available, and it is not always obvious which model `best' describes a dataset. This paper presents a Bayesian model selection approach using Bayesian quadrature. The model evidence is adopted as the selection metric, choosing the simplest model that describes the data, in the spirit of Occam's razor. However, estimating this requires integral computations over parameter space, which is usually prohibitively expensive. Bayesian quadrature offers sample-efficient integration via model-based inference that minimises the number of battery model evaluations. The posterior distribution of model parameters can also be inferred as a byproduct without further computation. Here, the simplest lithium-ion battery models, equivalent circuit models, were used to analyse the sensitivity of the selection criterion to given different datasets and model configurations. We show that popular model selection criteria, such as root-mean-square error and Bayesian informa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;FrozenQubits&#31639;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#28909;&#28857;&#33410;&#28857;&#23558;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#31354;&#38388;&#24182;&#29420;&#31435;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;QAOA&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.17037</link><description>&lt;p&gt;
FrozenQubits: &#36890;&#36807;&#36339;&#36807;&#28909;&#28857;&#33410;&#28857;&#25552;&#21319;QAOA&#30340;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
FrozenQubits: Boosting Fidelity of QAOA by Skipping Hotspot Nodes. (arXiv:2210.17037v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;FrozenQubits&#31639;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#28909;&#28857;&#33410;&#28857;&#23558;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#31354;&#38388;&#24182;&#29420;&#31435;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;QAOA&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#26159;&#23637;&#31034;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#20855;&#22791;&#37327;&#23376;&#20248;&#21183;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#39640;&#35774;&#22791;&#35823;&#24046;&#29575;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#20110;&#36229;&#36807;&#20960;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#38382;&#39064;&#21487;&#38752;&#22320;&#36816;&#34892;QAOA&#30005;&#36335;&#12290;&#22312;QAOA&#20013;&#65292;&#38382;&#39064;&#22270;&#34987;&#36716;&#25442;&#20026;&#19968;&#20010;&#37327;&#23376;&#30005;&#36335;&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#23545;&#24212;&#20110;&#30005;&#36335;&#27599;&#23618;&#20013;&#30340;&#20004;&#20010;2&#37327;&#23376;&#27604;&#29305;CNOT&#25805;&#20316;&#12290;&#30001;&#20110;CNOT&#26497;&#26131;&#20986;&#38169;&#65292;&#22240;&#27492;QAOA&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#30001;&#38382;&#39064;&#22270;&#20013;&#36793;&#30340;&#25968;&#37327;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22810;&#25968;&#23545;&#24212;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#22270;&#36981;&#24490;&#8220;&#24130;&#24459;&#8221;&#20998;&#24067;&#65292;&#20854;&#20013;&#19968;&#20123;&#28909;&#28857;&#33410;&#28857;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#36830;&#25509;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#24182;&#25552;&#20986;&#8220;FrozenQubits&#8221;&#65292;&#20923;&#32467;&#28909;&#28857;&#33410;&#28857;&#25110;&#37327;&#23376;&#27604;&#29305;&#65292;&#23558;&#32473;&#23450;&#38382;&#39064;&#30340;&#29366;&#24577;&#31354;&#38388;&#26234;&#33021;&#22320;&#20998;&#25104;&#20960;&#20010;&#36739;&#23567;&#30340;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#29420;&#31435;&#22320;&#35299;&#20915;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Approximate Optimization Algorithm (QAOA) is one of the leading candidates for demonstrating the quantum advantage using near-term quantum computers. Unfortunately, high device error rates limit us from reliably running QAOA circuits for problems with more than a few qubits. In QAOA, the problem graph is translated into a quantum circuit such that every edge corresponds to two 2-qubit CNOT operations in each layer of the circuit. As CNOTs are extremely error-prone, the fidelity of QAOA circuits is dictated by the number of edges in the problem graph.  We observe that majority of graphs corresponding to real-world applications follow the ``power-law`` distribution, where some hotspot nodes have significantly higher number of connections. We leverage this insight and propose ``FrozenQubits`` that freezes the hotspot nodes or qubits and intelligently partitions the state-space of the given problem into several smaller sub-spaces which are then solved independently. The correspondi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#22312;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36215;&#21040;&#8220;&#20808;&#30693;&#8221;&#30340;&#20316;&#29992;&#65292;&#20801;&#35768;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.08323</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#22312;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36215;&#21040;&#8220;&#20808;&#30693;&#8221;&#30340;&#20316;&#29992;&#65292;&#20801;&#35768;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#21487;&#20197;&#23558;&#26041;&#27861;&#20998;&#20026;&#20004;&#31181;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#12290;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21407;&#21017;&#19978;&#21487;&#20197;&#20139;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#65292;&#20294;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#12290;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#36807;&#20110;&#20445;&#23432;&#65292;&#38590;&#20197;&#36229;&#36234;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#32487;&#25215;&#20102;&#27169;&#20223;&#24335;&#26041;&#27861;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#36923;&#36753;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#20197;&#30417;&#30563;&#21644;&#35299;&#32806;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#21578;&#35785;&#25191;&#34892;&#31574;&#30053;&#24212;&#35813;&#21435;&#21738;&#37324;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#26469;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#30340;&#36208;&#21521;&#65292;&#20316;&#20026;&#8220;&#20808;&#30693;&#8221;&#12290;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20801;&#35768;&#8220;&#29366;&#24577;&#32452;&#21512;&#24615;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \textit{Prophet}. By doing so, our algorithm allows \textit{state-compositionality} f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#35774;&#35745;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#35266;&#27979;&#22120;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.01476</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340; Luenberger &#35266;&#27979;&#22120;&#35774;&#35745;&#29992;&#20110;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning-based Design of Luenberger Observers for Autonomous Nonlinear Systems. (arXiv:2210.01476v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#35774;&#35745;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#35266;&#27979;&#22120;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340; Luenberger &#35266;&#27979;&#22120;&#28041;&#21450;&#23558;&#29366;&#24577;&#36716;&#25442;&#21040;&#19968;&#31181;&#21487;&#28176;&#36817;&#31283;&#23450;&#19988;&#22312;&#36755;&#20986;&#27880;&#20837;&#26041;&#38754;&#26159;&#32447;&#24615;&#30340;&#22791;&#36873;&#22352;&#26631;&#31995;&#65292;&#21487;&#33021;&#26159;&#26356;&#39640;&#32500;&#24230;&#30340;&#12290;&#28982;&#21518;&#65292;&#35266;&#27979;&#22120;&#36890;&#36807;&#21453;&#36716;&#36716;&#25442;&#26144;&#23556;&#22312;&#21407;&#22987;&#22352;&#26631;&#31995;&#20013;&#20272;&#35745;&#31995;&#32479;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25214;&#21040;&#19968;&#20010;&#36866;&#21512;&#30340;&#21487;&#36870;&#21464;&#25442;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21463;&#30417;&#30563;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#36716;&#25442;&#21450;&#20854;&#21453;&#28436;&#12290;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#35823;&#24046;&#19982;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing Luenberger observers for nonlinear systems involves the challenging task of transforming the state to an alternate coordinate system, possibly of higher dimensions, where the system is asymptotically stable and linear up to output injection. The observer then estimates the system's state in the original coordinates by inverting the transformation map. However, finding a suitable injective transformation whose inverse can be derived remains a primary challenge for general nonlinear systems. We propose a novel approach that uses supervised physics-informed neural networks to approximate both the transformation and its inverse. Our method exhibits superior generalization capabilities to contemporary methods and demonstrates robustness to both neural network's approximation errors and system uncertainties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;</title><link>http://arxiv.org/abs/2210.00875</link><description>&lt;p&gt;
&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#65306;&#26397;&#30528;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#21487;&#20197;&#35828;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#30410;&#20110;&#39640;&#36136;&#37327;&#65288;&#24320;&#28304;&#65289;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#22312;&#27492;&#22522;&#30784;&#19978;&#36731;&#26494;&#22320;&#35780;&#20272;&#21644;&#25913;&#36827;&#20182;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#32791;&#26102;&#29978;&#33267;&#26114;&#36149;&#30340;&#65292;&#22914;&#20309;&#20445;&#25252;&#20854;&#29256;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#30001;&#20110;&#26377;&#30446;&#26631;&#30340;&#21518;&#38376;&#27700;&#21360;&#30340;&#29305;&#24615;&#65292;&#20250;&#22312;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#27169;&#22411;&#34892;&#20026;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#20998;&#25955;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#22312;&#21463;&#27745;&#26579;&#26631;&#31614;&#21644;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#35774;&#32622;&#19979;&#35774;&#35745;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27700;&#21360;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#19978;&#37117;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#34987;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#65292;&#19981;&#20250;&#24341;&#20837;&#20219;&#20309;&#21487;&#26816;&#27979;&#30340;&#25197;&#26354;&#25110;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#40065;&#26834;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#25932;&#25163;&#27010;&#24565;&#65292;&#24314;&#31435;&#36215;&#26426;&#22120;&#20154;&#39044;&#27979;&#32773;&#19982;&#25932;&#25163;&#20043;&#38388;&#30340;&#38646;&#21644;&#20108;&#20154;&#21338;&#24328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#38752;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.10802</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#39044;&#27979;&#65306;&#19968;&#31181;&#21338;&#24328;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Forecasting for Robotic Control: A Game-Theoretic Approach. (arXiv:2209.10802v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#40065;&#26834;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#25932;&#25163;&#27010;&#24565;&#65292;&#24314;&#31435;&#36215;&#26426;&#22120;&#20154;&#39044;&#27979;&#32773;&#19982;&#25932;&#25163;&#20043;&#38388;&#30340;&#38646;&#21644;&#20108;&#20154;&#21338;&#24328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#38752;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#20154;&#38656;&#35201;&#20934;&#30830;&#30340;&#39044;&#27979;&#26469;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#20026;&#26426;&#22120;&#20154;&#25511;&#21046;&#29983;&#25104;&#40065;&#26834;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#27169;&#25311;&#24433;&#21709;&#26410;&#26469;&#39044;&#27979;&#30340;&#30495;&#23454;&#19990;&#30028;&#22240;&#32032;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25932;&#25163;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25200;&#20081;&#35266;&#23519;&#21040;&#30340;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#26469;&#22686;&#21152;&#26426;&#22120;&#20154;&#26368;&#32456;&#30340;&#25511;&#21046;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20132;&#20114;&#24314;&#27169;&#20026;&#26426;&#22120;&#20154;&#39044;&#27979;&#32773;&#21644;&#29702;&#24819;&#20013;&#30340;&#25932;&#25163;&#20043;&#38388;&#30340;&#38646;&#21644;&#20108;&#20154;&#21338;&#24328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21338;&#24328;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#27714;&#35299;&#21040;&#19968;&#20010;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern robots require accurate forecasts to make optimal decisions in the real world. For example, self-driving cars need an accurate forecast of other agents' future actions to plan safe trajectories. Current methods rely heavily on historical time series to accurately predict the future. However, relying entirely on the observed history is problematic since it could be corrupted by noise, have outliers, or not completely represent all possible outcomes. To solve this problem, we propose a novel framework for generating robust forecasts for robotic control. In order to model real-world factors affecting future forecasts, we introduce the notion of an adversary, which perturbs observed historical time series to increase a robot's ultimate control cost. Specifically, we model this interaction as a zero-sum two-player game between a robot's forecaster and this hypothetical adversary. We show that our proposed game may be solved to a local Nash equilibrium using gradient-based optimizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Max-Dependency-Min-Divergence (MDmD)&#20934;&#21017;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#31163;&#25955;&#21270;&#25968;&#25454;&#30340;&#21028;&#21035;&#20449;&#24687;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#20934;&#21017;&#21487;&#24212;&#29992;&#20110;&#31163;&#25955;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#12290;</title><link>http://arxiv.org/abs/2209.10095</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#30456;&#20851;&#26368;&#23567;&#24046;&#24322;&#26631;&#20934;&#30340;&#25968;&#25454;&#31163;&#25955;&#21270;&#21450;&#20854;&#22312;&#26420;&#32032;&#36125;&#21494;&#26031;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Max-relevance-min-divergence Criterion for Data Discretization with Applications on Naive Bayes. (arXiv:2209.10095v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Max-Dependency-Min-Divergence (MDmD)&#20934;&#21017;&#65292;&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#31163;&#25955;&#21270;&#25968;&#25454;&#30340;&#21028;&#21035;&#20449;&#24687;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#20934;&#21017;&#21487;&#24212;&#29992;&#20110;&#31163;&#25955;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#20026;&#26356;&#22909;&#22320;&#20272;&#35745;&#20854;&#20998;&#24067;&#65292;&#25968;&#25454;&#20250;&#34987;&#31163;&#25955;&#21270;&#12290;&#29616;&#26377;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#24448;&#24448;&#38024;&#23545;&#26368;&#22823;&#21270;&#31163;&#25955;&#21270;&#21518;&#25968;&#25454;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#25968;&#25454;&#31163;&#25955;&#21270;&#22312;&#20998;&#31867;&#20013;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20026;&#21516;&#26102;&#26368;&#22823;&#21270;&#31163;&#25955;&#21270;&#21518;&#25968;&#25454;&#30340;&#21028;&#21035;&#20449;&#24687;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Max-Dependency-Min-Divergence&#65288;MDmD&#65289;&#20934;&#21017;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#26368;&#22823;&#21270;&#20102;&#31163;&#25955;&#21270;&#25968;&#25454;&#19982;&#20998;&#31867;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#32780;&#26368;&#23567;&#20998;&#27495;&#20934;&#21017;&#22312;&#32473;&#23450;&#31163;&#25955;&#21270;&#26041;&#26696;&#26102;&#26174;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#39564;&#35777;&#25968;&#25454;&#20043;&#38388;&#30340;JS&#36317;&#31163;&#12290;&#25552;&#20986;&#30340;MDmD&#20934;&#21017;&#22312;&#25216;&#26415;&#19978;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many classification models, data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance. As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme. The proposed MDmD criterion is technically appealin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#32593;&#32476;&#24615;&#33021;&#65292;&#19988;&#30456;&#23545;&#35823;&#24046;&#38543;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2209.03487</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A simple approach for quantizing neural networks. (arXiv:2209.03487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26435;&#37325;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#32593;&#32476;&#24615;&#33021;&#65292;&#19988;&#30456;&#23545;&#35823;&#24046;&#38543;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#23436;&#20840;&#35757;&#32451;&#36807;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#39044;&#22788;&#29702;&#27493;&#39588;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#26080;&#35760;&#24518;&#26631;&#37327;&#37327;&#21270;&#26469;&#37327;&#21270;&#32593;&#32476;&#23618;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#36827;&#34892;&#31616;&#21333;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short note, we propose a new method for quantizing the weights of a fully trained neural network. A simple deterministic pre-processing step allows us to quantize network layers via memoryless scalar quantization while preserving the network performance on given training data. On one hand, the computational complexity of this pre-processing slightly exceeds that of state-of-the-art algorithms in the literature. On the other hand, our approach does not require any hyper-parameter tuning and, in contrast to previous methods, allows a plain analysis. We provide rigorous theoretical guarantees in the case of quantizing single network layers and show that the relative error decays with the number of parameters in the network if the training data behaves well, e.g., if it is sampled from suitable random distributions. The developed method also readily allows the quantization of deep networks by consecutive application to single layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#25110;&#36830;&#32493;&#26799;&#24230;&#27969;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#19988;&#26080;&#38656;&#38543;&#26426;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.02525</link><description>&lt;p&gt;
&#22522;&#20110;&#30830;&#23450;&#24615;PAC-Bayes&#30340;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisation under gradient descent via deterministic PAC-Bayes. (arXiv:2209.02525v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#25110;&#36830;&#32493;&#26799;&#24230;&#27969;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#19988;&#26080;&#38656;&#38543;&#26426;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#25110;&#36830;&#32493;&#26799;&#24230;&#27969;&#35757;&#32451;&#27169;&#22411;&#24314;&#31435;&#20102;&#32454;&#20998;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;PAC-Bayes&#35774;&#23450;&#20013;&#30340;&#26631;&#20934;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21435;&#38543;&#26426;&#21270;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#23436;&#20840;&#21487;&#35745;&#31639;&#30340;&#65292;&#21462;&#20915;&#20110;&#21021;&#22987;&#20998;&#24067;&#30340;&#23494;&#24230;&#21644;&#36712;&#36857;&#19978;&#35757;&#32451;&#30446;&#26631;&#30340;&#28023;&#26862;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12289;&#21160;&#37327;&#31639;&#27861;&#21644;&#38459;&#23612;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish disintegrated PAC-Bayesian generalisation bounds for models trained with gradient descent methods or continuous gradient flows. Contrary to standard practice in the PAC-Bayesian setting, our result applies to optimisation algorithms that are deterministic, without requiring any de-randomisation step. Our bounds are fully computable, depending on the density of the initial distribution and the Hessian of the training objective over the trajectory. We show that our framework can be applied to a variety of iterative optimisation algorithms, including stochastic gradient descent (SGD), momentum-based schemes, and damped Hamiltonian dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.03923</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#20960;&#20309;&#35282;&#24230;&#29702;&#35299;VAEs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#25915;&#20987;&#26102;&#65292;&#23545;&#25163;&#20250;&#25214;&#21040;&#19968;&#20010;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#23567;&#25200;&#21160;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21464;&#20854;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#19968;&#20010;&#22266;&#23450;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#24050;&#30693;&#30340;&#21407;&#22240;&#26159;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#23558;&#20854;&#32534;&#30721;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20302;/&#38646;&#23494;&#24230;&#21306;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#26080;&#38480;&#21046;&#30340;&#29983;&#25104;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;&#32534;&#30721;&#22120;&#30340;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#27979;&#37327;&#23427;&#20174;&#36755;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#24494;&#23567;&#28508;&#22312;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20998;&#26512;&#36755;&#20837;&#25200;&#21160;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#25928;&#26524;&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#23384;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;&#20449;&#24687;&#20256;&#36755;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#22336;&#20449;&#36947;&#30340;&#21472;&#21152;&#29305;&#24615;&#65292;&#36890;&#36807;&#20248;&#21270;&#38271;&#26399;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#22823;&#21270;&#20449;&#24687;&#20256;&#36755;&#25968;&#25454;&#36895;&#29575;&#65292;&#24182;&#20445;&#35777;&#32473;&#23450;&#30340;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.12884</link><description>&lt;p&gt;
CFLIT&#65306;&#20849;&#23384;&#30340;&#32852;&#37030;&#23398;&#20064;&#19982;&#20449;&#24687;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
CFLIT: Coexisting Federated Learning and Information Transfer. (arXiv:2207.12884v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#23384;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;&#20449;&#24687;&#20256;&#36755;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#22336;&#20449;&#36947;&#30340;&#21472;&#21152;&#29305;&#24615;&#65292;&#36890;&#36807;&#20248;&#21270;&#38271;&#26399;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#22823;&#21270;&#20449;&#24687;&#20256;&#36755;&#25968;&#25454;&#36895;&#29575;&#65292;&#24182;&#20445;&#35777;&#32473;&#23450;&#30340;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#26080;&#32447;&#32593;&#32476;&#23558;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#21644;&#26222;&#36941;&#25968;&#25454;&#20256;&#36755;&#31561;&#22810;&#26679;&#21270;&#30340;&#31227;&#21160;&#26381;&#21153;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20998;&#24067;&#24335;&#31227;&#21160;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#21327;&#20316;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#12290;&#21033;&#29992;&#22810;&#22336;&#20449;&#36947;&#30340;&#21472;&#21152;&#29305;&#24615;&#65292;&#31354;&#20013;&#35745;&#31639;&#20801;&#35768;&#22823;&#37327;&#35774;&#22791;&#22312;&#21516;&#19968;&#26080;&#32447;&#36164;&#28304;&#19978;&#24182;&#21457;&#19978;&#20256;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;&#21644;&#20256;&#32479;&#20449;&#24687;&#20256;&#36755;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#20849;&#23384;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#23384;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;&#20449;&#24687;&#20256;&#36755;&#36890;&#20449;&#26694;&#26550;&#65288;CFLIT&#65289;&#65292;&#20854;&#20013;&#32852;&#37030;&#23398;&#20064;&#21644;&#20449;&#24687;&#20256;&#36755;&#35774;&#22791;&#20849;&#20139;OFDM&#31995;&#32479;&#20013;&#30340;&#26080;&#32447;&#39057;&#35889;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#38271;&#26399;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#22823;&#21270;&#20449;&#24687;&#20256;&#36755;&#25968;&#25454;&#36895;&#29575;&#65292;&#24182;&#20445;&#35777;&#32473;&#23450;&#30340;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future wireless networks are expected to support diverse mobile services, including artificial intelligence (AI) services and ubiquitous data transmissions. Federated learning (FL), as a revolutionary learning approach, enables collaborative AI model training across distributed mobile edge devices. By exploiting the superposition property of multiple-access channels, over-the-air computation allows concurrent model uploading from massive devices over the same radio resources, and thus significantly reduces the communication cost of FL. In this paper, we study the coexistence of over-the-air FL and traditional information transfer (IT) in a mobile edge network. We propose a coexisting federated learning and information transfer (CFLIT) communication framework, where the FL and IT devices share the wireless spectrum in an OFDM system. Under this framework, we aim to maximize the IT data rate and guarantee a given FL convergence performance by optimizing the long-term radio resource alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.09098</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#23384;&#22312;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#35757;&#32451;&#40065;&#26834;&#24615;&#24378;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28982;&#32780;&#65292;&#23427;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#20026;&#20154;&#20204;&#25152;&#29087;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#37322;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21457;&#23637;&#30340;&#26032;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23558;&#20043;&#21069;&#24050;&#30693;&#30340;&#23545;&#25239;&#20998;&#31867;&#39118;&#38505;&#30340;&#23384;&#22312;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#20195;&#29702;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#22791;&#21463;&#24573;&#35270;&#65292;&#20294;&#25968;&#25454;&#30456;&#20851;&#30340;&#21021;&#22987;&#21270;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#36866;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2206.07630</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Initialization of the Sinkhorn Algorithm. (arXiv:2206.07630v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#22791;&#21463;&#24573;&#35270;&#65292;&#20294;&#25968;&#25454;&#30456;&#20851;&#30340;&#21021;&#22987;&#21270;&#21487;&#20197;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#24182;&#36866;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#26368;&#21021;&#34987;&#21046;&#23450;&#20026;&#32447;&#24615;&#35268;&#21010;&#65292;&#20294;&#28155;&#21152;&#29109;&#27491;&#21017;&#21270;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#26524;&#12290;Sinkhorn&#31639;&#27861;&#26159;&#35299;&#20915;&#36825;&#20010;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#26368;&#24120;&#29992;&#26041;&#27861;&#65292;&#24182;&#19988;&#24050;&#32463;&#26377;&#22810;&#27425;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;&#22914;&#27491;&#21017;&#21270;&#21442;&#25968;&#36864;&#28779;&#12289;&#21160;&#37327;&#25110;&#21152;&#36895;&#24230;&#26469;&#20943;&#23569;&#20854;&#36816;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#30340;&#21069;&#25552;&#26159;Sinkhorn&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#20851;&#27880;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20004;&#31181;&#20808;&#20837;&#20026;&#20027;&#30340;&#35266;&#24565;:&#30001;&#20110;&#27491;&#21017;&#21270;&#30340;OT&#38382;&#39064;&#26159;&#20984;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#20540;&#24471;&#35774;&#35745;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#65292;&#22240;&#20026;&#20219;&#20309;&#21021;&#22987;&#21270;&#37117;&#26159;&#21487;&#34892;&#30340;&#65307;&#20854;&#27425;&#65292;&#22240;&#20026;Sinkhorn&#31639;&#27861;&#30340;&#36755;&#20986;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#31649;&#36947;&#20013;&#23637;&#24320;&#65292;&#25152;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#21021;&#22987;&#21270;&#20250;&#23545;&#38597;&#21508;&#27604;&#35745;&#31639;&#36896;&#25104;&#20559;&#24046;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#20256;&#32479;&#26234;&#24935;&#65292;&#24182;&#23637;&#31034;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#21021;&#22987;&#21270;&#21487;&#20197;&#25552;&#39640;Sinkhorn&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36866;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the optimal transport (OT) problem was originally formulated as a linear program, the addition of entropic regularization has proven beneficial both computationally and statistically, for many applications. The Sinkhorn fixed-point algorithm is the most popular approach to solve this regularized problem, and, as a result, multiple attempts have been made to reduce its runtime using, e.g., annealing in the regularization parameter, momentum or acceleration. The premise of this work is that initialization of the Sinkhorn algorithm has received comparatively little attention, possibly due to two preconceptions: since the regularized OT problem is convex, it may not be worth crafting a good initialization, since any is guaranteed to work; secondly, because the outputs of the Sinkhorn algorithm are often unrolled in end-to-end pipelines, a data-dependent initialization would bias Jacobian computations. We challenge this conventional wisdom, and show that data-dependent initializers re
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2203.05711</link><description>&lt;p&gt;
&#30005;&#24433;&#21465;&#36848;&#25688;&#35201;&#65306;&#19968;&#20010;&#29992;&#20110;&#25925;&#20107;&#29702;&#35299;&#30340;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05711
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;-&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#29992;&#20110;&#25512;&#36827;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#26377;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25925;&#20107;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#19968;&#20010;&#35270;&#39057;&#35821;&#35328;&#25925;&#20107;&#25968;&#25454;&#38598;SYMON&#65292;&#20854;&#20013;&#21253;&#21547;5,193&#20010;&#27969;&#34892;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#25688;&#35201;&#12290;SYMON&#25429;&#25417;&#20102;&#30001;&#20154;&#31867;&#21019;&#20316;&#32773;&#21046;&#20316;&#30340;&#38754;&#21521;&#20154;&#31867;&#35266;&#20247;&#30340;&#33258;&#28982;&#25925;&#20107;&#21465;&#36848;&#35270;&#39057;&#12290;&#20316;&#20026;&#19968;&#20010;&#21407;&#22411;&#21644;&#33258;&#28982;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;SYMON&#20855;&#26377;&#39640;&#35206;&#30422;&#30340;&#22810;&#27169;&#24577;&#25925;&#20107;&#20107;&#20214;&#12289;&#20016;&#23500;&#30340;&#24515;&#29702;&#29366;&#24577;&#25551;&#36848;&#21644;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#22823;&#35821;&#20041;&#24046;&#36317;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#21644;&#30005;&#24433;&#25688;&#35201;&#35270;&#39057;&#30340;&#38646;&#26679;&#26412;&#23545;&#40784;&#30340;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#22312;&#25925;&#20107;&#29702;&#35299;&#20013;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SYMON&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#22810;&#27169;&#24577;&#25925;&#20107;&#29702;&#35299;&#30340;&#36827;&#23637;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances of AI, story understanding remains an open and under-investigated problem. We collect, preprocess, and publicly release a video-language story dataset, Synopses of Movie Narratives (SYMON), containing 5,193 video summaries of popular movies and TV series. SYMON captures naturalistic story-telling videos for human audience made by human creators. As a prototypical and naturalistic story dataset, SYMON features high coverage of multimodal story events, abundant mental-state descriptions, and large semantic gaps between the visual and the textual modalities. We establish benchmarks on video-text retrieval and zero-shot alignment on movie summary videos, which showcase the importance of in-domain data in story understanding. With SYMON, we hope to lay the groundwork for progress in multimodal story understanding.
&lt;/p&gt;</description></item><item><title>GraphTune&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20801;&#35768;&#25105;&#20204;&#35843;&#25972;&#29983;&#25104;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#20540;&#20316;&#20026;&#26465;&#20214;&#65292;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2201.11494</link><description>&lt;p&gt;
GraphTune&#65306;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#32467;&#26500;&#29305;&#24449;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphTune: A Learning-based Graph Generative Model with Tunable Structural Features. (arXiv:2201.11494v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11494
&lt;/p&gt;
&lt;p&gt;
GraphTune&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20801;&#35768;&#25105;&#20204;&#35843;&#25972;&#29983;&#25104;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#20540;&#20316;&#20026;&#26465;&#20214;&#65292;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20960;&#21313;&#24180;&#21069;&#24320;&#22987;&#30740;&#31350;&#29983;&#25104;&#22270;&#30340;&#26041;&#27861;&#20197;&#26469;&#65292;&#36825;&#19968;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#24320;&#22987;&#20851;&#27880;&#33021;&#22815;&#37325;&#29616;&#23454;&#38469;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#26159;&#22312;&#26465;&#20214;&#29983;&#25104;&#36890;&#29992;&#22270;&#24418;&#26041;&#38754;&#36824;&#26377;&#24456;&#22810;&#25506;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;GraphTune&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#23558;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#20540;&#20316;&#20026;&#26465;&#20214;&#36827;&#34892;&#35843;&#25972;&#12290;GraphTune&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#20351;&#24471;&#21487;&#20197;&#35843;&#25972;&#29983;&#25104;&#22270;&#20013;&#30340;&#20219;&#20309;&#32467;&#26500;&#29305;&#24449;&#30340;&#20540;&#12290;&#20351;&#29992;&#30495;&#23454;&#22270;&#24418;&#25968;&#25454;&#38598;&#23545;GraphTune&#21644;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;GraphTune&#21487;&#20197;&#26356;&#28165;&#26224;&#22320;&#35843;&#25972;&#29983;&#25104;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#20540;&#65292;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for graphs have been actively studied for decades, and they have a wide range of applications. Recently, learning-based graph generation that reproduces real-world graphs has been attracting the attention of many researchers. Although several generative models that utilize modern machine learning technologies have been proposed, conditional generation of general graphs has been less explored in the field. In this paper, we propose a generative model that allows us to tune the value of a global-level structural feature as a condition. Our model, called GraphTune, makes it possible to tune the value of any structural feature of generated graphs using Long Short Term Memory (LSTM) and a Conditional Variational AutoEncoder (CVAE). We performed comparative evaluations of GraphTune and conventional models on a real graph dataset. The evaluations show that GraphTune makes it possible to more clearly tune the value of a global-level structural feature better than conventional
&lt;/p&gt;</description></item><item><title>CFU Playground&#26159;&#19968;&#20010;&#20840;&#26632;&#24320;&#28304;&#26694;&#26550;&#65292;&#20026;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#24555;&#36895;&#36845;&#20195;&#35774;&#35745;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;CFU Playground&#65292;&#21487;&#20197;&#23450;&#21046;&#21644;&#21327;&#21516;&#20248;&#21270;&#23454;&#39564;&#24615;&#21644;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#30456;&#23545;&#36739;&#23567;&#30340;&#23450;&#21046;&#25237;&#36164;&#21363;&#21487;&#35753;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21644;&#36719;&#20214;&#24320;&#21457;&#32773;&#33719;&#24471;&#26174;&#33879;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2201.01863</link><description>&lt;p&gt;
CFU Playground: &#38754;&#21521;FPGA&#30340;TinyML&#21152;&#36895;&#30340;&#20840;&#26632;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CFU Playground: Full-Stack Open-Source Framework for Tiny Machine Learning (tinyML) Acceleration on FPGAs. (arXiv:2201.01863v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01863
&lt;/p&gt;
&lt;p&gt;
CFU Playground&#26159;&#19968;&#20010;&#20840;&#26632;&#24320;&#28304;&#26694;&#26550;&#65292;&#20026;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#24555;&#36895;&#36845;&#20195;&#35774;&#35745;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;CFU Playground&#65292;&#21487;&#20197;&#23450;&#21046;&#21644;&#21327;&#21516;&#20248;&#21270;&#23454;&#39564;&#24615;&#21644;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#30456;&#23545;&#36739;&#23567;&#30340;&#23450;&#21046;&#25237;&#36164;&#21363;&#21487;&#35753;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21644;&#36719;&#20214;&#24320;&#21457;&#32773;&#33719;&#24471;&#26174;&#33879;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#39640;&#25928;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#30340;&#38656;&#27714;&#20652;&#29983;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#21457;&#23637;&#12290;&#19987;&#29992;&#30828;&#20214;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20986;&#38656;&#35201;&#26356;&#28789;&#27963;&#30340;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#39046;&#22495;&#29305;&#23450;&#20248;&#21270;&#24037;&#20855;&#27969;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CFU Playground&#65306;&#19968;&#31181;&#20840;&#26632;&#24320;&#28304;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#36845;&#20195;&#35774;&#35745;&#21644;&#35780;&#20272;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#24320;&#28304;&#30340;FPGA&#21644;&#26410;&#26469;&#31995;&#32479;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#27969;&#31243;&#12290;&#36825;&#20010;&#20840;&#26632;&#26694;&#26550;&#20351;&#29992;&#25143;&#21487;&#20197;&#35775;&#38382;&#23450;&#21046;&#21644;&#21327;&#21516;&#20248;&#21270;&#30340;&#23454;&#39564;&#24615;&#21644;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#65292;&#29305;&#21035;&#38024;&#23545;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;&#20351;&#29992;CFU Playground&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#22238;&#36335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#22238;&#25253;&#65292;&#32780;&#30456;&#23545;&#36739;&#23567;&#30340;&#23450;&#21046;&#25237;&#36164;&#21363;&#21487;&#35753;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21644;&#36719;&#20214;&#24320;&#21457;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Need for the efficient processing of neural networks has given rise to the development of hardware accelerators. The increased adoption of specialized hardware has highlighted the need for more agile design flows for hardware-software co-design and domain-specific optimizations. In this paper, we present CFU Playground: a full-stack open-source framework that enables rapid and iterative design and evaluation of machine learning (ML) accelerators for embedded ML systems. Our tool provides a completely open-source end-to-end flow for hardware-software co-design on FPGAs and future systems research. This full-stack framework gives the users access to explore experimental and bespoke architectures that are customized and co-optimized for embedded ML. Our rapid, deploy-profile-optimization feedback loop lets ML hardware and software developers achieve significant returns out of a relatively small investment in customization. Using CFU Playground's design and evaluation loop, we show substan
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#22312;&#20551;&#35774;Quantified SETH&#30340;&#24773;&#20917;&#19979;&#65292;&#25490;&#38500;&#20102;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#22235;&#27425;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.03222</link><description>&lt;p&gt;
&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Complexity of 1-Center in Various Metrics. (arXiv:2112.03222v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03222
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24230;&#37327;&#19979;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#22312;&#20551;&#35774;Quantified SETH&#30340;&#24773;&#20917;&#19979;&#65292;&#25490;&#38500;&#20102;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#22235;&#27425;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#65306;&#32473;&#23450;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#32452;n&#20010;&#28857;P&#65292;&#25214;&#21040;&#36317;&#31163;&#20854;&#20182;P&#20013;&#28857;&#36317;&#31163;&#26368;&#22823;&#30340;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;d&#32500;lp&#24230;&#37327;&#20013;&#20197;&#21450;&#22522;&#20110;&#23383;&#31526;&#20018;&#38271;&#24230;&#20026;d&#30340;&#32534;&#36753;&#21644;Ulam&#24230;&#37327;&#20013;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;1-&#20013;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#25353;d&#36827;&#34892;&#20998;&#31867;&#12290;&#23567;d&#26102;&#65292;&#22312;&#20551;&#35774;&#21629;&#20013;&#38598;&#21512;&#29468;&#24819;&#65288;HSC&#65289;&#25104;&#31435;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;d=&#969;&#65288;logn&#65289;&#26102;&#65292;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;lp&#24230;&#37327;&#25110;&#32534;&#36753;&#25110;Ulam&#24230;&#37327;&#23454;&#29616;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#27425;&#20108;&#27425;&#31639;&#27861;&#12290;&#22823;d&#26102;&#65292;&#25105;&#20204;&#23558;&#26377;&#26465;&#20214;&#30340;&#19979;&#30028;&#25512;&#24191;&#21040;&#22522;&#20110;&#32534;&#36753;&#24230;&#37327;&#20013;&#30340;1-&#20013;&#24515;&#38382;&#39064;&#65292;&#25490;&#38500;&#20102;&#27425;&#22235;&#27425;&#31639;&#27861;&#65288;&#20551;&#35774;Quantified SETH&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;Ulam&#24230;&#37327;&#20013;1-&#20013;&#24515;&#38382;&#39064;&#30340;&#65288;1+&#949;&#65289;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20801;&#35768;&#36817;&#20284;&#26469;&#21152;&#24378;&#20102;&#19968;&#20123;&#20043;&#21069;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classic 1-center problem: Given a set $P$ of $n$ points in a metric space find the point in $P$ that minimizes the maximum distance to the other points of $P$. We study the complexity of this problem in $d$-dimensional $\ell_p$-metrics and in edit and Ulam metrics over strings of length $d$. Our results for the 1-center problem may be classified based on $d$ as follows.  $\bullet$ Small $d$: Assuming the hitting set conjecture (HSC), we show that when $d=\omega(\log n)$, no subquadratic algorithm can solve 1-center problem in any of the $\ell_p$-metrics, or in edit or Ulam metrics.  $\bullet$ Large $d$: When $d=\Omega(n)$, we extend our conditional lower bound to rule out subquartic algorithms for 1-center problem in edit metric (assuming Quantified SETH). On the other hand, we give a $(1+\epsilon)$-approximation for 1-center in Ulam metric with running time $\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$.  We also strengthen some of the above lower bounds by allowing approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#26469;&#22788;&#29702;Hessian&#30697;&#38453;&#65292;&#24182;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2112.01401</link><description>&lt;p&gt;
&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Newton methods based convolution neural networks using parallel processing. (arXiv:2112.01401v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#26469;&#22788;&#29702;Hessian&#30697;&#38453;&#65292;&#24182;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26159;&#19968;&#20010;&#39640;&#32500;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22312;&#26080;&#27861;&#33258;&#20449;&#22320;&#35774;&#32622;&#21442;&#25968;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#29575;&#24456;&#20302;&#12290;&#36807;&#21435;&#30340;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#29275;&#39039;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29275;&#39039;&#26041;&#27861;&#28041;&#21450;&#22797;&#26434;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#30340;Hessian&#29275;&#39039;&#26041;&#27861;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29275;&#39039;&#26041;&#27861;&#22788;&#29702;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#32780;&#19981;&#26159;&#19968;&#27425;&#21482;&#22788;&#29702;&#37096;&#20998;&#25968;&#25454;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#22788;&#29702;&#32780;&#19981;&#26159;&#20018;&#34892;&#22788;&#29702;&#26469;&#36827;&#34892;&#23567;&#25209;&#37327;&#35745;&#31639;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#24182;&#34892;&#22788;&#29702;&#24471;&#21040;&#30340;&#32467;&#26524;&#20248;&#20110;&#20043;&#21069;&#25152;&#29992;&#26041;&#27861;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of convolutional neural networks is a high dimensional and a non-convex optimization problem. At present, it is inefficient in situations where parametric learning rates can not be confidently set. Some past works have introduced Newton methods for training deep neural networks. Newton methods for convolutional neural networks involve complicated operations. Finding the Hessian matrix in second-order methods becomes very complex as we mainly use the finite differences method with the image data. Newton methods for convolutional neural networks deals with this by using the sub-sampled Hessian Newton methods. In this paper, we have used the complete data instead of the sub-sampled methods that only handle partial data at a time. Further, we have used parallel processing instead of serial processing in mini-batch computations. The results obtained using parallel processing in this study, outperform the time taken by the previous approach.
&lt;/p&gt;</description></item><item><title>CyTran &#26159;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#23545;&#25239;&#21367;&#31215; Transformer &#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#38750;&#23545;&#27604;&#24230;&#21644;&#23545;&#27604;&#24230; CT &#25195;&#25551;&#30340;&#32763;&#35793;&#65292;&#21487;&#20026;&#19981;&#33021;&#27880;&#23556;&#23545;&#27604;&#21058;&#30340;&#24739;&#32773;&#33258;&#21160;&#29983;&#25104;&#23545;&#27604; CT &#25195;&#25551;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27604;&#21644;&#38750;&#23545;&#27604; CT &#25195;&#25551;&#30340;&#23545;&#40784;&#12290; CyTran outperforms state-of-the-art models for both non-contrast to contrast and contrast to non-contrast CT translation tasks.</title><link>http://arxiv.org/abs/2110.06400</link><description>&lt;p&gt;
CyTran: &#19968;&#31181;&#22810;&#32423;&#36830;&#36143;&#24615;&#30340;&#24490;&#29615;&#19968;&#33268; Transformer &#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#23545;&#27604;&#24230;&#21040;&#23545;&#27604;&#24230; CT &#25195;&#25551;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CyTran: A Cycle-Consistent Transformer with Multi-Level Consistency for Non-Contrast to Contrast CT Translation. (arXiv:2110.06400v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06400
&lt;/p&gt;
&lt;p&gt;
CyTran &#26159;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#23545;&#25239;&#21367;&#31215; Transformer &#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#38750;&#23545;&#27604;&#24230;&#21644;&#23545;&#27604;&#24230; CT &#25195;&#25551;&#30340;&#32763;&#35793;&#65292;&#21487;&#20026;&#19981;&#33021;&#27880;&#23556;&#23545;&#27604;&#21058;&#30340;&#24739;&#32773;&#33258;&#21160;&#29983;&#25104;&#23545;&#27604; CT &#25195;&#25551;&#65292;&#21516;&#26102;&#22686;&#24378;&#23545;&#27604;&#21644;&#38750;&#23545;&#27604; CT &#25195;&#25551;&#30340;&#23545;&#40784;&#12290; CyTran outperforms state-of-the-art models for both non-contrast to contrast and contrast to non-contrast CT translation tasks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#23545;&#27604; CT &#25195;&#25551;&#21644;&#23545;&#27604; CT &#25195;&#25551;&#30456;&#20114;&#32763;&#35793;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26377;&#20004;&#20010;&#37325;&#35201;&#24212;&#29992;&#65306;(i) &#20026;&#37027;&#20123;&#19981;&#33021;&#27880;&#23556;&#23545;&#27604;&#21058;&#30340;&#24739;&#32773;&#33258;&#21160;&#29983;&#25104;&#23545;&#27604; CT &#25195;&#25551;&#65292;(ii) &#36890;&#36807;&#20943;&#23569;&#23545;&#27604;&#21058;&#24341;&#36215;&#30340;&#24046;&#24322;&#26469;&#22686;&#24378;&#23545;&#27604;&#21644;&#38750;&#23545;&#27604; CT &#25195;&#25551;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#23545;&#25239;&#21367;&#31215; Transformer &#27169;&#22411;&#65292;&#31616;&#31216; CyTran&#12290;&#30001;&#20110;&#25972;&#21512;&#20102;&#22810;&#32423;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#22312;&#26410;&#25104;&#23545;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38500;&#20102;&#22312;&#22270;&#20687;&#32423;&#21035;&#24212;&#29992;&#26631;&#20934;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#20013;&#38388;&#29305;&#24449;&#34920;&#36798;&#20043;&#38388;&#24212;&#29992;&#39069;&#22806;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#24378;&#21046;&#27169;&#22411;&#22312;&#22810;&#20010;&#34920;&#31034;&#23618;&#38754;&#19978;&#20445;&#25345;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#24212;&#23545;&#38750;&#23545;&#27604;&#24230;&#21644;&#23545;&#27604;&#24230; CT &#25195;&#25551;&#20043;&#38388;&#30340;&#39640;&#20998;&#36776;&#29575;&#32454;&#33410;&#21644;&#22823;&#21464;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#32763;&#35793;&#21644;&#30446;&#26631;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CyTran &#22312;&#38750;&#23545;&#27604;&#24230;&#21040;&#23545;&#27604;&#24230;&#21644;&#23545;&#27604;&#24230;&#21040;&#38750;&#23545;&#27604;&#24230; CT &#25195;&#25551;&#30340;&#32763;&#35793;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast CT scans and the other way around. Solving this task has two important applications: (i) to automatically generate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to enhance the alignment between contrast and non-contrast CT by reducing the differences induced by the contrast substance before registration. Our approach is based on cycle-consistent generative adversarial convolutional transformers, for short, CyTran. Our neural model can be trained on unpaired images, due to the integration of a multi-level cycle-consistency loss. Aside from the standard cycle-consistency loss applied at the image level, we propose to apply additional cycle-consistency losses between intermediate feature representations, which enforces the model to be cycle-consistent at multiple representations levels, leading to superior results. To deal with high-re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31232;&#30095;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;(FedSubAvg)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#23548;&#33268;&#30340;&#35745;&#31639;&#19979;&#38477;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#24179;&#22343;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#26032;&#24230;&#37327;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2109.07704</link><description>&lt;p&gt;
Federated Submodel Optimization for Hot and Cold Data Features&#65288;&#28909;&#28857;&#21644;&#20919;&#38376;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Federated Submodel Optimization for Hot and Cold Data Features. (arXiv:2109.07704v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31232;&#30095;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;(FedSubAvg)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#23548;&#33268;&#30340;&#35745;&#31639;&#19979;&#38477;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#24179;&#22343;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#26032;&#24230;&#37327;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23454;&#38469;&#25968;&#25454;&#29305;&#24449;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20855;&#26377;&#31232;&#30095;&#29305;&#24449;&#65292;&#24182;&#19988;&#26576;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#36890;&#24120;&#20165;&#28041;&#21450;&#23436;&#25972;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#31216;&#20026;&#23376;&#27169;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#25110;&#20854;&#21464;&#20307;&#23558;&#20005;&#37325;&#20943;&#24930;&#65292;&#22240;&#20026;&#22312;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#26102;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#38500;&#20854;&#23376;&#27169;&#22411;&#22806;&#30340;&#38646;&#26356;&#26032;&#34987;&#19981;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#65288;FedSubAvg&#65289;&#65292;&#30830;&#20445;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#30340;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19968;&#20010;&#31216;&#20026;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#30340;&#26032;&#24230;&#37327;&#19978;&#30028;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;FedSubAvg&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20010;&#26032;&#24230;&#37327;&#21487;&#20197;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#65292;&#32780;&#20256;&#32479;&#30340;&#24179;&#26041;&#26799;&#24230;&#24230;&#37327;&#21017;&#26080;&#27861;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study practical data characteristics underlying federated learning, where non-i.i.d. data from clients have sparse features, and a certain client's local data normally involves only a small part of the full model, called a submodel. Due to data sparsity, the classical federated averaging (FedAvg) algorithm or its variants will be severely slowed down, because when updating the global model, each client's zero update of the full model excluding its submodel is inaccurately aggregated. Therefore, we propose federated submodel averaging (FedSubAvg), ensuring that the expectation of the global update of each model parameter is equal to the average of the local updates of the clients who involve it. We theoretically proved the convergence rate of FedSubAvg by deriving an upper bound under a new metric called the element-wise gradient norm. In particular, this new metric can characterize the convergence of federated optimization over sparse data, while the conventional metric of squared g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion SB (DSB) &#30340;&#36924;&#36817;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#31243;&#24207;&#65292;&#29992;&#20110;&#35299;&#20915;Schr&#246;dinger Bridge&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#20135;&#29983;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#32780;&#26377;&#38480;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2106.01357</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#30340;&#25193;&#25955;Schr&#246;dinger&#26725;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion SB (DSB) &#30340;&#36924;&#36817;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#31243;&#24207;&#65292;&#29992;&#20110;&#35299;&#20915;Schr&#246;dinger Bridge&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#20135;&#29983;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#32780;&#26377;&#38480;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#24212;&#29992;&#39640;&#26031;&#22122;&#22768;&#21487;&#20197;&#23558;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#36817;&#20284;&#39640;&#26031;&#20998;&#24067;&#12290;&#21453;&#21521;&#23450;&#20041;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#12290;&#24403;&#27491;&#21521;&#22122;&#22768;&#36807;&#31243;&#30001;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#32473;&#20986;&#26102;&#65292;Song&#31561;&#20154;&#65288;2021&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#20272;&#35745;&#30456;&#24212;&#26102;&#38388;&#19981;&#22343;&#21248;&#28418;&#31227;&#30340;&#21453;&#21521;&#26102;&#38388;SDE&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#36816;&#34892;&#36275;&#22815;&#38271;&#26102;&#38388;&#30340;&#27491;&#21521;&#26102;&#38388;SDE&#65292;&#25165;&#33021;&#20351;&#26368;&#32456;&#20998;&#24067;&#36817;&#20284;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35299;&#20915;Schr&#246;dinger&#26725;&#38382;&#39064;(SB)&#65292;&#21363;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#30340;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#21487;&#20197;&#20135;&#29983;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#32780;&#26377;&#38480;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;SB&#65288;DSB&#65289;&#65292;&#19968;&#20010;&#35299;&#20915;SB&#38382;&#39064;&#30340;&#21407;&#22987;&#36817;&#20284;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#31243;&#24207;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#20998;&#26512;&#21644;&#29983;&#25104;&#24314;&#27169;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21482;&#38656;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#26377;&#25928;&#38477;&#20302;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2105.11453</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Semiconductor Device Modeling for Electronic Design Automation by Machine Learning Techniques. (arXiv:2105.11453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21482;&#38656;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#26377;&#25928;&#38477;&#20302;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#25216;&#26415;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;TCAD&#65289;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;&#23545;&#21322;&#23548;&#20307;&#34892;&#19994;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#30001;&#20110;&#22120;&#20214;&#21046;&#36896;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#21322;&#23548;&#20307;&#34892;&#19994;&#20013;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#25216;&#26415;&#25913;&#36827;&#22522;&#20110;ML&#30340;&#22120;&#20214;&#24314;&#27169;&#12290;&#36825;&#20123;&#25216;&#26415;&#21482;&#38656;&#35201;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;TCAD&#24037;&#20855;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#38227;&#27694;&#22120;&#20214;&#20013;&#27431;&#22982;&#30005;&#38459;&#20540;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#26102;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#38477;&#20302;&#20102;70%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22266;&#26377;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semiconductors industry benefits greatly from the integration of Machine Learning (ML)-based techniques in Technology Computer-Aided Design (TCAD) methods. The performance of ML models however relies heavily on the quality and quantity of training datasets. They can be particularly difficult to obtain in the semiconductor industry due to the complexity and expense of the device fabrication. In this paper, we propose a self-augmentation strategy for improving ML-based device modeling using variational autoencoder-based techniques. These techniques require a small number of experimental data points and does not rely on TCAD tools. To demonstrate the effectiveness of our approach, we apply it to a deep neural network-based prediction task for the Ohmic resistance value in Gallium Nitride devices. A 70% reduction in mean absolute error when predicting experimental results is achieved. The inherent flexibility of our approach allows easy adaptation to various tasks, thus making it highl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;AMAFQI&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#26041;&#27861;FQI&#65292;AMAFQI&#30340;&#35745;&#31639;&#37327;&#22686;&#38271;&#26356;&#20026;&#32531;&#24930;&#65292;&#20197;&#32447;&#24615;&#22686;&#38271;&#65292;&#19988;&#20223;&#30495;&#23454;&#39564;&#26174;&#31034;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#31867;&#20284;&#12290;</title><link>http://arxiv.org/abs/2104.09343</link><description>&lt;p&gt;
&#36817;&#20284;&#30340;&#22810;&#26234;&#33021;&#20307;&#25311;&#21512;Q&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Approximated Multi-Agent Fitted Q Iteration. (arXiv:2104.09343v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;AMAFQI&#65292;&#21487;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#26041;&#27861;FQI&#65292;AMAFQI&#30340;&#35745;&#31639;&#37327;&#22686;&#38271;&#26356;&#20026;&#32531;&#24930;&#65292;&#20197;&#32447;&#24615;&#22686;&#38271;&#65292;&#19988;&#20223;&#30495;&#23454;&#39564;&#26174;&#31034;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#21363;&#36817;&#20284;&#30340;&#22810;&#26234;&#33021;&#20307;&#25311;&#21512;Q&#36845;&#20195;&#65288;AMAFQI&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31574;&#30053;&#25628;&#32034;&#65292;&#24182;&#23637;&#31034;&#20854;&#24471;&#21040;&#30340;&#31574;&#30053;&#20855;&#26377;&#22810;&#20010;&#38598;&#20013;&#23398;&#20064;&#21040;&#30340;Q&#20989;&#25968;&#30340;&#36817;&#20284;&#30340;&#36138;&#23146;&#29305;&#24615;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#21644;&#31574;&#30053;&#35780;&#20272;&#20013;&#65292;AMAFQI&#38656;&#35201;&#30340;&#35745;&#31639;&#37327;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#21576;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;&#25311;&#21512;Q&#36845;&#20195;&#65288;FQI&#65289;&#38656;&#35201;&#30340;&#35745;&#31639;&#37327;&#21017;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;FQI&#26159;&#22312;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;AMAFQI&#30340;&#36825;&#20010;&#29305;&#24615;&#23545;&#20110;&#35774;&#35745;&#21487;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#20223;&#30495;&#20013;&#35780;&#20272;&#20102;AMAFQI&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;FQI&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#20351;&#29992;AMAFQI&#32780;&#19981;&#26159;FQI&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#35777;&#23454;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#31867;&#20284;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate an efficient approximation for multi-agent batch reinforcement learning, the approximated multi-agent fitted Q iteration (AMAFQI). We present a detailed derivation of our approach. We propose an iterative policy search and show that it yields a greedy policy with respect to multiple approximations of the centralized, learned Q-function. In each iteration and policy evaluation, AMAFQI requires a number of computations that scales linearly with the number of agents whereas the analogous number of computations increase exponentially for the fitted Q iteration (FQI), a commonly used approaches in batch reinforcement learning. This property of AMAFQI is fundamental for the design of a tractable multi-agent approach. We evaluate the performance of AMAFQI and compare it to FQI in numerical simulations. The simulations illustrate the significant computation time reduction when using AMAFQI instead of FQI in multi-agent problems and corroborate the similar performance of both appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36135;&#36816;&#39044;&#35746;&#25511;&#21046;&#26041;&#38754;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20316;&#32773;&#25351;&#20986;&#65292;&#24815;&#20363;&#19978;&#22312;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#35299;&#20915;&#36816;&#33829;&#38382;&#39064;&#21487;&#33021;&#22826;&#32791;&#36153;&#26102;&#38388;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2102.00092</link><description>&lt;p&gt;
&#29992;&#20110;&#36135;&#36816;&#39044;&#35746;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Freight Booking Control Problems. (arXiv:2102.00092v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36135;&#36816;&#39044;&#35746;&#25511;&#21046;&#26041;&#38754;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20316;&#32773;&#25351;&#20986;&#65292;&#24815;&#20363;&#19978;&#22312;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#35299;&#20915;&#36816;&#33829;&#38382;&#39064;&#21487;&#33021;&#22826;&#32791;&#36153;&#26102;&#38388;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35746;&#25511;&#21046;&#38382;&#39064;&#26159;&#25910;&#30410;&#31649;&#29702;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#36135;&#36816;&#39044;&#35746;&#25511;&#21046;&#20391;&#37325;&#20110;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#25110;&#25298;&#32477;&#39044;&#35746;&#35831;&#27714;&#65306;&#22312;&#26377;&#38480;&#30340;&#23481;&#37327;&#19979;&#65292;&#25509;&#21463;&#39044;&#35746;&#35831;&#27714;&#25110;&#25298;&#32477;&#23427;&#20197;&#20445;&#30041;&#23481;&#37327;&#20379;&#26410;&#26469;&#21487;&#33021;&#20855;&#26377;&#26356;&#39640;&#25910;&#30410;&#30340;&#39044;&#35746;&#20351;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#65292;&#20854;&#20013;&#25509;&#21463;&#19968;&#32452;&#35831;&#27714;&#23548;&#33268;&#22312;&#39044;&#35746;&#26399;&#32467;&#26463;&#26102;&#33719;&#24471;&#30340;&#21033;&#28070;&#65292;&#35813;&#21033;&#28070;&#21462;&#20915;&#20110;&#23653;&#34892;&#25509;&#21463;&#30340;&#39044;&#35746;&#30340;&#25104;&#26412;&#12290;&#23545;&#20110;&#35768;&#22810;&#36135;&#36816;&#24212;&#29992;&#65292;&#28385;&#36275;&#35831;&#27714;&#30340;&#25104;&#26412;&#26159;&#36890;&#36807;&#35299;&#20915;&#36816;&#33829;&#20915;&#31574;&#38382;&#39064;&#33719;&#24471;&#30340;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#24120;&#35268;&#35299;&#20915;&#27492;&#31867;&#36816;&#33829;&#38382;&#39064;&#21487;&#33021;&#22826;&#32791;&#36153;&#26102;&#38388;&#12290;&#22823;&#22810;&#25968;&#39044;&#35746;&#25511;&#21046;&#31574;&#30053;&#26159;&#36890;&#36807;&#35299;&#20915;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Booking control problems are sequential decision-making problems that occur in the domain of revenue management. More precisely, freight booking control focuses on the problem of deciding to accept or reject bookings: given a limited capacity, accept a booking request or reject it to reserve capacity for future bookings with potentially higher revenue. This problem can be formulated as a finite-horizon stochastic dynamic program, where accepting a set of requests results in a profit at the end of the booking period that depends on the cost of fulfilling the accepted bookings. For many freight applications, the cost of fulfilling requests is obtained by solving an operational decision-making problem, which often requires the solutions to mixed-integer linear programs. Routinely solving such operational problems when deploying reinforcement learning algorithms may be too time consuming. The majority of booking control policies are obtained by solving problem-specific mathematical program
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25112;&#30053;&#20195;&#29702;&#30340;&#27835;&#30103;&#20998;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20248;&#35268;&#21017;&#21487;&#20197;&#28041;&#21450;&#38543;&#26426;&#21270;&#65292;&#23545;&#37027;&#20123;&#24179;&#22343;&#19978;&#23545;&#27835;&#30103;&#26377;&#31215;&#26497;&#21453;&#24212;&#30340;&#20010;&#20307;&#20063;&#21487;&#20197;&#20998;&#37197;&#19981;&#21040;100%&#30340;&#27835;&#30103;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2011.06528</link><description>&lt;p&gt;
&#20855;&#26377;&#25112;&#30053;&#20195;&#29702;&#30340;&#27835;&#30103;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Treatment Allocation with Strategic Agents. (arXiv:2011.06528v5 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.06528
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#25112;&#30053;&#20195;&#29702;&#30340;&#27835;&#30103;&#20998;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20248;&#35268;&#21017;&#21487;&#20197;&#28041;&#21450;&#38543;&#26426;&#21270;&#65292;&#23545;&#37027;&#20123;&#24179;&#22343;&#19978;&#23545;&#27835;&#30103;&#26377;&#31215;&#26497;&#21453;&#24212;&#30340;&#20010;&#20307;&#20063;&#21487;&#20197;&#20998;&#37197;&#19981;&#21040;100%&#30340;&#27835;&#30103;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#20010;&#20307;&#29305;&#24449;&#30340;&#27835;&#30103;&#20998;&#37197;&#20135;&#29983;&#20852;&#36259;&#65292;&#20363;&#22914;&#23450;&#21521;&#33829;&#38144;&#12289;&#20010;&#24615;&#21270;&#20449;&#29992;&#25253;&#20215;&#21644;&#24322;&#36136;&#24615;&#23450;&#20215;&#31561;&#12290;&#27835;&#30103;&#20010;&#24615;&#21270;&#24341;&#20837;&#20102;&#20010;&#20307;&#20462;&#25913;&#20854;&#34892;&#20026;&#20197;&#33719;&#24471;&#26356;&#22909;&#27835;&#30103;&#30340;&#28608;&#21169;&#12290;&#25112;&#30053;&#34892;&#20026;&#20250;&#25913;&#21464;&#20849;&#21464;&#37327;&#21644;&#28508;&#22312;&#32467;&#26524;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#22312;&#27809;&#26377;&#25112;&#30053;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#35268;&#21017;&#23558;&#20998;&#37197;&#27835;&#30103;&#23545;&#20110;&#26377;&#31215;&#26497;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#20010;&#20307;&#12290;&#32780;&#22312;&#23384;&#22312;&#25112;&#30053;&#34892;&#20026;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#35268;&#21017;&#21487;&#20197;&#28041;&#21450;&#38543;&#26426;&#21270;&#65292;&#21363;&#20351;&#26159;&#23545;&#37027;&#20123;&#24179;&#22343;&#19978;&#23545;&#27835;&#30103;&#26377;&#31215;&#26497;&#21453;&#24212;&#30340;&#20010;&#20307;&#65292;&#20063;&#21487;&#20197;&#20998;&#37197;&#19981;&#21040;100%&#30340;&#27835;&#30103;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39034;&#24207;&#23454;&#39564;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#20010;&#20307;&#25112;&#30053;&#34892;&#20026;&#21442;&#25968;&#20551;&#35774;&#19979;&#65292;&#25910;&#25947;&#21040;&#26368;&#20248;&#27835;&#30103;&#20998;&#37197;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in allocating treatments based on observed individual characteristics: examples include targeted marketing, individualized credit offers, and heterogeneous pricing. Treatment personalization introduces incentives for individuals to modify their behavior to obtain a better treatment. Strategic behavior shifts the joint distribution of covariates and potential outcomes. The optimal rule without strategic behavior allocates treatments only to those with a positive Conditional Average Treatment Effect. With strategic behavior, we show that the optimal rule can involve randomization, allocating treatments with less than 100% probability even to those who respond positively on average to the treatment. We propose a sequential experiment based on Bayesian Optimization that converges to the optimal treatment rule without parametric assumptions on individual strategic behavior.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#22797;&#26434;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#26032;&#22411;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#23384;&#22312;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#25910;&#25947;&#21450;&#19981;&#31283;&#23450;&#24615;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;GANs&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2005.00065</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#32508;&#36848;&#65306;&#25361;&#25112;&#65292;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs Survey): Challenges, Solutions, and Future Directions. (arXiv:2005.00065v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00065
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#22797;&#26434;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#26032;&#22411;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#23384;&#22312;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#25910;&#25947;&#21450;&#19981;&#31283;&#23450;&#24615;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;GANs&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;GANs&#21487;&#20197;&#38544;&#24335;&#22320;&#23398;&#20064;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#39640;&#32500;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#24403;&#35774;&#35745;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#20351;&#29992;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#36873;&#25321;&#65292;GANs&#30340;&#35757;&#32451;&#23384;&#22312;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#22914;&#27169;&#24335;&#23849;&#28291;&#65292;&#19981;&#25910;&#25947;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22522;&#20110;&#37325;&#26032;&#35774;&#35745;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#26367;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26356;&#22909;&#30340;GANs&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31687;&#19987;&#38376;&#20851;&#27880;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#27867;&#21644;&#31995;&#32479;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GANs&#35774;&#35745;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) is a novel class of deep generative models which has recently gained significant attention. GANs learns complex and high-dimensional distributions implicitly over images, audio, and data. However, there exists major challenges in training of GANs, i.e., mode collapse, non-convergence and instability, due to inappropriate design of network architecture, use of objective function and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges.
&lt;/p&gt;</description></item></channel></rss>