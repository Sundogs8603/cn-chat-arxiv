<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01849</link><description>&lt;p&gt;
&#20132;&#21449;&#25193;&#25955;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#37319;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#20013;&#65292;&#24182;&#20174;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#29305;&#24322;&#33021;&#21147;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30446;&#26631;&#26469;&#22686;&#24378;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#12290;&#26631;&#20934;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#21160;&#20316;&#24207;&#21015;&#65292;&#26465;&#20214;&#26159;&#35270;&#35273;&#35266;&#27979;&#21644;&#20854;&#20182;&#20302;&#32500;&#29366;&#24577;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#30721;&#22120;&#65292;&#20174;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#37325;&#26500;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#65288;&#21644;&#20854;&#20182;&#29366;&#24577;&#20449;&#24687;&#65289;&#65292;&#24182;&#20351;&#29992;SSL&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Crossway Diffusion&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#20854;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#28151;&#21512;&#24577;&#37325;&#26500;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#19979;&#19981;&#21516;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26356;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.01840</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#29366;&#24577;&#37325;&#26500;&#30340;&#32463;&#39564;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Empirical Sample Complexity of Neural Network Mixed State Reconstruction. (arXiv:2307.01840v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#28151;&#21512;&#24577;&#37325;&#26500;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#19979;&#19981;&#21516;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26356;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#37327;&#23376;&#24577;&#36827;&#34892;&#37327;&#23376;&#29366;&#24577;&#37325;&#26500;&#34987;&#25552;&#20986;&#20316;&#20026;&#20943;&#23569;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37327;&#23376;&#20987;&#31359;&#22797;&#26434;&#24615;&#30340;&#21487;&#34892;&#24037;&#20855;&#65292;&#24182;&#19988;&#22312;&#20027;&#35201;&#20851;&#27880;&#26080;&#22122;&#22768;&#24773;&#20917;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28151;&#21512;&#24577;&#65306;&#26377;&#38480;&#28201;&#24230;&#20234;&#36763;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#25968;&#23383;&#19978;&#30340;&#24615;&#33021;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31995;&#32479;&#22320;&#20943;&#23569;&#31639;&#27861;&#30340;&#37327;&#23376;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#65292;&#21363;&#31070;&#32463;&#23494;&#24230;&#31639;&#31526;&#21644;&#27491;&#31639;&#31526;&#20540;&#27979;&#37327;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#24577;&#28151;&#21512;&#31243;&#24230;&#21464;&#21270;&#26102;&#30340;&#19981;&#21516;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#33539;&#22260;&#20869;&#65292;&#26576;&#20123;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#25351;&#20986;&#35774;&#35745;&#26356;&#21152;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#65292;&#26080;&#35770;&#26159;&#20174;&#35745;&#31639;&#22797;&#26434;&#24615;&#36824;&#26159;&#20174;&#24615;&#33021;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques. Then, we compare the two leading neural quantum state encodings of the state, namely, the Neural Density Operator and the positive operator-valued measurement representation, and illustrate their different performance as the mixedness of the target state varies. We find that certain encodings are more efficient in different regimes of mixedness and point out the need for designing more efficient encodings in terms of both cla
&lt;/p&gt;</description></item><item><title>DiT-3D&#26159;&#19968;&#31181;&#38024;&#23545;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#22411;&#25193;&#25955;Transformer&#65292;&#36890;&#36807;&#22312;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#32467;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#32858;&#21512;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.01831</link><description>&lt;p&gt;
DiT-3D: &#25506;&#32034;&#29992;&#20110;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#32431;&#25193;&#25955;Transformer
&lt;/p&gt;
&lt;p&gt;
DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01831
&lt;/p&gt;
&lt;p&gt;
DiT-3D&#26159;&#19968;&#31181;&#38024;&#23545;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#22411;&#25193;&#25955;Transformer&#65292;&#36890;&#36807;&#22312;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#32467;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#32858;&#21512;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25193;&#25955;Transformer&#65288;&#20363;&#22914;DiT&#65289;&#24050;&#32463;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25928;&#26524;&#12290;&#20294;&#26159;&#65292;&#26159;&#21542;Transformer&#26550;&#26500;&#22312;3D&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#21516;&#26679;&#26377;&#25928;&#20173;&#28982;&#26377;&#24453;&#30830;&#23450;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;3D&#25193;&#25955;&#26041;&#27861;&#22823;&#37096;&#20998;&#37319;&#29992;&#20102;U-Net&#26550;&#26500;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#25193;&#25955;Transformer&#65292;&#31216;&#20026;DiT-3D&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#22312;&#29992;&#20110;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;U-Net&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;DiT-3D&#22312;&#27169;&#22411;&#23610;&#23544;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#20135;&#29983;&#30340;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#26356;&#39640;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DiT-3D&#37319;&#29992;&#20102;DiT&#30340;&#35774;&#35745;&#29702;&#24565;&#65292;&#20294;&#36890;&#36807;&#34701;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26469;&#33258;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#20943;&#23569;3D&#24418;&#29366;&#29983;&#25104;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#22312;Transformer&#22359;&#20013;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#21152;3D&#20196;&#29260;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token le
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#21487;&#20197;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#23545;&#32593;&#32476;&#26131;&#21463;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.01827</link><description>&lt;p&gt;
&#35299;&#26500;&#25968;&#25454;&#37325;&#24314;&#65306;&#22810;&#31867;&#21035;&#12289;&#26435;&#37325;&#34928;&#20943;&#21644;&#36890;&#29992;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses. (arXiv:2307.01827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#21487;&#20197;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#23545;&#32593;&#32476;&#26131;&#21463;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#36816;&#20316;&#30340;&#29702;&#35299;&#36824;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26368;&#36817;&#65292;Haim&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#22810;&#23618;&#24863;&#30693;&#22120;&#20108;&#20803;&#20998;&#31867;&#22120;&#20013;&#37325;&#24314;&#35757;&#32451;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#32593;&#32476;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#22823;&#37096;&#20998;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#26041;&#21521;&#19978;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#21457;&#29616;&#65292;&#21253;&#25324;&#20174;&#22810;&#31867;&#21035;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#37325;&#24314;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#37325;&#24314;&#26041;&#26696;&#65292;&#21487;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#22238;&#24402;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24433;&#21709;&#32593;&#32476;&#26131;&#21463;&#27492;&#31867;&#37325;&#24314;&#26041;&#26696;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#20250;&#22686;&#21152;&#37325;&#24314;&#33021;&#21147;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#37327;&#36824;&#26159;&#36136;&#37327;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#39564;&#20102;&#31070;&#32463;&#20803;&#25968;&#37327;&#30456;&#23545;&#20110;&#35757;&#32451;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24179;&#34913;&#21644;&#38543;&#26426;&#28216;&#36208;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#30340;&#32467;&#26500;&#24179;&#34913;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#21160;&#24577;&#23646;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#21457;&#29616;&#24403;&#32593;&#32476;&#26159;&#32467;&#26500;&#24179;&#34913;&#30340;&#26102;&#20505;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#24403;&#32593;&#32476;&#20005;&#26684;&#19981;&#24179;&#34913;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01813</link><description>&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24179;&#34913;&#21644;&#38543;&#26426;&#28216;&#36208;&#19982;&#22797;&#25968;&#26435;&#37325;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Structural Balance and Random Walks on Complex Networks with Complex Weights. (arXiv:2307.01813v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24179;&#34913;&#21644;&#38543;&#26426;&#28216;&#36208;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#30340;&#32467;&#26500;&#24179;&#34913;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#21160;&#24577;&#23646;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#21457;&#29616;&#24403;&#32593;&#32476;&#26159;&#32467;&#26500;&#24179;&#34913;&#30340;&#26102;&#20505;&#65292;&#21487;&#20197;&#23454;&#29616;&#23616;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#24403;&#32593;&#32476;&#20005;&#26684;&#19981;&#24179;&#34913;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#25968;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#23450;&#20041;&#20102;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37327;&#23376;&#29289;&#29702;&#20013;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#38750;&#23545;&#35282;&#39033;&#23601;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22312;&#36793;&#30340;&#26435;&#37325;&#26159;&#22797;&#25968;&#26102;&#25299;&#23637;&#32593;&#32476;&#31185;&#23398;&#24037;&#20855;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26435;&#37325;&#30697;&#38453;&#26159;&#21380;&#31859;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#24182;&#30740;&#31350;&#20102;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#23646;&#24615;&#12290;&#22522;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#24179;&#34913;&#30340;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#31181;&#31867;&#22411;&#20013;&#30340;&#20849;&#20139;&#35889;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#23545;&#22797;&#25968;&#26435;&#37325;&#32593;&#32476;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#21270;&#65292;&#24403;&#22270;&#26159;&#32467;&#26500;&#24179;&#34913;&#30340;&#26102;&#20505;&#65292;&#23616;&#37096;&#19968;&#33268;&#24615;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#65292;&#32780;&#24403;&#22270;&#20005;&#26684;&#19981;&#24179;&#34913;&#26102;&#65292;&#20840;&#23616;&#19968;&#33268;&#24615;&#23558;&#34987;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;p&#20540;....&#65288;&#21407;&#25991;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Complex numbers define the relationship between entities in many situations. A canonical example would be the off-diagonal terms in a Hamiltonian matrix in quantum physics. Recent years have seen an increasing interest to extend the tools of network science when the weight of edges are complex numbers. Here, we focus on the case when the weight matrix is Hermitian, a reasonable assumption in many applications, and investigate both structural and dynamical properties of the complex-weighted networks. Building on concepts from signed graphs, we introduce a classification of complex-weighted networks based on the notion of structural balance, and illustrate the shared spectral properties within each type. We then apply the results to characterise the dynamics of random walks on complex-weighted networks, where local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. Finally, we explore p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#25429;&#25417;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#23616;&#37096;&#28201;&#24230;&#28436;&#21464;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;R^2&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01804</link><description>&lt;p&gt;
&#36890;&#36807;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#25429;&#25417;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#23616;&#37096;&#28201;&#24230;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators. (arXiv:2307.01804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#25429;&#25417;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#23616;&#37096;&#28201;&#24230;&#28436;&#21464;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;R^2&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20445;&#30495;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#27169;&#25311;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#28909;&#34892;&#20026;&#65292;&#23545;&#20110;&#25552;&#39640;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#22312;&#38646;&#20214;&#35774;&#35745;&#12289;&#24037;&#33402;&#35268;&#21010;&#12289;&#30417;&#25511;&#21644;&#25511;&#21046;&#26041;&#38754;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38646;&#20214;&#20960;&#20309;&#24418;&#29366;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;&#27169;&#22411;&#24456;&#38590;&#22312;&#24191;&#27867;&#30340;&#20960;&#20309;&#24418;&#29366;&#33539;&#22260;&#20869;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27169;&#22411;&#25253;&#21578;&#20102;&#25972;&#20010;&#39046;&#22495;&#65288;&#38646;&#20214;&#65289;&#30340;&#20302;&#22343;&#26041;&#24046;&#65288;MSE&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#38500;&#20102;&#36817;&#26399;&#27785;&#31215;&#30340;&#28909;&#24433;&#21709;&#21306;&#22495;&#22806;&#65292;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#21306;&#22495;&#30340;&#28201;&#24230;&#21464;&#21270;&#24182;&#19981;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;MSE&#30340;&#27169;&#22411;&#20445;&#30495;&#24230;&#27979;&#37327;&#21487;&#33021;&#34987;&#39640;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#25429;&#25417;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#23616;&#37096;&#28201;&#24230;&#28436;&#21464;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#25552;&#20986;&#20351;&#29992;R^2&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#65292;&#35813;&#25351;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
High-fidelity, data-driven models that can quickly simulate thermal behavior during additive manufacturing (AM) are crucial for improving the performance of AM technologies in multiple areas, such as part design, process planning, monitoring, and control. However, the complexities of part geometries make it challenging for current models to maintain high accuracy across a wide range of geometries. Additionally, many models report a low mean square error (MSE) across the entire domain (part). However, in each time step, most areas of the domain do not experience significant changes in temperature, except for the heat-affected zones near recent depositions. Therefore, the MSE-based fidelity measurement of the models may be overestimated.  This paper presents a data-driven model that uses Fourier Neural Operator to capture the local temperature evolution during the additive manufacturing process. In addition, the authors propose to evaluate the model using the $R^2$ metric, which provides
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;&#38750;&#23545;&#27604;&#24230;MRI&#19978;&#23545;&#32925;&#32959;&#30244;&#36827;&#34892;&#23450;&#37327;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#36793;&#32536;&#24863;&#30693;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01798</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#38750;&#23545;&#27604;&#24230;MRI&#19978;&#32925;&#32959;&#30244;&#30340;&#23450;&#37327;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI. (arXiv:2307.01798v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;&#38750;&#23545;&#27604;&#24230;MRI&#19978;&#23545;&#32925;&#32959;&#30244;&#36827;&#34892;&#23450;&#37327;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#36793;&#32536;&#24863;&#30693;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#38750;&#23545;&#27604;&#24230;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;NCMRI&#65289;&#19978;&#21516;&#26102;&#36827;&#34892;&#32925;&#32959;&#30244;&#30340;&#22810;&#25351;&#26631;&#23450;&#37327;&#12289;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#20934;&#30830;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#32570;&#20047;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;NCMRI&#34701;&#21512;&#26426;&#21046;&#21644;&#20934;&#30830;&#30340;&#36793;&#30028;&#20449;&#24687;&#25429;&#25417;&#26426;&#21046;&#65292;&#20351;&#24471;&#36825;&#20123;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;&#22810;&#20219;&#21153;&#32593;&#32476;&#65288;EaMtNet&#65289;&#65292;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;NCMRI&#19978;&#20851;&#32852;&#32925;&#32959;&#30244;&#30340;&#22810;&#25351;&#26631;&#23450;&#37327;&#12289;&#20998;&#21106;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;EaMtNet&#37319;&#29992;&#20004;&#20010;&#24179;&#34892;&#30340;CNN&#32534;&#30721;&#22120;&#21644;Sobel&#28388;&#27874;&#22120;&#26469;&#25552;&#21462;&#23616;&#37096;&#29305;&#24449;&#21644;&#36793;&#32536;&#22270;&#20687;&#12290;&#26032;&#35774;&#35745;&#30340;&#36793;&#32536;&#24863;&#30693;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;EaFA&#65289;&#29992;&#20110;&#29305;&#24449;&#34701;&#21512;&#21644;&#36873;&#25321;&#65292;&#36890;&#36807;&#25429;&#25417;&#29305;&#24449;&#21644;&#36793;&#32536;&#22270;&#20687;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20351;&#32593;&#32476;&#20855;&#26377;&#36793;&#32536;&#24863;&#30693;&#33021;&#21147;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#21033;&#29992;&#39044;&#27979;&#24046;&#24322;&#26469;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#24182;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods lack an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, making these tasks challenging. To address these issues, this paper proposes a unified framework, namely edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs two parallel CNN encoders and the Sobel filters to extract local features and edge maps, respectively. The newly designed edge-aware feature aggregation module (EaFA) is used for feature fusion and selection, making the network edge-aware by capturing long-range dependency between feature and edge maps. Multi-tasking leverages prediction discrepancy to estimate uncertainty and improve s
&lt;/p&gt;</description></item><item><title>GHOST&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#21152;&#36895;GNN&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21152;&#36895;&#22120;&#30340;&#32570;&#28857;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01782</link><description>&lt;p&gt;
GHOST:&#19968;&#31181;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
GHOST: A Graph Neural Network Accelerator using Silicon Photonics. (arXiv:2307.01782v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01782
&lt;/p&gt;
&lt;p&gt;
GHOST&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#21152;&#36895;GNN&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21152;&#36895;&#22120;&#30340;&#32570;&#28857;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24314;&#27169;&#21644;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;GNN&#30340;&#33021;&#21147;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#31561;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#21152;&#36895;&#21644;&#39640;&#25928;&#22788;&#29702;GNN&#38656;&#35201;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36825;&#26159;&#30001;&#20110;GNN&#20855;&#26377;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;CMOS&#24179;&#21488;&#32553;&#25918;&#30340;&#20943;&#36895;&#20063;&#25512;&#21160;&#20102;&#23545;&#26367;&#20195;&#23454;&#29616;&#22522;&#20307;&#30340;&#23547;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GHOST&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;GNN&#30340;&#30789;&#20809;&#23376;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;GHOST&#39640;&#25928;&#22320;&#20943;&#36731;&#20102;&#19982;&#39030;&#28857;&#21644;&#36793;&#25805;&#20316;&#30456;&#20851;&#30340;&#25104;&#26412;&#12290;&#23427;&#22312;&#20809;&#23398;&#39046;&#22495;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;&#36816;&#34892;GNN&#25152;&#28041;&#21450;&#30340;&#19977;&#20010;&#20027;&#35201;&#38454;&#27573;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, 
&lt;/p&gt;</description></item><item><title>FedHIL&#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#35774;&#22791;&#30340;&#40065;&#26834;&#23460;&#20869;&#23450;&#20301;&#30340;&#24322;&#26500;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#23460;&#20869;&#23450;&#20301;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#25552;&#39640;&#22312;&#35774;&#22791;&#24322;&#26500;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.01780</link><description>&lt;p&gt;
FedHIL: &#38754;&#21521;&#31227;&#21160;&#35774;&#22791;&#30340;&#40065;&#26834;&#23460;&#20869;&#23450;&#20301;&#30340;&#24322;&#26500;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices. (arXiv:2307.01780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01780
&lt;/p&gt;
&lt;p&gt;
FedHIL&#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#35774;&#22791;&#30340;&#40065;&#26834;&#23460;&#20869;&#23450;&#20301;&#30340;&#24322;&#26500;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#23460;&#20869;&#23450;&#20301;&#21644;&#32852;&#37030;&#23398;&#20064;&#26469;&#25552;&#39640;&#22312;&#35774;&#22791;&#24322;&#26500;&#29615;&#22659;&#20013;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#22312;&#32039;&#24613;&#21709;&#24212;&#12289;&#20179;&#24211;&#31649;&#29702;&#21644;&#22686;&#24378;&#29616;&#23454;&#31561;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#21508;&#31181;&#23460;&#20869;&#21644;&#22320;&#19979;&#29615;&#22659;&#20013;&#36827;&#34892;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#22534;&#26632;&#30340;&#24322;&#26500;&#24615;&#65292;&#31934;&#30830;&#30340;&#23460;&#20869;&#23450;&#20301;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20301;&#32622;&#20272;&#35745;&#19981;&#19968;&#33268;&#21644;&#19981;&#20934;&#30830;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#20005;&#37325;&#20381;&#36182;&#20110;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#20854;&#22312;&#23460;&#20869;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#26102;&#26131;&#21463;&#24615;&#33021;&#19979;&#38477;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#35774;&#22791;&#24322;&#26500;&#24615;&#21644;&#32570;&#20047;&#36866;&#24212;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;FedHIL&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#23460;&#20869;&#23450;&#20301;&#21644;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#35774;&#22791;&#24322;&#26500;&#29615;&#22659;&#20013;&#25552;&#39640;&#23460;&#20869;&#23450;&#20301;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization plays a vital role in applications such as emergency response, warehouse management, and augmented reality experiences. By deploying machine learning (ML) based indoor localization frameworks on their mobile devices, users can localize themselves in a variety of indoor and subterranean environments. However, achieving accurate indoor localization can be challenging due to heterogeneity in the hardware and software stacks of mobile devices, which can result in inconsistent and inaccurate location estimates. Traditional ML models also heavily rely on initial training data, making them vulnerable to degradation in performance with dynamic changes across indoor environments. To address the challenges due to device heterogeneity and lack of adaptivity, we propose a novel embedded ML framework called FedHIL. Our framework combines indoor localization and federated learning (FL) to improve indoor localization accuracy in device-heterogeneous environments while also preserv
&lt;/p&gt;</description></item><item><title>Shapley Sets&#26159;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20989;&#25968;&#20998;&#35299;&#30340;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#23427;&#36991;&#20813;&#20102;Shapley&#20540;&#29305;&#24449;&#24402;&#22240;&#20013;&#30340;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#32467;&#26500;&#30340;&#25968;&#25454;&#31867;&#22411;&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01777</link><description>&lt;p&gt;
Shapley Sets: &#36890;&#36807;&#36882;&#24402;&#20989;&#25968;&#20998;&#35299;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Shapley Sets: Feature Attribution via Recursive Function Decomposition. (arXiv:2307.01777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01777
&lt;/p&gt;
&lt;p&gt;
Shapley Sets&#26159;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20989;&#25968;&#20998;&#35299;&#30340;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#23427;&#36991;&#20813;&#20102;Shapley&#20540;&#29305;&#24449;&#24402;&#22240;&#20013;&#30340;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#32467;&#26500;&#30340;&#25968;&#25454;&#31867;&#22411;&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Shapley&#20540;&#29305;&#24449;&#24402;&#22240;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#24341;&#23548;&#35823;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24402;&#22240;&#26041;&#27861;&#65292;&#21363;Shapley Sets&#65292;&#23427;&#23558;&#20215;&#20540;&#25480;&#20104;&#29305;&#24449;&#32452;&#21512;&#12290;Shapley Sets&#20351;&#29992;&#20855;&#26377;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#36882;&#24402;&#20989;&#25968;&#20998;&#35299;&#31639;&#27861;&#23558;&#24213;&#23618;&#27169;&#22411;&#20998;&#35299;&#20026;&#19981;&#21487;&#20998;&#30340;&#21464;&#37327;&#32452;&#12290; Shapley Sets&#20026;&#27599;&#20010;&#19981;&#21487;&#20998;&#21464;&#37327;&#32452;&#20998;&#37197;&#20854;&#23545;&#20110;&#29305;&#23450;&#39044;&#27979;&#30340;&#32452;&#21512;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;Shapley Sets&#31561;&#25928;&#20110;&#23545;&#21464;&#25442;&#21518;&#30340;&#29305;&#24449;&#38598;&#36827;&#34892;Shapley&#20540;&#35745;&#31639;&#65292;&#22240;&#27492;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#21516;&#26679;&#20844;&#29702;&#12290; Shapley Sets&#23545;&#20540;&#20989;&#25968;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;Shapley Sets&#22914;&#20309;&#36991;&#20813;&#19982;&#22522;&#20110;Shapley&#20540;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#20851;&#32852;&#30340;&#38519;&#38449;&#65292;&#24182;&#22312;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#32467;&#26500;&#30340;&#25968;&#25454;&#31867;&#22411;&#20013;&#20855;&#26377;&#29305;&#27530;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their ubiquitous use, Shapley value feature attributions can be misleading due to feature interaction in both model and data. We propose an alternative attribution approach, Shapley Sets, which awards value to sets of features. Shapley Sets decomposes the underlying model into non-separable variable groups using a recursive function decomposition algorithm with log linear complexity in the number of variables. Shapley Sets attributes to each non-separable variable group their combined value for a particular prediction. We show that Shapley Sets is equivalent to the Shapley value over the transformed feature set and thus benefits from the same axioms of fairness. Shapley Sets is value function agnostic and we show theoretically and experimentally how Shapley Sets avoids pitfalls associated with Shapley value based alternatives and are particularly advantageous for data types with complex dependency structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#36827;&#34892;&#36817;&#20284;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#30340;&#20195;&#29702;&#36317;&#31163;min-SWGG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20256;&#36755;&#35745;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#36866;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01770</link><description>&lt;p&gt;
&#24555;&#36895;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#23454;&#29616;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#29255;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#36827;&#34892;&#36817;&#20284;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#30340;&#20195;&#29702;&#36317;&#31163;min-SWGG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#20256;&#36755;&#35745;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#36866;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wassserstein&#36317;&#31163;&#21644;&#30456;&#20851;&#30340;&#26368;&#20248;&#36755;&#36816;&#35745;&#21010;&#22312;&#35768;&#22810;&#28041;&#21450;&#27010;&#29575;&#24230;&#37327;&#30340;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#26041;Wasserstein&#36317;&#31163;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;min-SWGG&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#36755;&#20837;&#20998;&#24067;&#30340;&#19968;&#32500;&#26368;&#20248;&#25237;&#24433;&#24341;&#23548;&#30340;&#36816;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;min-SWGG&#21644;Wasserstein&#24191;&#20041;&#27979;&#22320;&#32447;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#65292;&#20854;&#20013;&#26530;&#32445;&#27979;&#24230;&#22312;&#19968;&#26465;&#30452;&#32447;&#19978;&#24471;&#21040;&#25903;&#25345;&#12290;&#25105;&#20204;&#29305;&#21035;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#38381;&#21512;&#24418;&#24335;&#30340;&#31934;&#30830;Wasserstein&#36317;&#31163;&#65292;&#22312;&#20854;&#20013;&#19968;&#20010;&#20998;&#24067;&#25903;&#25345;&#22312;&#19968;&#26465;&#30452;&#32447;&#19978;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;min-SWGG&#26159;WD&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#23427;&#20855;&#26377;&#19982;Sliced-Wasserstein&#31867;&#20284;&#30340;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#65292;&#22914;&#36317;&#31163;&#24615;&#12289;&#24369;&#25910;&#25947;&#12289;&#35745;&#31639;&#21644;&#25299;&#25169;&#24615;&#36136;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21152;&#32435;&#20840;&#29983;&#21629;&#21608;&#26399;&#20316;&#29289;&#30142;&#30149;&#35782;&#21035;&#26696;&#20363;&#65292;&#23637;&#31034;&#20102;&#26412;&#22320;&#21270;&#25968;&#25454;&#24037;&#20316;&#20316;&#20026;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#20844;&#20849;&#39046;&#22495;&#20219;&#21153;&#22914;&#20892;&#19994;&#29983;&#20135;&#21147;&#21644;&#39135;&#21697;&#23433;&#20840;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01767</link><description>&lt;p&gt;
&#20316;&#20026;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#26412;&#22320;&#21270;&#25968;&#25454;&#24037;&#20316;&#65306;&#21152;&#32435;&#20840;&#29983;&#21629;&#21608;&#26399;&#20316;&#29289;&#30142;&#30149;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana. (arXiv:2307.01767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21152;&#32435;&#20840;&#29983;&#21629;&#21608;&#26399;&#20316;&#29289;&#30142;&#30149;&#35782;&#21035;&#26696;&#20363;&#65292;&#23637;&#31034;&#20102;&#26412;&#22320;&#21270;&#25968;&#25454;&#24037;&#20316;&#20316;&#20026;&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#20844;&#20849;&#39046;&#22495;&#20219;&#21153;&#22914;&#20892;&#19994;&#29983;&#20135;&#21147;&#21644;&#39135;&#21697;&#23433;&#20840;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#32435;&#20154;&#24037;&#26234;&#33021;&#33136;&#26524;&#30142;&#30149;&#35782;&#21035;&#39033;&#30446;&#23637;&#31034;&#20102;&#22312;&#20892;&#19994;&#29983;&#20135;&#21147;&#21644;&#39135;&#21697;&#23433;&#20840;&#31561;&#20844;&#20849;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#30340;&#26412;&#22320;&#21270;&#25968;&#25454;&#24037;&#20316;&#20316;&#20026;&#23454;&#29616;&#26377;&#29992;&#30340;&#12289;&#38024;&#23545;&#26412;&#22320;&#38656;&#27714;&#30340;&#25968;&#25454;&#20013;&#24515;&#35299;&#20915;&#26041;&#26696;&#30340;&#20808;&#20915;&#26465;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#26080;&#20154;&#26426;&#25910;&#38598;&#30340;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30830;&#23450;&#20316;&#29289;&#30340;&#21387;&#21147;&#22240;&#32032;&#65292;&#24182;&#20849;&#21516;&#24320;&#21457;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26368;&#32456;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#26700;&#38754;&#24212;&#29992;&#31243;&#24207;&#21521;&#24403;&#22320;&#20892;&#27665;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65292;&#21033;&#29992;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01759</link><description>&lt;p&gt;
&#21482;&#38656;&#39044;&#35757;&#32451;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65292;&#21033;&#29992;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31934;&#31070;&#30142;&#30149;&#65292;&#34920;&#29616;&#20026;&#38750;&#20856;&#22411;&#30340;&#35748;&#30693;&#12289;&#24773;&#32490;&#21644;&#31038;&#20132;&#27169;&#24335;&#12290;&#21450;&#26102;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;ASD&#24739;&#32773;&#30340;&#26377;&#25928;&#24178;&#39044;&#21644;&#25913;&#21892;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65288;METAFormer&#65289;&#29992;&#20110;ASD&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;ABIDE I&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#65292;&#21253;&#25324;406&#21517;ASD&#24739;&#32773;&#21644;476&#21517;&#20856;&#22411;&#23545;&#29031;&#65288;TC&#65289;&#21463;&#35797;&#32773;&#12290;METAFormer&#37319;&#29992;&#20102;&#22810;atlas&#26041;&#27861;&#65292;&#20854;&#20013;&#26469;&#33258;AAL&#12289;CC200&#21644;DOS160&#22270;&#35889;&#30340;&#23637;&#24179;&#36830;&#25509;&#30697;&#38453;&#20316;&#20026;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20174;&#36755;&#20837;&#20013;&#37325;&#24314;&#25513;&#30721;&#20540;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25110;&#29420;&#31435;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#24182;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;</title><link>http://arxiv.org/abs/2307.01753</link><description>&lt;p&gt;
&#26469;&#33258;DESI&#20142;&#32418;&#26143;&#31995;&#22823;&#23610;&#24230;&#32858;&#31867;&#30340;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies. (arXiv:2307.01753v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Dark Energy Spectroscopic Instrument (DESI)&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#26469;&#38480;&#21046;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#21253;&#25324;&#36229;&#36807;1200&#19975;&#20010;&#30446;&#26631;&#65292;&#35206;&#30422;&#20102;14000&#24179;&#26041;&#24230;&#30340;&#22825;&#31354;&#21306;&#22495;&#65292;&#32418;&#31227;&#33539;&#22260;&#20026;0.2 &lt; z &lt; 1.35&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38134;&#27827;&#28040;&#20809;&#12289;&#35843;&#26597;&#28145;&#24230;&#21644;&#35266;&#27979;&#26465;&#20214;&#26159;&#31995;&#32479;&#35823;&#24046;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#24182;&#37319;&#29992;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#22823;&#23610;&#24230;&#19978;&#30340;&#38750;&#23431;&#23449;&#23398;&#36807;&#24230;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32463;&#36807;&#20102;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;fNL&#21644;&#31995;&#32479;&#35823;&#24046;&#30340;&#23545;&#25968;&#27491;&#24577;&#27169;&#25311;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22312;&#20943;&#23567;&#21097;&#20313;&#31995;&#32479;&#35823;&#24046;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;fNL&#30340;68\%&#65288;95\%&#65289;&#32622;&#20449;&#21306;&#38388;&#20026;fNL = 47^{+14(+29)}_{-11(-22)}&#12290;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25152;&#26377;&#25104;&#20687;&#22270;&#38598;&#36827;&#34892;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;
We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2&lt; z &lt; 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;SRCD&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01750</link><description>&lt;p&gt;
SRCD&#65306;&#22810;&#20010;&#22797;&#21512;&#39046;&#22495;&#30340;&#35821;&#20041;&#25512;&#29702;&#29992;&#20110;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;SRCD&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#21333;&#39046;&#22495;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#65288;&#21363;Single-DGOD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#32500;&#25252;&#33258;&#22686;&#24191;&#20041;&#36328;&#39046;&#22495;&#26679;&#26412;&#30340;&#35821;&#20041;&#32467;&#26500;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24863;&#20852;&#36259;&#12290;&#19982;&#22312;&#22810;&#20010;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;DGOD&#19981;&#21516;&#65292;Single-DGOD&#35201;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#28304;&#39046;&#22495;&#24456;&#38590;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#22810;&#20010;&#30446;&#26631;&#39046;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#37319;&#29992;&#20102;&#19982;DGOD&#31867;&#20284;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#36890;&#36807;&#35299;&#32806;&#21512;&#25110;&#21387;&#32553;&#35821;&#20041;&#31354;&#38388;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#26497;&#24230;&#31232;&#32570;&#30340;&#21333;&#39046;&#22495;&#25968;&#25454;&#65292;&#20266;&#23646;&#24615;-&#26631;&#31614;&#30456;&#20851;&#24615;&#65307;2&#65289;&#36890;&#24120;&#24573;&#30053;&#35821;&#20041;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#25105;&#20204;&#21457;&#29616;&#31034;&#20363;&#20013;&#30340;&#23454;&#20363;&#32423;&#35821;&#20041;&#20851;&#31995;&#30340;&#30456;&#20284;&#24615;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;Single-DGOD&#30340;&#22797;&#21512;&#39046;&#22495;&#30340;&#35821;&#20041;&#25512;&#29702;&#65288;SRCD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. 
&lt;/p&gt;</description></item><item><title>RRCNN&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21019;&#26032;&#30340;&#26041;&#24335;&#35745;&#31639;&#20449;&#21495;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01725</link><description>&lt;p&gt;
RRCNN:&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network. (arXiv:2307.01725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01725
&lt;/p&gt;
&lt;p&gt;
RRCNN&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21019;&#26032;&#30340;&#26041;&#24335;&#35745;&#31639;&#20449;&#21495;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#20449;&#21495;&#30340;&#20998;&#35299;&#26159;&#20449;&#21495;&#26102;&#39057;&#20998;&#26512;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#65292;&#22914;1998&#24180;&#30001;&#40644;&#23439;&#31077;&#31561;&#20154;&#39318;&#21019;&#30340;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#27861;&#65292;&#24050;&#34987;&#19981;&#21516;&#30340;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#36890;&#24120;&#23481;&#26131;&#20135;&#29983;&#36793;&#30028;&#21644;&#27169;&#24335;&#28151;&#21512;&#25928;&#24212;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#19981;&#22826;&#40065;&#26834;&#12290;&#21463;&#28145;&#24230;&#23398;&#20064;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#37492;&#20110;&#25991;&#29486;&#20013;&#32570;&#20047;&#30452;&#25509;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23558;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#20026;&#31616;&#21333;&#25391;&#33633;&#20998;&#37327;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20197;&#21019;&#26032;&#30340;&#26041;&#24335;&#35745;&#31639;&#20449;&#21495;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24182;&#30740;&#31350;&#19968;&#31181;&#26032;&#30340;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decomposition of non-stationary signals is an important and challenging task in the field of signal time-frequency analysis. In the recent two decades, many signal decomposition methods led by the empirical mode decomposition, which was pioneered by Huang et al. in 1998, have been proposed by different research groups. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning in fields like image processing and natural language processing, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the fram
&lt;/p&gt;</description></item><item><title>MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01719</link><description>&lt;p&gt;
MOPO-LSI&#65306;&#29992;&#25143;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01719
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#21487;&#25345;&#32493;&#25237;&#36164;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;MOPO-LSI&#29256;&#26412;1.0&#30340;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#37197;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;</title><link>http://arxiv.org/abs/2307.01717</link><description>&lt;p&gt;
&#20851;&#20110;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#32463;&#24120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#29992;&#20110;&#22686;&#21152;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#24182;&#21019;&#24314;&#30001;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#20998;&#24067;&#30456;&#20284;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30495;&#23454;&#24615;&#65289;&#20197;&#21450;&#28385;&#36275;&#19968;&#23450;&#25968;&#20540;&#32422;&#26463;&#26159;&#21453;&#20107;&#23454;&#26102;&#38388;&#24207;&#21015;&#22330;&#26223;&#29983;&#25104;&#35831;&#27714;&#20013;&#24120;&#35265;&#30340;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#32654;&#32852;&#20648;&#21457;&#24067;&#20102;&#32473;&#23450;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#25104;&#24066;&#22330;&#21387;&#21147;&#24773;&#26223;&#65292;&#20379;&#37329;&#34701;&#26426;&#26500;&#35780;&#20272;&#20854;&#22312;&#20551;&#35774;&#24615;&#34928;&#36864;&#20013;&#30340;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24809;&#32602;&#26469;&#24378;&#21046;&#28385;&#36275;&#32422;&#26463;&#65292;&#24182;&#25298;&#32477;&#19981;&#31526;&#21512;&#32422;&#26463;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#25913;&#21464;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#25298;&#32477;&#25277;&#26679;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01708</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36866;&#24403;&#30340;&#20215;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#39118;&#38505;&#20013;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#65292;&#20294;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#38024;&#23545;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#26159;&#35745;&#31639;&#22797;&#26434;&#65307;&#21478;&#19968;&#20010;&#26159;&#23454;&#38469;&#30340;&#21464;&#20307;&#65292;&#20801;&#35768;&#36873;&#25321;&#21487;&#20197;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#34920;&#26684;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#26469;&#23637;&#31034;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.01689</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#21644;&#20351;&#29992;ERM&#39044;&#35328;&#26426;&#35299;&#20915;&#26080;&#31351;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;ERM&#36275;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#24182;&#38750;&#22914;&#27492;&#65292;&#36890;&#24120;&#30340;&#27010;&#24565;&#31867;&#31639;&#27861;&#20381;&#36182;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#30340;&#39044;&#35328;&#26426;&#65292;&#22914;&#26631;&#20934;&#26368;&#20248;&#31639;&#27861;(SOA)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#20108;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;(regret)&#65292;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36890;&#36807;&#24213;&#23618;&#27010;&#24565;&#31867;&#30340;Littlestone&#21644;&#38408;&#20540;&#32500;&#24230;&#26469;&#38480;&#21046;&#36951;&#25022;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#65292;&#20854;&#20013;ERM&#39044;&#35328;&#26426;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#65292;&#26681;&#25454;&#20854;&#20182;&#29609;&#23478;&#30340;&#28216;&#25103;&#21382;&#21490;&#25214;&#21040;&#19968;&#20010;&#29609;&#23478;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#36164;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#30340;&#26550;&#26500;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01684</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38654;&#26381;&#21153;&#22120;&#20026;&#26234;&#33021;&#29289;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#22270;&#31070;&#32463;&#32593;&#32476;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services. (arXiv:2307.01684v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#36164;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#30340;&#26550;&#26500;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#22270;&#32467;&#26500;&#19978;&#25552;&#21462;&#28508;&#22312;&#34920;&#31034;&#33021;&#21147;&#30340;&#31361;&#20986;&#20248;&#21183;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#20026;&#29289;&#32852;&#32593;&#39537;&#21160;&#30340;&#26234;&#33021;&#24212;&#29992;&#25552;&#20379;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#26381;&#21153;&#33539;&#20363;&#36890;&#24120;&#36890;&#36807;&#23436;&#20840;&#19978;&#20256;&#22320;&#29702;&#20998;&#24067;&#30340;&#36755;&#20837;&#25968;&#25454;&#21040;&#36828;&#31243;&#25968;&#25454;&#20013;&#24515;&#26469;&#20381;&#36182;&#20113;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#37327;&#26174;&#31034;&#36825;&#31181;&#22522;&#20110;&#20113;&#30340;&#26381;&#21153;&#23384;&#22312;&#26174;&#33879;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#24212;&#29992;&#26032;&#20852;&#30340;&#38654;&#35745;&#31639;&#30340;&#28508;&#22312;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#24102;&#26469;&#30340;&#26550;&#26500;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#12290;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;Fograph&#30340;&#35774;&#35745;&#19982;GNN&#26381;&#21153;&#30340;&#29420;&#29305;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN servin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20540;&#21270;&#32593;&#32476;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01683</link><description>&lt;p&gt;
&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20540;&#21270;&#32593;&#32476;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#20108;&#20540;&#21270;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#26356;&#24555;&#30340;&#20301;&#36816;&#31639;&#26367;&#20195;&#35745;&#31639;&#22797;&#26434;&#30340;&#28014;&#28857;&#25805;&#20316;&#26469;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#65292;&#21487;&#20197;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#35757;&#32451;&#20855;&#26377;&#31163;&#25955;&#26435;&#37325;&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#20801;&#35768;&#31163;&#25955;&#28608;&#27963;&#12290;&#21407;&#22987;&#26041;&#27861;&#20248;&#21270;&#20102;&#31163;&#25955;&#26435;&#37325;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#23558;&#39044;&#28608;&#27963;&#36817;&#20284;&#20026;&#36830;&#32493;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27010;&#29575;&#24314;&#27169;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#31163;&#25955;&#28608;&#27963;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#23545;&#27604;&#21457;&#25955;&#65288;DCD&#65289;&#35757;&#32451;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#23545;&#27604;&#21457;&#25955;&#65288;CD&#65289;&#65292;DCD&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26356;&#39640;&#65292;&#24182;&#19988;&#19981;&#21463;&#38750;&#21487;&#24573;&#30053;&#26799;&#24230;&#39033;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.01668</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#23545;&#27604;&#21457;&#25955;&#35757;&#32451;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#23545;&#27604;&#21457;&#25955;&#65288;DCD&#65289;&#35757;&#32451;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#23545;&#27604;&#21457;&#25955;&#65288;CD&#65289;&#65292;DCD&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26356;&#39640;&#65292;&#24182;&#19988;&#19981;&#21463;&#38750;&#21487;&#24573;&#30053;&#26799;&#24230;&#39033;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#12290;&#20256;&#32479;&#30340;&#23545;&#27604;&#21457;&#25955;&#65288;CD&#65289;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65288;MCMCs&#65289;&#20174;EBM&#20013;&#37319;&#26679;&#65292;&#36825;&#23548;&#33268;&#20102;&#35745;&#31639;&#36127;&#25285;&#21644;CD&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#19981;&#21487;&#35843;&#21644;&#30340;&#25240;&#34935;&#12290;MCMCs&#30340;&#25910;&#25947;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#30701;&#26399;&#36816;&#34892;&#30340;MCMC&#20250;&#24341;&#20837;&#38590;&#20197;&#22788;&#29702;&#30340;&#39069;&#22806;&#21442;&#25968;&#26799;&#24230;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#25955;&#23545;&#27604;&#21457;&#25955;&#65288;DCD&#65289;&#31995;&#21015;&#30340;&#19968;&#33324;&#35299;&#37322;&#65292;&#23558;CD&#35270;&#20026;DCD&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#22312;CD&#20013;&#20351;&#29992;&#19981;&#21516;&#20110;Langevin&#21160;&#21147;&#23398;&#30340;EBM&#21442;&#25968;&#33258;&#30001;&#25193;&#25955;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#21457;&#25955;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DCD&#27604;CD&#26356;&#21152;&#35745;&#31639;&#39640;&#25928;&#65292;&#24182;&#19988;&#19981;&#21463;&#38750;&#21487;&#24573;&#30053;&#26799;&#24230;&#39033;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01649</link><description>&lt;p&gt;
&#20302;&#32500;&#27969;&#24418;&#19978;&#36807;&#21442;&#25968;&#21270;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;(ConvResNets)&#34429;&#28982;&#36807;&#21442;&#25968;&#21270;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#33021;&#22815;&#33719;&#24471;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36825;&#19981;&#33021;&#34987;&#24120;&#35268;&#26234;&#24935;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#20998;&#31867;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#30340;ConvResNeXts&#65288;&#35206;&#30422;ConvResNets&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20801;&#35768;ConvResNeXts&#20013;&#26377;&#26080;&#38480;&#22810;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#24182;&#26174;&#31034;&#26435;&#37325;&#34928;&#20943;&#38544;&#21547;&#22320;&#24378;&#21046;&#36825;&#20123;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#65292;&#28982;&#21518;&#35777;&#26126;ConvResNeXts&#21487;&#20197;&#36866;&#24212;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21644;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#32780;&#19981;&#21463;&#32500;&#24230;&#35781;&#21650;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37096;&#20998;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;ConvResNeXts&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#20114;&#36830;&#24615;&#25351;&#26631;&#12290;&#31639;&#27861;&#36890;&#36807;&#24314;&#27169;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24182;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#28982;&#21518;&#21033;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#35745;&#31639;&#20114;&#36830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01639</link><description>&lt;p&gt;
&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heuristic Algorithms for the Approximation of Mutual Coherence. (arXiv:2307.01639v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#20114;&#36830;&#24615;&#25351;&#26631;&#12290;&#31639;&#27861;&#36890;&#36807;&#24314;&#27169;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24182;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#28982;&#21518;&#21033;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#35745;&#31639;&#20114;&#36830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#36830;&#24615;&#26159;&#34913;&#37327;&#20004;&#31181;&#35266;&#28857;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#12290;&#23613;&#31649;&#35813;&#27010;&#24565;&#26469;&#33258;&#21746;&#23398;&#65292;&#20294;&#23427;&#22312;&#24191;&#27867;&#30340;&#25216;&#26415;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;Wahl-O-Mat&#31995;&#32479;&#12290;&#22312;&#24503;&#22269;&#65292;&#35813;&#31995;&#32479;&#24110;&#21161;&#36873;&#27665;&#25214;&#21040;&#26368;&#31526;&#21512;&#20182;&#20204;&#25919;&#27835;&#20559;&#22909;&#30340;&#20505;&#36873;&#20154;&#12290;&#30001;&#20110;&#35201;&#36845;&#20195;&#36941;&#21382;&#35266;&#28857;&#30340;&#25152;&#26377;&#23376;&#38598;&#65292;&#20934;&#30830;&#35745;&#31639;&#20114;&#36830;&#24615;&#38750;&#24120;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#27599;&#20010;&#23376;&#38598;&#65292;&#24517;&#39035;&#35299;&#20915;&#19968;&#20010;SAT&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#36825;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#30828;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26159;&#21152;&#36895;&#36825;&#31181;&#35745;&#31639;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#25152;&#35859;&#30340;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24314;&#27169;&#20026;&#19977;&#20010;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#20114;&#36830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20123;&#31639;&#27861;&#26159;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#65292;&#20854;&#20182;&#31639;&#27861;&#21482;&#38656;&#35201;&#35299;&#20915;&#23569;&#37327;&#30340;SA&#27169;&#22411;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HAGNN&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#22270;&#30340;&#28151;&#21512;&#32858;&#21512;&#26041;&#27861;&#12290;HAGNN&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#65292;&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.01636</link><description>&lt;p&gt;
HAGNN: &#28151;&#21512;&#32858;&#21512;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks. (arXiv:2307.01636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAGNN&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#22270;&#30340;&#28151;&#21512;&#32858;&#21512;&#26041;&#27861;&#12290;HAGNN&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#65292;&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#24322;&#26500;&#22270;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#29616;&#26377;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20803;&#36335;&#24452;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#31616;&#21333;&#30340;&#26080;&#20803;&#36335;&#24452;&#30340;&#21516;&#36136;&#22270;&#27169;&#22411;&#20063;&#21487;&#20197;&#21462;&#24471;&#30456;&#36817;&#30340;&#32467;&#26524;&#65292;&#36825;&#23545;&#20803;&#36335;&#24452;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#22522;&#20110;&#20803;&#36335;&#24452;&#21644;&#26080;&#20803;&#36335;&#24452;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#65292;&#21363;&#22914;&#20309;&#36873;&#25321;&#33410;&#28857;&#32858;&#21512;&#30340;&#37051;&#23621;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#20016;&#23500;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;HAGNN&#65288;&#28151;&#21512;&#32858;&#21512;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;HAGNN&#30340;&#26680;&#24515;&#26159;&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#12290;HAGNN&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#31639;&#27861;&#65292;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.01622</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#31639;&#27861;&#65292;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#65292;&#24182;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#33021;&#22815;&#24110;&#21161;&#37197;&#30005;&#32593;&#32476;&#26356;&#21152;&#39640;&#25928;&#21644;&#21487;&#38752;&#22320;&#36816;&#34892;&#65292;&#24182;&#26377;&#25928;&#22320;&#25972;&#21512;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39044;&#27979;&#12289;&#20248;&#21270;&#21644;&#25511;&#21046;/&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#38656;&#27714;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#24490;&#29615;&#36235;&#21183;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#35843;&#24230;&#39044;&#27979;&#65288;rTPNN-FES&#65289;&#65292;&#29992;&#20110;&#25552;&#20379;&#39640;&#25928;&#30340;&#20303;&#23429;&#38656;&#27714;&#25511;&#21046;&#12290;rTPNN-FES&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#24182;&#23433;&#25490;&#23478;&#30005;&#30340;&#20351;&#29992;&#26102;&#38388;&#12290;&#36890;&#36807;&#20854;&#23884;&#20837;&#24335;&#32467;&#26500;&#65292;rTPNN-FES&#28040;&#38500;&#20102;&#20351;&#29992;&#21333;&#29420;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#21644;&#35843;&#24230;&#30340;&#38656;&#35201;&#65292;&#24182;&#29983;&#25104;&#23545;&#39044;&#27979;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#35843;&#24230;&#23433;&#25490;&#12290;&#26412;&#25991;&#36824;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#22312;&#29289;&#32852;&#32593;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;rTPNN-FES&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01616</link><description>&lt;p&gt;
SageFormer&#65306;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#37325;&#35201;&#24615;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;SageFormer&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26377;&#25928;&#22320;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#20197;&#21450;&#20943;&#23569;&#24207;&#21015;&#20043;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#35758;&#30340;&#31995;&#21015;&#24863;&#30693;&#26694;&#26550;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SageFormer&#30456;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36798;&#21040;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.01610</link><description>&lt;p&gt;
&#36807;&#24230;&#33258;&#20449;&#26159;&#19968;&#20214;&#21361;&#38505;&#30340;&#20107;&#24773;&#65306;&#36890;&#36807;&#24378;&#21046;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#26469;&#32531;&#35299;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36798;&#21040;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#30830;&#23450;&#32473;&#23450;&#30340;&#36755;&#20837;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#21162;&#21147;&#26469;&#32531;&#35299;MIAs&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#21463;&#21040;&#26377;&#38480;&#30340;&#38544;&#31169;&#20445;&#25252;&#12289;&#22823;&#24133;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;/&#25110;&#38656;&#35201;&#38590;&#20197;&#33719;&#24471;&#30340;&#39069;&#22806;&#25968;&#25454;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#19981;&#21516;&#24418;&#24335;&#30340;MIAs&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#21487;&#20197;&#32479;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;ML&#27169;&#22411;&#22312;&#36890;&#36807;&#19981;&#21516;&#30340;&#20195;&#29702;&#39044;&#27979;&#35757;&#32451;&#26679;&#26412;&#26102;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#24378;&#21046;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#19978;&#34920;&#29616;&#31867;&#20284;&#12290;HAMP&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#29109;&#36719;&#26631;&#31614;&#21644;&#22522;&#20110;&#29109;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32422;&#26463;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ProtoAD&#65292;&#21033;&#29992;&#21407;&#22411;&#20316;&#20026;&#35299;&#37322;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#36879;&#26126;&#24230;&#21644;&#30452;&#35266;&#29702;&#35299;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;</title><link>http://arxiv.org/abs/2307.01601</link><description>&lt;p&gt;
&#29992;&#21407;&#22411;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prototypes as Explanation for Time Series Anomaly Detection. (arXiv:2307.01601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ProtoAD&#65292;&#21033;&#29992;&#21407;&#22411;&#20316;&#20026;&#35299;&#37322;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#36879;&#26126;&#24230;&#21644;&#30452;&#35266;&#29702;&#35299;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22823;&#25968;&#25454;&#24212;&#29992;&#20013;&#65292;&#26816;&#27979;&#20559;&#31163;&#19968;&#23450;&#35268;&#24459;&#37325;&#22797;&#27169;&#24335;&#30340;&#24322;&#24120;&#27169;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26631;&#31614;&#12289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#24615;&#20197;&#21450;&#24847;&#24819;&#19981;&#21040;&#30340;&#24322;&#24120;&#34892;&#20026;&#20351;&#24471;&#26816;&#27979;&#36807;&#31243;&#20805;&#28385;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#31070;&#31192;&#26426;&#21046;&#25104;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#26032;&#25361;&#25112;&#12290;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#32570;&#22833;&#38459;&#30861;&#20102;&#36825;&#20123;&#39046;&#22495;&#36827;&#19968;&#27493;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;ProtoAD&#65292;&#21033;&#29992;&#21407;&#22411;&#20316;&#20026;&#22522;&#20110;&#31034;&#20363;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#36807;&#31243;&#20013;&#30340;&#27491;&#24120;&#27169;&#24335;&#29366;&#24577;&#12290;&#22312;&#19981;&#26174;&#33879;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22411;&#29031;&#20142;&#20102;&#28145;&#40657;&#30418;&#27169;&#22411;&#65292;&#24182;&#20026;&#39046;&#22495;&#19987;&#23478;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#25193;&#23637;&#21040;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;bo
&lt;/p&gt;
&lt;p&gt;
Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing bo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CryptoRLPM&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#38142;&#19978;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65292;&#32972;&#27979;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19977;&#20010;&#25237;&#36164;&#32452;&#21512;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.01599</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#38142;&#19978;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#30340;&#21487;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management. (arXiv:2307.01599v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01599
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CryptoRLPM&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#38142;&#19978;&#25968;&#25454;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65292;&#32972;&#27979;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19977;&#20010;&#25237;&#36164;&#32452;&#21512;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#20844;&#21496;&#22522;&#26412;&#38754;&#30340;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#38142;&#19978;&#25968;&#25454;&#65288;&#25351;&#26631;&#65289;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#30340;&#37325;&#35201;&#21644;&#20840;&#38754;&#30340;&#35265;&#35299;&#12290;&#23613;&#31649;&#20855;&#26377;&#20449;&#24687;&#24615;&#36136;&#65292;&#20294;&#38142;&#19978;&#25968;&#25454;&#23578;&#26410;&#22312;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#31995;&#32479;&#20013;&#24471;&#21040;&#21033;&#29992;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#35838;&#39064;&#26159;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#38142;&#19978;&#25968;&#25454;&#30340;&#21033;&#29992;&#31243;&#24230;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#22238;&#25253;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CryptoRLPM&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#23558;&#38142;&#19978;&#25968;&#25454;&#32435;&#20837;&#31471;&#21040;&#31471;&#30340;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;CryptoRLPM&#30001;&#20116;&#20010;&#21333;&#20803;&#32452;&#25104;&#65292;&#20174;&#20449;&#24687;&#29702;&#35299;&#21040;&#20132;&#26131;&#35746;&#21333;&#25191;&#34892;&#12290;&#22312;CryptoRLPM&#20013;&#65292;&#38142;&#19978;&#25968;&#25454;&#32463;&#36807;&#27979;&#35797;&#21644;&#25351;&#23450;&#65292;&#20197;&#35299;&#20915;&#25351;&#26631;&#26080;&#25928;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;CryptoRLPM&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#24471;&#21487;&#20197;&#38543;&#26102;&#26356;&#25913;&#25237;&#36164;&#32452;&#21512;&#20013;&#30340;&#21152;&#23494;&#36135;&#24065;&#12290;&#22312;&#19977;&#20010;&#25237;&#36164;&#32452;&#21512;&#19978;&#30340;&#22238;&#27979;&#32467;&#26524;&#34920;&#26126;&#65292;CryptoRLPM&#30340;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-chain data (metrics) of blockchain networks, akin to company fundamentals, provide crucial and comprehensive insights into the networks. Despite their informative nature, on-chain data have not been utilized in reinforcement learning (RL)-based systems for cryptocurrency (crypto) portfolio management (PM). An intriguing subject is the extent to which the utilization of on-chain data can enhance an RL-based system's return performance compared to baselines. Therefore, in this study, we propose CryptoRLPM, a novel RL-based system incorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of five units, spanning from information comprehension to trading order execution. In CryptoRLPM, the on-chain data are tested and specified for each crypto to solve the issue of ineffectiveness of metrics. Moreover, the scalable nature of CryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting results on three portfolios indicate that CryptoRLPM outperforms all th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01597</link><description>&lt;p&gt;
&#22312;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;: Seq2Peak&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#65288;PHSF&#65289;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;PHSF&#20013;&#21364;&#38590;&#20197;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20013;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#30452;&#25509;&#39044;&#27979;&#27604;&#26631;&#20934;&#30340;TSF&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25163;&#21160;&#20174;&#24120;&#35268;&#39044;&#27979;&#32467;&#26524;&#20013;&#25552;&#21462;&#26368;&#22823;&#20540;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#26368;&#23567;&#21270;&#24179;&#22343;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#65292;&#19968;&#20010;&#19987;&#20026;PHSF&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#22312;TSF&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;Seq2Peak&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;CyclicNorm&#27969;&#31243;&#26469;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33258;&#30001;&#23792;&#20540;&#23567;&#26102;&#35299;&#30721;&#22120;&#65292;&#37319;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#21407;&#22987;&#24207;&#21015;&#21644;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;CECS&#65292;&#29992;&#20110;&#35299;&#20915;&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#36328;&#20803;&#32032;&#20132;&#20114;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01593</link><description>&lt;p&gt;
&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#30340;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising. (arXiv:2307.01593v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;CECS&#65292;&#29992;&#20110;&#35299;&#20915;&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#36328;&#20803;&#32032;&#20132;&#20114;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#21578;&#21019;&#24847;&#30340;&#26377;&#25928;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#35270;&#35273;&#22806;&#35266;&#30340;&#24433;&#21709;&#12290;&#24191;&#21578;&#24179;&#21488;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#24191;&#21578;&#21019;&#24847;&#20013;&#30340;&#19981;&#21516;&#20803;&#32032;&#26469;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#22806;&#35266;&#30340;&#24191;&#21578;&#21019;&#24847;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24191;&#21578;&#21019;&#24847;&#20803;&#32032;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20174;&#26080;&#25968;&#21487;&#33021;&#24615;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#21512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34892;&#19994;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#29420;&#31435;&#36873;&#25321;&#21508;&#20010;&#21019;&#24847;&#20803;&#32032;&#65292;&#36825;&#32463;&#24120;&#24573;&#35270;&#20102;&#24314;&#27169;&#36807;&#31243;&#20013;&#21019;&#24847;&#20803;&#32032;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#30340;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;&#65292;&#31216;&#20026;CECS&#12290;&#22312;&#32534;&#30721;&#22120;&#36807;&#31243;&#20013;&#65292;&#37319;&#29992;&#20102;&#36328;&#20803;&#32032;&#20132;&#20114;&#65292;&#26681;&#25454;&#24403;&#21069;&#20505;&#36873;&#21019;&#24847;&#21160;&#24577;&#35843;&#25972;&#21333;&#20010;&#21019;&#24847;&#20803;&#32032;&#30340;&#34920;&#36798;&#12290;&#22312;&#35299;&#30721;&#22120;&#36807;&#31243;&#20013;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#21644;&#34920;&#24449;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#26446;&#32676;&#23545;&#31216;&#21464;&#25442;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#36873;&#25321;&#21644;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01583</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26446;&#32676;&#23545;&#31216;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Learning Lie Group Symmetry Transformations with Neural Networks. (arXiv:2307.01583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#21644;&#34920;&#24449;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#26446;&#32676;&#23545;&#31216;&#21464;&#25442;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#36873;&#25321;&#21644;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#37327;&#21270;&#23545;&#31216;&#24615;&#30340;&#38382;&#39064;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#12289;&#29983;&#25104;&#24314;&#27169;&#21644;&#25968;&#25454;&#20998;&#26512;&#31561;&#26041;&#38754;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#31574;&#30053;&#21644;&#24320;&#28304;&#30340;IAdet&#24037;&#20855;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#33021;&#22815;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20813;&#36153;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01582</link><description>&lt;p&gt;
IAdet&#65306;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#31574;&#30053;&#21644;&#24320;&#28304;&#30340;IAdet&#24037;&#20855;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#33021;&#22815;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20813;&#36153;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#27880;&#25968;&#25454;&#30340;&#21516;&#26102;&#35757;&#32451;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#12290;IA&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#65288;1&#65289;&#36741;&#21161;&#25968;&#25454;&#26631;&#27880;&#65292;&#65288;2&#65289;&#32972;&#26223;&#27169;&#22411;&#35757;&#32451;&#21644;&#65288;3&#65289;&#20027;&#21160;&#36873;&#25321;&#19979;&#19968;&#20010;&#25968;&#25454;&#28857;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#29305;&#23450;&#20110;&#21333;&#31867;&#30446;&#26631;&#26816;&#27979;&#30340;IAdet&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#36825;&#31181;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#22312;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#22522;&#20110;&#19968;&#20010;&#25925;&#24847;&#35774;&#35745;&#38750;&#24120;&#31616;&#21333;&#30340;IAdet&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;IAdet&#26131;&#20110;&#25913;&#36827;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#20013;&#30340;&#20108;&#20803;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#29992;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01578</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#30340;&#26368;&#20248;&#39640;&#25928;&#20108;&#20803;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#20013;&#30340;&#20108;&#20803;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#29992;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#27880;&#37322;&#23545;&#35299;&#37322;&#33021;&#21147;&#12289;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#22914;&#20027;&#21160;&#23398;&#20064;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#37117;&#38598;&#20013;&#22312;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#19978;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21478;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#33719;&#24471;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#38024;&#23545;&#31616;&#21333;&#30340;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#38382;&#39064;&#34987;&#26500;&#24314;&#20026;&#32473;&#23450;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#25317;&#26377;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#19968;&#33324;&#20108;&#20803;&#38382;&#39064;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#32534;&#30721;&#29702;&#35770;&#20013;&#25214;&#21040;&#65292;&#20854;&#20013;&#26368;&#20248;&#30340;&#25552;&#38382;&#31574;&#30053;&#30001;&#21487;&#33021;&#30340;&#26631;&#31614;&#32534;&#30721;&#30340;&#38669;&#22827;&#26364;&#32534;&#30721;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20063;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31181;&#21551;&#21457;&#24335;&#21644;&#21069;&#30651;&#24615;&#26368;&#23567;&#21270;&#30340;&#26367;&#20195;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AIM&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#65292;&#29305;&#21035;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;AIM&#31639;&#27861;&#21033;&#29992;&#36817;&#20284;&#29109;&#26799;&#24230;&#26469;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;&#35201;&#25289;&#21160;&#30340;&#25163;&#33218;&#65292;&#19982;Infomax&#21644;Thompson&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#36895;&#24230;&#12289;&#30830;&#23450;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#32463;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;AIM&#31639;&#27861;&#31526;&#21512;Lai-Robbins&#28176;&#36827;&#30028;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#20808;&#39564;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01563</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#30340;&#36817;&#20284;&#20449;&#24687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate information for efficient exploration-exploitation strategies. (arXiv:2307.01563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AIM&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#65292;&#29305;&#21035;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;AIM&#31639;&#27861;&#21033;&#29992;&#36817;&#20284;&#29109;&#26799;&#24230;&#26469;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;&#35201;&#25289;&#21160;&#30340;&#25163;&#33218;&#65292;&#19982;Infomax&#21644;Thompson&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#36895;&#24230;&#12289;&#30830;&#23450;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#32463;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;AIM&#31639;&#27861;&#31526;&#21512;Lai-Robbins&#28176;&#36827;&#30028;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#20808;&#39564;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20915;&#31574;&#20013;&#28508;&#22312;&#30340;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#65292;&#37325;&#28857;&#30740;&#31350;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#19968;&#20010;&#20195;&#29702;&#20915;&#23450;&#26159;&#21542;&#21033;&#29992;&#24403;&#21069;&#30340;&#30693;&#35782;&#20197;&#33719;&#21462;&#21363;&#26102;&#25910;&#30410;&#65292;&#36824;&#26159;&#25506;&#32034;&#26032;&#30340;&#36884;&#24452;&#20197;&#33719;&#24471;&#28508;&#22312;&#30340;&#38271;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21363;&#36817;&#20284;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;AIM&#65289;&#65292;&#23427;&#21033;&#29992;&#29109;&#26799;&#24230;&#30340;&#35299;&#26512;&#36817;&#20284;&#26469;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;&#35201;&#25289;&#21160;&#30340;&#25163;&#33218;&#12290;AIM&#22312;&#24615;&#33021;&#19978;&#19982;Infomax&#21644;Thompson&#25277;&#26679;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35745;&#31639;&#36895;&#24230;&#12289;&#30830;&#23450;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#23545;AIM&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20854;&#31526;&#21512;Lai-Robbins&#28176;&#36827;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#19968;&#31995;&#21015;&#20808;&#39564;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#34920;&#36798;&#24335;&#21487;&#35843;&#33410;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36827;&#34892;&#20855;&#20307;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the exploration-exploitation dilemma inherent in decision-making, focusing on multi-armed bandit problems. The problems involve an agent deciding whether to exploit current knowledge for immediate gains or explore new avenues for potential long-term rewards. We here introduce a novel algorithm, approximate information maximization (AIM), which employs an analytical approximation of the entropy gradient to choose which arm to pull at each point in time. AIM matches the performance of Infomax and Thompson sampling while also offering enhanced computational speed, determinism, and tractability. Empirical evaluation of AIM indicates its compliance with the Lai-Robbins asymptotic bound and demonstrates its robustness for a range of priors. Its expression is tunable, which allows for specific optimization in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#36793;&#32536;-&#38654;&#25191;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#20887;&#20313;&#25191;&#34892;&#38543;&#26426;&#23376;&#32593;&#32476;&#26469;&#39564;&#35777;&#38654;&#35745;&#31639;&#12290;&#19982;&#23436;&#20840;&#22312;&#26426;&#36733;&#19978;&#36816;&#34892;&#30340;&#26368;&#20808;&#36827;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#30456;&#27604;&#65292;&#37319;&#29992;&#20998;&#24067;&#24335;&#26041;&#24335;&#25191;&#34892;&#30340;&#26356;&#22823;&#32593;&#32476;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25915;&#20987;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2307.01559</link><description>&lt;p&gt;
&#22823;&#22411;&#26080;&#20154;&#26426;&#23618;&#32423;&#20113;&#35745;&#31639;&#23433;&#20840;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#24067;&#24335;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones. (arXiv:2307.01559v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#36793;&#32536;-&#38654;&#25191;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#20887;&#20313;&#25191;&#34892;&#38543;&#26426;&#23376;&#32593;&#32476;&#26469;&#39564;&#35777;&#38654;&#35745;&#31639;&#12290;&#19982;&#23436;&#20840;&#22312;&#26426;&#36733;&#19978;&#36816;&#34892;&#30340;&#26368;&#20808;&#36827;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#30456;&#27604;&#65292;&#37319;&#29992;&#20998;&#24067;&#24335;&#26041;&#24335;&#25191;&#34892;&#30340;&#26356;&#22823;&#32593;&#32476;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25915;&#20987;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#25484;&#22823;&#23567;&#30340;&#32435;&#31859;&#26080;&#20154;&#26426;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36793;&#32536;&#33410;&#28857;&#31867;&#21035;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#26080;&#27861;&#22312;&#26426;&#36733;&#19978;&#36816;&#34892;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#37319;&#29992;&#36793;&#32536;-&#38654;&#35745;&#31639;&#33539;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#21040;&#38654;&#20013;&#65292;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#33021;&#20449;&#20219;&#38654;&#33410;&#28857;&#25110;&#36890;&#20449;&#38142;&#36335;&#65292;&#36825;&#23558;&#24102;&#26469;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#36793;&#32536;-&#38654;&#25191;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#32435;&#31859;&#26080;&#20154;&#26426;&#19978;&#20887;&#20313;&#25191;&#34892;&#38543;&#26426;&#23376;&#32593;&#32476;&#26469;&#39564;&#35777;&#38654;&#35745;&#31639;&#12290;&#19982;&#23436;&#20840;&#22312;&#26426;&#36733;&#19978;&#36816;&#34892;&#30340;&#26368;&#20808;&#36827;&#35270;&#35273;&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#30456;&#27604;&#65292;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#25191;&#34892;&#30340;&#26356;&#22823;&#32593;&#32476;&#30340;$R^2$&#24471;&#20998;&#25552;&#39640;&#20102;+0.19&#65307;&#22312;&#25915;&#20987;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;2&#31186;&#20869;&#20197;95%&#30340;&#27010;&#29575;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Palm-sized nano-drones are an appealing class of edge nodes, but their limited computational resources prevent running large deep-learning models onboard. Adopting an edge-fog computational paradigm, we can offload part of the computation to the fog; however, this poses security concerns if the fog node, or the communication link, can not be trusted. To tackle this concern, we propose a novel distributed edge-fog execution scheme that validates fog computation by redundantly executing a random subnetwork aboard our nano-drone. Compared to a State-of-the-Art visual pose estimation network that entirely runs onboard, a larger network executed in a distributed way improves the $R^2$ score by +0.19; in case of attack, our approach detects it within 2s with 95% probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25237;&#24433;&#31639;&#23376;&#20197;&#21450;&#26680;&#20989;&#25968;&#36827;&#34892;&#30456;&#20851;&#24615;&#34913;&#37327;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01558</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#25237;&#24433;&#31639;&#23376;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable variable selection for two-view learning tasks with projection operators. (arXiv:2307.01558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25237;&#24433;&#31639;&#23376;&#20197;&#21450;&#26680;&#20989;&#25968;&#36827;&#34892;&#30456;&#20851;&#24615;&#34913;&#37327;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#35270;&#22270;&#35774;&#32622;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#22411;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#26679;&#26412;&#25968;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;&#30334;&#19975;&#32423;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#19982;&#36755;&#20986;&#21464;&#37327;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#20808;&#21069;&#36873;&#25321;&#30340;&#21464;&#37327;&#26080;&#20851;&#30340;&#21464;&#37327;&#26469;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#12290;&#20026;&#20102;&#34913;&#37327;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#25237;&#24433;&#31639;&#23376;&#21450;&#20854;&#20195;&#25968;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25237;&#24433;&#31639;&#23376;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#30456;&#20851;&#24615;&#20063;&#21487;&#20197;&#36890;&#36807;&#26680;&#20989;&#25968;&#26469;&#34920;&#36798;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#25152;&#36873;&#25321;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued mea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35299;&#21387;&#32553;&#25805;&#20316;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01524</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;&#30340;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35299;&#21387;&#32553;&#25805;&#20316;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20986;&#34892;&#30340;&#26041;&#24335;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#36710;&#36742;&#30446;&#21069;&#20381;&#36182;&#20110;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#26469;&#26816;&#27979;&#21644;&#36319;&#36394;&#21608;&#22260;&#30340;&#29289;&#20307;&#12290;&#20174;&#36710;&#36742;&#25910;&#38598;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#21457;&#36865;&#21040;&#20113;&#26381;&#21153;&#22120;&#20197;&#20415;&#20110;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#25345;&#32493;/&#32456;&#36523;&#23398;&#20064;&#12290;&#32771;&#34385;&#21040;&#24102;&#23485;&#38480;&#21046;&#65292;&#25968;&#25454;&#22312;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#34987;&#21387;&#32553;&#65292;&#36890;&#24120;&#20250;&#22312;&#35757;&#32451;&#21644;&#20998;&#26512;&#26102;&#36827;&#34892;&#35299;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#26469;&#20943;&#23569;&#26631;&#20934;&#27969;&#27700;&#32447;&#20013;&#35299;&#21387;&#32553;&#25805;&#20316;&#25152;&#20135;&#29983;&#30340;&#26102;&#24310;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;&#21387;&#32553;&#34920;&#31034;&#36824;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#65292;&#20197;&#33719;&#21462;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#65292;&#22312;&#20854;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#21387;&#32553;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression facto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Attention Q-Network&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#27835;&#30103;&#26041;&#26696;&#65292;&#36890;&#36807;&#39640;&#25928;&#25972;&#21512;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#24403;&#21069;&#35266;&#23519;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01519</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#28145;&#24230;&#27880;&#24847;&#21147;Q&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Attention Q-Network&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#27835;&#30103;&#26041;&#26696;&#65292;&#36890;&#36807;&#39640;&#25928;&#25972;&#21512;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#24403;&#21069;&#35266;&#23519;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#23545;&#20110;&#27599;&#20010;&#30149;&#24739;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#36798;&#21040;&#26368;&#20339;&#30340;&#21307;&#30103;&#25928;&#26524;&#26041;&#38754;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#20381;&#38752;&#24403;&#21069;&#30149;&#24739;&#30340;&#35266;&#23519;&#20449;&#24687;&#65288;&#29983;&#21629;&#20307;&#24449;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65289;&#20316;&#20026;&#30149;&#24739;&#30340;&#29366;&#24577;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#30149;&#24739;&#30340;&#30495;&#23454;&#20581;&#24247;&#29366;&#20917;&#12290;&#36825;&#31181;&#38480;&#21046;&#22952;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#27880;&#24847;&#21147;Q&#32593;&#32476;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#39640;&#25928;&#22320;&#25972;&#21512;&#20102;&#25152;&#26377;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#36133;&#34880;&#30151;&#21644;&#24613;&#24615;&#20302;&#34880;&#21387;&#30149;&#20154;&#32676;&#20013;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/stevenmsm/RL-ICU-DAQN&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01514</link><description>&lt;p&gt;
SelfFed: &#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#22312;&#34892;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#26410;&#26631;&#35760;&#20294;&#23396;&#31435;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;&#26631;&#31614;&#31232;&#32570;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65289;&#26041;&#38754;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#30340;SelfFed&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;SelfFed&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26159;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;Swin Transformer&#30340;&#32534;&#30721;&#22120;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#22686;&#24378;&#24314;&#27169;&#12290;SelfFed&#26694;&#26550;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26377;&#21161;&#20110;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#38454;&#27573;&#26159;&#24494;&#35843;&#33539;&#24335;&#65292;&#24341;&#20837;&#23545;&#27604;&#32593;&#32476;&#21644;&#19968;&#31181;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#22411;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20998;&#25955;&#35757;&#32451;&#12290;&#36825;&#20010;&#24494;&#35843;&#38454;&#27573;&#20811;&#26381;&#20102;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;RaSECo&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#30340;&#33647;&#29289;&#22270;&#21644;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#20110;&#26032;&#33647;&#29289;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01507</link><description>&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#22312;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction. (arXiv:2307.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;RaSECo&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#30340;&#33647;&#29289;&#22270;&#21644;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#20110;&#26032;&#33647;&#29289;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#23545;&#20110;&#39044;&#27979;&#22810;&#20851;&#31995;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#38750;&#24120;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#22823;&#37096;&#20998;&#37117;&#23616;&#38480;&#20110;&#23398;&#20064;&#29616;&#26377;&#33647;&#29289;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#23548;&#33268;&#22312;&#28041;&#21450;&#26032;&#33647;&#29289;&#30340;&#27979;&#35797;DDIs&#20013;&#20986;&#29616;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#20851;&#31995;&#30340;&#23376;&#22270;&#23884;&#20837;&#19982;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#22411;DDI&#39044;&#27979;&#26041;&#27861;&#65292;&#21363;RaSECo&#12290;RaSECo&#26500;&#24314;&#20102;&#20004;&#20010;&#24322;&#26500;&#33647;&#29289;&#22270;&#65306;&#22810;&#20851;&#31995;DDI&#22270;&#21644;&#22522;&#20110;&#22810;&#23646;&#24615;&#30340;&#33647;&#29289;&#30456;&#20284;&#24230;&#65288;DDS&#65289;&#22270;&#12290;&#36825;&#20004;&#20010;&#22270;&#20998;&#21035;&#29992;&#20110;&#23398;&#20064;&#21644;&#20256;&#25773;&#33647;&#29289;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#20174;&#32780;&#30830;&#20445;&#25152;&#26377;&#33647;&#29289;&#65292;&#21253;&#25324;&#26032;&#33647;&#29289;&#65292;&#33021;&#22815;&#32858;&#21512;&#26377;&#25928;&#30340;&#23376;&#22270;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#26426;&#21046;&#26469;&#22686;&#24378;&#33647;&#29289;&#23545;&#65288;DP&#65289;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation-aware subgraph embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware subgraph embeddings (RaSEs) of drugs from the DDI graph. However, most existing approaches are usually limited in learning RaSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, We propose a novel DDI prediction method based on relation-aware subgraph embedding with co-contrastive learning, RaSECo. RaSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attributes-based drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaSEs of drugs, thereby ensuring that all drugs, including new ones, can aggregate effective RaSEs. Additionally, we employ a cross-view contrastive mechanism to enhance drug-pair (DP) embedding. RaSEC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01504</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#20219;&#21153;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#36890;&#29992;&#30340;&#22270;&#30693;&#35782;&#26469;&#32531;&#35299;&#27599;&#20010;&#24212;&#29992;&#20013;&#32570;&#20047;&#22270;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#30340;&#22270;&#20219;&#21153;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#39044;&#25991;&#26412;&#36890;&#24120;&#19982;&#36825;&#20123;&#22810;&#20219;&#21153;&#19981;&#20860;&#23481;&#12290;&#36825;&#31181;&#24046;&#36317;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#8220;&#36127;&#36801;&#31227;&#8221;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#24050;&#32463;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22635;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#24046;&#36317;&#30340;&#25552;&#31034;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#20196;&#29260;&#12289;&#20196;&#29260;&#32467;&#26500;&#21644;&#25554;&#20837;&#27169;&#24335;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01497</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19968;&#33324;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31867;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#35266;&#27979;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#19982;&#31639;&#27861;&#20135;&#29983;&#30340;&#36817;&#20284;&#35299;&#30340;"&#20122;&#26368;&#20248;&#24615;" &#30456;&#20851;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#32479;&#35745;&#23398;&#20013;&#30340;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#8212;&#8212;&#38543;&#26426;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;SAGD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#22806;&#25512;&#65288;SGE&#65289;&#8212;&#8212;&#23427;&#20204;&#20855;&#26377;&#19968;&#31181;&#29305;&#27530;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#21331;&#21644;Windows&#31995;&#32479;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#19981;&#21516;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#19978;&#22343;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01494</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#21331;&#21644;Windows&#31995;&#32479;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of Deep Learning-based Malware Detection for Android and Windows System. (arXiv:2307.01494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#21331;&#21644;Windows&#31995;&#32479;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#19981;&#21516;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#19978;&#22343;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#36776;&#24694;&#24847;&#36719;&#20214;&#30340;&#34892;&#20026;&#21644;&#23041;&#32961;&#32423;&#21035;&#23545;&#20110;&#21046;&#23450;&#38450;&#24481;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#21508;&#31181;&#21453;&#24694;&#24847;&#36719;&#20214;&#31995;&#32479;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#29992;&#20110;&#21306;&#20998;&#19981;&#21516;&#30340;&#24694;&#24847;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26368;&#36817;&#30340;&#24694;&#24847;&#36719;&#20214;&#37117;&#20855;&#22791;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#28151;&#28102;&#25216;&#26415;&#27450;&#39575;&#20256;&#32479;&#30340;&#21453;&#24694;&#24847;&#36719;&#20214;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#21482;&#26377;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21453;&#24694;&#24847;&#36719;&#20214;&#31995;&#32479;&#23545;&#25239;&#36825;&#20123;&#25216;&#26415;&#26356;&#21152;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#26816;&#27979;&#21040;&#24694;&#24847;&#27963;&#21160;&#25152;&#28041;&#21450;&#30340;&#24694;&#24847;&#36719;&#20214;&#25991;&#20214;&#20013;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20004;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#29992;&#20110;&#22312;Windows&#21644;&#23433;&#21331;&#25805;&#20316;&#31995;&#32479;&#20013;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#22312;&#26816;&#27979;&#21508;&#31181;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#26102;&#22343;&#23454;&#29616;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiating malware is important to determine their behaviors and level of threat; as well as to devise defensive strategy against them. In response, various anti-malware systems have been developed to distinguish between different malwares. However, most of the recent malware families are Artificial Intelligence (AI) enable and can deceive traditional anti-malware systems using different obfuscation techniques. Therefore, only AI-enabled anti-malware system is robust against these techniques and can detect different features in the malware files that aid in malicious activities. In this study we review two AI-enabled techniques for detecting malware in Windows and Android operating system, respectively. Both the techniques achieved perfect accuracy in detecting various malware families.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01482</link><description>&lt;p&gt;
Nexus sine qua non&#65306;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#36830;&#25509;&#30340;&#26102;&#31354;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19981;&#20165;&#26377;&#21161;&#20110;&#20174;&#19994;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#21152;&#28145;&#25105;&#20204;&#23545;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#25104;&#20026;&#23398;&#20064;&#26102;&#31354;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;STGNNs&#30340;&#26550;&#26500;&#24448;&#24448;&#36890;&#36807;&#22534;&#21472;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#23618;&#27425;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#35774;&#35745;&#30340;&#27169;&#22411;&#21487;&#33021;&#22810;&#20313;&#25110;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#32473;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#20195;STGNNs&#30340;&#35774;&#35745;&#65292;&#24182;&#30830;&#23450;&#23545;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#39044;&#27979;&#22120;&#26377;&#25152;&#36129;&#29486;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23436;&#20840;&#30001;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23450;&#20041;&#65292;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#65292;&#27809;&#26377;&#20219;&#20309;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#65292;&#20363;&#22914;TCNs&#65292;RNNs&#21644;Transformers&#12290;&#36890;&#36807;&#23454;&#35777;&#37325;&#26032;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
&lt;/p&gt;</description></item><item><title>DOM2&#26159;&#19968;&#31181;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#36824;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#21482;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.01472</link><description>&lt;p&gt;
&#36229;&#36234;&#20445;&#23432;&#20027;&#20041;&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning. (arXiv:2307.01472v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01472
&lt;/p&gt;
&lt;p&gt;
DOM2&#26159;&#19968;&#31181;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#36824;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#21482;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#65288;DOM2&#65289;&#65292;&#29992;&#20110;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#19982;&#29616;&#26377;&#31639;&#27861;&#22312;&#31574;&#30053;&#35774;&#35745;&#20013;&#20027;&#35201;&#20381;&#36182;&#20445;&#23432;&#20027;&#20041;&#19981;&#21516;&#65292;DOM2&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#20102;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#32435;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#20851;&#38190;&#22240;&#32032;&#20351;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29615;&#22659;&#21464;&#21270;&#26041;&#38754;&#26356;&#21152;&#31283;&#20581;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#21482;&#20351;&#29992;$20+$&#20493;&#23569;&#30340;&#25968;&#25454;&#19979;&#65292;&#23601;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#22522;&#26412;&#30693;&#35782;&#12289;&#20272;&#35745;&#26041;&#27861;&#20197;&#21450;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#31639;&#27861;&#25216;&#26415;&#65292;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.01470</link><description>&lt;p&gt;
&#23545;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#21644;&#22312;&#20957;&#35270;&#34892;&#20026;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#22522;&#26412;&#30693;&#35782;&#12289;&#20272;&#35745;&#26041;&#27861;&#20197;&#21450;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#31639;&#27861;&#25216;&#26415;&#65292;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#22312;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#26816;&#27979;&#12289;&#35270;&#35273;&#20998;&#24515;&#26816;&#27979;&#12289;&#20957;&#35270;&#34892;&#20026;&#29702;&#35299;&#21644;&#26500;&#24314;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#31561;&#20957;&#35270;&#22522;&#20110;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#22522;&#30784;&#30693;&#35782;&#12289;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#26041;&#27861;&#21450;&#20854;&#22312;&#29616;&#23454;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#24635;&#32467;&#12290;&#39318;&#20808;&#25105;&#20204;&#35752;&#35770;&#19982;&#39550;&#39542;&#21592;&#20957;&#35270;&#30456;&#20851;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#21253;&#25324;&#22836;&#25140;&#24335;&#21644;&#36828;&#31243;&#35774;&#32622;&#30340;&#20957;&#35270;&#20272;&#35745;&#20197;&#21450;&#27599;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#25152;&#20351;&#29992;&#30340;&#26415;&#35821;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21015;&#20030;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#39550;&#39542;&#21592;&#20957;&#35270;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#25152;&#20351;&#29992;&#30340;&#35774;&#22791;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#20110;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#20027;&#35201;&#28041;&#21450;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#25509;&#30528;&#65292;&#20272;&#35745;&#30340;&#39550;&#39542;&#21592;&#20957;&#35270;&#34987;&#29992;&#20110;&#29702;&#35299;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#30340;&#20957;&#35270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuver
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2307.01452</link><description>&lt;p&gt;
&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01452
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#33539;&#24335;&#12290;&#23613;&#31649;&#36817;&#20960;&#21313;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#22240;&#27492;&#24517;&#39035;&#36890;&#36807;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#23398;&#20064;&#12290;&#20182;&#20204;&#21487;&#33021;&#22312;&#35299;&#37322;&#33258;&#24049;&#30340;&#20915;&#31574;&#20197;&#21450;&#25512;&#24191;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#23427;&#21487;&#20197;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#19981;&#21464;&#24615;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#36825;&#23548;&#33268;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#23427;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22240;&#26524;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#26469;&#22686;&#24378;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcemen
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.01449</link><description>&lt;p&gt;
&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#35266;&#27979;&#25968;&#25454;&#32467;&#21512;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#36890;&#24120;&#30001;&#20110;&#26080;&#27861;&#27979;&#35797;&#30340;&#20551;&#35774;&#32780;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36739;&#36731;&#30340;&#20551;&#35774;&#19979;&#27979;&#35797;&#22806;&#37096;&#25928;&#24230;&#21644;&#21487;&#24573;&#35270;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;&#24403;&#21482;&#26377;&#19968;&#20010;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#24378;&#35843;&#20102;&#20934;&#30830;&#35782;&#21035;&#36829;&#21453;&#30340;&#20551;&#35774;&#23545;&#19968;&#33268;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Prompt Production System&#65288;PRopS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#33021;&#22815;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#31163;&#25955;&#35268;&#21017;&#30340;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;PRopS&#22312;PLM&#36866;&#24212;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01446</link><description>&lt;p&gt;
&#20851;&#20110;&#26465;&#20214;&#21644;&#32452;&#21512;&#35821;&#35328;&#27169;&#22411;&#21487;&#24494;&#25552;&#31034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Prompt Production System&#65288;PRopS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#33021;&#22815;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#31163;&#25955;&#35268;&#21017;&#30340;&#23398;&#20064;&#65292;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;PRopS&#22312;PLM&#36866;&#24212;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25552;&#31034;&#21487;&#20197;&#30001;&#20154;&#24037;&#35774;&#35745;&#30340;&#35789;&#24207;&#21015;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#36830;&#32493;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26465;&#20214;&#21644;&#32452;&#21512;&#30340;&#21487;&#24494;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;Prompt Production System&#65288;PRopS&#65289;&#65292;&#23427;&#23398;&#20064;&#23558;&#20219;&#21153;&#35828;&#26126;&#25110;&#36755;&#20837;&#20803;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#28608;&#21457;PLM&#20135;&#29983;&#20219;&#21153;&#29305;&#23450;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#25105;&#20204;&#23545;&#20110;&#20135;&#21697;&#31995;&#32479;&#30340;&#31070;&#32463;&#24418;&#24335;&#21270;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#31163;&#25955;&#35268;&#21017;&#8212;&#8212;&#31070;&#32463;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#23398;&#20064;&#19987;&#38376;&#23558;&#29305;&#23450;&#30340;&#25552;&#31034;&#36755;&#20837;&#27169;&#24335;&#36716;&#21270;&#20026;&#29305;&#23450;&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;PRopS&#22987;&#32456;&#36229;&#36234;&#20854;&#20182;PLM&#36866;&#24212;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#24120;&#25913;&#36827;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#25351;&#38024;&#32593;&#32476;&#23398;&#20064;&#20998;&#25903;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#34920;&#31034;&#27714;&#35299;&#22120;&#29366;&#24577;&#65292;&#24182;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#36890;&#36807;&#27169;&#20223;&#32463;&#20856;&#30340;&#24378;&#20998;&#25903;&#19987;&#23478;&#35268;&#21017;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01434</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#25351;&#38024;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20998;&#25903;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning to Branch in Combinatorial Optimization with Graph Pointer Networks. (arXiv:2307.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#25351;&#38024;&#32593;&#32476;&#23398;&#20064;&#20998;&#25903;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#34920;&#31034;&#27714;&#35299;&#22120;&#29366;&#24577;&#65292;&#24182;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#36890;&#36807;&#27169;&#20223;&#32463;&#20856;&#30340;&#24378;&#20998;&#25903;&#19987;&#23478;&#35268;&#21017;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25903;&#23450;&#30028;&#26159;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#25351;&#38024;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#25903;&#23450;&#30028;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#22270;&#29305;&#24449;&#12289;&#20840;&#23616;&#29305;&#24449;&#21644;&#21382;&#21490;&#29305;&#24449;&#26469;&#34920;&#31034;&#27714;&#35299;&#22120;&#29366;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#27714;&#35299;&#22120;&#29366;&#24577;&#26144;&#23556;&#21040;&#20998;&#25903;&#21464;&#37327;&#20915;&#31574;&#19978;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#35774;&#35745;&#30340;&#21069;k&#20010;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#20197;&#27169;&#20223;&#32463;&#20856;&#30340;&#24378;&#20998;&#25903;&#19987;&#23478;&#35268;&#21017;&#12290;&#19968;&#31995;&#21015;&#22522;&#20934;&#38382;&#39064;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#19987;&#23478;&#35774;&#35745;&#30340;&#20998;&#25903;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#19978;&#22312;&#27714;&#35299;&#36895;&#24230;&#21644;&#25628;&#32034;&#26641;&#22823;&#23567;&#26041;&#38754;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23454;&#20363;&#21644;&#21487;&#25193;&#23637;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Branch-and-bound is a typical way to solve combinatorial optimization problems. This paper proposes a graph pointer network model for learning the variable selection policy in the branch-and-bound. We extract the graph features, global features and historical features to represent the solver state. The proposed model, which combines the graph neural network and the pointer mechanism, can effectively map from the solver state to the branching variable decisions. The model is trained to imitate the classic strong branching expert rule by a designed top-k Kullback-Leibler divergence loss function. Experiments on a series of benchmark problems demonstrate that the proposed approach significantly outperforms the widely used expert-designed branching rules. Our approach also outperforms the state-of-the-art machine-learning-based branch-and-bound methods in terms of solving speed and search tree size on all the test instances. In addition, the model can generalize to unseen instances and sca
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#26032;&#35270;&#35282;&#65292;&#23545;&#20110;&#20855;&#26377;&#26126;&#30830;&#32452;&#21512;&#32467;&#26500;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#23558;&#37319;&#26679;&#35270;&#20026;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#33021;&#22815;&#28040;&#38500;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#39640;&#24230;&#22810;&#27169;&#24335;&#20998;&#24067;&#20013;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01422</link><description>&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;: &#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks: a Markov Chain Perspective. (arXiv:2307.01422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#26032;&#35270;&#35282;&#65292;&#23545;&#20110;&#20855;&#26377;&#26126;&#30830;&#32452;&#21512;&#32467;&#26500;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#23558;&#37319;&#26679;&#35270;&#20026;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#33021;&#22815;&#28040;&#38500;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#39640;&#24230;&#22810;&#27169;&#24335;&#20998;&#24067;&#20013;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65288;MCMC&#65289;&#20026;&#20174;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#24615;&#26694;&#26550;&#65292;&#20294;&#24403;&#30446;&#26631;&#20998;&#24067;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#25910;&#25947;&#32531;&#24930;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#37319;&#26679;&#35270;&#20026;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#24403;&#26679;&#26412;&#20855;&#26377;&#26126;&#30830;&#30340;&#32452;&#21512;&#32467;&#26500;&#26102;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20174;&#27969;&#32593;&#32476;&#30340;&#35282;&#24230;&#24341;&#20837;&#30340;&#65292;&#20294;&#26368;&#36817;GFlowNets&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#36234;&#26469;&#36234;&#22810;&#22320;&#21463;&#21040;&#39532;&#23572;&#21487;&#22827;&#38142;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#23436;&#20840;&#32469;&#36807;&#20102;&#27969;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#36825;&#31181;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#26032;&#35270;&#35282;&#26469;&#35299;&#37322;GFlowNets&#30340;&#32479;&#19968;&#35270;&#22270;&#65292;&#26080;&#35770;&#29366;&#24577;&#31354;&#38388;&#30340;&#24615;&#36136;&#22914;&#20309;&#65292;&#37117;&#21487;&#20197;&#35270;&#20026;&#26159;&#32463;&#24120;&#25442;&#25163;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#23558;GFlowNets&#23450;&#20301;&#21040;&#19982;MCMC&#26041;&#27861;&#30456;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#19979;&#65292;&#20063;&#20801;&#35768;...
&lt;/p&gt;
&lt;p&gt;
While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36339;&#36830;&#25509;&#30340;&#36125;&#21494;&#26031;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30001;&#33021;&#65292;&#25581;&#31034;&#20102;&#20854;&#19981;&#20381;&#36182;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#20855;&#26377;&#31867;&#20284;&#30340;&#27867;&#21270;&#35823;&#24046;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.01417</link><description>&lt;p&gt;
&#20855;&#26377;&#36339;&#36830;&#25509;&#30340;&#36125;&#21494;&#26031;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30001;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Free energy of Bayesian Convolutional Neural Network with Skip Connection. (arXiv:2307.01417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36339;&#36830;&#25509;&#30340;&#36125;&#21494;&#26031;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30001;&#33021;&#65292;&#25581;&#31034;&#20102;&#20854;&#19981;&#20381;&#36182;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#20855;&#26377;&#31867;&#20284;&#30340;&#27867;&#21270;&#35823;&#24046;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Residual Network(ResNet)&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#35768;&#22810;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#26550;&#26500;&#37117;&#37319;&#29992;&#20102;&#36339;&#36830;&#25509;&#12290;&#34429;&#28982;&#36339;&#36830;&#25509;&#30340;CNN&#30340;&#27867;&#21270;&#24615;&#33021;&#24050;&#22312;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#19979;&#24471;&#21040;&#35299;&#37322;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#30340;&#20381;&#36182;&#24615;&#23578;&#26410;&#25581;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#65292;&#26377;&#36339;&#36830;&#25509;&#21644;&#26080;&#36339;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#33258;&#30001;&#33021;&#12290;&#20855;&#26377;&#36339;&#36830;&#25509;&#30340;&#36125;&#21494;&#26031;CNN&#30340;&#33258;&#30001;&#33021;&#19978;&#30028;&#19981;&#20381;&#36182;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#32780;&#36125;&#21494;&#26031;CNN&#30340;&#27867;&#21270;&#35823;&#24046;&#20063;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters have not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.
&lt;/p&gt;</description></item><item><title>&#22810;&#39044;&#27979;&#22120;&#34701;&#21512;&#31639;&#27861;&#65288;MPF&#65289;&#36890;&#36807;&#23558;&#23398;&#20064;&#22411;&#21644;&#35268;&#21017;&#22411;&#30340;&#39044;&#27979;&#22120;&#20197;&#27010;&#29575;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#19968;&#33268;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01408</link><description>&lt;p&gt;
&#22810;&#39044;&#27979;&#22120;&#34701;&#21512;&#65306;&#32467;&#21512;&#22522;&#20110;&#23398;&#20064;&#21644;&#35268;&#21017;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors. (arXiv:2307.01408v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01408
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39044;&#27979;&#22120;&#34701;&#21512;&#31639;&#27861;&#65288;MPF&#65289;&#36890;&#36807;&#23558;&#23398;&#20064;&#22411;&#21644;&#35268;&#21017;&#22411;&#30340;&#39044;&#27979;&#22120;&#20197;&#27010;&#29575;&#32452;&#21512;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#19968;&#33268;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#27169;&#22359;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#23433;&#20840;&#39640;&#25928;&#35268;&#21010;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#65292;&#23588;&#20854;&#22312;&#39640;&#24230;&#20114;&#21160;&#20132;&#36890;&#22330;&#26223;&#19979;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;&#36890;&#36807;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#39044;&#27979;&#22120;&#34701;&#21512;&#65288;MPF&#65289;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#28385;&#36275;&#22522;&#20110;&#36923;&#36753;&#35268;&#21017;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#26469;&#22686;&#24378;&#23398;&#20064;&#22411;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;MPF&#36890;&#36807;&#23558;&#26469;&#33258;&#29420;&#31435;&#39044;&#27979;&#22120;&#30340;&#36712;&#36857;&#25353;&#29031;&#20449;&#24565;&#20998;&#24067;&#28151;&#21512;&#65292;&#27010;&#29575;&#22320;&#32452;&#21512;&#20102;&#22522;&#20110;&#23398;&#20064;&#21644;&#35268;&#21017;&#30340;&#39044;&#27979;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MPF&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#20248;&#20110;&#20004;&#20010;&#29420;&#31435;&#39044;&#27979;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction modules are key enablers for safe and efficient planning of autonomous vehicles (AVs), particularly in highly interactive traffic scenarios. Recently, learning-based trajectory predictors have experienced considerable success in providing state-of-the-art performance due to their ability to learn multimodal behaviors of other agents from data. In this paper, we present an algorithm called multi-predictor fusion (MPF) that augments the performance of learning-based predictors by imbuing them with motion planners that are tasked with satisfying logic-based rules. MPF probabilistically combines learning- and rule-based predictors by mixing trajectories from both standalone predictors in accordance with a belief distribution that reflects the online performance of each predictor. In our results, we show that MPF outperforms the two standalone predictors on various metrics and delivers the most consistent performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#26500;&#24314;&#21943;&#27668;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;&#20934;&#30830;&#31354;&#38388;&#36755;&#20986;&#26367;&#20195;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#36755;&#20986;&#25968;&#25454;&#32858;&#31867;&#24182;&#20026;&#27599;&#20010;&#32858;&#31867;&#26500;&#24314;&#29420;&#31435;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;&#24403;&#31354;&#38388;&#22495;&#30001;&#25968;&#30334;&#19975;&#20010;&#26629;&#26684;&#28857;&#34920;&#31034;&#26102;&#65292;&#25968;&#25454;&#30340;&#32858;&#31867;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.01400</link><description>&lt;p&gt;
&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#21943;&#27668;&#30340;&#26102;&#31354;&#26367;&#20195;&#29289;&#65306;&#31532;&#20108;&#37096;&#20998;--&#23545;&#39640;&#32500;&#26629;&#26684;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II -- Clustering Extremely High-Dimensional Grid-Based Data. (arXiv:2307.01400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01400
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#26500;&#24314;&#21943;&#27668;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;&#20934;&#30830;&#31354;&#38388;&#36755;&#20986;&#26367;&#20195;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#36755;&#20986;&#25968;&#25454;&#32858;&#31867;&#24182;&#20026;&#27599;&#20010;&#32858;&#31867;&#26500;&#24314;&#29420;&#31435;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;&#24403;&#31354;&#38388;&#22495;&#30001;&#25968;&#30334;&#19975;&#20010;&#26629;&#26684;&#28857;&#34920;&#31034;&#26102;&#65292;&#25968;&#25454;&#30340;&#32858;&#31867;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#19968;&#20010;&#20934;&#30830;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#26102;&#31354;&#36755;&#20986;&#30340;&#26367;&#20195;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25913;&#36827;&#26367;&#20195;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26159;&#22522;&#20110;&#30456;&#20284;&#24615;&#23558;&#36755;&#20986;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#20026;&#27599;&#20010;&#32858;&#31867;&#26500;&#24314;&#19968;&#20010;&#29420;&#31435;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#24403;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#36755;&#20986;&#20855;&#26377;&#20013;&#31561;&#22823;&#23567;&#26102;&#65292;&#36825;&#31181;&#32858;&#31867;&#30456;&#23545;&#31616;&#21333;&#12290;&#28982;&#32780;&#65292;&#24403;&#31354;&#38388;&#22495;&#30001;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#26629;&#26684;&#28857;&#34920;&#31034;&#26102;&#65292;&#25968;&#25454;&#30340;&#32858;&#31867;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#21943;&#27969;&#30340;&#36755;&#20986;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#31354;&#38388;&#22495;&#20013;&#21487;&#29992;&#65292;&#22312;&#31354;&#38388;&#22352;&#26631;&#19978;&#21464;&#21270;&#65292;&#24182;&#20197;&#27599;&#20010;&#27169;&#25311;&#26102;&#38388;&#27493;&#39588;&#22312;&#22810;&#20010;&#25991;&#20214;&#20013;&#20998;&#24067;&#36755;&#20986;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#22312;&#32858;&#31867;&#20043;&#21069;&#22914;&#20309;&#23558;&#36825;&#20123;&#25968;&#25454;&#32479;&#19968;&#20026;&#19968;&#33268;&#30340;&#26684;&#24335;&#12290;&#20174;&#25968;&#25454;&#30340;&#38543;&#26426;&#25237;&#24433;&#20013;&#20511;&#37492;&#20102;&#38543;&#26426;&#25237;&#24433;&#30340;&#24819;&#27861;
&lt;/p&gt;
&lt;p&gt;
Building an accurate surrogate model for the spatio-temporal outputs of a computer simulation is a challenging task. A simple approach to improve the accuracy of the surrogate is to cluster the outputs based on similarity and build a separate surrogate model for each cluster. This clustering is relatively straightforward when the output at each time step is of moderate size. However, when the spatial domain is represented by a large number of grid points, numbering in the millions, the clustering of the data becomes more challenging. In this report, we consider output data from simulations of a jet interacting with high explosives. These data are available on spatial domains of different sizes, at grid points that vary in their spatial coordinates, and in a format that distributes the output across multiple files at each time step of the simulation. We first describe how we bring these data into a consistent format prior to clustering. Borrowing the idea of random projections from data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.01394</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes. (arXiv:2307.01394v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#22823;&#25968;&#25454;&#38761;&#21629;&#65292;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#37117;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#25193;&#23637;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32473;&#25968;&#25454;&#24037;&#31243;&#24212;&#29992;&#24102;&#26469;&#20102;&#26356;&#22810;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#29616;&#22312;&#34987;&#38598;&#25104;&#21040;&#25968;&#25454;&#22788;&#29702;&#31649;&#36947;&#20013;&#20197;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#36890;&#24120;&#65292;&#22312;&#36825;&#20123;&#31649;&#36947;&#20013;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#22240;&#27492;&#25552;&#39640;&#20854;&#25928;&#29575;&#30452;&#25509;&#24433;&#21709;&#25972;&#20307;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#31038;&#21306;&#24050;&#32463;&#25509;&#21463;&#20102;&#25968;&#25454;&#26694;&#20316;&#20026;&#20107;&#23454;&#19978;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#25805;&#20316;&#30340;&#25968;&#25454;&#32467;&#26500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#65288;R&#12289;pandas&#65289;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#25105;&#20204;&#30456;&#20449;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#20108;&#32500;&#38382;&#39064;&#25506;&#35752;&#20102;&#22914;&#20309;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#27169;&#25311;&#22797;&#26434;&#29616;&#35937;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#21943;&#27969;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01393</link><description>&lt;p&gt;
&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#21943;&#27969;&#30340;&#26102;&#31354;&#26367;&#20195;&#27169;&#22411;&#65306;&#31532;&#19968;&#37096;&#20998; - &#23567;&#26679;&#26412;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I -- Analysis with a Small Sample Size. (arXiv:2307.01393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#20108;&#32500;&#38382;&#39064;&#25506;&#35752;&#20102;&#22914;&#20309;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#27169;&#25311;&#22797;&#26434;&#29616;&#35937;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#21943;&#27969;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#27169;&#25311;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#29616;&#35937;&#65292;&#21487;&#33021;&#38656;&#35201;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#29702;&#35299;&#19968;&#20010;&#29616;&#35937;&#65292;&#38656;&#35201;&#36816;&#34892;&#22810;&#20010;&#27169;&#25311;&#65292;&#27599;&#20010;&#27169;&#25311;&#37117;&#26377;&#19981;&#21516;&#30340;&#27169;&#25311;&#36755;&#20837;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#25554;&#20540;&#22120;&#25110;&#26367;&#20195;&#27169;&#22411;&#65292;&#23558;&#27169;&#25311;&#36755;&#20986;&#19982;&#30456;&#24212;&#30340;&#36755;&#20837;&#30456;&#20851;&#32852;&#12290;&#24403;&#36755;&#20837;&#21644;&#36755;&#20986;&#26159;&#26631;&#37327;&#26102;&#65292;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23601;&#36275;&#22815;&#20102;&#12290;&#28982;&#32780;&#65292;&#24403;&#27169;&#25311;&#36755;&#20986;&#26159;&#20197;&#20108;&#32500;&#25110;&#19977;&#32500;&#31354;&#38388;&#32500;&#24230;&#20026;&#20301;&#32622;&#30340;&#21521;&#37327;&#20540;&#65292;&#24182;&#19988;&#36890;&#24120;&#20855;&#26377;&#26102;&#38388;&#32452;&#20214;&#26102;&#65292;&#21019;&#24314;&#26367;&#20195;&#27169;&#22411;&#23601;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20108;&#32500;&#30340;&#21943;&#27969;&#19982;&#39640;&#29190;&#28856;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#26469;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#29305;&#28857;&#29420;&#29305; - &#27599;&#20010;&#27169;&#25311;&#20135;&#29983;&#30340;&#21521;&#37327;&#20540;&#36755;&#20986;&#22312;&#36229;&#36807;&#20004;&#30334;&#19975;&#20010;&#31354;&#38388;&#20301;&#32622;&#21487;&#29992;&#65307;&#27599;&#20010;&#27169;&#25311;&#36816;&#34892;&#30340;&#26102;&#38388;&#30456;&#23545;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer simulations, especially of complex phenomena, can be expensive, requiring high-performance computing resources. Often, to understand a phenomenon, multiple simulations are run, each with a different set of simulation input parameters. These data are then used to create an interpolant, or surrogate, relating the simulation outputs to the corresponding inputs. When the inputs and outputs are scalars, a simple machine learning model can suffice. However, when the simulation outputs are vector valued, available at locations in two or three spatial dimensions, often with a temporal component, creating a surrogate is more challenging. In this report, we use a two-dimensional problem of a jet interacting with high explosives to understand how we can build high-quality surrogates. The characteristics of our data set are unique - the vector-valued outputs from each simulation are available at over two million spatial locations; each simulation is run for a relatively small number of ti
&lt;/p&gt;</description></item><item><title>&#27450;&#35784;&#26816;&#27979;&#26159;&#25968;&#25454;&#32463;&#27982;&#30340;&#20851;&#38190;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#22312;&#38754;&#23545;&#27450;&#35784;&#27963;&#21160;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#22312;&#29305;&#23450;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23558;&#20854;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#21644;&#24212;&#29992;&#26041;&#38754;&#36824;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.01390</link><description>&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#23545;&#25239;&#23398;&#20064;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives. (arXiv:2307.01390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01390
&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26159;&#25968;&#25454;&#32463;&#27982;&#30340;&#20851;&#38190;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#22312;&#38754;&#23545;&#27450;&#35784;&#27963;&#21160;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#22312;&#29305;&#23450;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23558;&#20854;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#21644;&#24212;&#29992;&#26041;&#38754;&#36824;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32463;&#27982;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#32780;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21017;&#21463;&#21040;&#35784;&#39575;&#27963;&#21160;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#20174;&#32780;&#23041;&#32961;&#21040;&#20854;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#12290;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20852;&#36259;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#26174;&#33879;&#22686;&#38271;&#65292;&#25581;&#31034;&#20102;&#26377;&#25928;&#25915;&#20987;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#23398;&#20064;&#24212;&#29992;&#12290;&#23613;&#31649;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#26089;&#26399;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#22270;&#20687;&#22788;&#29702;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22914;&#20309;&#23558;&#23545;&#25239;&#24615;&#25216;&#26415;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#21644;&#24212;&#29992;&#26041;&#38754;&#30340;&#30740;&#31350;&#25991;&#29486;&#21644;&#23454;&#36341;&#20013;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#27450;&#35784;&#26816;&#27979;&#26159;&#25968;&#25454;&#32463;&#27982;&#30340;&#20851;&#38190;&#38450;&#24481;&#26426;&#21046;&#65292;&#20063;&#23545;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#27450;&#35784;&#26816;&#27979;&#31995;&#32479;&#30340;&#25915;&#20987;&#19982;&#20854;&#20182;&#31995;&#32479;&#30340;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data economy relies on data-driven systems and complex machine learning applications are fueled by them. Unfortunately, however, machine learning models are exposed to fraudulent activities and adversarial attacks, which threaten their security and trustworthiness. In the last decade or so, the research interest on adversarial machine learning has grown significantly, revealing how learning applications could be severely impacted by effective attacks. Although early results of adversarial machine learning indicate the huge potential of the approach to specific domains such as image processing, still there is a gap in both the research literature and practice regarding how to generalize adversarial techniques in other domains and applications. Fraud detection is a critical defense mechanism for data economy, as it is for other applications as well, which poses several challenges for machine learning. In this work, we describe how attacks against fraud detection systems differ from other
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25512;&#26029;&#26041;&#27861;&#25581;&#31034;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#31215;&#32047;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2307.01389</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25512;&#26029;&#35782;&#21035;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#31215;&#32047;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference. (arXiv:2307.01389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01389
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25512;&#26029;&#26041;&#27861;&#25581;&#31034;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#31215;&#32047;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#22987;&#20110;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#27785;&#31215;&#65292;&#38543;&#21518;&#24341;&#21457;&#31070;&#32463;&#20803;&#20002;&#22833;&#21644;&#32467;&#26500;&#12289;&#21151;&#33021;&#20197;&#21450;&#35748;&#30693;&#30340;&#24694;&#21270;&#12290;&#36890;&#36807;18F-florbetapir (AV45) &#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25668;&#24433;&#65288;PET&#65289;&#25104;&#20687;&#27979;&#37327;&#30340;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#22312;&#22823;&#33041;&#20013;&#30340;&#31215;&#32047;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;AD&#30340;&#26089;&#26399;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#31215;&#32047;&#19982;AD&#30149;&#29702;&#29983;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#65292;&#22240;&#27492;&#38656;&#35201;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26469;&#25581;&#31034;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#27700;&#24179;&#22914;&#20309;&#24433;&#21709;AD&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#27700;&#24179;&#19979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#22270;&#24418;&#21464;&#31995;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;GVCNet&#65289;&#65292;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21253;&#25324;GVCNet&#65292;&#22312;&#27979;&#37327;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;&#31215;&#32047;&#19982;AD&#30149;&#29702;&#29983;&#29702;&#20043;&#38388;&#30340;&#21306;&#22495;&#22240;&#26524;&#36830;&#25509;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning with amyloidosis, followed by neuronal loss and deterioration in structure, function, and cognition. The accumulation of amyloid-beta in the brain, measured through 18F-florbetapir (AV45) positron emission tomography (PET) imaging, has been widely used for early diagnosis of AD. However, the relationship between amyloid-beta accumulation and AD pathophysiology remains unclear, and causal inference approaches are needed to uncover how amyloid-beta levels can impact AD development. In this paper, we propose a graph varying coefficient neural network (GVCNet) for estimating the individual treatment effect with continuous treatment levels using a graph convolutional neural network. We highlight the potential of causal inference approaches, including GVCNet, for measuring the regional causal connections between amyloid-beta accumulation and AD pathophysiology, which may serve as a robust tool for early diagnosis and 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26679;&#26412;&#25512;&#26029;&#20013;&#23384;&#22312;&#31995;&#32479;&#20559;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#25968;&#32676;&#20307;&#20013;&#65292;&#23548;&#33268;&#30446;&#26631;&#29305;&#24449;&#27424;&#39044;&#27979;&#12290;&#36825;&#31181;&#27424;&#39044;&#27979;&#27169;&#24335;&#26159;&#23567;&#26679;&#26412;&#32479;&#35745;&#25512;&#26029;&#30340;&#21487;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01384</link><description>&lt;p&gt;
&#26679;&#26412;&#25512;&#26029;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#21450;&#20854;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Systematic Bias in Sample Inference and its Effect on Machine Learning. (arXiv:2307.01384v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01384
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26679;&#26412;&#25512;&#26029;&#20013;&#23384;&#22312;&#31995;&#32479;&#20559;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#25968;&#32676;&#20307;&#20013;&#65292;&#23548;&#33268;&#30446;&#26631;&#29305;&#24449;&#27424;&#39044;&#27979;&#12290;&#36825;&#31181;&#27424;&#39044;&#27979;&#27169;&#24335;&#26159;&#23567;&#26679;&#26412;&#32479;&#35745;&#25512;&#26029;&#30340;&#21487;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#24120;&#35266;&#23519;&#21040;&#30340;&#19968;&#20010;&#27169;&#24335;&#26159;&#30446;&#26631;&#29305;&#24449;&#30340;&#27424;&#39044;&#27979;&#65292;&#23545;&#20110;&#32473;&#23450;&#31867;&#21035;&#30340;&#25104;&#21592;&#65292;&#27169;&#22411;&#39044;&#27979;&#30340;&#30446;&#26631;&#29575;&#36890;&#24120;&#20302;&#20110;&#35757;&#32451;&#38598;&#20013;&#35813;&#31867;&#21035;&#25104;&#21592;&#30340;&#23454;&#38469;&#30446;&#26631;&#29575;&#12290;&#36825;&#31181;&#27424;&#39044;&#27979;&#22312;&#23569;&#25968;&#32676;&#20307;&#30340;&#25104;&#21592;&#20013;&#36890;&#24120;&#26356;&#26126;&#26174;&#65307;&#20363;&#22914;&#65292;&#22312;&#8220;adult&#8221;&#25968;&#25454;&#38598;&#20013;&#65292;&#26080;&#35770;&#26159;&#30007;&#24615;&#36824;&#26159;&#22899;&#24615;&#30340;&#25910;&#20837;&#27700;&#24179;&#37117;&#34987;&#27424;&#39044;&#27979;&#65292;&#20294;&#23545;&#20110;&#22899;&#24615;&#65288;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#23569;&#25968;&#32676;&#20307;&#65289;&#65292;&#27424;&#39044;&#27979;&#31243;&#24230;&#26126;&#26174;&#26356;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#38024;&#23545;&#23569;&#25968;&#32676;&#20307;&#30340;&#27424;&#39044;&#27979;&#27169;&#24335;&#26159;&#23545;&#23567;&#26679;&#26412;&#32479;&#35745;&#25512;&#26029;&#30340;&#21487;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#23545;&#19968;&#20010;&#26032;&#20010;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#30340;&#25512;&#26029;&#19981;&#26159;&#22522;&#20110;&#25972;&#20010;&#35757;&#32451;&#38598;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20010;&#19982;&#26032;&#20010;&#20307;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#30456;&#20284;&#30340;&#23376;&#38598;&#65292;&#36825;&#20123;&#23376;&#38598;&#30340;&#22823;&#23567;&#36890;&#24120;&#36981;&#24490;&#24130;&#24459;&#20998;&#24067;&#65292;&#22823;&#22810;&#25968;&#37117;&#24456;&#23567;&#65288;&#24182;&#19988;&#36825;&#20123;&#23376;&#38598;&#24517;&#39035;&#26159;&#24517;&#28982;&#23567;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A commonly observed pattern in machine learning models is an underprediction of the target feature, with the model's predicted target rate for members of a given category typically being lower than the actual target rate for members of that category in the training set. This underprediction is usually larger for members of minority groups; while income level is underpredicted for both men and women in the 'adult' dataset, for example, the degree of underprediction is significantly higher for women (a minority in that dataset). We propose that this pattern of underprediction for minorities arises as a predictable consequence of statistical inference on small samples. When presented with a new individual for classification, an ML model performs inference not on the entire training set, but on a subset that is in some way similar to the new individual, with sizes of these subsets typically following a power law distribution so that most are small (and with these subsets being necessarily 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.01381</link><description>&lt;p&gt;
&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#29992;&#20110;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20154;&#31867;&#20132;&#27969;&#20219;&#21153;&#65292;&#21363;&#22312;&#36827;&#34892;&#35821;&#38899;&#36755;&#20837;&#30340;&#21516;&#26102;&#29983;&#25104;&#32763;&#35793;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27969;&#24335;&#20219;&#21153;&#65292;&#20351;&#29992;&#22359;&#22788;&#29702;&#23558;&#36755;&#20837;&#24207;&#21015;&#20998;&#21106;&#25104;&#29255;&#27573;&#30340;Transformer&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20801;&#35768;&#20449;&#24687;&#22312;&#29255;&#27573;&#20043;&#38388;&#20256;&#25773;&#65292;&#21253;&#25324;&#24038;&#19978;&#19979;&#25991;&#21644;&#23384;&#20648;&#22120;&#24211;&#65292;&#20294;&#23427;&#20204;&#26082;&#26159;&#19981;&#20805;&#20998;&#30340;&#34920;&#31034;&#21448;&#26159;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#24038;&#19978;&#19979;&#25991;&#26041;&#27861;&#38544;&#24335;&#20445;&#30041;&#35760;&#24518;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#29992;&#23384;&#20648;&#22120;&#24211;&#26174;&#24335;&#34920;&#31034;&#35760;&#24518;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#21069;&#19968;&#20010;&#29255;&#27573;&#30340;&#27880;&#24847;&#21147;&#36755;&#20986;&#29983;&#25104;&#24038;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#21253;&#21547;&#22312;&#24403;&#21069;&#29255;&#27573;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#38190;&#21644;&#20540;&#20013;&#12290;&#23545;MuST-C&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#38544;&#24335;&#20869;&#23384;&#21464;&#25442;&#22120;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#23569;&#37327;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#23384;&#22312;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#65292;&#35813;&#26041;&#26696;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#20110;Augmented Memory Transformer&#21518;&#21487;&#20197;&#25552;&#39640;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.01377</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#65306;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#23384;&#22312;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#65292;&#35813;&#26041;&#26696;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#20110;Augmented Memory Transformer&#21518;&#21487;&#20197;&#25552;&#39640;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#65292;&#20351;&#29992;&#20998;&#27573;&#22788;&#29702;&#30340;Transformer&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#29615;&#22659;&#20043;&#38388;&#21019;&#24314;&#20102;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#28508;&#22312;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22987;&#32456;&#32500;&#25345;&#19968;&#33268;&#30340;&#27573;&#33853;&#21644;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#21363;&#20351;&#30001;&#20110;&#27969;&#24335;&#32763;&#35793;&#30340;&#24615;&#36136;&#23548;&#33268;&#37096;&#20998;&#22635;&#20805;&#30340;&#27573;&#33853;&#23384;&#22312;&#12290;&#21487;&#36716;&#31227;&#19978;&#19979;&#25991;&#22312;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#27573;Transformer&#20013;&#20063;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;&#25105;&#20204;&#22312;MUST-C&#25968;&#25454;&#38598;&#30340;&#33521;&#24503;&#12289;&#33521;&#27861;&#21644;&#33521;&#35199;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#35813;&#26041;&#26696;&#24212;&#29992;&#20110;Augmented Memory Transformer&#65288;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65289;&#21487;&#20197;&#24179;&#22343;&#25552;&#39640;2.09&#12289;1.83&#21644;1.95&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.01357</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#22238;&#24402;(PCR)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22266;&#23450;&#35774;&#35745;&#35823;&#24046;&#21464;&#37327;&#22238;&#24402;&#25216;&#26415;&#65292;&#23427;&#26159;&#32447;&#24615;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#35266;&#27979;&#30340;&#21327;&#21464;&#37327;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#25552;&#20379;&#20102;&#22312;&#32447;&#65288;&#27491;&#21017;&#21270;&#65289;PCR&#30340;&#31532;&#19968;&#27425;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#30001;&#20110;&#20998;&#26512;&#22266;&#23450;&#35774;&#35745;&#20013;PCR&#30340;&#35777;&#26126;&#25216;&#26415;&#26080;&#27861;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23558;&#29616;&#20195;&#38789;&#27987;&#24230;&#30340;&#24037;&#20855;&#36866;&#24212;&#21040;&#35823;&#24046;&#21464;&#37327;&#35774;&#32622;&#20013;&#12290;&#20316;&#20026;&#25105;&#20204;&#30028;&#38480;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#38754;&#26495;&#25968;&#25454;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#65292;&#24403;&#24178;&#39044;&#34987;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26694;&#26550;&#30340;&#27867;&#21270;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#24178;&#39044;&#20998;&#37197;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;Buckley-Leverett PDE&#30340;&#27969;&#20989;&#25968;&#31354;&#38388;&#19982;&#35299;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20351;&#29992;Physics-Informed DeepONets (PI-DeepONets)&#22312;&#27809;&#26377;&#37197;&#23545;&#36755;&#20837;-&#36755;&#20986;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36719;&#32422;&#26463;&#30340;&#24213;&#23618;&#29289;&#29702;&#23450;&#24459;&#26469;&#21152;&#36895;&#22810;&#23380;&#20171;&#36136;&#20013;&#27969;&#20307;&#27969;&#21160;&#21644;&#20256;&#36755;&#30340;&#25968;&#20540;&#27169;&#25311;&#12290;a</title><link>http://arxiv.org/abs/2307.01354</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#20989;&#25968;&#31639;&#23376;&#23398;&#20064;&#22810;&#30456;&#20171;&#36136;&#20013;&#30340;&#20256;&#36755;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator. (arXiv:2307.01354v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01354
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;Buckley-Leverett PDE&#30340;&#27969;&#20989;&#25968;&#31354;&#38388;&#19982;&#35299;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20351;&#29992;Physics-Informed DeepONets (PI-DeepONets)&#22312;&#27809;&#26377;&#37197;&#23545;&#36755;&#20837;-&#36755;&#20986;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36719;&#32422;&#26463;&#30340;&#24213;&#23618;&#29289;&#29702;&#23450;&#24459;&#26469;&#21152;&#36895;&#22810;&#23380;&#20171;&#36136;&#20013;&#27969;&#20307;&#27969;&#21160;&#21644;&#20256;&#36755;&#30340;&#25968;&#20540;&#27169;&#25311;&#12290;a
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#29992;&#20110;&#27169;&#25311;&#22810;&#23380;&#20171;&#36136;&#20013;&#27969;&#20307;&#27969;&#21160;&#21644;&#20256;&#36755;&#30340;&#25968;&#20540;&#26041;&#26696;&#21487;&#33021;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#31185;&#23398;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20855;&#26377;&#28508;&#21147;&#24110;&#21161;&#21152;&#24555;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#27169;&#25311;&#26102;&#38388;&#12290;DeepONet&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#31639;&#23376;(&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;)&#26469;&#21152;&#36895;&#27714;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;Buckley-Leverett PDE&#30340;&#27969;&#20989;&#25968;&#31354;&#38388;&#19982;&#35299;&#31354;&#38388;(&#39281;&#21644;&#24230;)&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;DeepONets (PI-DeepONets)&#26469;&#23454;&#29616;&#36825;&#31181;&#26144;&#23556;&#65292;&#38500;&#20102;&#19968;&#32452;&#32473;&#23450;&#30340;&#21021;&#22987;&#26465;&#20214;&#25110;&#36793;&#30028;&#26465;&#20214;&#20043;&#22806;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#25104;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35266;&#27979;&#65292;&#22240;&#27492;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#36719;&#32422;&#26463;&#30340;&#24213;&#23618;&#29289;&#29702;&#23450;&#24459;&#65292;&#31867;&#20284;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#20197;&#21450;&#19968;&#20010;u
&lt;/p&gt;
&lt;p&gt;
Traditional numerical schemes for simulating fluid flow and transport in porous media can be computationally expensive. Advances in machine learning for scientific computing have the potential to help speed up the simulation time in many scientific and engineering fields. DeepONet has recently emerged as a powerful tool for accelerating the solution of partial differential equations (PDEs) by learning operators (mapping between function spaces) of PDEs. In this work, we learn the mapping between the space of flux functions of the Buckley-Leverett PDE and the space of solutions (saturations). We use Physics-Informed DeepONets (PI-DeepONets) to achieve this mapping without any paired input-output observations, except for a set of given initial or boundary conditions; ergo, eliminating the expensive data generation process. By leveraging the underlying physical laws via soft penalty constraints during model training, in a manner similar to Physics-Informed Neural Networks (PINNs), and a u
&lt;/p&gt;</description></item><item><title>Patch-CNN&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20165;&#20845;&#20010;&#26041;&#21521;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#20013;&#31934;&#30830;&#20272;&#35745;&#25193;&#25955;&#24352;&#37327;&#65292;&#20811;&#26381;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#22823;&#21644;&#26080;&#27861;&#20272;&#35745;&#32420;&#32500;&#26041;&#21521;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.01346</link><description>&lt;p&gt;
Patch-CNN: &#20174;&#26368;&#23569;&#25193;&#25955;&#21327;&#35758;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#25193;&#25955;&#24352;&#37327;&#20272;&#35745;&#30340;&#25968;&#25454;&#26377;&#25928;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols. (arXiv:2307.01346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01346
&lt;/p&gt;
&lt;p&gt;
Patch-CNN&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20165;&#20845;&#20010;&#26041;&#21521;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;&#20013;&#31934;&#30830;&#20272;&#35745;&#25193;&#25955;&#24352;&#37327;&#65292;&#20811;&#26381;&#20102;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#22823;&#21644;&#26080;&#27861;&#20272;&#35745;&#32420;&#32500;&#26041;&#21521;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Patch-CNN&#65292;&#29992;&#20110;&#20174;&#20165;&#20845;&#20010;&#26041;&#21521;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#20687;(DWI)&#20013;&#20272;&#35745;&#25193;&#25955;&#24352;&#37327;(DT)&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;dMRI&#21442;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#20307;&#32032;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;(FCN)&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#22312;&#21387;&#21147;&#29978;&#22823;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#30340;&#21387;&#21147;&#38480;&#21046;&#20102;&#25104;&#20687;&#26041;&#21521;&#30340;&#25968;&#37327;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#22270;&#20687;&#20307;&#31215;(&#22522;&#20110;&#22270;&#20687;&#30340;CNN)&#65292;&#35201;&#20040;&#26080;&#27861;&#20272;&#35745;&#32420;&#32500;&#26041;&#21521;(&#22522;&#20110;&#20307;&#32032;&#30340;FCN)&#65292;&#26080;&#27861;&#36827;&#34892;&#26463;&#36857;&#20272;&#35745;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Patch-CNN&#65292;&#23427;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#23567;(&#38750;&#22522;&#20110;&#20307;&#32032;&#30340;)&#21367;&#31215;&#26680;(3&#215;3&#215;3)&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;FCN&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#30340;&#20248;&#21183;&#22312;&#20110;&#21487;&#20197;&#21033;&#29992;&#23616;&#37096;&#35299;&#21078;&#20449;&#24687;&#12290;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;CNN&#30456;&#27604;&#65292;&#26368;&#23567;&#30340;&#21367;&#31215;&#26680;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33322;&#28023;&#39046;&#22495;&#20013;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;Dropout&#33719;&#24471;&#30340;&#31867;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#32467;&#21512;&#65292;&#23545;&#33322;&#28023;&#23545;&#35937;&#36827;&#34892;&#20102;&#40065;&#26834;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.01325</link><description>&lt;p&gt;
&#23545;&#33322;&#28023;&#23545;&#35937;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Uncertainty Estimation for Classification of Maritime Objects. (arXiv:2307.01325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33322;&#28023;&#39046;&#22495;&#20013;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;Dropout&#33719;&#24471;&#30340;&#31867;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#32467;&#21512;&#65292;&#23545;&#33322;&#28023;&#23545;&#35937;&#36827;&#34892;&#20102;&#40065;&#26834;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33322;&#28023;&#39046;&#22495;&#20013;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#65288;CIFAR10&#65289;&#19978;&#30340;&#21151;&#25928;&#65292;&#24182;&#22312;&#25105;&#20204;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;SHIPS&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;Dropout&#33719;&#24471;&#30340;&#31867;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19982;&#20854;&#22312;CIFAR10&#21644;&#23454;&#38469;&#24212;&#29992;&#29615;&#22659;&#20013;&#30340;&#24037;&#20316;&#34920;&#29616;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#27809;&#26377;&#31163;&#32676;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#23558;FPR95&#25552;&#39640;&#20102;8&#65285;&#65292;&#30456;&#27604;&#20110;&#30446;&#21069;&#26368;&#39640;&#24615;&#33021;&#30340;&#24037;&#20316;&#12290;&#30456;&#27604;&#20110;&#24120;&#35268;&#23454;&#29616;&#30340;Wide ResNet&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;77&#65285;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;SHIPS&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19982;&#22522;&#32447;&#30456;&#27604;&#23558;FPR95&#25552;&#39640;&#20102;44.2&#65285;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of uncertainty estimation in the maritime domain, showing the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset, SHIPS. We present a method joining the intra-class uncertainty achieved using Monte Carlo Dropout, with recent discoveries in the field of outlier detection, to gain more holistic uncertainty measures. We explore the relationship between the introduced uncertainty measures and examine how well they work on CIFAR10 and in a real-life setting. Our work improves the FPR95 by 8% compared to the current highest-performing work when the models are trained without out-of-distribution data. We increase the performance by 77% compared to a vanilla implementation of the Wide ResNet. We release the SHIPS dataset and show the effectiveness of our method by improving the FPR95 by 44.2% with respect to the baseline. Our approach is model agnostic, easy to implement, and often does not require model retraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#34892;&#30340;&#31034;&#20363;&#26469;&#35757;&#32451;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#19982;&#24402;&#19968;&#21270;&#27969;&#22312;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly. (arXiv:2307.01317v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#33258;&#30465;&#24335;&#26426;&#22120;&#20154;&#35013;&#37197;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#34892;&#30340;&#31034;&#20363;&#26469;&#35757;&#32451;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#20013;&#38656;&#35201;&#23545;&#39044;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#33258;&#30465;&#65292;&#21363;&#21028;&#26029;&#20854;&#21487;&#34892;&#24615;&#65292;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#25928;&#29575;&#38477;&#20302;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20135;&#21697;&#21464;&#20307;&#26102;&#65292;&#25910;&#38598;&#36275;&#22815;&#30340;&#19981;&#21487;&#34892;&#31034;&#20363;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;&#21487;&#34892;&#31034;&#20363;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#21487;&#34892;&#24615;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#20998;&#24067;&#21306;&#20998;&#65292;&#24402;&#19968;&#21270;&#27969;&#26159;&#29992;&#20110;&#20272;&#35745;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#24378;&#22823;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#35013;&#37197;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#21333;&#20998;&#31867;&#22522;&#32447;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#20986;&#19981;&#21487;&#34892;&#30340;&#35013;&#37197;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#25454;&#28857;&#20013;&#35299;&#20915;&#20248;&#21270;&#23398;&#20064;&#20013;&#30340;Chebyshev&#36793;&#30028;&#38382;&#39064;&#12290;&#31639;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#20551;&#35774;&#31354;&#38388;&#30340;Chebyshev&#21322;&#24452;&#21644;Chebyshev&#20013;&#24515;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26368;&#20248;&#24674;&#22797;&#20989;&#25968;&#30340;&#30446;&#26631;&#12290;&#31639;&#27861;&#22522;&#20110;&#26377;&#38024;&#23545;&#24615;&#25277;&#26679;&#30340;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01304</link><description>&lt;p&gt;
&#19968;&#20010;&#27714;&#35299;&#20248;&#21270;&#23398;&#20064;&#20013;Chebyshev&#36793;&#30028;&#30340;&#25968;&#20540;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A numerical algorithm for attaining the Chebyshev bound in optimal learning. (arXiv:2307.01304v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#26377;&#38480;&#25968;&#25454;&#28857;&#20013;&#35299;&#20915;&#20248;&#21270;&#23398;&#20064;&#20013;&#30340;Chebyshev&#36793;&#30028;&#38382;&#39064;&#12290;&#31639;&#27861;&#33021;&#22815;&#35745;&#31639;&#20986;&#20551;&#35774;&#31354;&#38388;&#30340;Chebyshev&#21322;&#24452;&#21644;Chebyshev&#20013;&#24515;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26368;&#20248;&#24674;&#22797;&#20989;&#25968;&#30340;&#30446;&#26631;&#12290;&#31639;&#27861;&#22522;&#20110;&#26377;&#38024;&#23545;&#24615;&#25277;&#26679;&#30340;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;Banach&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#32039;&#33268;&#23376;&#38598;&#65292;Chebyshev&#20013;&#24515;&#38382;&#39064;&#26159;&#25214;&#21040;&#19968;&#20010;&#26368;&#23567;&#30340;&#21253;&#22260;&#23376;&#38598;&#30340;&#29699;&#12290;&#26412;&#25991;&#22312;&#26377;&#38480;&#25968;&#25454;&#28857;&#30340;&#26368;&#20248;&#23398;&#20064;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#25968;&#20540;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Chebyshev&#20013;&#24515;&#38382;&#39064;&#12290;&#23545;&#20110;&#19968;&#20010;&#20551;&#35774;&#31354;&#38388;&#65292;&#23427;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;&#32039;&#33268;&#20294;&#19981;&#19968;&#23450;&#26159;&#20984;&#38598;&#30340;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#30340;&#23376;&#38598;&#65292;&#22312;&#27492;&#31639;&#27861;&#20013;&#35745;&#31639;&#20102;&#20551;&#35774;&#31354;&#38388;&#30340;Chebyshev&#21322;&#24452;&#21644;Chebyshev&#20013;&#24515;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20174;&#25968;&#25454;&#20013;&#26368;&#20248;&#24674;&#22797;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#26412;&#36523;&#22522;&#20110;&#26368;&#36817;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#25277;&#26679;&#23454;&#29616;&#30340;&#20989;&#25968;&#36817;&#20284;&#38382;&#39064;&#30340;&#36817;&#20284;&#26368;&#20248;&#35299;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#35828;&#26126;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26412;&#25991;&#36824;&#21253;&#25324;&#20102;&#20960;&#20010;&#25968;&#20540;&#35745;&#31639;Chebyshev&#20013;&#24515;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a compact subset of a Banach space, the Chebyshev center problem consists of finding a minimal circumscribing ball containing the set. In this article we establish a numerically tractable algorithm for solving the Chebyshev center problem in the context of optimal learning from a finite set of data points. For a hypothesis space realized as a compact but not necessarily convex subset of a finite-dimensional subspace of some underlying Banach space, this algorithm computes the Chebyshev radius and the Chebyshev center of the hypothesis space, thereby solving the problem of optimal recovery of functions from data. The algorithm itself is based on, and significantly extends, recent results for near-optimal solutions of convex semi-infinite problems by means of targeted sampling, and it is of independent interest. Several examples of numerical computations of Chebyshev centers are included in order to illustrate the effectiveness of the algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01292</link><description>&lt;p&gt;
Pareto-&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PSML&#65289;&#65306;&#25351;&#32441;&#21644;&#20445;&#25252;&#25512;&#26029;&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#23558;&#26597;&#35810;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#24182;&#25351;&#23450;&#25152;&#38656;&#30340;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#31561;&#65289;&#12290;&#26381;&#21153;&#22120;&#22312;&#21518;&#31471;&#32500;&#25252;&#19968;&#32452;&#27169;&#22411;&#65288;&#27169;&#22411;&#24211;&#65289;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#25351;&#26631;&#25552;&#20379;&#26597;&#35810;&#26381;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#38544;&#34255;&#22312;&#25512;&#29702;&#26381;&#21153;&#25509;&#21475;&#32972;&#21518;&#30340;&#27169;&#22411;&#24211;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#30830;&#23450;&#20351;&#29992;&#30340;&#26159;&#21738;&#20010;&#27169;&#22411;&#12290;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#27493;&#39588;&#26469;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#37117;&#33021;&#24471;&#21040;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#27169;&#22411;&#25552;&#21462;&#21487;&#20197;&#20855;&#26377;&#20445;&#30495;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#20998;&#27495;&#38382;&#39064;&#65292;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.01288</link><description>&lt;p&gt;
&#29992;&#20849;&#35782;&#26041;&#24335;&#35299;&#20915;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#27495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#20998;&#27495;&#38382;&#39064;&#65292;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20215;&#20540;&#36890;&#24120;&#36890;&#36807;&#20854;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#19982;&#20854;&#20934;&#30830;&#24615;&#21516;&#31561;&#37325;&#35201;&#12290;&#20026;&#20102;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#35299;&#37322;&#24615;&#31639;&#27861;&#26159;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#22810;&#31181;&#31639;&#27861;&#21487;&#20379;&#36873;&#25321;&#65292;&#23427;&#20204;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#20998;&#27495;&#65292;&#23548;&#33268;&#30456;&#20114;&#30683;&#30462;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#27169;&#22411;&#34987;&#35299;&#37322;&#20043;&#21518;&#21487;&#20197;&#24212;&#29992;&#20849;&#35782;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#38382;&#39064;&#24182;&#27809;&#26377;&#23436;&#20840;&#35299;&#20915;&#65292;&#22240;&#20026;&#26368;&#32456;&#32467;&#26524;&#23558;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#20849;&#35782;&#20989;&#25968;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20808;&#21069;&#22312;&#22235;&#20010;&#24050;&#30693;&#20869;&#37096;&#35268;&#21017;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20849;&#35782;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.01238</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#24739;&#32773;&#24517;&#39035;&#23494;&#20999;&#30417;&#27979;&#20182;&#20204;&#30340;&#34880;&#31958;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#36827;&#39184;&#21518;&#12290;&#34880;&#31958;&#35843;&#33410;&#38656;&#35201;&#27491;&#30830;&#32452;&#21512;&#30340;&#39135;&#29289;&#25668;&#20837;&#21644;&#33008;&#23707;&#32032;&#27880;&#23556;&#12290;&#34880;&#31958;&#39044;&#27979;&#23545;&#20110;&#36991;&#20813;&#27835;&#30103;&#31958;&#23615;&#30149;&#24739;&#32773;&#39184;&#21518;&#24182;&#21457;&#30151;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#30001;&#20110;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#26377;&#26102;&#19981;&#36866;&#29992;&#20110;&#21307;&#29983;&#24320;&#21457;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#35843;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#34880;&#31958;&#39044;&#27979;&#26041;&#27861;&#65306;&#21487;&#35299;&#37322;&#31232;&#30095;&#35782;&#21035;&#36890;&#36807;&#35821;&#27861;&#28436;&#21270;&#12290;&#32467;&#21512;&#20808;&#21069;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#38480;&#24046;&#20998;&#26041;&#31243;&#26469;&#39044;&#27979;&#39184;&#21518;&#20004;&#23567;&#26102;&#20869;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#20998;&#20026;&#22235;&#23567;&#26102;&#30340;&#27573;&#65292;&#26681;&#25454;&#36827;&#39184;&#21069;&#20004;&#23567;&#26102;&#30340;&#34880;&#31958;&#20540;&#36827;&#34892;&#32858;&#31867;&#12290;&#39044;&#27979;&#27169;&#22411;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
People with diabetes must carefully monitor their blood glucose levels, especially after eating. Blood glucose regulation requires a proper combination of food intake and insulin boluses. Glucose prediction is vital to avoid dangerous post-meal complications in treating individuals with diabetes. Although traditional methods, such as artificial neural networks, have shown high accuracy rates, sometimes they are not suitable for developing personalised treatments by physicians due to their lack of interpretability. In this study, we propose a novel glucose prediction method emphasising interpretability: Interpretable Sparse Identification by Grammatical Evolution. Combined with a previous clustering stage, our approach provides finite difference equations to predict postprandial glucose levels up to two hours after meals. We divide the dataset into four-hour segments and perform clustering based on blood glucose values for the twohour window before the meal. Prediction models are traine
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21160;&#24577;&#22270;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(DynGESN)&#19982;&#31216;&#20026;&#24555;&#29031;&#21512;&#24182;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20256;&#25773;&#36807;&#31243;&#20998;&#31867;(DPC)&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#29031;&#21512;&#24182;&#21644;&#22810;&#20010;&#20648;&#23618;&#32534;&#30721;&#22120;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#36923;&#36753;&#22238;&#24402;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.01237</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#19982;&#24555;&#29031;&#21512;&#24182;&#29992;&#20110;&#20256;&#25773;&#36807;&#31243;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification. (arXiv:2307.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21160;&#24577;&#22270;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(DynGESN)&#19982;&#31216;&#20026;&#24555;&#29031;&#21512;&#24182;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20256;&#25773;&#36807;&#31243;&#20998;&#31867;(DPC)&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#29031;&#21512;&#24182;&#21644;&#22810;&#20010;&#20648;&#23618;&#32534;&#30721;&#22120;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#36923;&#36753;&#22238;&#24402;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#36807;&#31243;&#20998;&#31867;(DPC)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26102;&#38388;&#22270;&#20998;&#31867;&#24212;&#29992;&#12290;DPC&#30340;&#30446;&#26631;&#26159;&#23545;&#30001;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#22270;&#34920;&#31034;&#30340;&#31038;&#21306;&#20013;&#30340;&#20449;&#24687;&#25110;&#30123;&#24773;&#20256;&#25773;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20648;&#23618;&#35745;&#31639;&#30340;&#27169;&#22411;&#31216;&#20026;&#21160;&#24577;&#22270;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(DynGESN)&#65292;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#36739;&#39640;&#26377;&#25928;&#24615;&#21644;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#26102;&#38388;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#31216;&#20026;&#24555;&#29031;&#21512;&#24182;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#19982;DynGESN&#30456;&#32467;&#21512;&#29992;&#20110;&#22788;&#29702;DPC&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#24555;&#29031;&#21512;&#24182;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#21512;&#24182;&#30456;&#37051;&#30340;&#24555;&#29031;&#26469;&#24418;&#25104;&#26032;&#30340;&#24555;&#29031;&#65292;&#28982;&#21518;&#35774;&#32622;&#22810;&#20010;&#20648;&#23618;&#32534;&#30721;&#22120;&#20197;&#20174;&#21512;&#24182;&#30340;&#24555;&#29031;&#20013;&#25429;&#33719;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#37319;&#29992;&#36923;&#36753;&#22238;&#24402;&#23558;&#27714;&#21644;&#27744;&#21270;&#30340;&#23884;&#20837;&#35299;&#30721;&#20026;&#20998;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Dissemination Process Classification (DPC) is a popular application of temporal graph classification. The aim of DPC is to classify different spreading patterns of information or pestilence within a community represented by discrete-time temporal graphs. Recently, a reservoir computing-based model named Dynamical Graph Echo State Network (DynGESN) has been proposed for processing temporal graphs with relatively high effectiveness and low computational costs. In this study, we propose a novel model which combines a novel data augmentation strategy called snapshot merging with the DynGESN for dealing with DPC tasks. In our model, the snapshot merging strategy is designed for forming new snapshots by merging neighboring snapshots over time, and then multiple reservoir encoders are set for capturing spatiotemporal features from merged snapshots. After those, the logistic regression is adopted for decoding the sum-pooled embeddings into the classification results. Experimental results o
&lt;/p&gt;</description></item><item><title>Rockmate&#26159;&#19968;&#20010;&#39640;&#25928;&#12289;&#24555;&#36895;&#12289;&#33258;&#21160;&#21644;&#36890;&#29992;&#30340;PyTorch&#37325;&#26032;&#26448;&#26009;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#26816;&#27979;&#35745;&#31639;&#21644;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#65292;&#23558;&#27169;&#22411;&#37325;&#20889;&#20026;&#22797;&#26434;&#22359;&#30340;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#19982;Checkmate&#21644;Rotor&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.01236</link><description>&lt;p&gt;
Rockmate: &#19968;&#20010;&#39640;&#25928;&#12289;&#24555;&#36895;&#12289;&#33258;&#21160;&#21644;&#36890;&#29992;&#30340;PyTorch&#37325;&#26032;&#26448;&#26009;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch. (arXiv:2307.01236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01236
&lt;/p&gt;
&lt;p&gt;
Rockmate&#26159;&#19968;&#20010;&#39640;&#25928;&#12289;&#24555;&#36895;&#12289;&#33258;&#21160;&#21644;&#36890;&#29992;&#30340;PyTorch&#37325;&#26032;&#26448;&#26009;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#26816;&#27979;&#35745;&#31639;&#21644;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#65292;&#23558;&#27169;&#22411;&#37325;&#20889;&#20026;&#22797;&#26434;&#22359;&#30340;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#19982;Checkmate&#21644;Rotor&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Rockmate&#26469;&#25511;&#21046;&#35757;&#32451;PyTorch&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;Rockmate&#26159;&#19968;&#20010;&#33258;&#21160;&#24037;&#20855;&#65292;&#20174;&#27169;&#22411;&#20195;&#30721;&#24320;&#22987;&#65292;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#28608;&#27963;&#20869;&#23384;&#37327;&#29983;&#25104;&#19968;&#20010;&#31561;&#25928;&#27169;&#22411;&#65292;&#20197;&#23569;&#37327;&#37325;&#26032;&#35745;&#31639;&#20026;&#20195;&#20215;&#12290;Rockmate&#33258;&#21160;&#26816;&#27979;&#35745;&#31639;&#21644;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#65292;&#24182;&#23558;&#21021;&#22987;&#27169;&#22411;&#37325;&#20889;&#20026;&#22797;&#26434;&#22359;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#26679;&#30340;&#32467;&#26500;&#24456;&#26222;&#36941;&#65292;&#22312;&#35768;&#22810;&#25991;&#29486;&#30340;&#27169;&#22411;&#20013;&#37117;&#21487;&#20197;&#25214;&#21040;&#65288;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;ResNet&#65292;RegNets&#31561;&#65289;&#12290;&#36825;&#31181;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#22359;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;Checkmate&#30340;&#25913;&#32534;&#65288;&#22312;&#25972;&#20010;&#27169;&#22411;&#19978;&#36895;&#24230;&#22826;&#24930;&#20294;&#36890;&#29992;&#65289;&#65292;&#24182;&#22312;&#24207;&#21015;&#26412;&#36523;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;Rotor&#30340;&#25913;&#32534;&#65288;&#36895;&#24230;&#24555;&#20294;&#20165;&#38480;&#20110;&#39034;&#24207;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35768;&#22810;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Rockmate&#19982;Rotor&#19968;&#26679;&#24555;&#65292;&#19982;Checkmate&#19968;&#26679;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
We propose Rockmate to control the memory requirements when training PyTorch DNN models. Rockmate is an automatic tool that starts from the model code and generates an equivalent model, using a predefined amount of memory for activations, at the cost of a few re-computations. Rockmate automatically detects the structure of computational and data dependencies and rewrites the initial model as a sequence of complex blocks. We show that such a structure is widespread and can be found in many models in the literature (Transformer based models, ResNet, RegNets,...). This structure allows us to solve the problem in a fast and efficient way, using an adaptation of Checkmate (too slow on the whole model but general) at the level of individual blocks and an adaptation of Rotor (fast but limited to sequential models) at the level of the sequence itself. We show through experiments on many models that Rockmate is as fast as Rotor and as efficient as Checkmate, and that it allows in many cases to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#20102;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#31561;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.01234</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Internet of Things Fault Detection and Classification via Multitask Learning. (arXiv:2307.01234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#20102;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#31561;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24037;&#19994;&#29289;&#32852;&#32593;&#24212;&#29992;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#26631;&#27880;&#12289;&#31639;&#27861;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#30340;&#24037;&#19994;&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#25105;&#20204;&#23545;11&#20010;&#39044;&#23450;&#20041;&#30340;&#25925;&#38556;&#31867;&#21035;&#36827;&#34892;&#20102;&#19977;&#20010;&#38454;&#27573;&#30340;&#25968;&#25454;&#25910;&#38598;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SMTCNN&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;SMTCNN&#22312;&#29305;&#24322;&#24615;&#65288;3.5%&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive investigation into developing a fault detection and classification system for real-world IIoT applications. The study addresses challenges in data collection, annotation, algorithm development, and deployment. Using a real-world IIoT system, three phases of data collection simulate 11 predefined fault categories. We propose SMTCNN for fault detection and category classification in IIoT, evaluating its performance on real-world data. SMTCNN achieves superior specificity (3.5%) and shows significant improvements in precision, recall, and F1 measures compared to existing techniques.
&lt;/p&gt;</description></item><item><title>RobustL2S&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23558;&#21767;&#35821;&#36716;&#25442;&#20026;&#35821;&#38899;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#35821;&#38899;&#20869;&#23481;&#24182;&#23545;&#20854;&#36827;&#34892;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01233</link><description>&lt;p&gt;
RobustL2S: &#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#20010;&#21035;&#35762;&#35805;&#32773;&#21767;&#35821;&#21512;&#25104;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations. (arXiv:2307.01233v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01233
&lt;/p&gt;
&lt;p&gt;
RobustL2S&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23558;&#21767;&#35821;&#36716;&#25442;&#20026;&#35821;&#38899;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#35821;&#38899;&#20869;&#23481;&#24182;&#23545;&#20854;&#36827;&#34892;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#21035;&#35762;&#35805;&#32773;&#21767;&#35821;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26088;&#22312;&#20174;&#26080;&#22768;&#30340;&#35828;&#35805;&#20154;&#38754;&#37096;&#35270;&#39057;&#20013;&#29983;&#25104;&#35821;&#38899;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#65292;&#30452;&#25509;&#20174;&#21767;&#37096;&#34920;&#31034;&#39044;&#27979;mel&#39057;&#35889;&#22270;&#25110;&#38899;&#39057;&#27874;&#24418;&#12290;&#25105;&#20204;&#20551;&#35774;&#30452;&#25509;mel&#39044;&#27979;&#20250;&#22240;&#35757;&#32451;/&#27169;&#22411;&#25928;&#29575;&#21463;&#21040;&#35821;&#38899;&#20869;&#23481;&#19982;&#29615;&#22659;&#20449;&#24687;&#21644;&#35762;&#35805;&#32773;&#29305;&#24449;&#30340;&#32416;&#32544;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RobustL2S&#65292;&#19968;&#20010;&#29992;&#20110;&#21767;&#35821;&#21512;&#25104;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#38750;&#33258;&#22238;&#24402;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#35299;&#32806;&#30340;&#35821;&#38899;&#20869;&#23481;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#22768;&#30721;&#22120;&#23558;&#35821;&#38899;&#29305;&#24449;&#36716;&#25442;&#20026;&#21407;&#22987;&#27874;&#24418;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#35774;&#32622;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#26080;&#32422;&#26463;&#30340;Lip2Wav&#25968;&#25454;&#38598;&#21644;&#32422;&#26463;&#30340;GRID&#21644;TCD-TIMIT&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant progress has been made in speaker dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Spee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#30340;&#40065;&#26834;&#27169;&#22411;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#26234;&#33021;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#32452;&#35013;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.01232</link><description>&lt;p&gt;
&#26080;&#22122;&#22768;&#25968;&#25454;&#20013;&#20869;&#38236;&#35270;&#39057;&#30340;&#40065;&#26834;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data. (arXiv:2307.01232v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#30340;&#40065;&#26834;&#27169;&#22411;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#26234;&#33021;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#32452;&#35013;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#22806;&#31185;&#25968;&#25454;&#31185;&#23398;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26032;&#20852;&#30340;ML&#25216;&#26415;&#22312;&#20998;&#26512;&#25163;&#26415;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#25163;&#26415;&#35760;&#24405;&#65292;&#29992;&#20110;&#25968;&#23383;&#21270;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#21151;&#33021;&#65292;&#22914;&#26415;&#21069;&#35745;&#21010;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#20915;&#31574;&#21644;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#32570;&#20047;&#20195;&#34920;&#24615;&#21644;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#20013;&#38388;ML&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#26631;&#31614;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#21487;&#38752;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#24320;&#21457;&#29992;&#20110;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;(1)&#26234;&#33021;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#26368;&#23567;&#25968;&#25454;&#38598;&#30340;&#35782;&#21035;&#21644;&#20154;&#24037;&#19987;&#23478;&#30340;&#26631;&#31614;&#20462;&#27491;&#65307;(2)&#29992;&#20110;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#32452;&#35013;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, surgical data science has attracted substantial interest from the machine learning (ML) community. Various studies have demonstrated the efficacy of emerging ML techniques in analysing surgical data, particularly recordings of procedures, for digitizing clinical and non-clinical functions like preoperative planning, context-aware decision-making, and operating skill assessment. However, this field is still in its infancy and lacks representative, well-annotated datasets for training robust models in intermediate ML tasks. Also, existing datasets suffer from inaccurate labels, hindering the development of reliable models. In this paper, we propose a systematic methodology for developing robust models for surgical tool detection using noisy data. Our methodology introduces two key innovations: (1) an intelligent active learning strategy for minimal dataset identification and label correction by human experts; and (2) an assembling strategy for a student-teacher m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.01231</link><description>&lt;p&gt;
&#23545;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;(ER)&#26159;&#35782;&#21035;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#25351;&#21521;&#30456;&#21516;&#23454;&#20307;&#30340;&#35760;&#24405;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#35299;&#20915;ER&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21305;&#37197;&#38454;&#27573;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#23545;&#23454;&#39564;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#36827;&#34892;&#26816;&#26597;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#20010;&#24050;&#24314;&#31435;&#25968;&#25454;&#38598;&#30340;&#38590;&#24230;&#21644;&#36866;&#29992;&#24615;&#65306;&#20004;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#28041;&#21450;&#26032;&#30340;&#32447;&#24615;&#24230;&#37327;&#21644;&#29616;&#26377;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20197;&#21450;&#20004;&#31181;&#23454;&#38469;&#26041;&#27861;&#65306;&#26368;&#20339;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#21305;&#37197;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#20339;&#23398;&#20064;&#21305;&#37197;&#22120;&#21644;&#23436;&#32654;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#25968;&#25454;&#38598;&#37117;&#25552;&#20986;&#20102;&#30456;&#24403;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.01230</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#20855;&#26377;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35770;&#25991;&#12289;&#22270;&#20687;&#12289;&#38899;&#20048;&#29978;&#33267;&#19977;&#32500;&#36164;&#20135;&#30340;&#33021;&#21147;&#65292;&#20026;&#22810;&#23398;&#31185;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25991;&#26412;&#21040;&#19977;&#32500;&#27169;&#22411;&#22312;&#24037;&#31243;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#35745;&#31639;&#27169;&#25311;&#35774;&#35745;&#20248;&#21270;&#20013;&#25972;&#21512;&#21644;&#20132;&#20114;&#19977;&#32500;&#36164;&#20135;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25968;&#20540;&#34920;&#31034;&#30340;&#19977;&#32500;&#20960;&#20309;&#35774;&#35745;&#20248;&#21270;&#19981;&#21516;&#65292;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#23545;&#21464;&#24322;&#31639;&#23376;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#20943;&#36731;&#21644;&#28608;&#21457;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#30340;&#36827;&#21270;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26368;&#36817;&#25512;&#20986;&#30340;Shap-E&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently pu
&lt;/p&gt;</description></item><item><title>EmoGen&#26159;&#19968;&#31181;&#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#20027;&#35266;&#20559;&#24046;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#29983;&#25104;&#20998;&#20026;&#24773;&#24863;&#21040;&#23646;&#24615;&#30340;&#26144;&#23556;&#20197;&#21450;&#23646;&#24615;&#21040;&#38899;&#20048;&#30340;&#29983;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2307.01229</link><description>&lt;p&gt;
EmoGen: &#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#20027;&#35266;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
EmoGen: Eliminating Subjective Bias in Emotional Music Generation. (arXiv:2307.01229v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01229
&lt;/p&gt;
&lt;p&gt;
EmoGen&#26159;&#19968;&#31181;&#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#20027;&#35266;&#20559;&#24046;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#29983;&#25104;&#20998;&#20026;&#24773;&#24863;&#21040;&#23646;&#24615;&#30340;&#26144;&#23556;&#20197;&#21450;&#23646;&#24615;&#21040;&#38899;&#20048;&#30340;&#29983;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#29992;&#20110;&#20256;&#36798;&#24773;&#24863;&#65292;&#22240;&#27492;&#22312;&#33258;&#21160;&#29983;&#25104;&#38899;&#20048;&#26102;&#29983;&#25104;&#24773;&#24863;&#38899;&#20048;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#20851;&#20110;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#30340;&#24037;&#20316;&#30452;&#25509;&#20351;&#29992;&#26631;&#27880;&#30340;&#24773;&#24863;&#26631;&#31614;&#20316;&#20026;&#25511;&#21046;&#20449;&#21495;&#65292;&#20294;&#23384;&#22312;&#20027;&#35266;&#20559;&#24046;&#65306;&#19981;&#21516;&#30340;&#20154;&#21487;&#33021;&#20250;&#22312;&#21516;&#26679;&#30340;&#38899;&#20048;&#19978;&#26631;&#27880;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#21516;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#20063;&#21487;&#33021;&#24863;&#21463;&#21040;&#19981;&#21516;&#30340;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#30452;&#25509;&#23558;&#24773;&#24863;&#26631;&#31614;&#26144;&#23556;&#21040;&#38899;&#20048;&#24207;&#21015;&#20013;&#20250;&#28151;&#28102;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#38459;&#30861;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EmoGen&#65292;&#19968;&#31181;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#24773;&#24863;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#23558;&#29983;&#25104;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22522;&#20110;&#30417;&#30563;&#32858;&#31867;&#30340;&#24773;&#24863;&#21040;&#23646;&#24615;&#26144;&#23556;&#20197;&#21450;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#26159;&#26377;&#30410;&#30340;&#65306;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#32858;&#31867;&#21608;&#22260;&#30340;&#23646;&#24615;&#20540;&#26377;&#21161;&#20110;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#31532;&#20108;&#20010;&#38454;&#27573;&#21017;&#23454;&#29616;&#20102;&#38899;&#20048;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learning process and hinder the model from generating music with general emotions. In this paper, we propose EmoGen, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music, and divides the generation into two stages: emotion-to-attribute mapping with supervised clustering, and attribute-to-music generation with self-supervised learning. Both stages are beneficial: in the first stage, the attribute values around the clusterin
&lt;/p&gt;</description></item><item><title>ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01227</link><description>&lt;p&gt;
ESGCN: &#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01227
&lt;/p&gt;
&lt;p&gt;
ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20132;&#36890;&#27969;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Edge Squeeze Graph Convolutional Network (ESGCN)&#30340;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#20010;&#22320;&#21306;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;ESGCN&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;W&#27169;&#22359;&#21644;ES&#27169;&#22359;&#12290;W&#27169;&#22359;&#26159;&#19968;&#20010;&#23436;&#20840;&#20197;&#33410;&#28857;&#20026;&#22522;&#30784;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;&#23427;&#20998;&#21035;&#23545;&#27599;&#20010;&#20132;&#36890;&#21306;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#29305;&#24449;&#12290;ES&#27169;&#22359;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26102;&#24207;&#29305;&#24449;&#29983;&#25104;&#33258;&#36866;&#24212;&#37051;&#25509;&#30697;&#38453;(AAM)&#12290;&#20026;&#20102;&#25552;&#39640;AAM&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#12290;1&#65289;&#20351;&#29992;&#36793;&#32536;&#29305;&#24449;&#30452;&#25509;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#26102;&#31354;&#27969;&#21160;&#34920;&#31034;&#12290;2&#65289;&#23558;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#24212;&#29992;&#20110;GCN&#65292;&#20174;&#36793;&#32536;&#29305;&#24449;&#20013;&#25552;&#21462;AAM&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.01225</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22914;BERT&#12289;Roberta&#12289;T5&#21644;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#24456;&#38590;&#29702;&#35299;&#23545;&#25239;&#24615;&#20998;&#31867;&#24182;&#35782;&#21035;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#12290;&#23427;&#19987;&#27880;&#20110;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26102;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;IT-DT&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#24615;&#26816;&#27979;&#12290;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#12290;&#22312;&#36716;&#25442;&#38454;&#27573;&#65292;IT-DT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#12290;&#36890;&#36807;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#25442;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;</title><link>http://arxiv.org/abs/2307.01217</link><description>&lt;p&gt;
FedCP:&#36890;&#36807;&#26465;&#20214;&#31574;&#30053;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#22312;&#38544;&#31169;&#20445;&#25252;&#12289;&#21327;&#20316;&#23398;&#20064;&#20197;&#21450;&#35299;&#20915;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20363;&#22914;&#21307;&#38498;&#12289;&#31227;&#21160;&#26234;&#33021;&#25163;&#26426;&#31561;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#20391;&#37325;&#20110;&#21033;&#29992;&#23458;&#25143;&#31471;&#32423;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#26159;&#36825;&#20004;&#31181;&#20449;&#24687;&#30340;&#28304;&#22836;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26465;&#20214;&#31574;&#30053;&#65288;FedCP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20998;&#31163;&#20854;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#28982;&#21518;&#20998;&#21035;&#36890;&#36807;&#20840;&#23616;&#22836;&#21644;&#20010;&#24615;&#21270;&#22836;&#36827;&#34892;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#30456;&#27604;&#65292;FedCP&#26356;&#21152;&#32454;&#31890;&#24230;&#22320;&#32771;&#34385;&#20010;&#24615;&#21270;&#30340;&#26679;&#26412;&#29305;&#23450;&#26041;&#24335;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FedCP&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#25512;&#33616;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#23574;&#23792;&#24418;&#25104;&#12290;&#36890;&#36807;&#25552;&#20986;&#24230;&#37327;&#26631;&#20934;&#24182;&#36827;&#34892;&#25968;&#23398;&#35777;&#26126;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23574;&#23792;&#24418;&#25104;&#19982;&#19981;&#21516;&#20869;&#37096;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#31038;&#21306;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#29992;&#20363;&#65292;&#25506;&#35752;&#20102;&#22312;&#28155;&#21152;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38899;&#20048;&#23884;&#20837;&#30340;&#30456;&#20284;&#39033;&#30446;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.01212</link><description>&lt;p&gt;
&#20851;&#20110;&#23574;&#38160;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#21644;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Of Spiky SVDs and Music Recommendation. (arXiv:2307.01212v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#25512;&#33616;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#29616;&#35937;&#65306;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#23574;&#23792;&#24418;&#25104;&#12290;&#36890;&#36807;&#25552;&#20986;&#24230;&#37327;&#26631;&#20934;&#24182;&#36827;&#34892;&#25968;&#23398;&#35777;&#26126;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23574;&#23792;&#24418;&#25104;&#19982;&#19981;&#21516;&#20869;&#37096;&#27969;&#34892;&#24230;&#30340;&#39033;&#30446;&#31038;&#21306;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#29992;&#20363;&#65292;&#25506;&#35752;&#20102;&#22312;&#28155;&#21152;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38899;&#20048;&#23884;&#20837;&#30340;&#30456;&#20284;&#39033;&#30446;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#26159;&#38899;&#20048;&#25512;&#33616;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#26816;&#32034;&#31867;&#20284;&#29289;&#21697;&#25110;&#20026;&#19979;&#28216;&#20219;&#21153;&#23884;&#20837;&#38899;&#20048;&#39033;&#30446;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35768;&#22810;&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#19968;&#31181;&#26377;&#36259;&#25928;&#24212;&#65306;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#23574;&#23792;&#24418;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#36825;&#31181;&#23574;&#23792;&#32452;&#32455;&#24378;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#28982;&#21518;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#20854;&#36215;&#28304;&#19982;&#20869;&#37096;&#27969;&#34892;&#24230;&#21508;&#24322;&#30340;&#39033;&#30446;&#31038;&#21306;&#26377;&#20851;&#12290;&#20973;&#20511;&#36825;&#31181;&#26032;&#33719;&#24471;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#25105;&#20204;&#26368;&#21518;&#20197;&#19968;&#31181;&#24037;&#19994;&#29992;&#20363;&#24320;&#25918;&#20102;&#35813;&#20027;&#39064;&#65292;&#29992;&#20110;&#20272;&#35745;&#22312;&#28155;&#21152;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38899;&#20048;&#23884;&#20837;&#30340;&#21069;k&#20010;&#30456;&#20284;&#39033;&#30446;&#23558;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truncated singular value decomposition is a widely used methodology in music recommendation for direct similar-item retrieval or embedding musical items for downstream tasks. This paper investigates a curious effect that we show naturally occurring on many recommendation datasets: spiking formations in the embedding space. We first propose a metric to quantify this spiking organization's strength, then mathematically prove its origin tied to underlying communities of items of varying internal popularity. With this new-found theoretical understanding, we finally open the topic with an industrial use case of estimating how music embeddings' top-k similar items will change over time under the addition of data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27721;&#23383;&#38899;&#38901;&#23398;&#20013;&#33719;&#21462;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#21644;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#24182;&#23637;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01209</link><description>&lt;p&gt;
&#27721;&#23383;&#38899;&#31995;&#30340;&#22810;&#26041;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27721;&#23383;&#38899;&#38901;&#23398;&#20013;&#33719;&#21462;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#21644;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#24182;&#23637;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35821;&#35328;&#21644;&#38899;&#38901;&#31561;&#35937;&#24449;&#31995;&#32479;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#22312;&#27721;&#23383;&#21382;&#21490;&#38899;&#38901;&#23398;&#20013;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#20219;&#21153;&#21253;&#25324;&#26041;&#35328;&#27604;&#36739;&#21644;&#21407;&#22987;&#35821;&#35328;&#31995;&#32479;&#30340;&#37325;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33719;&#21462;&#27721;&#23383;&#38899;&#33410;&#30340;&#22810;&#26041;&#35328;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#32467;&#26500;&#21270;&#38899;&#38901;&#25968;&#25454;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#24212;&#29992;&#30693;&#35782;&#24211;&#23398;&#20064;&#20013;&#30340;BoxE&#25216;&#26415;&#12290;&#25105;&#20204;&#24212;&#29992;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#25216;&#26415;&#23545;&#25152;&#24471;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#35266;&#23519;&#65292;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#25429;&#25417;&#21040;&#36755;&#20837;&#26041;&#35328;&#30340;&#38899;&#20301;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20998;&#31867;&#22120;&#26469;&#36827;&#34892;&#26080;&#27861;&#35266;&#23519;&#30340;&#20013;&#21476;&#27721;&#35821;&#26631;&#31614;&#30340;&#25512;&#29702;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#25581;&#31034;&#21476;&#32769;&#30340;&#21407;&#22987;&#35821;&#35328;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#23545;&#30862;&#29255;&#21270;&#30340;&#27721;&#23383;&#38899;&#38901;&#36827;&#34892;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data, then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations' potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#38024;&#23545;&#22312;&#32447;&#21644;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#23454;&#26045;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;&#31038;&#20132;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#25512;&#33616;&#20219;&#21153;&#65292;&#24182;&#25551;&#36848;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#23436;&#20840;&#20998;&#24067;&#29615;&#22659;&#19979;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.01207</link><description>&lt;p&gt;
&#22312;&#32447;&#21644;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems for Online and Mobile Social Networks: A survey. (arXiv:2307.01207v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#38024;&#23545;&#22312;&#32447;&#21644;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#23454;&#26045;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;&#31038;&#20132;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#25512;&#33616;&#20219;&#21153;&#65292;&#24182;&#25551;&#36848;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#23436;&#20840;&#20998;&#24067;&#29615;&#22659;&#19979;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30446;&#21069;&#22312;&#22312;&#32447;&#26381;&#21153;&#20013;&#25198;&#28436;&#30528;&#22522;&#30784;&#24037;&#20855;&#30340;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#65288;OSN&#65289;&#30340;&#20986;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#20869;&#23481;&#65292;&#24456;&#23481;&#26131;&#34987;&#26080;&#29992;&#20449;&#24687;&#28153;&#27809;&#12290;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#20063;&#26159;&#34920;&#24449;&#20869;&#23481;&#21644;&#29992;&#25143;&#20852;&#36259;&#30340;&#37325;&#35201;&#20449;&#24687;&#28304;&#12290;RS&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#36827;&#19968;&#27493;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#25913;&#36827;&#25512;&#33616;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20026;&#22312;&#32447;&#21644;&#31227;&#21160;&#31038;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#23454;&#26045;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35843;&#30740;&#65292;&#37325;&#28857;&#25551;&#36848;&#20102;&#31038;&#20132;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#25512;&#33616;&#20219;&#21153;&#65292;&#20197;&#21450;&#26631;&#20934;&#31639;&#27861;&#22312;&#23436;&#20840;&#20998;&#24067;&#29615;&#22659;&#65288;&#22914;&#26426;&#20250;&#32593;&#32476;&#65289;&#20013;&#22914;&#20309;&#36827;&#34892;&#25913;&#36827;&#21644;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#12289;&#30446;&#26631;&#39046;&#22495;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24615;&#33021;&#35780;&#20272;&#26469;&#25551;&#36848;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems (RS) currently represent a fundamental tool in online services, especially with the advent of Online Social Networks (OSN). In this case, users generate huge amounts of contents and they can be quickly overloaded by useless information. At the same time, social media represent an important source of information to characterize contents and users' interests. RS can exploit this information to further personalize suggestions and improve the recommendation process. In this paper we present a survey of Recommender Systems designed and implemented for Online and Mobile Social Networks, highlighting how the use of social context information improves the recommendation task, and how standard algorithms must be enhanced and optimized to run in a fully distributed environment, as opportunistic networks. We describe advantages and drawbacks of these systems in terms of algorithms, target domains, evaluation metrics and performance evaluations. Eventually, we present some open
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32622;&#20449;&#24230;&#25490;&#24207;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#22312;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#24341;&#20837;&#20102;&#32622;&#20449;&#24230;&#25490;&#24207;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#20844;&#20849;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01206</link><description>&lt;p&gt;
CTR&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Confidence Ranking for CTR Prediction. (arXiv:2307.01206v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32622;&#20449;&#24230;&#25490;&#24207;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#22312;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#24341;&#20837;&#20102;&#32622;&#20449;&#24230;&#25490;&#24207;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#20844;&#20849;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#28436;&#36827;&#21644;&#25968;&#25454;&#30340;&#25345;&#32493;&#21487;&#29992;&#24615;&#26159;&#20004;&#20010;&#24120;&#35265;&#29616;&#35937;&#65292;&#20363;&#22914;&#24191;&#21578;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#20102;&#36866;&#24212;&#36825;&#31181;&#24773;&#20917;&#65292;&#23454;&#38469;&#31995;&#32479;&#36890;&#24120;&#20250;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#20197;&#26399;&#23450;&#26399;&#26356;&#26032;&#27169;&#22411;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32622;&#20449;&#24230;&#25490;&#24207;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20248;&#21270;&#30446;&#26631;&#35774;&#35745;&#20026;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#27169;&#22411;&#30340;&#25490;&#24207;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32622;&#20449;&#24230;&#25490;&#24207;&#25439;&#22833;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#19981;&#21516;&#20984;&#26367;&#20195;&#20989;&#25968;&#65288;&#20363;&#22914;AUC&#21644;&#20934;&#30830;&#24230;&#65289;&#30340;logits&#36755;&#20986;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#30446;&#26631;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22312;&#20844;&#20849;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#30340;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#24341;&#20837;&#32622;&#20449;&#24230;&#25490;&#24207;&#25439;&#22833;&#21487;&#20197;&#32988;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24050;&#22312;&#20140;&#19996;&#24191;&#21578;&#31995;&#32479;&#20013;&#37096;&#32626;&#65292;&#20197;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model evolution and constant availability of data are two common phenomena in large-scale real-world machine learning applications, e.g. ads and recommendation systems. To adapt, the real-world system typically retrain with all available data and online learn with recently available data to update the models periodically with the goal of better serving performance. In this paper, we propose a novel framework, named Confidence Ranking, which designs the optimization objective as a ranking function with two different models. Our confidence ranking loss allows direct optimization of the logits output for different convex surrogate functions of metrics, e.g. AUC and Accuracy depending on the target task and dataset. Armed with our proposed methods, our experiments show that the introduction of confidence ranking loss can outperform all baselines on the CTR prediction tasks of public and industrial datasets. This framework has been deployed in the advertisement system of JD.com to serve the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#65292;&#22312;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01204</link><description>&lt;p&gt;
&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach. (arXiv:2307.01204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#65292;&#22312;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#29992;&#23569;&#26679;&#26412;&#30340;&#38142;&#25509;&#26469;&#39044;&#27979;&#26410;&#35265;&#23454;&#20307;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#20256;&#23548;&#22330;&#26223;&#65292;&#21363;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#23454;&#20307;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#26410;&#35265;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;&#30340;&#24402;&#32435;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#23376;&#22270;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#26080;&#27861;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24402;&#32435;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#65292;&#31216;&#20026;RawNP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20989;&#25968;&#24314;&#27169;&#20026;&#28789;&#27963;&#30340;&#20998;&#24067;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict missing links for unseen entities with few-shot links observed. Previous methods are limited to transductive scenarios, where entities exist in the knowledge graphs, so they are unable to handle unseen entities. Therefore, recent inductive methods utilize the sub-graphs around unseen entities to obtain the semantics and predict links inductively. However, in the few-shot setting, the sub-graphs are often sparse and cannot provide meaningful inductive patterns. In this paper, we propose a novel relational anonymous walk-guided neural process for few-shot inductive link prediction on knowledge graphs, denoted as RawNP. Specifically, we develop a neural process-based method to model a flexible distribution over link prediction functions. This enables the model to quickly adapt to new entities and estimate the uncertainty when making predictions. To capture general inductive patterns, we present a relational anony
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01202</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#19987;&#21033;&#23398;&#65306;&#20351;&#29992;ChatGPT&#25216;&#26415;&#39044;&#27979;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;
&lt;/p&gt;
&lt;p&gt;
Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26041;&#27861;&#23545;&#20110;&#21019;&#26032;&#30340;&#20998;&#26512;&#22312;&#24191;&#27867;&#30340;&#32467;&#26500;&#21464;&#37327;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;ChatGPT&#25216;&#26415;&#37319;&#29992;LLM&#26041;&#27861;&#23545;&#19987;&#21033;&#36827;&#34892;&#20998;&#26512;&#65292;&#31361;&#30772;&#20102;&#36793;&#30028;&#12290;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#22815;&#35775;&#38382;&#20851;&#20110;&#27599;&#20010;&#21457;&#26126;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#29992;&#20110;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#12290;&#32454;&#33268;&#30340;&#23884;&#20837;&#20351;&#39044;&#27979;&#19987;&#21033;&#20215;&#20540;&#30340;R-squared&#25552;&#39640;&#20102;24&#65285;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#26368;&#24046;&#21644;&#26368;&#20339;&#24212;&#29992;&#31243;&#24207;&#20998;&#31163;&#24320;&#26469;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#27599;&#24180;&#23454;&#29616;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#39640;&#36798;3.3%&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#24066;&#22330;&#26080;&#27861;&#21450;&#26102;&#25972;&#21512;&#26377;&#20851;&#24212;&#29992;&#31243;&#24207;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#27169;&#22411;&#20026;&#38761;&#21629;&#24615;&#25913;&#21464;&#31185;&#26681;&#12289;&#24085;&#24085;&#23612;&#31185;&#27931;&#12289;&#22622;&#40065;&#21644;&#26031;&#25176;&#22827;&#26364;&#65288;2017&#65289;&#23545;&#19987;&#21033;&#20272;&#20540;&#30340;&#26356;&#27491;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutioniz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36951;&#24536;&#22312;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#35821;&#35328;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#23558;PLMs&#24212;&#29992;&#20110;&#26032;&#35821;&#35328;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#36825;&#26159;&#20351;&#23427;&#20204;&#30340;&#33021;&#21147;&#26222;&#36941;&#21487;&#35775;&#38382;&#30340;&#22721;&#22418;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20026;&#26032;&#35821;&#35328;&#23398;&#20064;&#26032;&#30340;&#23884;&#20837;&#23618;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#20570;&#26082;&#28010;&#36153;&#25968;&#25454;&#21448;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#65292;&#20316;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;PLMs&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#27599;K&#27425;&#26356;&#26032;&#26102;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#25105;&#20204;&#40723;&#21169;PLM&#22312;&#26377;&#38480;&#27425;&#26356;&#26032;&#20869;&#25552;&#39640;&#23398;&#20064;&#26032;&#23884;&#20837;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;RoBERTa&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#36951;&#24536;&#26426;&#21046;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#36866;&#24212;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20302;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
&lt;/p&gt;</description></item><item><title>AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01146</link><description>&lt;p&gt;
AVSegFormer: &#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01146
&lt;/p&gt;
&lt;p&gt;
AVSegFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#38899;&#35270;&#39057;&#20998;&#21106;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#26469;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#35270;&#35273;&#29305;&#24449;&#65292;&#36824;&#20351;&#29992;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#32467;&#21512;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#23450;&#20301;&#21644;&#20998;&#21106;&#32473;&#23450;&#35270;&#39057;&#20013;&#30340;&#26377;&#22768;&#23545;&#35937;&#12290;&#36825;&#20010;&#20219;&#21153;&#39318;&#27425;&#35201;&#27714;&#22312;&#20687;&#32032;&#32423;&#21035;&#23545;&#38899;&#39057;&#39537;&#21160;&#30340;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AVSegFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;AVS&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#38899;&#39057;&#26597;&#35810;&#21644;&#21487;&#23398;&#20064;&#26597;&#35810;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#24863;&#20852;&#36259;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38899;&#39057;-&#35270;&#35273;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30456;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#21644;&#25233;&#21046;&#26080;&#20851;&#30340;&#31354;&#38388;&#36890;&#36947;&#26469;&#21160;&#24577;&#35843;&#25972;&#35270;&#35273;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#38388;&#25513;&#27169;&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#30417;&#30563;&#65292;&#40723;&#21169;&#32593;&#32476;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#39318;&#27425;&#23545;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#21512;&#35268;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#22823;&#22823;&#19979;&#38477;&#65292;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01088</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#25968;&#25454;&#19979;&#65292;&#23545;&#29616;&#20195;&#35270;&#35273;&#26550;&#26500;&#36827;&#34892;&#21512;&#35268;&#39044;&#27979;&#30340;&#32463;&#39564;&#35777;&#23454;
&lt;/p&gt;
&lt;p&gt;
Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#39318;&#27425;&#23545;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#21512;&#35268;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#22823;&#22823;&#19979;&#38477;&#65292;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21487;&#38752;&#22320;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#23433;&#20840;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#24615;&#33021;&#24050;&#30693;&#22312;&#20998;&#24067;&#36716;&#31227;&#21644;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#20250;&#19979;&#38477;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#32463;&#24120;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#20960;&#31181;&#20107;&#21518;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#21512;&#35268;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#34920;&#24449;&#65292;&#24182;&#39318;&#27425;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#21512;&#35268;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20013;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#36829;&#21453;&#23433;&#20840;&#20445;&#35777;&#26102;&#22823;&#22823;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#22312;&#38271;&#23614;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#31867;&#21035;&#30340;&#20445;&#35777;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#37096;&#32626;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.
&lt;/p&gt;</description></item><item><title>CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.00859</link><description>&lt;p&gt;
CardiGraphormer: &#25581;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00859
&lt;/p&gt;
&lt;p&gt;
CardiGraphormer&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#39072;&#35206;&#20102;&#33647;&#29289;&#21457;&#29616;&#30340;&#26041;&#24335;&#12290;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#25191;&#34892;&#21508;&#31181;&#19982;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#38420;&#30340;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#65292;&#24050;&#30693;&#33647;&#29289;&#32422;&#26377;15,000&#31181;&#65292;&#20294;&#21482;&#26377;&#22823;&#32422;4,200&#31181;&#24471;&#21040;&#20102;&#25209;&#20934;&#65292;&#21270;&#23398;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#25552;&#20379;&#20102;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#25104;&#20026;&#20102;&#26377;&#21147;&#30340;&#20249;&#20276;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#20173;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CardiGraphormer&#65292;&#36825;&#26159;&#19968;&#31181;&#21010;&#26102;&#20195;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#39072;&#35206;&#33647;&#29289;&#21457;&#29616;&#12290;CardiGraphormer&#26159;Graphormer&#21644;&#20445;&#25345;&#22522;&#25968;&#27880;&#24847;&#21147;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#21033;&#29992;SSL&#23398;&#20064;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;GNN&#25552;&#21462;&#20998;&#23376;&#25351;&#32441;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#23427;&#22312;&#22788;&#29702;&#20998;&#23376;&#32467;&#26500;&#31561;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#25191;&#34892;&#19982;&#33410;&#28857;&#12289;&#33410;&#28857;&#23545;&#12289;&#23376;&#22270;&#25110;&#25972;&#20010;&#22270;&#32467;&#26500;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.00668</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Active Sensing with Predictive Coding and Uncertainty Minimization. (arXiv:2307.00668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20840;&#27969;&#31243;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20219;&#20309;&#20219;&#21153;&#26080;&#20851;&#21644;&#20869;&#22312;&#39537;&#21160;&#30340;&#24773;&#22659;&#19979;&#24212;&#29992;&#20110;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#22522;&#30784;&#30340;&#36716;&#25442;&#20998;&#24067;&#24182;&#37325;&#24314;&#29615;&#22659;&#30340;&#31354;&#38388;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#20027;&#21160;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#20027;&#21160;&#37319;&#26679;&#20854;&#35270;&#35273;&#29615;&#22659;&#20197;&#33719;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#26080;&#30417;&#30563;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20027;&#21160;&#37319;&#26679;&#21644;&#39640;&#25928;&#20998;&#31867;&#24863;&#30693;&#22330;&#26223;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#30340;&#36755;&#20837;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#23398;&#20064;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#20302;&#30340;&#21442;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an end-to-end procedure for embodied exploration based on two biologically inspired computations: predictive coding and uncertainty minimization. The procedure can be applied to any exploration setting in a task-independent and intrinsically driven manner. We first demonstrate our approach in a maze navigation task and show that our model is capable of discovering the underlying transition distribution and reconstructing the spatial features of the environment. Second, we apply our model to the more complex task of active vision, where an agent must actively sample its visual environment to gather information. We show that our model is able to build unsupervised representations that allow it to actively sample and efficiently categorize sensory scenes. We further show that using these representations as input for downstream classification leads to superior data efficiency and learning speed compared to other baselines, while also maintaining lower parameter complexity. Final
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#19988;&#32467;&#26524;&#23545;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#38750;&#31354;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.00426</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#19988;&#32467;&#26524;&#23545;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#38750;&#31354;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#20855;&#20307;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#21069;&#21521;&#28145;&#24230;ReLU&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21033;&#29992;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20943;&#23567;&#26377;&#25928;&#27169;&#22411;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#20986;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26681;&#26412;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#27169;&#22411;&#23454;&#29616;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#33539;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#20013;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#20808;&#39564;&#32467;&#21512;&#26102;&#20063;&#33021;&#24471;&#21040;&#38750;&#31354;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.00165</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20004;&#31181;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#32972;&#26223;&#19979;&#65292;&#23427;&#20204;&#30340;&#20851;&#31995;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20849;&#21516;&#24314;&#27169;&#36825;&#20004;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#65288;&#31070;&#32463;&#65289;&#36923;&#36753;&#25512;&#29702;&#20004;&#31181;&#37325;&#35201;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#65292;&#23427;&#36890;&#36807;&#36827;&#34892;&#21453;&#20107;&#23454;&#36923;&#36753;&#25512;&#29702;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;CCR&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#8220;&#22256;&#38590;&#8221;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#19982;&#21407;&#22987;&#30340;&#35757;&#32451;&#26679;&#26412;&#19968;&#36215;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability -- counterfactual reasoning and (neural) logical reasoning -- we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate "difficult" counterfactual training examples for data augmentation, which -together with the original training examples -- can enhance the model performance. Since the 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#36317;&#31163;&#20989;&#25968;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#27809;&#26377;&#39044;&#20808;&#20102;&#35299;&#25968;&#25454;&#27969;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#21644;Canberra&#36317;&#31163;&#30340;&#32452;&#21512;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.00106</link><description>&lt;p&gt;
&#36317;&#31163;&#20989;&#25968;&#22312;&#27969;&#22330;&#26223;&#19979;&#30340;&#35268;&#33539;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distance Functions and Normalization Under Stream Scenarios. (arXiv:2307.00106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00106
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20843;&#31181;&#36317;&#31163;&#20989;&#25968;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#21457;&#29616;&#22312;&#27809;&#26377;&#39044;&#20808;&#20102;&#35299;&#25968;&#25454;&#27969;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#21644;Canberra&#36317;&#31163;&#30340;&#32452;&#21512;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35268;&#33539;&#21270;&#26159;&#24314;&#27169;&#20998;&#31867;&#31995;&#32479;&#26102;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#65292;&#30001;&#20110;&#21487;&#33021;&#26080;&#27861;&#39044;&#20808;&#20102;&#35299;&#29305;&#24449;&#30340;&#23646;&#24615;&#65288;&#22914;&#26368;&#23567;/&#26368;&#22823;&#20540;&#65289;&#65292;&#19988;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#22240;&#27492;&#25968;&#25454;&#35268;&#33539;&#21270;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20843;&#31181;&#33879;&#21517;&#30340;&#36317;&#31163;&#20989;&#25968;&#22312;&#27809;&#26377;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#27969;&#20013;&#29983;&#25104;&#30340;&#20934;&#30830;&#24230;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#25509;&#25910;&#21040;&#30340;&#31532;&#19968;&#25209;&#25968;&#25454;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#32771;&#34385;&#21040;&#19978;&#19968;&#25209;&#25968;&#25454;&#30340;&#35268;&#33539;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23558;&#20840;&#27969;&#31243;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#23454;&#39564;&#21327;&#35758;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#24182;&#20250;&#23548;&#33268;&#20559;&#20506;&#21644;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20107;&#20808;&#19981;&#20102;&#35299;&#25968;&#25454;&#27969;&#30340;&#20449;&#24687;&#26102;&#65292;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#27969;&#32780;&#19981;&#24212;&#29992;&#35268;&#33539;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;Canberra&#36317;&#31163;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#22909;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data normalization is an essential task when modeling a classification system. When dealing with data streams, data normalization becomes especially challenging since we may not know in advance the properties of the features, such as their minimum/maximum values, and these properties may change over time. We compare the accuracies generated by eight well-known distance functions in data streams without normalization, normalized considering the statistics of the first batch of data received, and considering the previous batch received. We argue that experimental protocols for streams that consider the full stream as normalized are unrealistic and can lead to biased and poor results. Our results indicate that using the original data stream without applying normalization, and the Canberra distance, can be a good combination when no information about the data stream is known beforehand.
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.16208</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;q-learning&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;q-learning&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#29992;&#20110;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;q&#20989;&#25968;&#30340;&#23384;&#22312;&#21450;&#20854;&#31215;&#20998;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;q-learning&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;McKean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;&#19982;Jia&#21644;Zhou&#65288;2022c&#65289;&#30340;&#21333;&#20010;&#20195;&#29702;&#25511;&#21046;&#38382;&#39064;&#19981;&#21516;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#22343;&#22330;&#30456;&#20114;&#20316;&#29992;&#20351;&#24471;q&#20989;&#25968;&#30340;&#23450;&#20041;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#28982;&#20135;&#29983;&#20004;&#31181;&#19981;&#21516;q&#20989;&#25968;&#30340;&#24773;&#20917;&#65306;&#65288;i&#65289;&#34987;&#31216;&#20026;&#38598;&#25104;q&#20989;&#25968;&#65288;&#29992;$q$&#34920;&#31034;&#65289;&#65292;&#20316;&#20026;Gu&#12289;Guo&#12289;Wei&#21644;Xu&#65288;2023&#65289;&#24341;&#20837;&#30340;&#38598;&#25104;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#21487;&#20197;&#36890;&#36807;&#28041;&#21450;&#27979;&#35797;&#31574;&#30053;&#30340;&#24369;&#38789;&#26465;&#20214;&#36827;&#34892;&#23398;&#20064;&#65307;&#65288;ii&#65289;&#20316;&#20026;&#31574;&#30053;&#25913;&#36827;&#36845;&#20195;&#20013;&#25152;&#20351;&#29992;&#30340;&#23454;&#36136;q&#20989;&#25968;&#65288;&#29992;$q_e$&#34920;&#31034;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;q&#20989;&#25968;&#22312;&#25152;&#26377;&#27979;&#35797;&#31574;&#30053;&#19979;&#36890;&#36807;&#31215;&#20998;&#34920;&#31034;&#30456;&#20851;&#32852;&#12290;&#22522;&#20110;&#38598;&#25104;q&#20989;&#25968;&#30340;&#24369;&#38789;&#26465;&#20214;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#23398;&#20064;&#20004;&#20010;q&#20989;&#25968;&#20197;&#35299;&#20915;Mckean-Vlasov&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the q-learning, recently coined as the continuous-time counterpart of Q-learning by Jia and Zhou (2022c), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2022c), the mean-field interaction of agents render the definition of q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023) that can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition of the integrated q-function and our proposed searching method of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#26041;&#27861;&#25581;&#31034;&#20102;Ti-Al&#31995;&#32479;&#20013;&#30340;&#30028;&#38754;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;TiAl3&#26230;&#30028;&#22312;&#23454;&#39564;&#28909;&#22788;&#29702;&#26465;&#20214;&#19979;Ti&#21644;Al&#21407;&#23376;&#30340;&#34892;&#20026;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#25581;&#31034;&#20102;&#28909;&#22788;&#29702;&#26089;&#26399;&#38454;&#27573;Al&#21407;&#23376;&#36890;&#36807;TiAl3&#26230;&#30028;&#26397;Ti&#34920;&#38754;&#25193;&#25955;&#26159;&#20027;&#35201;&#36807;&#31243;&#12290;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#21017;&#21487;&#20197;&#23450;&#20301;&#24182;&#30830;&#35748;&#20851;&#38190;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2306.14568</link><description>&lt;p&gt;
&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#25581;&#31034;Ti-Al&#31995;&#32479;&#30340;&#30028;&#38754;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Elucidating Interfacial Dynamics of Ti-Al Systems Using Molecular Dynamics Simulation and Markov State Modeling. (arXiv:2306.14568v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#26041;&#27861;&#25581;&#31034;&#20102;Ti-Al&#31995;&#32479;&#20013;&#30340;&#30028;&#38754;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;TiAl3&#26230;&#30028;&#22312;&#23454;&#39564;&#28909;&#22788;&#29702;&#26465;&#20214;&#19979;Ti&#21644;Al&#21407;&#23376;&#30340;&#34892;&#20026;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#25581;&#31034;&#20102;&#28909;&#22788;&#29702;&#26089;&#26399;&#38454;&#27573;Al&#21407;&#23376;&#36890;&#36807;TiAl3&#26230;&#30028;&#26397;Ti&#34920;&#38754;&#25193;&#25955;&#26159;&#20027;&#35201;&#36807;&#31243;&#12290;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#21017;&#21487;&#20197;&#23450;&#20301;&#24182;&#30830;&#35748;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20248;&#24322;&#30340;&#26426;&#26800;&#21644;&#21270;&#23398;&#24615;&#33021;&#65292;Ti-Al&#22522;&#26448;&#26009;&#22312;&#27773;&#36710;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#22269;&#38450;&#31561;&#20247;&#22810;&#24037;&#31243;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30001;&#20110;&#20854;&#20302;&#23494;&#24230;&#12289;&#39640;&#24378;&#24230;&#20197;&#21450;&#25239;&#33104;&#34432;&#21644;&#25239;&#27687;&#21270;&#24615;&#65292;&#36825;&#20123;&#37329;&#23646;&#38388;&#21270;&#21512;&#29289;&#21512;&#37329;&#21644;&#22797;&#21512;&#37329;&#23646;-&#37329;&#23646;&#22797;&#21512;&#26448;&#26009;&#24050;&#32463;&#25214;&#21040;&#20102;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;Ti-Al&#31995;&#32479;&#30340;&#30028;&#38754;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;TiAl3&#26230;&#30028;&#22312;&#23454;&#39564;&#28909;&#22788;&#29702;&#26465;&#20214;&#19979;Ti&#21644;Al&#21407;&#23376;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#27169;&#22411;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#23545;&#24418;&#25104;TiAl3&#25152;&#28041;&#21450;&#30340;&#21160;&#21147;&#23398;&#36807;&#31243;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#34920;&#26126;&#65292;&#22312;&#28909;&#22788;&#29702;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;Al&#21407;&#23376;&#36890;&#36807;TiAl3&#26230;&#30028;&#26397;Ti&#34920;&#38754;&#25193;&#25955;&#26159;&#20027;&#35201;&#36807;&#31243;&#12290;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24314;&#27169;&#21017;&#21487;&#20197;&#23450;&#20301;&#24182;&#30830;&#35748;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their remarkable mechanical and chemical properties, Ti-Al based materials are attracting considerable interest in numerous fields of engineering, such as automotive, aerospace, and defense. With their low density, high strength, and resistance to corrosion and oxidation, these intermetallic alloys and compound metal-metallic composites have found diverse applications. The present study delves into the interfacial dynamics of these Ti-Al systems, particularly focusing on the behavior of Ti and Al atoms in the presence of TiAl$_3$ grain boundaries under experimental heat treatment conditions. Using a combination of Molecular Dynamics and Markov State Model analyses, we scrutinize the kinetic processes involved in the formation of TiAl$_3$. The Molecular Dynamics simulation indicates that at the early stage of heat treatment, the predominating process is the diffusion of Al atoms towards the Ti surface through the TiAl$_3$ grain boundaries. The Markov State Modeling identifies thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.13695</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#30456;&#20301;&#23637;&#24320;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24515;&#33039;&#34880;&#27969;&#30340;&#23454;&#26102;&#20449;&#24687;&#12290;&#22312;&#24038;&#24515;&#23460;&#38271;&#36724;&#35270;&#22270;&#20013;&#65292;&#24425;&#33394;&#22810;&#26222;&#21202;&#23481;&#26131;&#20986;&#29616;&#30456;&#20301;&#21253;&#35065;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#33039;&#25910;&#32553;&#21644;&#33298;&#24352;&#26399;&#12290;&#24403;&#22522;&#20110;&#24425;&#33394;&#22810;&#26222;&#21202;&#30340;&#23450;&#37327;&#26041;&#27861;&#26102;&#65292;&#24517;&#39035;&#32416;&#27491;&#36825;&#31181;&#21253;&#35065;&#20266;&#24433;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#21253;(&#21435;&#20266;&#24433;)&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65292;&#23558;&#20854;&#26377;&#25928;&#24615;&#19982;&#22522;&#20110;nnU-Net&#21644;Transformer&#27169;&#22411;&#30340;&#20004;&#31181;&#26368;&#26032;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#33258;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#27599;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;nnU-Net&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#21435;&#20266;&#24433;&#32467;&#26524;&#65292;&#20854;&#27425;&#26159;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21644;&#22522;&#20110;Transformer&#30340;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#25317;&#26377;&#26174;&#33879;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#24615;&#33021;&#20173;&#33021;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respec
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12680</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#26032;&#21457;&#23637;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25216;&#26415;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#39046;&#22495;&#20869;&#29616;&#29366;&#30340;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;&#35813;&#30740;&#31350;&#39318;&#20808;&#20840;&#38754;&#24635;&#32467;&#20102;&#20027;&#35201;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#21644;&#32676;&#32452;&#25512;&#33616;&#31995;&#32479;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#33616;&#31995;&#32479;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#24635;&#32467;&#20102;&#35780;&#20272;&#24230;&#37327;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#30340;&#35265;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12070</link><description>&lt;p&gt;
&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#24615;&#30340;&#20219;&#21153;&#40065;&#26834;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#40065;&#26834;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#24182;&#24212;&#29992;&#26497;&#23567;&#26497;&#22823;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#20851;&#24515;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#21512;&#29702;&#30340;&#26465;&#20214;&#21464;&#21270;&#19979;&#30340;&#34892;&#20026;&#12290;&#24403;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#26102;&#65292;&#21516;&#26679;&#30340;&#21746;&#23398;&#20063;&#36866;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#20250;&#22312;&#19968;&#31995;&#21015;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#21248;&#22320;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#32771;&#34385;&#39044;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#20445;&#35777;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#22343;&#21248;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31216;&#27492;&#30446;&#26631;&#20026;&#19979;&#28216;&#20219;&#21153;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#19978;&#28216;&#20219;&#21153;&#20998;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#30340;minimax loss &#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as $\textit{downstream-task robustness}$. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minim
&lt;/p&gt;</description></item><item><title>Segment Anything Model (SAM) &#22312;&#20020;&#24202;&#25918;&#23556;&#27835;&#30103;&#20013;&#30340;&#24615;&#33021;&#24471;&#21040;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#21160;&#20998;&#21106;&#20013;&#30340;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#38024;&#23545;&#19981;&#21516;&#37096;&#20301;&#30340;&#19981;&#21516;&#22120;&#23448;&#30340;&#31934;&#30830;&#25551;&#32472;&#12290;</title><link>http://arxiv.org/abs/2306.11730</link><description>&lt;p&gt;
&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#27835;&#30103;&#30340;Segment Anything Model (SAM)
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) for Radiation Oncology. (arXiv:2306.11730v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11730
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) &#22312;&#20020;&#24202;&#25918;&#23556;&#27835;&#30103;&#20013;&#30340;&#24615;&#33021;&#24471;&#21040;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#21160;&#20998;&#21106;&#20013;&#30340;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#38024;&#23545;&#19981;&#21516;&#37096;&#20301;&#30340;&#19981;&#21516;&#22120;&#23448;&#30340;&#31934;&#30830;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Segment Anything Model (SAM) &#22312;&#20020;&#24202;&#25918;&#23556;&#27835;&#30103;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#30340;&#8220;segment anything&#8221;&#27169;&#24335;&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#39118;&#38505;&#22120;&#23448;&#65288;OARs&#65289;&#19978;&#23454;&#29616;&#20020;&#24202;&#21487;&#25509;&#21463;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;Dice&#20998;&#25968;&#39640;&#20110;0.7&#12290;SAM&#30340;&#8220;box prompt&#8221;&#27169;&#24335;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;Dice&#20998;&#25968;0.1&#33267;0.5&#20010;&#21333;&#20301;&#12290;&#32771;&#34385;&#21040;&#22120;&#23448;&#30340;&#22823;&#23567;&#21644;&#36793;&#30028;&#30340;&#28165;&#26224;&#24230;&#65292;SAM&#22312;&#22823;&#23567;&#36866;&#20013;&#19988;&#36793;&#30028;&#28165;&#26224;&#30340;&#22120;&#23448;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#36793;&#30028;&#19981;&#28165;&#26224;&#30340;&#23567;&#22120;&#23448;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#30001;&#20110;SAM&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#20020;&#24202;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#22788;&#29702;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;OARs&#30340;&#25551;&#32472;&#65292;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;SAM&#22312;&#25918;&#23556;&#27835;&#30103;&#30340;&#33258;&#21160;&#20998;&#21106;&#20013;&#20855;&#26377;&#19968;&#33268;&#20934;&#30830;&#24615;&#30340;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;SAM&#21487;&#20197;&#36890;&#36807;&#36890;&#29992;&#30340;&#33258;&#21160;&#20998;&#21106;&#27169;&#22411;&#23454;&#29616;&#23545;&#19981;&#21516;&#37096;&#20301;&#30340;&#19981;&#21516;OARs&#30340;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we evaluate the performance of the Segment Anything Model (SAM) in clinical radiotherapy. Our results indicate that SAM's 'segment anything' mode can achieve clinically acceptable segmentation results in most organs-at-risk (OARs) with Dice scores higher than 0.7. SAM's 'box prompt' mode further improves the Dice scores by 0.1 to 0.5. Considering the size of the organ and the clarity of its boundary, SAM displays better performance for large organs with clear boundaries but performs worse for smaller organs with unclear boundaries. Given that SAM, a model pre-trained purely on natural images, can handle the delineation of OARs from medical images with clinically acceptable accuracy, these results highlight SAM's robust generalization capabilities with consistent accuracy in automatic segmentation for radiotherapy. In other words, SAM can achieve delineation of different OARs at different sites using a generic automatic segmentation model. SAM's generalization capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#30740;&#31350;&#20102;&#24120;&#27493;&#38271;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#25910;&#25947;&#20110;&#19968;&#20010;&#19981;&#21464;&#20998;&#24067;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.11497</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#24120;&#27493;&#38271;SGD&#30340;&#25910;&#25947;&#21644;&#38598;&#20013;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Convergence and concentration properties of constant step-size SGD through Markov chains. (arXiv:2306.11497v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#30740;&#31350;&#20102;&#24120;&#27493;&#38271;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#36845;&#20195;&#25910;&#25947;&#20110;&#19968;&#20010;&#19981;&#21464;&#20998;&#24067;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#24120;&#27493;&#38271;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#24179;&#28369;&#19988;&#24378;&#20984;&#30340;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#30740;&#31350;&#20854;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#36731;&#24494;&#21463;&#25511;&#26041;&#24046;&#30340;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#65292;&#36845;&#20195;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#25910;&#25947;&#20110;&#19968;&#20010;&#19981;&#21464;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#22312;&#19982;&#20197;&#21069;&#24037;&#20316;&#30456;&#27604;&#26799;&#24230;&#22122;&#22768;&#20998;&#24067;&#30340;&#25918;&#23485;&#20551;&#35774;&#19979;&#65292;&#22312;Wasserstein-2&#36317;&#31163;&#19979;&#24314;&#31435;&#20102;&#36825;&#31181;&#25910;&#25947;&#24615;&#12290;&#30001;&#20110;&#26497;&#38480;&#20998;&#24067;&#30340;&#19981;&#21464;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#36825;&#20123;&#23545;&#20110;&#26799;&#24230;&#25104;&#31435;&#26102;&#65292;&#21518;&#32773;&#32487;&#25215;&#20102;&#20122;&#39640;&#26031;&#25110;&#20122;&#25351;&#25968;&#27987;&#24230;&#29305;&#24615;&#12290;&#36825;&#20801;&#35768;&#25512;&#23548;&#20986;&#23545;&#20110;&#26368;&#32456;&#20272;&#35745;&#30340;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#65292;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;Polyak-Ruppert&#24207;&#21015;&#30340;&#23614;&#37096;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#26080;&#32500;&#24230;&#20559;&#24046;&#38480;&#21046;&#12290;&#25152;&#26377;&#32467;&#26524;&#22343;&#20026;&#38750;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization of a smooth and strongly convex objective using constant step-size stochastic gradient descent (SGD) and study its properties through the prism of Markov chains. We show that, for unbiased gradient estimates with mildly controlled variance, the iteration converges to an invariant distribution in total variation distance. We also establish this convergence in Wasserstein-2 distance under a relaxed assumption on the gradient noise distribution compared to previous work. Thanks to the invariance property of the limit distribution, our analysis shows that the latter inherits sub-Gaussian or sub-exponential concentration properties when these hold true for the gradient. This allows the derivation of high-confidence bounds for the final estimate. Finally, under such conditions in the linear case, we obtain a dimension-free deviation bound for the Polyak-Ruppert average of a tail sequence. All our results are non-asymptotic and their consequences are discussed thr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrAMMI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#65292;GrAMMI&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11168</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#23398;&#20064;&#23545;&#25239;&#20195;&#29702;&#34892;&#20026;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Models of Adversarial Agent Behavior under Partial Observability. (arXiv:2306.11168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrAMMI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#65292;GrAMMI&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27604;&#22914;&#32844;&#19994;&#20307;&#32946;&#12289;&#35270;&#39057;&#28216;&#25103;&#35774;&#35745;&#21644;&#27602;&#21697;&#25130;&#33719;&#20013;&#65292;&#23545;&#25163;&#24314;&#27169;&#21644;&#36319;&#36394;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#23545;&#25239;&#24314;&#27169;&#26041;&#27861;&#65292;&#21517;&#20026;GrAMMI&#65292;&#29992;&#20110;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;GrAMMI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#12290;&#20026;&#20102;&#35780;&#20272;GrAMMI&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#24322;&#26500;&#20195;&#29702;&#22242;&#38431;&#30340;&#20219;&#21153;&#26159;&#36861;&#36394;&#21644;&#25130;&#33719;&#19968;&#20010;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#65292;&#32780;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#24517;&#39035;&#22312;&#21516;&#26102;&#36798;&#21040;&#33258;&#24049;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#36867;&#36991;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#20114;&#20449;&#24687;&#30340;&#24418;&#24335;&#21270;&#65292;GrAMMI&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#20102;&#26410;&#26469;&#23545;&#25239;&#30340;&#24179;&#22343;&#23545;&#25968;&#20284;&#28982;&#20540;&#25552;&#39640;&#20102;31.68%&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for opponent modeling and tracking arises in several real-world scenarios, such as professional sports, video game design, and drug-trafficking interdiction. In this work, we present Graph based Adversarial Modeling with Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent agent. GrAMMI is a novel graph neural network (GNN) based approach that uses mutual information maximization as an auxiliary objective to predict the current and future states of an adversarial opponent with partial observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion domains inspired by real-world scenarios, where a team of heterogeneous agents is tasked with tracking and interdicting a single adversarial agent, and the adversarial agent must evade detection while achieving its own objectives. With the mutual information formulation, GrAMMI outperforms all baselines in both domains and achieves 31.68% higher log-likelihood on average for future adversarial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>DRCFS&#26159;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07024</link><description>&lt;p&gt;
DRCFS&#65306;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DRCFS: Doubly Robust Causal Feature Selection. (arXiv:2306.07024v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07024
&lt;/p&gt;
&lt;p&gt;
DRCFS&#26159;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#23545;&#20110;&#29305;&#23450;&#30446;&#26631;&#21464;&#37327;&#38750;&#24120;&#30456;&#20851;&#30340;&#22797;&#26434;&#31995;&#32479;&#29305;&#24449;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#32447;&#24615;&#35774;&#32622;&#65292;&#26377;&#26102;&#32570;&#20047;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26080;&#27861;&#25193;&#23637;&#21040;&#38656;&#35201;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRCFS&#65292;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#35774;&#32622;&#20013;&#35782;&#21035;&#22240;&#26524;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#38416;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;DRCFS&#22312;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#29305;&#24449;&#30340;&#25216;&#24039;&#38750;&#24120;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the features of a complex system that are highly relevant to a particular target variable is of fundamental interest in many areas of science. Existing approaches are often limited to linear settings, sometimes lack guarantees, and in most cases, do not scale to the problem at hand, in particular to images. We propose DRCFS, a doubly robust feature selection method for identifying the causal features even in nonlinear and high dimensional settings. We provide theoretical guarantees, illustrate necessary conditions for our assumptions, and perform extensive experiments across a wide range of simulated and semi-synthetic datasets. DRCFS significantly outperforms existing state-of-the-art methods, selecting robust features even in challenging highly non-linear and high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02210</link><description>&lt;p&gt;
GPT-FL: &#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02210
&lt;/p&gt;
&lt;p&gt;
GPT-FL&#26159;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#32467;&#21512;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#31561;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;FL&#35757;&#32451;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-FL&#65292;&#19968;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#36741;&#21161;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#12290;GPT-FL&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#29983;&#25104;&#30340;&#25968;&#25454;&#29992;&#20110;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#19979;&#28216;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#26631;&#20934;FL&#26694;&#26550;&#19979;&#20351;&#29992;&#31169;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-FL&#22312;&#27169;&#22411;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23458;&#25143;&#31471;&#37319;&#26679;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;FL&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#19979;&#28216;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#26799;&#24230;&#22810;&#26679;&#24615;&#30340;&#26041;&#21521;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;GPT-FL&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#25552;&#21319;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#30446;&#26631;&#25968;&#25454;&#26159;&#21542;&#22312;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#25110;&#22806;&#65292;GPT-FL&#22987;&#32456;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.20030</link><description>&lt;p&gt;
&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#65306;&#19968;&#31181;&#19981;&#21487;&#35265;&#19988;&#20581;&#22766;&#30340;&#25193;&#25955;&#22270;&#20687;&#25351;&#32441;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#23383;&#27700;&#21360;&#26159;&#36861;&#36394;&#29256;&#26435;&#21644;&#38450;&#27490;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#23041;&#32961;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;&#25968;&#23383;&#27700;&#21360;&#23558;&#22270;&#20687;&#29983;&#25104;&#25152;&#20351;&#29992;&#30340;&#21021;&#22987;&#22122;&#22768;&#21521;&#37327;&#20013;&#23884;&#20837;&#19968;&#20010;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#20013;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#21367;&#31215;&#12289;&#35009;&#21098;&#12289;&#33192;&#32960;&#12289;&#32763;&#36716;&#21644;&#26059;&#36716;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22270;&#20687;&#29983;&#25104;&#21518;&#65292;&#36890;&#36807;&#21453;&#28436;&#25193;&#25955;&#36807;&#31243;&#26469;&#26816;&#27979;&#27700;&#21360;&#20449;&#21495;&#65292;&#20197;&#26816;&#32034;&#23884;&#20837;&#20449;&#21495;&#30340;&#22122;&#22768;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#24847;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diff
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19525</link><description>&lt;p&gt;
&#21457;&#29616;&#26032;&#30340;&#21487;&#35299;&#37322;&#20445;&#23432;&#24459;&#20316;&#20026;&#31232;&#30095;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19525
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse Invariant Detector&#65288;SID&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20445;&#23432;&#24459;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#30340;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#65292;&#24182;&#19988;&#24050;&#21457;&#29616;&#30340;&#20445;&#23432;&#24459;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32473;&#23450;&#21160;&#21147;&#31995;&#32479;&#30340;&#20445;&#23432;&#24459;&#26159;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#35774;&#32622;&#65288;&#24050;&#30693;&#24494;&#20998;&#26041;&#31243;&#21644;&#22522;&#20989;&#25968;&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sparse Invariant Detector&#65288;SID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#33258;&#21160;&#21457;&#29616;&#20445;&#23432;&#24459;&#30340;&#31639;&#27861;&#12290;&#20854;&#31639;&#27861;&#31616;&#21333;&#24615;&#30830;&#20445;&#20102;&#24050;&#21457;&#29616;&#20445;&#23432;&#25968;&#37327;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SID&#33021;&#22815;&#22312;&#21508;&#31181;&#31995;&#32479;&#20013;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#20445;&#23432;&#24459;&#65292;&#29978;&#33267;&#21457;&#29616;&#26032;&#30340;&#20445;&#23432;&#24459;&#12290;&#22312;&#27969;&#20307;&#21147;&#23398;&#21644;&#22823;&#27668;&#21270;&#23398;&#30340;&#20004;&#20010;&#20363;&#23376;&#20013;&#65292;SID&#20998;&#21035;&#21457;&#29616;&#20102;14&#20010;&#21644;3&#20010;&#23432;&#24658;&#37327;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#19987;&#23478;&#20808;&#21069;&#21482;&#30693;&#36947;12&#20010;&#21644;2&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#27700;&#21160;&#21147;&#27169;&#22411;&#65292;&#25104;&#21151;&#21152;&#36895;&#24182;&#25552;&#21319;&#20102;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12052</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27946;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;DL Hydro-FRAN
&lt;/p&gt;
&lt;p&gt;
Deep Learning Hydrodynamic Forecasting for Flooded Region Assessment in Near-Real-Time (DL Hydro-FRAN). (arXiv:2305.12052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#27700;&#21160;&#21147;&#27169;&#22411;&#65292;&#25104;&#21151;&#21152;&#36895;&#24182;&#25552;&#21319;&#20102;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21160;&#21147;&#27946;&#27700;&#24314;&#27169;&#25552;&#39640;&#20102;&#26292;&#38632;&#20107;&#20214;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#20998;&#36776;&#29575;&#27700;&#21160;&#21147;&#23398;&#25152;&#38656;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#25968;&#20540;&#35299;&#36890;&#24120;&#20250;&#38459;&#30861;&#22312;&#36817;&#23454;&#26102;&#27946;&#27700;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#29992;&#20110;&#20248;&#21270;&#27946;&#27700;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;2D HEC-RAS&#27700;&#21160;&#21147;&#27169;&#22411;&#22312;&#20302;&#28023;&#25300;&#12289;&#39640;&#20998;&#36776;&#29575;&#22478;&#24066;&#29615;&#22659;&#20013;&#27169;&#25311;&#20102;&#20960;&#27425;&#26292;&#38632;&#20107;&#20214;&#12290;&#23558;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#29992;&#20110;DNN&#35757;&#32451;&#38598;&#65292;&#39044;&#27979;&#27946;&#27700;&#28145;&#24230;&#21644;&#36895;&#24230;&#12290;&#19982;&#27700;&#21160;&#21147;&#27946;&#27700;&#27169;&#22411;&#30456;&#27604;&#65292;DNN&#27169;&#22411;&#30340;&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#22312;&#30740;&#31350;&#21306;&#22495;&#20869;&#32454;&#32990;&#27946;&#27700;&#28145;&#24230;&#20013;&#65292;&#20013;&#20301;&#25968;RMSE&#32422;&#20026;2&#27627;&#31859;&#12290;&#21516;&#26102;&#65292;DNN&#27169;&#22411;&#39044;&#27979;&#30340;&#35745;&#31639;&#26102;&#38388;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#39044;&#27979;&#26102;&#38388;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;34.2&#21040;72.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hydrodynamic flood modeling improves hydrologic and hydraulic prediction of storm events. However, the computationally intensive numerical solutions required for high-resolution hydrodynamics have historically prevented their implementation in near-real-time flood forecasting. This study examines whether several Deep Neural Network (DNN) architectures are suitable for optimizing hydrodynamic flood models. Several pluvial flooding events were simulated in a low-relief high-resolution urban environment using a 2D HEC-RAS hydrodynamic model. These simulations were assembled into a training set for the DNNs, which were then used to forecast flooding depths and velocities. The DNNs' forecasts were compared to the hydrodynamic flood models, and showed good agreement, with a median RMSE of around 2 mm for cell flooding depths in the study area. The DNNs also improved forecast computation time significantly, with the DNNs providing forecasts between 34.2 and 72.4 times faster than conventional
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22270;&#24418;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(GESN)&#35299;&#20915;&#24322;&#36136;&#24615;&#22270;&#19978;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;GESN&#26159;&#19968;&#31181;&#20648;&#22791;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#33410;&#28857;&#23884;&#20837;&#26469;&#22788;&#29702;&#22312;&#24322;&#36136;&#24615;&#22270;&#19978;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08233</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#35299;&#20915;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Heterophily in Node Classification with Graph Echo State Networks. (arXiv:2305.08233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22270;&#24418;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(GESN)&#35299;&#20915;&#24322;&#36136;&#24615;&#22270;&#19978;&#33410;&#28857;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;GESN&#26159;&#19968;&#31181;&#20648;&#22791;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#33410;&#28857;&#23884;&#20837;&#26469;&#22788;&#29702;&#22312;&#24322;&#36136;&#24615;&#22270;&#19978;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23436;&#20840;&#35757;&#32451;&#30340;&#28145;&#24230;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#22810;&#27425;&#32858;&#21512;&#33410;&#28857;&#38468;&#36817;&#30340;&#37051;&#23621;&#26469;&#23454;&#29616;&#12290;&#22312;&#23637;&#31034;&#39640;&#27604;&#20363;&#20869;&#31867;&#36793;&#32536;&#30340;&#22270;&#24418;&#19978;&#26377;&#25928;&#65292;&#20294;&#22312;&#24322;&#36136;&#24615;&#30456;&#21453;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#23646;&#20110;&#21516;&#19968;&#31867;&#30340;&#33410;&#28857;&#36890;&#24120;&#30456;&#38548;&#36739;&#36828;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#20855;&#26377;&#39640;&#24230;&#24322;&#36136;&#24615;&#30340;&#22270;&#24418;&#20013;&#65292;&#21367;&#31215;&#27169;&#22411;&#35745;&#31639;&#30340;&#22522;&#20110;&#32039;&#23494;&#37051;&#23621;&#30340;&#24179;&#28369;&#34920;&#31034;&#19981;&#20877;&#26377;&#25928;&#12290;&#30446;&#21069;&#65292;&#22312;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#20307;&#31995;&#32467;&#26500;&#21464;&#21270;&#20197;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#25110;&#37325;&#26032;&#24067;&#32447;&#36755;&#20837;&#22270;&#20197;&#25913;&#21892;&#38271;&#31243;&#28040;&#24687;&#20256;&#36882;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#24418;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;(GESN)&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#22270;&#24418;&#30340;&#25361;&#25112;&#20197;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#12290;GESN&#26159;&#19968;&#31181;&#22270;&#24418;&#30340;&#20648;&#22791;&#35745;&#31639;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#33410;&#28857;&#23884;&#20837;&#26159;&#36890;&#36807;&#26410;&#32463;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#20989;&#25968;&#36882;&#24402;&#35745;&#31639;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification tasks on graphs are addressed via fully-trained deep message-passing models that learn a hierarchy of node representations via multiple aggregations of a node's neighbourhood. While effective on graphs that exhibit a high ratio of intra-class edges, this approach poses challenges in the opposite case, i.e. heterophily, where nodes belonging to the same class are usually further apart. In graphs with a high degree of heterophily, the smoothed representations based on close neighbours computed by convolutional models are no longer effective. So far, architectural variations in message-passing models to reduce excessive smoothing or rewiring the input graph to improve longer-range message passing have been proposed. In this paper, we address the challenges of heterophilic graphs with Graph Echo State Network (GESN) for node classification. GESN is a reservoir computing model for graphs, where node embeddings are recursively computed by an untrained message-passing func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02657</link><description>&lt;p&gt;
&#28145;&#24230;&#23485;&#26494;&#24347;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;$\mathcal X \subset \mathbb R^{d}$&#19978;&#30340;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#34987;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22238;&#24402;&#25152;&#23436;&#20840;&#25551;&#32472;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#35889;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22312;$\mathcal{X}$&#19978;&#20026;&#27491;&#23450;&#65292;&#20854;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#20026;$(d+1)/d$&#12290;&#30001;&#20110;&#26680;&#22238;&#24402;&#20013;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02562</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional and Residual Methods in Scalable Coding for Humans and Machines. (arXiv:2305.02562v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02562
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#32534;&#30721;&#20013;&#30340;&#26465;&#20214;&#32534;&#30721;&#21644;&#27531;&#24046;&#32534;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#26469;&#20248;&#21270;&#37325;&#26500;&#20219;&#21153;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20449;&#24687;&#20998;&#26512;&#65292;&#25552;&#20379;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#26465;&#20214;&#32534;&#30721;&#65292;&#20855;&#22791;&#22686;&#21152;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#31867;&#20284;&#30340;&#21487;&#25805;&#20316;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#65292;&#20854;&#20013;&#19968;&#31181;&#23454;&#39564;&#20351;&#29992;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#30340;&#35821;&#20041;&#20998;&#21106;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#30340;&#30446;&#26631;&#26816;&#27979;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26465;&#20214;&#21644;&#27531;&#24046;&#26041;&#27861;&#20043;&#38388;&#30340;&#31867;&#20284;&#24615;&#33021;&#65292;&#24471;&#21040;&#30340;&#36895;&#29575;&#22833;&#30495;&#26354;&#32447;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#22522;&#32447;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present methods for conditional and residual coding in the context of scalable coding for humans and machines. Our focus is on optimizing the rate-distortion performance of the reconstruction task using the information available in the computer vision task. We include an information analysis of both approaches to provide baselines and also propose an entropy model suitable for conditional coding with increased modelling capacity and similar tractability as previous work. We apply these methods to image reconstruction, using, in one instance, representations created for semantic segmentation on the Cityscapes dataset, and in another instance, representations created for object detection on the COCO dataset. In both experiments, we obtain similar performance between the conditional and residual methods, with the resulting rate-distortion curves contained within our baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.02506</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#35299;&#23494;&#24230;&#30340;&#23383;&#31526;&#20018;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#23450;&#20041;&#22312;&#38543;&#26426;&#21464;&#37327;&#38598;&#19978;&#30340;&#32852;&#21512;&#23494;&#24230;&#30340;&#33539;&#30068;&#21450;&#20854;&#24847;&#20041;&#65292;&#20197;&#24110;&#21161;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;&#25193;&#23637;&#23450;&#21521;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#24517;&#35201;&#24615;&#12290;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#27169;&#22411;&#37117;&#23450;&#20041;&#20102;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#31232;&#30095;&#32467;&#26500;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#26377;&#20851;&#27010;&#29575;&#26144;&#23556;&#30340;&#39532;&#23572;&#21487;&#22827;&#33539;&#30068;&#30340;&#24037;&#20316;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#33539;&#30068;&#65292;&#20854;&#24577;&#23556;&#23558;&#20998;&#21035;&#30001;&#27599;&#20010;&#26679;&#26412;&#31354;&#38388;&#20998;&#35299;&#30340;&#32852;&#21512;&#23494;&#24230;&#19982;&#20174;&#26679;&#26412;&#21040;&#36820;&#22238;&#20540;&#30340;&#30830;&#23450;&#24615;&#26144;&#23556;&#32452;&#21512;&#12290;&#36825;&#26159;&#36808;&#21521;&#26368;&#36817;&#30340;&#33539;&#30068;&#35770;&#27010;&#29575;&#27979;&#24230;&#25551;&#36848;&#21644;&#36890;&#24120;&#22312;&#27010;&#29575;&#32534;&#31243;&#21644;&#22240;&#26524;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#20998;&#35299;&#23494;&#24230;&#30340;&#25805;&#20316;&#23450;&#20041;&#20043;&#38388;&#30340;&#32553;&#23567;&#24046;&#36317;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14997</link><description>&lt;p&gt;
&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20498;&#25512;&#20102;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#34892;&#20026;&#12290;&#36825;&#20123;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#30740;&#31350;&#32773;&#30340;&#30452;&#35273;&#65292;&#36825;&#20351;&#24471;&#24212;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#20102;&#35299;&#24403;&#21069;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#22797;&#26434;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#26680;&#24515;&#24037;&#20316;&#27969;&#31243;&#38750;&#24120;&#30456;&#20284;&#12290;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#65292;&#35825;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#36866;&#24403;&#30340;&#25277;&#35937;&#21333;&#20803;&#65292;&#26367;&#25442;&#36825;&#20123;&#21333;&#20803;&#30340;&#28608;&#27963;&#20197;&#30830;&#23450;&#21738;&#20123;&#21442;&#19982;&#20102;&#34892;&#20026;&#65292;&#28982;&#21518;&#35299;&#37322;&#36825;&#20123;&#21333;&#20803;&#23454;&#26045;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#21644;&#24453;&#30740;&#31350;&#30340;&#21333;&#20803;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#29702;&#35299;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#21306;&#22495;&#30340;&#21151;&#33021;&#21644;&#23427;&#20204;&#32452;&#25104;&#30340;&#30005;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#65288;ACDC&#65289;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.09872</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#23545;&#20559;&#24207;&#38598;&#21512;&#36827;&#34892;&#25551;&#36848;&#24615;&#20998;&#26512;&#12290;&#23613;&#31649;&#28145;&#24230;&#20989;&#25968;&#22312;&#32447;&#24615;&#21644;&#24230;&#37327;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20559;&#24207;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33879;&#21517;&#30340;&#21333;&#32431;&#28145;&#24230;&#30340;&#20559;&#24207;&#29256;&#26412;-&#26080;&#24182;&#36890;&#29992;&#28145;&#24230;&#65288;ufg depth&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340; ufg depth &#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#19981;&#21516;&#20998;&#31867;&#22120;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28608;&#28872;&#36777;&#35770;&#22686;&#21152;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;&#25968;&#25454;&#20013;&#30340;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#24182;&#32500;&#25252;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#65292;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;&#20102;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.06808</link><description>&lt;p&gt;
&#27969;&#24335;&#25968;&#25454;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;&#25968;&#25454;&#20013;&#30340;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#24182;&#32500;&#25252;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#65292;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;&#20102;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20027;&#21160;&#26631;&#35760;&#27969;&#25968;&#25454;&#38382;&#39064;&#65292;&#20854;&#20013;&#20027;&#21160;&#23398;&#20064;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#25968;&#25454;&#28857;&#65292;&#24182;&#24517;&#39035;&#36890;&#36807;&#26114;&#36149;&#30340;&#23454;&#39564;&#31934;&#24515;&#36873;&#25321;&#21738;&#20123;&#28857;&#36827;&#34892;&#26631;&#35760;&#65292;&#27492;&#31867;&#38382;&#39064;&#24120;&#24120;&#20986;&#29616;&#22312;&#21307;&#30103;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#30340;&#26159;&#25968;&#25454;&#36755;&#20837;&#23646;&#20110;$K$&#20010;&#31163;&#25955;&#20998;&#24067;&#20043;&#19968;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#21270;&#25551;&#36848;&#27492;&#38382;&#39064;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#20102;&#26631;&#35760;&#25104;&#26412;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#24403;&#26631;&#35760;&#25104;&#26412;&#20026;$B$&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#65292;&#20165;&#22312;&#19981;&#30830;&#23450;&#24615;&#22823;&#20110;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#26102;&#36827;&#34892;&#65292;&#21487;&#20197;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#19978;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#21040;&#36798;&#27169;&#24335;&#26356;&#26377;&#21033;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21040;&#36798;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34917;&#20805;&#20102;&#20004;&#20010;&#19978;&#30028;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27969;&#25968;&#25454;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#26631;&#35760;&#27969;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19982;&#21069;&#38754;&#24773;&#20917;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
&lt;/p&gt;</description></item><item><title>DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00133</link><description>&lt;p&gt;
DeforestVis&#65306;&#20351;&#29992;&#20195;&#29702;&#20915;&#31574;&#26641;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00133
&lt;/p&gt;
&lt;p&gt;
DeforestVis&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20195;&#29702;&#20915;&#31574;&#26641;&#65292;&#24635;&#32467;&#20102;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#65288;&#21644;&#20851;&#38190;&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#26131;&#35299;&#37322;&#21644;&#21487;&#20449;&#36182;&#30340;ML&#12290;&#35299;&#37322;&#22797;&#26434;ML&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65288;&#20363;&#22914;&#35268;&#21017;&#38598;&#21644;&#20915;&#31574;&#26641;&#65289;&#65292;&#20197;&#36275;&#22815;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#65292;&#20294;&#26356;&#31616;&#21333;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#35268;&#21017;&#38598;&#21487;&#20197;&#21464;&#24471;&#38750;&#24120;&#20887;&#38271;&#65292;&#21253;&#21547;&#35768;&#22810;if-else&#35821;&#21477;&#65292;&#32780;&#20915;&#31574;&#26641;&#30340;&#28145;&#24230;&#20250;&#38543;&#30528;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;ML&#27169;&#22411;&#32780;&#36805;&#36895;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#20854;&#26680;&#24515;&#30446;&#26631;&#65292;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DeforestVis&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#65288;AdaBoost&#65289;&#25216;&#26415;&#29983;&#25104;&#30340;&#20195;&#29702;&#20915;&#31574;&#26641;&#65288;&#19968;&#32423;&#20915;&#31574;&#26641;&#65289;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#22797;&#26434;ML&#27169;&#22411;&#34892;&#20026;&#30340;&#21451;&#22909;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.11789</link><description>&lt;p&gt;
&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65306;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#23454;&#26102;&#30340;&#22270;&#19978;&#35266;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25910;&#25947;&#24615;&#36716;&#21270;&#20026;&#24102;&#26377;L2&#26377;&#30028;&#38789;&#24046;&#20998;&#39033;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#38543;&#26426;&#26102;&#21464;&#24046;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#23637;&#20102;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#22270;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#65292;&#21017;&#25152;&#26377;&#33410;&#28857;&#30340;&#20272;&#35745;&#22343;&#20026;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#30340;&#12290;&#36890;&#36807;&#23558;RKHS&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;RKHS&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#23558;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.08325</link><description>&lt;p&gt;
FairAdaBN&#65306;&#33258;&#36866;&#24212;&#25209;&#24402;&#19968;&#21270;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#21450;&#20854;&#22312;&#30382;&#32932;&#30149;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#23558;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27491;&#22312;&#21307;&#23398;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#21516;&#26102;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#65292;&#29978;&#33267;&#21253;&#25324;&#20851;&#38190;&#30340;&#35786;&#26029;&#20915;&#31574;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#19981;&#21516;&#20154;&#21475;&#23646;&#24615;&#23376;&#32452;&#20043;&#38388;&#30340;&#26174;&#33879;&#24615;&#33021;&#24046;&#24322;&#65292;&#31216;&#20026;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#33268;&#21147;&#20110;&#31934;&#24515;&#35774;&#35745;&#20248;&#38597;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#35299;&#20915;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#35757;&#32451;&#36127;&#25285;&#12289;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairAdaBN&#65292;&#36890;&#36807;&#20351;&#25209;&#24402;&#19968;&#21270;&#36866;&#24212;&#25935;&#24863;&#23646;&#24615;&#65292;&#21487;&#20197;&#23558;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;&#24212;&#29992;&#21040;&#21407;&#26412;&#19981;&#20102;&#35299;&#20844;&#24179;&#24615;&#30340;&#22810;&#20010;&#20998;&#31867;&#20027;&#24178;&#20013;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#23567;&#25209;&#37327;&#23376;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#34913;&#65292;&#40723;&#21169;&#27169;&#22411;&#20197;&#30456;&#24403;&#20844;&#24179;&#30340;&#26041;&#24335;&#25910;&#25947;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;HAM10000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#33719;&#21462;&#30382;&#32932;&#30149;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#20998;&#31867;&#19971;&#31181;&#24120;&#35265;&#30340;&#30382;&#32932;&#30149;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FairAdaBN&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24320;&#38144;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called model unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attribute. This simple but effective design can be adopted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270;&#65288;CDPs&#65289;&#26469;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;CDPs&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#21487;&#20197;&#27169;&#22359;&#21270;&#22320;&#32467;&#21512;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20123;&#22270;&#34920;&#21487;&#20197;&#25104;&#20026;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24182;&#23545;&#30456;&#20851;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.04209</link><description>&lt;p&gt;
&#22240;&#26524;&#20381;&#36182;&#22270;
&lt;/p&gt;
&lt;p&gt;
Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270;&#65288;CDPs&#65289;&#26469;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;CDPs&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#21487;&#20197;&#27169;&#22359;&#21270;&#22320;&#32467;&#21512;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20123;&#22270;&#34920;&#21487;&#20197;&#25104;&#20026;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24182;&#23545;&#30456;&#20851;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#22823;&#12290;&#20026;&#20102;&#26126;&#26234;&#22320;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#19982;&#19990;&#30028;&#20114;&#21160;&#65292;&#21253;&#25324;&#23427;&#20204;&#22312;&#25968;&#25454;&#36755;&#20837;&#19978;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270; (CDPs)&#65292;&#29992;&#20110;&#21487;&#35270;&#21270;&#19968;&#20010;&#21464;&#37327;&#65288;&#32467;&#26524;&#65289;&#22914;&#20309;&#38543;&#21478;&#19968;&#20010;&#21464;&#37327;&#65288;&#39044;&#27979;&#22120;&#65289;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#20197;&#21450;&#20854;&#20182;&#39044;&#27979;&#22120;&#21464;&#37327;&#30340;&#22240;&#26524;&#21464;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;CDPs&#19982;&#22522;&#20110;&#20445;&#25345;&#20854;&#20182;&#39044;&#27979;&#22120;&#24658;&#23450;&#25110;&#20551;&#35774;&#23427;&#20204;&#29420;&#31435;&#30340;&#26631;&#20934;&#26041;&#27861;&#19981;&#21516;&#12290;CDPs&#21033;&#29992;&#36741;&#21161;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#22240;&#26524;&#32467;&#35770;&#38656;&#35201;&#22240;&#26524;&#20551;&#35774;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CDPs&#21487;&#20197;&#19982;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#32467;&#21512;&#20351;&#29992;&#12290;&#30001;&#20110;&#20154;&#20204;&#32463;&#24120;&#22312;&#36755;&#20837;-&#36755;&#20986;&#20381;&#36182;&#24615;&#26041;&#38754;&#36827;&#34892;&#22240;&#26524;&#24605;&#32771;&#65292;CDPs&#21487;&#20197;&#25104;&#20026;xAI&#25110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#24182;&#23545;&#24212;&#29992;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to appl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13849</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#24314;&#35758;&#21644;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#36827;&#34892;&#26368;&#20248;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#32463;&#20856;&#30340;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#30830;&#23450;&#24615;&#23398;&#20064;&#22120;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#21487;&#20197;&#36890;&#36807;Littlestone&#32500;&#24230;&#26469;&#23454;&#29616;&#65288;Littlestone '88&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#23398;&#20064;&#22120;&#30340;&#31867;&#20284;&#32467;&#26524;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035; $\mathcal{H}$&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#65292;&#21363;&#23384;&#22312;&#19968;&#20010;&#30001; $\mathcal{H}$ &#25171;&#30862;&#30340;&#26641;&#65292;&#20854;&#24179;&#22343;&#28145;&#24230;&#20026; $2d$&#65292;&#32780; $d$ &#26159;&#26368;&#22823;&#30340;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982; $\mathcal{H}$ &#20013;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968; $k$ &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;Littlestone&#32500;&#24230; $d$ &#30340;&#31867;&#21035;&#23398;&#20064;&#30340;&#26368;&#20248;&#38543;&#26426;&#21270;&#38169;&#35823;&#36793;&#30028;&#26159; $k + \Theta (\sqrt{k d} + d )$&#12290;&#36825;&#20063;&#24847;&#21619;&#30528;&#30830;&#23450;&#24615;&#23398;&#20064;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#26159; $2k + O (\sqrt{k d} + d )$&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Auer&#21644;Long ['99]&#30740;&#31350;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#38382;&#39064;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied by Auer and Long ['99].  As an application of our theory, we revisit the classical problem of prediction 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#12290;&#36825;&#23545;&#20110;&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2302.13259</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36991;&#20813;&#21452;&#19979;&#38477;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we avoid Double Descent in Deep Neural Networks?. (arXiv:2302.13259v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#12290;&#36825;&#23545;&#20110;&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#38750;&#24120;&#37325;&#35201;&#19988;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#33410;&#33021;&#26041;&#26696;&#20013;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#8220;&#21452;&#19979;&#38477;&#8221;&#65292;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#30028;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#39318;&#20808;&#21464;&#24046;&#65292;&#28982;&#21518;&#24674;&#22797;&#25552;&#21319;&#12290;&#36825;&#23545;&#20110;&#32500;&#25345;&#39640;&#27867;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#22823;&#23567;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#38382;&#39064;&#65306;&#27169;&#22411;&#38656;&#35201;&#36275;&#22815;&#30340;&#36229;&#21442;&#25968;&#21270;&#65292;&#20294;&#28155;&#21152;&#36807;&#22810;&#30340;&#21442;&#25968;&#20250;&#28010;&#36153;&#35757;&#32451;&#36164;&#28304;&#12290;&#26159;&#21542;&#21487;&#33021;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20339;&#25240;&#34935;&#26041;&#26696;&#65311;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#33021;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20294;&#26368;&#32456;&#31572;&#26696;&#20173;&#24453;&#30830;&#23450;&#12290;&#25105;&#20204;&#32463;&#39564;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26377;&#26395;&#36991;&#24320;&#21452;&#19979;&#38477;&#65292;&#31616;&#21333;&#30340;$\ell_2$&#27491;&#21017;&#21270;&#24050;&#32463;&#23545;&#27492;&#26377;&#31215;&#26497;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the ``double descent'', has caught the attention of the deep learning community. As the model's size grows, the performance gets first worse, and then goes back to improving. It raises serious questions about the optimal model's size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple $\ell_2$ regularization is already positively contributing to such a perspective.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312; exp-concave &#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24191;&#27867;&#31867;&#21035;&#30340;&#26377;&#30028; exp-concave &#25439;&#22833;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#65292;&#32500;&#24230;&#21644;&#26679;&#26412;&#22823;&#23567;&#23545;&#32467;&#26524;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;&#22522;&#20110;&#32479;&#19968;&#20960;&#20309;&#20551;&#35774;&#21644;&#26412;&#22320;&#35268;&#33539;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.10726</link><description>&lt;p&gt;
&#22312; exp-concave &#32479;&#35745;&#23398;&#20064;&#20013;&#25506;&#32034;&#26412;&#22320;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Exploring Local Norms in Exp-concave Statistical Learning. (arXiv:2302.10726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312; exp-concave &#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24191;&#27867;&#31867;&#21035;&#30340;&#26377;&#30028; exp-concave &#25439;&#22833;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#65292;&#32500;&#24230;&#21644;&#26679;&#26412;&#22823;&#23567;&#23545;&#32467;&#26524;&#26377;&#24433;&#21709;&#65292;&#24182;&#19988;&#22522;&#20110;&#32479;&#19968;&#20960;&#20309;&#20551;&#35774;&#21644;&#26412;&#22320;&#35268;&#33539;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#19968;&#20010;&#20984;&#31867;&#20013;&#22788;&#29702;&#24102;&#26377; exp-concave &#25439;&#22833;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22238;&#31572;&#20102;&#19968;&#20123;&#20043;&#21069;&#30740;&#31350;&#20013;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#26377;&#30028; exp-concave &#25439;&#22833;&#30340; $O(d/n + \log(1/\delta)/n)$ &#36807;&#37327;&#39118;&#38505;&#30028;&#65292;&#20854;&#20013; $d$ &#26159;&#20984;&#21442;&#32771;&#38598;&#30340;&#32500;&#24230;&#65292;$n$ &#26159;&#26679;&#26412;&#22823;&#23567;&#65292;$\delta$ &#26159;&#32622;&#20449;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#23545;&#25439;&#22833;&#26799;&#24230;&#30340;&#32479;&#19968;&#20960;&#20309;&#20551;&#35774;&#21644;&#26412;&#22320;&#35268;&#33539;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of stochastic convex optimization with exp-concave losses using Empirical Risk Minimization in a convex class. Answering a question raised in several prior works, we provide a $O( d / n + \log( 1 / \delta) / n )$ excess risk bound valid for a wide class of bounded exp-concave losses, where $d$ is the dimension of the convex reference set, $n$ is the sample size, and $\delta$ is the confidence level. Our result is based on a unified geometric assumption on the gradient of losses and the notion of local norms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedSpeed&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;prox&#26657;&#27491;&#39033;&#21644;&#21512;&#24182;&#39069;&#22806;&#26799;&#24230;&#19978;&#21319;&#27493;&#39588;&#30340;&#25200;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#24046;&#21644;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10429</link><description>&lt;p&gt;
FedSpeed: &#26356;&#22823;&#30340;&#26412;&#22320;&#38388;&#38548;&#65292;&#26356;&#23569;&#30340;&#36890;&#20449;&#36718;&#27425;&#65292;&#26356;&#39640;&#30340;&#27867;&#21270;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedSpeed&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;prox&#26657;&#27491;&#39033;&#21644;&#21512;&#24182;&#39069;&#22806;&#26799;&#24230;&#19978;&#21319;&#27493;&#39588;&#30340;&#25200;&#21160;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#24046;&#21644;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#37327;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26412;&#22320;&#35774;&#22791;&#32852;&#21512;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20854;&#24615;&#33021;&#21463;&#21040;&#26412;&#22320;&#19981;&#19968;&#33268;&#26368;&#20248;&#24341;&#20837;&#30340;&#38750;&#28040;&#22833;&#20559;&#24046;&#21644;&#26412;&#22320;&#36807;&#25311;&#21512;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#23458;&#25143;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;FedSpeed&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;FedSpeed&#22312;&#24403;&#21069;&#26412;&#22320;&#26356;&#26032;&#19978;&#24212;&#29992;&#20102;prox&#26657;&#27491;&#39033;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;prox&#39033;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;prox&#39033;&#26159;&#19968;&#31181;&#24517;&#35201;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#20445;&#25345;&#24378;&#28872;&#30340;&#26412;&#22320;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;FedSpeed&#23558;&#32431;&#38543;&#26426;&#26799;&#24230;&#19982;&#37051;&#22495;&#20013;&#39069;&#22806;&#26799;&#24230;&#19978;&#21319;&#27493;&#39588;&#35745;&#31639;&#30340;&#25200;&#21160;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#20943;&#36731;&#26412;&#22320;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25910;&#25947;&#36895;&#24230;&#19982;&#36890;&#20449;&#36718;&#27425;$T$&#21644;&#26412;&#22320;&#38388;&#38548;$K$&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems. Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.09532</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#33258;&#20449;&#30340;&#39044;&#27979;&#29983;&#25104;&#38468;&#21152;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#31867;&#30446;&#26631;&#23545;&#32473;&#23450;&#26631;&#31614;&#30340;&#25935;&#24863;&#24615;&#65292;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21487;&#38752;&#30340;&#20998;&#31867;&#30417;&#30563;&#8220;&#19968;&#20010;&#33410;&#28857;&#23646;&#20110;&#29305;&#23450;&#31867;&#8221;&#65292;&#25105;&#20204;&#26356;&#21916;&#27426;&#23481;&#38169;&#24615;&#23545;&#27604;&#30417;&#30563;&#8220;&#20004;&#20010;&#33410;&#28857;&#19981;&#23646;&#20110;&#21516;&#19968;&#31867;&#8221;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#25918;&#26494;&#30340;&#29256;&#26412;&#65292;&#21363;&#35782;&#21035;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNNs&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#20266;&#23545;&#27604;&#23398;&#20064;(PCL)&#12290;&#23427;&#23558;&#30446;&#26631;&#20026;&#30456;&#21516;&#31867;&#30340;&#27491;&#20266;&#26631;&#31614;&#21644;&#36127;&#20266;&#26631;&#31614;&#30340;&#20004;&#20010;&#33410;&#28857;&#20998;&#24320;&#12290;&#20026;&#20102;&#23558;&#25299;&#25169;&#30693;&#35782;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25299;&#25169;&#21152;&#26435;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pseudo Labeling is a technique used to improve the performance of semi-supervised Graph Neural Networks (GNNs) by generating additional pseudo-labels based on confident predictions. However, the quality of generated pseudo-labels has been a longstanding concern due to the sensitivity of the classification objective with respect to the given labels. To avoid the untrustworthy classification supervision indicating ``a node belongs to a specific class,'' we favor the fault-tolerant contrasting supervision demonstrating ``two nodes do not belong to the same class.'' Thus, the problem of generating high-quality pseudo-labels is then transformed into a relaxed version, i.e., identifying reliable negative pairs. To achieve this, we propose a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It separates two nodes whose positive and negative pseudo-labels target the same class. To incorporate topological knowledge into learning, we devise a topologically weighted contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#21160;&#24577;&#36951;&#25022;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#39318;&#27425;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.08652</link><description>&lt;p&gt;
&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Minimizing Dynamic Regret on Geodesic Metric Spaces. (arXiv:2302.08652v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#21160;&#24577;&#36951;&#25022;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20048;&#35266;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#39318;&#27425;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23436;&#22791;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#26368;&#23567;&#21270;&#19968;&#33324;&#21160;&#24577;&#36951;&#25022;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;&#20248;&#21270;&#22312;&#36825;&#26679;&#19968;&#20010;&#39046;&#22495;&#65292;&#20063;&#34987;&#31216;&#20026;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#65292;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#32447;&#35774;&#32622;&#21364;&#24341;&#36215;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#23578;&#26410;&#30830;&#23450;&#22312;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#27431;&#20960;&#37324;&#24503;&#35774;&#23450;&#20013;&#30340;&#32467;&#35770;&#65288;&#20363;&#22914;&#26354;&#29575;&#65289;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20855;&#26377;&#38750;&#27491;&#26354;&#29575;&#30340;&#27969;&#24418;&#19978;&#25214;&#21040;&#20048;&#35266;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21482;&#35201;&#20801;&#35768;&#19981;&#36866;&#24403;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#33258;&#36866;&#24212;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#32771;&#34385;&#19968;&#33324;&#21160;&#24577;&#36951;&#25022;&#24182;&#24320;&#21457;&#21487;&#24212;&#29992;&#20110;&#27979;&#22320;&#24230;&#37327;&#31354;&#38388;&#30340;&#8220;&#20048;&#35266;&#8221;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the sequential decision problem where the goal is to minimize the general dynamic regret on a complete Riemannian manifold. The task of offline optimization on such a domain, also known as a geodesic metric space, has recently received significant attention. The online setting has received significantly less attention, and it has remained an open question whether the body of results that hold in the Euclidean setting can be transplanted into the land of Riemannian manifolds where new challenges (e.g., curvature) come into play. In this paper, we show how to get optimistic regret bound on manifolds with non-positive curvature whenever improper learning is allowed and propose an array of adaptive no-regret algorithms. To the best of our knowledge, this is the first work that considers general dynamic regret and develops "optimistic" online learning algorithms which can be employed on geodesic metric spaces.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.08468</link><description>&lt;p&gt;
LEVER: &#20351;&#29992;&#25191;&#34892;&#36827;&#34892;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#23398;&#20064;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;code LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22312;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27492;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#23558;LLM&#35299;&#30721;&#19982;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#25110;&#22522;&#20110;&#25191;&#34892;&#32467;&#26524;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26679;&#26412;&#20462;&#21098;&#21644;&#37325;&#26032;&#25490;&#24207;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#24212;&#29992;&#26469;&#35828;&#65292;&#33719;&#21462;&#27979;&#35797;&#29992;&#20363;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#25429;&#25417;&#25191;&#34892;&#32467;&#26524;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#27604;&#22914;&#25968;&#25454;&#31867;&#22411;&#21644;&#20540;&#33539;&#22260;&#65292;&#36825;&#24448;&#24448;&#34920;&#26126;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEVER&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20174;&#32780;&#25913;&#36827;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#20174;LLM&#20013;&#25277;&#26679;&#30340;&#31243;&#24207;&#26159;&#21542;&#27491;&#30830;&#12290;&#36890;&#36807;&#23558;&#39564;&#35777;&#20998;&#25968;&#19982;LLM&#29983;&#25104;&#20998;&#25968;&#30456;&#32467;&#21512;&#65292;&#23545;&#25277;&#26679;&#30340;&#31243;&#24207;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#21442;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#21487;&#20197;&#37325;&#24314;&#27604;&#21407;&#32593;&#32476;&#23567;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2302.07937</link><description>&lt;p&gt;
&#20165;&#35843;&#25972;&#24402;&#19968;&#21270;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#21442;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#21487;&#20197;&#37325;&#24314;&#27604;&#21407;&#32593;&#32476;&#23567;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#19968;&#21270;&#36716;&#25442;&#65292;&#22914;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#23618;&#24402;&#19968;&#21270;&#65292;&#24050;&#25104;&#20026;&#24403;&#20170;&#20808;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20851;&#20110;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#36825;&#20123;&#20223;&#23556;&#21464;&#25442;&#30340;&#21442;&#25968;&#23601;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24341;&#21457;&#20102;&#23545;&#35843;&#25972;&#20923;&#32467;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#23545;&#20110;&#38543;&#26426;ReLU&#32593;&#32476;&#65292;&#20165;&#24494;&#35843;&#20854;&#24402;&#19968;&#21270;&#23618;&#21487;&#20197;&#37325;&#24314;&#20219;&#20309;&#22823;&#23567;&#20026;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#23567;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#31232;&#30095;&#32593;&#32476;&#20013;&#65292;&#22312;&#36275;&#22815;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#32467;&#35770;&#20063;&#25104;&#31435;&#65292;&#19982;&#20808;&#21069;&#30340;&#23454;&#35777;&#24037;&#20316;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07832</link><description>&lt;p&gt;
&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Anomaly Detection under Labeling Budget Constraints. (arXiv:2302.07832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#65292;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#19987;&#23478;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21307;&#23398;&#35786;&#26029;&#25110;&#27450;&#35784;&#26816;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#29702;&#35770;&#26465;&#20214;&#65292;&#20351;&#24471;&#20174;&#26631;&#35760;&#30340;&#26597;&#35810;&#21040;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#24322;&#24120;&#20998;&#25968;&#33021;&#22815;&#25512;&#24191;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#20855;&#26377;&#26368;&#20248;&#25968;&#25454;&#35206;&#30422;&#30340;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#22270;&#20687;&#12289;&#34920;&#26684;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting informative data points for expert feedback can significantly improve the performance of anomaly detection (AD) in various contexts, such as medical diagnostics or fraud detection. In this paper, we determine a set of theoretical conditions under which anomaly scores generalize from labeled queries to unlabeled data. Motivated by these results, we propose a data labeling strategy with optimal data coverage under labeling budget constraints. In addition, we propose a new learning framework for semi-supervised AD. Extensive experiments on image, tabular, and video data sets show that our approach results in state-of-the-art semi-supervised AD performance under labeling budget constraints.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#24773;&#26223;&#19979;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;&#20445;&#25345;&#20986;&#33394;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21516;&#26102;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05118</link><description>&lt;p&gt;
&#36229;&#36234;&#39046;&#22495;&#24773;&#26223;&#65306;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#24773;&#26223;&#19979;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;&#20445;&#25345;&#20986;&#33394;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21516;&#26102;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#26657;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#33719;&#24471;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#39044;&#27979;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#22312;&#39046;&#22495;&#20869;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24773;&#26223;&#19979;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;k&#36817;&#37051;&#30340;DAC&#65288;Density-Aware Calibration&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;&#19982;&#29616;&#26377;&#30340;&#20107;&#21518;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#38544;&#34255;&#23618;&#20316;&#20026;&#19982;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;DAC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#19982;&#26368;&#20808;&#36827;&#30340;&#20107;&#21518;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;DAC&#25552;&#39640;&#20102;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;OOD&#24773;&#26223;&#19979;&#26657;&#20934;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20986;&#33394;&#30340;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DAC&#33021;&#22815;&#19968;&#30452;&#24341;&#23548;&#30528;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep learning models to yield uncertainty-aware predictions is crucial as deep neural networks get increasingly deployed in safety-critical applications. While existing post-hoc calibration methods achieve impressive results on in-domain test datasets, they are limited by their inability to yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD) scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN). In contrast to existing post-hoc methods, we utilize hidden layers of classifiers as a source for uncertainty-related information and study their importance. We show that DAC is a generic method that can readily be combined with state-of-the-art post-hoc methods. DAC boosts the robustness of calibration performance in domain-shift and OOD, while maintaining excellent in-domain predictive uncertainty estimates. We demonstrate that DAC leads to consistently b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#65292;&#21487;&#20197;&#22312;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2302.02787</link><description>&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#20004;&#31181;&#30495;&#23454;&#21010;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative models for two-ground-truth partitions in networks. (arXiv:2302.02787v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#65292;&#21487;&#20197;&#22312;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38543;&#26426;&#20132;&#21449;&#22359;&#27169;&#22411;&#65288;SCBM&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#22522;&#20934;&#32593;&#32476;&#30340;&#20013;&#23610;&#24230;&#32467;&#26500;&#20013;&#21516;&#26102;&#26500;&#24314;&#20004;&#20010;&#19981;&#21516;&#30340;&#21010;&#20998;&#12290;&#36890;&#36807;&#35780;&#20272;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20934;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of approaches have been proposed to characterise the mesoscale structure of networks - most often as a partition based on patterns variously called communities, blocks, or clusters. Clearly, distinct methods designed to detect different types of patterns may provide a variety of answers to the network's mesoscale structure. Yet, even multiple runs of a given method can sometimes yield diverse and conflicting results, producing entire landscapes of partitions which potentially include multiple (locally optimal) mesoscale explanations of the network. Such ambiguity motivates a closer look at the ability of these methods to find multiple qualitatively different 'ground truth' partitions in a network. Here, we propose the stochastic cross-block model (SCBM), a generative model which allows for two distinct partitions to be built into the mesoscale structure of a single benchmark network. We demonstrate a use case of the benchmark model by appraising the power of stochastic block m
&lt;/p&gt;</description></item><item><title>ProtST&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#21644;&#29702;&#35299;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33719;&#24471;&#19981;&#21516;&#31890;&#24230;&#30340;&#34507;&#30333;&#36136;&#23646;&#24615;&#20449;&#24687;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.12040</link><description>&lt;p&gt;
ProtST: &#34507;&#30333;&#36136;&#24207;&#21015;&#19982;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts. (arXiv:2301.12040v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12040
&lt;/p&gt;
&lt;p&gt;
ProtST&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#21644;&#29702;&#35299;&#65292;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33719;&#24471;&#19981;&#21516;&#31890;&#24230;&#30340;&#34507;&#30333;&#36136;&#23646;&#24615;&#20449;&#24687;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20027;&#35201;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#23398;&#20064;&#34507;&#30333;&#36136;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20849;&#36827;&#21270;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#26126;&#30830;&#33719;&#24471;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36825;&#26159;&#34507;&#30333;&#36136;&#34920;&#24449;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#23545;&#20110;&#35768;&#22810;&#34507;&#30333;&#36136;&#26469;&#35828;&#65292;&#23427;&#20204;&#30340;&#25991;&#26412;&#23646;&#24615;&#25551;&#36848;&#26159;&#21487;&#29992;&#30340;&#65292;&#20854;&#20013;&#20063;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#21508;&#31181;&#21151;&#33021;&#12290;&#21463;&#21040;&#36825;&#20010;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;ProtDescribe&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#21151;&#33021;&#21644;&#20854;&#20182;&#37325;&#35201;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProtST&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#26469;&#22686;&#24378;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#21644;&#29702;&#35299;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21363;&#21333;&#27169;&#24577;&#25513;&#30721;&#39044;&#27979;&#12289;&#22810;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#21644;&#22810;&#27169;&#24577;&#25513;&#30721;&#39044;&#27979;&#65292;&#20197;&#22686;&#24378;PLM&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#34507;&#30333;&#36136;&#23646;&#24615;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;PLM&#30340;&#21407;&#22987;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original represen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#30456;&#23545;&#20960;&#20309;&#20851;&#31995;&#20197;&#21450;&#23884;&#20837;&#21305;&#37197;&#20219;&#21153;&#26469;&#23545;&#40784;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.12005</link><description>&lt;p&gt;
EmbedDistill: &#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#20960;&#20309;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval. (arXiv:2301.12005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#30456;&#23545;&#20960;&#20309;&#20851;&#31995;&#20197;&#21450;&#23884;&#20837;&#21305;&#37197;&#20219;&#21153;&#26469;&#23545;&#40784;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#65288;&#22914;Transformers&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#37096;&#32626;&#12290;&#21463;&#21040;&#25105;&#20204;&#23545;IR&#27169;&#22411;&#30340;&#24072;&#29983;&#27867;&#21270;&#24046;&#36317;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#23398;&#21040;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#23545;&#20960;&#20309;&#20851;&#31995;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25945;&#24072;&#20998;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#23884;&#20837;&#21305;&#37197;&#20219;&#21153;&#65292;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#20449;&#21495;&#26469;&#23545;&#40784;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21033;&#29992;&#26597;&#35810;&#29983;&#25104;&#26469;&#25506;&#32034;&#25968;&#25454;&#27969;&#24418;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#23398;&#29983;&#21644;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25512;&#21160;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large neural models (such as Transformers) achieve state-of-the-art performance for information retrieval (IR). In this paper, we aim to improve distillation methods that pave the way for the resource-efficient deployment of such models in practice. Inspired by our theoretical analysis of the teacher-student generalization gap for IR models, we propose a novel distillation approach that leverages the relative geometry among queries and documents learned by the large teacher model. Unlike existing teacher score-based distillation methods, our proposed approach employs embedding matching tasks to provide a stronger signal to align the representations of the teacher and student models. In addition, it utilizes query generation to explore the data manifold to reduce the discrepancies between the student and the teacher where training data is sparse. Furthermore, our analysis also motivates novel asymmetric architectures for student models which realizes better embedding alignment without i
&lt;/p&gt;</description></item><item><title>SOBER&#31639;&#27861;&#26159;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#36827;&#34892;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#65292;&#19988;&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11832</link><description>&lt;p&gt;
SOBER&#65306;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11832
&lt;/p&gt;
&lt;p&gt;
SOBER&#31639;&#27861;&#26159;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#36827;&#34892;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#65292;&#19988;&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#22788;&#29702;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#31215;&#20998;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#38656;&#24182;&#34892;&#26597;&#35810;&#26114;&#36149;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#25191;&#34892;&#20248;&#21270;&#21644;&#31215;&#20998;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#25209;&#37327;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;SOBER&#65292;&#23427;&#20801;&#35768;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#20351;&#29992;&#20219;&#24847;&#37319;&#38598;&#20989;&#25968;&#21644;&#20869;&#26680;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#20840;&#23616;&#20248;&#21270;&#30340;&#25209;&#37327;&#36873;&#25321;&#37325;&#26032;&#23450;&#20041;&#20026;&#31215;&#20998;&#38382;&#39064;&#65292;&#24182;&#23558;&#37319;&#38598;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#65288;&#38750;&#20984;&#65289;&#26494;&#24347;&#20026;&#20869;&#26680;&#37325;&#32452;&#65288;&#20984;&#65289;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;SOBER&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#21644;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11509</link><description>&lt;p&gt;
&#32454;&#35843;&#31070;&#32463;&#31639;&#31526;&#32467;&#26500;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#21644;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#32467;&#26524;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#31070;&#32463;&#31639;&#31526;&#65288;NOs&#65289;&#21450;&#20854;&#34893;&#29983;&#32467;&#26500;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#32463;&#39564;&#35780;&#20272;&#12289;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#20998;&#26512;&#20197;&#21450;&#23545;&#25439;&#22833;&#26223;&#35266;&#21487;&#35270;&#21270;&#30340;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#25552;&#39640;NOs&#27867;&#21270;&#33021;&#21147;&#30340;&#20462;&#25913;&#12290;&#21463;Transformer&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;${\textit{s}}{\text{NO}}+\varepsilon$&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#26680;&#31215;&#20998;&#31639;&#31526;&#26469;&#20195;&#26367;&#33258;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20276;&#38543;&#30528;&#25439;&#22833;&#26223;&#35266;&#21487;&#35270;&#21270;&#30340;&#23450;&#24615;&#21464;&#21270;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#20102;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#29468;&#27979;&#65292;Transformer&#30340;&#24067;&#23616;&#20351;&#20248;&#21270;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#26497;&#23567;&#20540;&#65292;&#24182;&#19988;&#38543;&#26426;&#28145;&#24230;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#30001;&#20110;&#35757;&#32451;&#21160;&#24577;&#30340;&#20005;&#26684;&#20998;&#26512;&#26159;&#28145;&#24230;&#23398;&#20064;&#26368;&#31361;&#20986;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#35838;&#31243;&#65292;&#37325;&#28857;&#26159;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
&lt;/p&gt;</description></item><item><title>&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.06535</link><description>&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#39640;&#38454;&#20132;&#20114;&#30340;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06535
&lt;/p&gt;
&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#25968;&#25454;&#39537;&#21160;&#30340;&#21327;&#21464;&#37327;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#27604;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#26696;&#20363;&#22522;&#30784;&#25277;&#26679;&#26694;&#26550;&#19982;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#26041;&#26696;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#33258;&#28982;&#22320;&#32771;&#34385;&#21040;&#25130;&#23614;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#26102;&#38388;&#36755;&#20837;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;CBNNs&#36890;&#36807;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#21051;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#26469;&#20272;&#35745;&#21361;&#38505;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#26102;&#38388;&#20381;&#36182;&#25351;&#26631;&#27604;&#36739;CBNNs&#19982;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#22797;&#26434;&#22522;&#32447;&#39118;&#38505;&#21644;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#25152;&#26377;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;CBNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#40723;&#21169;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10549</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#40723;&#21169;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#35832;&#22914;Winoground&#31561;&#32452;&#21512;&#27010;&#25324;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#21069;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20851;&#31995;&#32423;&#21035;&#30340;&#23545;&#40784;&#65306;&#21363;&#33021;&#22815;&#23558;&#25991;&#26412;&#20013;&#30340;&#23450;&#21521;&#35821;&#20041;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#8220;&#33609;&#22378;&#20013;&#30340;&#26479;&#23376;&#8221;&#65289;&#19982;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#26479;&#23376;&#30456;&#23545;&#20110;&#33609;&#22378;&#30340;&#20301;&#32622;&#65289;&#36827;&#34892;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#40723;&#21169;&#20174;&#8220;&#26479;&#23376;&#8221;&#21040;&#8220;&#33609;&#22378;&#8221;&#65288;&#25429;&#25417;&#35821;&#20041;&#20851;&#31995;&#8220;&#22312;&#8221;&#65289;&#30340;&#23450;&#21521;&#35821;&#35328;&#27880;&#24847;&#21147;&#19982;&#20174;&#26479;&#23376;&#21040;&#33609;&#22378;&#30340;&#23450;&#21521;&#35270;&#35273;&#27880;&#24847;&#21147;&#30456;&#21305;&#37197;&#26469;&#23454;&#29616;&#20851;&#31995;&#23545;&#40784;&#12290;&#36890;&#36807;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#36719;&#24615;&#22320;&#35782;&#21035;&#26631;&#35760;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36719;&#24615;&#20851;&#31995;&#23545;&#40784;&#30340;&#27010;&#24565;&#31561;&#21516;&#20110;&#22312;&#30001;&#36328;&#27169;&#24577;&#25552;&#20379;&#30340;&#8220;&#22522;&#24213;&#21464;&#25442;&#8221;&#19979;&#23454;&#26045;&#35270;&#35273;&#21644;&#35821;&#35328;&#27880;&#24847;&#21147;&#30697;&#38453;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., "mug in grass") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#20197;&#21450;&#39640;&#25928;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;&#30340;&#24555;&#36895;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2212.10503</link><description>&lt;p&gt;
&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#65306;&#36890;&#36807;&#23545;&#40784;&#30340;&#27973;&#23618;&#35757;&#32451;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;
&lt;/p&gt;
&lt;p&gt;
Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10503
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#20197;&#21450;&#39640;&#25928;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#19978;&#30340;&#24555;&#36895;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#26032;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20445;&#25345;transformer&#20027;&#20307;&#37096;&#20998;&#20923;&#32467;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;&#12290;&#23613;&#31649;&#21482;&#23398;&#20064;&#20102;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#24182;&#19981;&#39640;&#65292;&#22240;&#20026;&#35757;&#32451;&#26032;&#30340;&#23884;&#20837;&#21521;&#37327;&#38656;&#35201;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#23436;&#25972;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36855;&#20320;&#27169;&#22411;&#36866;&#24212;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#20013;&#26500;&#24314;&#19968;&#20010;&#27973;&#23618;&#36855;&#20320;&#27169;&#22411;&#12290;&#28982;&#21518;&#21487;&#20197;&#22312;&#36855;&#20320;&#27169;&#22411;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#26032;&#30340;&#35821;&#35328;&#29305;&#23450;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#25554;&#20837;&#21040;&#23545;&#40784;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#30340;&#36328;&#35821;&#35328;&#20256;&#36755;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#23398;&#20064;&#36855;&#20320;&#27169;&#22411;&#30340;&#26041;&#27861;&#65306;MiniJoint&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#20013;&#38388;&#23618;&#27425;&#19978;&#36741;&#21161;MLM&#22836;&#30340;&#21333;&#20010;transformer&#21516;&#26102;&#39044;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#36855;&#20320;&#27169;&#22411;&#12290;MiniPost&#21017;&#20174;&#24120;&#35268;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#22987;&#65292;&#36890;&#36807;&#25552;&#21462;&#21644;&#20923;&#32467;&#20960;&#23618;&#26469;&#26500;&#24314;&#36855;&#20320;&#27169;&#22411;&#65292;&#24182;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#19988;&#33021;&#32791;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2212.03369</link><description>&lt;p&gt;
&#25506;&#32034;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#19988;&#33021;&#32791;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19981;&#21516;&#20154;&#31867;&#27963;&#21160;&#25490;&#25918;&#24773;&#26223;&#30340;&#27668;&#20505;&#24433;&#21709;&#23545;&#20110;&#21046;&#23450;&#27668;&#20505;&#21464;&#21270;&#20943;&#32531;&#21644;&#36866;&#24212;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#23545;&#36825;&#20123;&#24433;&#21709;&#30340;&#35814;&#32454;&#27934;&#35265;&#65292;&#20294;&#27599;&#20010;&#24773;&#26223;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#36825;&#31181;&#24040;&#22823;&#30340;&#35745;&#31639;&#36127;&#25285;&#24341;&#21457;&#20102;&#23545;&#24320;&#21457;&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#22411;&#20223;&#30495;&#20219;&#21153;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#26500;&#24314;&#36825;&#20123;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;ClimateBench&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22810;&#23618;&#24863;&#30693;&#26426;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20013;&#20018;&#34892;&#36830;&#25509;&#30340;&#31264;&#23494;&#23618;&#26367;&#25442;&#20026;&#38543;&#26426;&#36830;&#25509;&#30340;&#31264;&#23494;&#23618;&#65292;&#24182;&#35780;&#20272;&#22312;&#25317;&#26377;100&#19975;&#21644;1000&#19975;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33021;&#32791;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#24403;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the climate impacts of various anthropogenic emissions scenarios is key to making informed decisions for climate change mitigation and adaptation. State-of-the-art Earth system models can provide detailed insight into these impacts, but have a large associated computational cost on a per-scenario basis. This large computational burden has driven recent interest in developing cheap machine learning models for the task of climate model emulation. In this manuscript, we explore the efficacy of randomly wired neural networks for this task. We describe how they can be constructed and compare them to their standard feedforward counterparts using the ClimateBench dataset. Specifically, we replace the serially connected dense layers in multilayer perceptrons, convolutional neural networks, and convolutional long short-term memory networks with randomly wired dense layers and assess the impact on model performance for models with 1 million and 10 million parameters. We find that model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02090</link><description>&lt;p&gt;
&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#26469;&#25171;&#30772;&#26465;&#20214;&#29983;&#25104;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25429;&#25417;&#26679;&#26412;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#32463;&#24120;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#32487;&#25215;&#20102;&#20266;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#22312;&#21478;&#19968;&#20010;&#28508;&#22312;&#23646;&#24615;&#19978;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20004;&#27493;&#31574;&#30053;&#12290; &#65288;a&#65289;&#20844;&#24179;&#24178;&#39044;&#65288;FI&#65289;&#65306;&#24378;&#35843;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30001;&#20110;&#20266;&#30456;&#20851;&#24615;&#38590;&#20197;&#29983;&#25104;&#30340;&#23569;&#25968;&#26679;&#26412;&#12290;&#65288;b&#65289;&#32416;&#27491;&#37319;&#26679;&#65288;CS&#65289;&#65306;&#26174;&#24335;&#36807;&#28388;&#29983;&#25104;&#30340;&#26679;&#26412;&#65292;&#24182;&#30830;&#20445;&#23427;&#20204;&#36981;&#24490;&#25152;&#38656;&#30340;&#28508;&#22312;&#23646;&#24615;&#20998;&#24067;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#20844;&#24179;&#24178;&#39044;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#31243;&#24230;&#30340;&#23545;&#20266;&#23646;&#24615;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FICS&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26465;&#20214;&#29983;&#25104;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
To capture the relationship between samples and labels, conditional generative models often inherit spurious correlations from the training dataset. This can result in label-conditional distributions that are imbalanced with respect to another latent attribute. To mitigate this issue, which we call spurious causality of conditional generation, we propose a general two-step strategy. (a) Fairness Intervention (FI): emphasize the minority samples that are hard to generate due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): explicitly filter the generated samples and ensure that they follow the desired latent attribute distribution. We have designed the fairness intervention to work for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results demonstrate that FICS can effectively resolve spurious causality of conditional generation across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#30340;&#24191;&#27867;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.15536</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#24847;&#35265;&#25366;&#25496;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#30340;&#24191;&#27867;&#25216;&#26415;&#65292;&#24182;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#25110;&#24847;&#35265;&#25366;&#25496;&#26377;&#21161;&#20110;&#35828;&#26126;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#30701;&#35821;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#36817;&#24180;&#26469;&#26368;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#30340;&#25216;&#26415;&#26469;&#23545;&#24773;&#24863;&#23545;&#31435;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#36807;&#31243;&#35299;&#37322;&#12290;&#36890;&#36807;&#20998;&#26512;&#30340;&#32467;&#26524;&#65292;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#20998;&#31867;&#21644;&#35780;&#35770;&#32423;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#24773;&#24863;&#20998;&#26512;&#30740;&#31350;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis or opinion mining help to illustrate the phrase NLP (Natural Language Processing). Sentiment analysis has been the most significant topic in recent years. The goal of this study is to solve the sentiment polarity classification challenges in sentiment analysis. A broad technique for categorizing sentiment opposition is presented, along with comprehensive process explanations. With the results of the analysis, both sentence-level classification and review-level categorization are conducted. Finally, we discuss our plans for future sentiment analysis research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2211.09325</link><description>&lt;p&gt;
TAX-Pose&#65306;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22522;&#20110;&#31034;&#33539;&#36716;&#31227;&#30456;&#20851;&#25216;&#33021;&#65311;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#25110;&#26410;&#35265;&#36807;&#30340;&#37197;&#32622;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#20114;&#23545;&#35937;&#30456;&#20851;&#37096;&#20998;&#30340;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#20851;&#31995;&#26159;&#19968;&#31181;&#21487;&#20197;&#36716;&#31227;&#21040;&#21516;&#19968;&#31867;&#21035;&#26032;&#29289;&#20307;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#27010;&#24565;&#65307;&#20363;&#22914;&#65292;&#24179;&#24213;&#38149;&#30456;&#23545;&#20110;&#28900;&#31665;&#30340;&#23039;&#21183;&#20851;&#31995;&#25110;&#32773;&#26479;&#23376;&#30456;&#23545;&#20110;&#26479;&#26550;&#30340;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#20026;&#8220;&#36328;&#23039;&#21183;&#8221;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#27010;&#24565;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#23398;&#20064;&#30340;&#23545;&#35937;&#38388;&#23545;&#24212;&#20851;&#31995;&#26469;&#23398;&#20064;&#20272;&#35745;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#12290;&#28982;&#21518;&#65292;&#20272;&#35745;&#30340;&#36328;&#23039;&#21183;&#29992;&#20110;&#24341;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#23545;&#35937;&#25805;&#32437;&#21040;&#25152;&#38656;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Sirius&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#20998;&#24037;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#20915;&#31574;&#24037;&#20316;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#22312;&#38656;&#35201;&#26102;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#31181;&#20154;&#26426;&#22242;&#38431;&#21487;&#20197;&#30830;&#20445;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#26469;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.08416</link><description>&lt;p&gt;
&#22312;&#24037;&#20316;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#65306;&#20154;&#20026;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23398;&#20064;&#21644;&#37096;&#32626;&#26399;&#38388;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment. (arXiv:2211.08416v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Sirius&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#20998;&#24037;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#20915;&#31574;&#24037;&#20316;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#22312;&#38656;&#35201;&#26102;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#31181;&#20154;&#26426;&#22242;&#38431;&#21487;&#20197;&#30830;&#20445;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#26469;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#35265;&#35777;&#20102;&#26032;&#22411;&#26426;&#22120;&#20154;&#33021;&#21147;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#34920;&#29616;&#20986;&#33030;&#24369;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25509;&#21463;&#23427;&#20204;&#30340;&#19981;&#23436;&#32654;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sirius&#65292;&#19968;&#20010;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#20998;&#24037;&#21512;&#20316;&#32780;&#35774;&#35745;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#22788;&#29702;&#22823;&#37096;&#20998;&#20915;&#31574;&#24037;&#20316;&#65292;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#23427;&#20204;&#33021;&#21487;&#38752;&#22320;&#24037;&#20316;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#30417;&#25511;&#36825;&#20010;&#36807;&#31243;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#26679;&#30340;&#20154;&#26426;&#22242;&#38431;&#30830;&#20445;&#20102;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;&#20174;&#20219;&#21153;&#25191;&#34892;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;&#31574;&#30053;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;&#36817;&#20284;&#20154;&#31867;&#34892;&#20026;&#30340;&#26679;&#26412;&#23545;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy's performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#28145;&#37096;&#12289;&#34920;&#27973;&#37096;&#21644;&#23567;&#33041;&#30333;&#36136;&#36830;&#25509;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#37096;&#30333;&#36136;&#23545;&#20110;&#24180;&#40836;&#39044;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.07398</link><description>&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#34920;&#29616;&#22312;&#28145;&#37096;&#12289;&#34920;&#27973;&#37096;&#21644;&#23567;&#33041;&#30333;&#36136;&#36830;&#25509;&#20013;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Age Prediction Performance Varies Across Deep, Superficial, and Cerebellar White Matter Connections. (arXiv:2211.07398v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#28145;&#37096;&#12289;&#34920;&#27973;&#37096;&#21644;&#23567;&#33041;&#30333;&#36136;&#36830;&#25509;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#37096;&#30333;&#36136;&#23545;&#20110;&#24180;&#40836;&#39044;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#30340;&#30333;&#36136;&#22312;&#20154;&#31867;&#23551;&#21629;&#26399;&#38388;&#32463;&#21382;&#21457;&#32946;&#21644;&#36864;&#34892;&#24615;&#36807;&#31243;&#12290;&#20026;&#20102;&#30740;&#31350;&#30333;&#36136;&#35299;&#21078;&#21306;&#22495;&#19982;&#24180;&#40836;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#36712;&#36857;&#22270;&#32454;&#20998;&#20026;&#28145;&#37096;&#12289;&#34920;&#27973;&#37096;&#21644;&#23567;&#33041;&#30333;&#36136;&#30340;&#32420;&#32500;&#31751;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#22823;&#21367;&#31215;&#26680;&#21644;&#20498;&#32622;&#29942;&#39048;&#12290;&#25105;&#20204;&#21033;&#29992;&#26032;&#22411;&#30340;&#31163;&#25955;&#22810;&#26041;&#38754;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#21644;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#24615;&#33021;&#65292;&#40723;&#21169;&#24180;&#40836;&#39044;&#27979;&#22312;&#39044;&#26399;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#20154;&#31867;&#36830;&#25509;&#32452;&#35745;&#21010;&#30340;965&#21517;&#20581;&#24247;&#24180;&#36731;&#25104;&#24180;&#20154;&#65288;22-37&#23681;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.59&#23681;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#38431;&#21015;&#20013;&#65292;&#28145;&#37096;&#30333;&#36136;&#23545;&#20110;&#24180;&#40836;&#39044;&#27979;&#26368;&#26377;&#20449;&#24687;&#65292;&#32780;&#34920;&#27973;&#37096;&#30333;&#36136;&#23545;&#20110;&#24180;&#40836;&#39044;&#27979;&#19981;&#22826;&#26377;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain's white matter (WM) undergoes developmental and degenerative processes during the human lifespan. To investigate the relationship between WM anatomical regions and age, we study diffusion magnetic resonance imaging tractography that is finely parcellated into fiber clusters in the deep, superficial, and cerebellar WM. We propose a deep-learning-based age prediction model that leverages large convolutional kernels and inverted bottlenecks. We improve performance using novel discrete multi-faceted mix data augmentation and a novel prior-knowledge-based loss function that encourages age predictions in the expected range. We study a dataset of 965 healthy young adults (22-37 years) derived from the Human Connectome Project (HCP). Experimental results demonstrate that the proposed model achieves a mean absolute error of 2.59 years and outperforms compared methods. We find that the deep WM is the most informative for age prediction in this cohort, while the superficial WM is the le
&lt;/p&gt;</description></item><item><title>DriftRec&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#38597;&#22320;&#20462;&#25913;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;DriftRec&#33021;&#22815;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.06757</link><description>&lt;p&gt;
DriftRec: &#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06757
&lt;/p&gt;
&lt;p&gt;
DriftRec&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#38597;&#22320;&#20462;&#25913;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;DriftRec&#33021;&#22815;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#35299;&#20915;&#30450;JPEG&#24674;&#22797;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25193;&#25955;&#27169;&#22411;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20248;&#38597;&#20462;&#25913;&#65292;&#20351;&#20854;&#36866;&#24212;&#27492;&#24674;&#22797;&#20219;&#21153;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;DriftRec&#12290;&#36890;&#36807;&#23558;DriftRec&#19982;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;$L_2$&#22238;&#24402;&#22522;&#32447;&#20197;&#21450;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;JPEG&#24674;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#20854;&#20182;&#26041;&#27861;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#30340;&#20542;&#21521;&#65292;&#24182;&#26174;&#33879;&#26356;&#21152;&#30495;&#23454;&#22320;&#24674;&#22797;&#20102;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#24178;&#20928;/&#25439;&#22351;&#22270;&#20687;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#20351;&#24471;&#23427;&#22312;&#20854;&#20182;&#24674;&#22797;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#19982;&#20854;&#20182;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24178;&#20928;&#22270;&#20687;&#21644;&#25439;&#22351;&#22270;&#20687;&#30340;&#20998;&#24067;&#24444;&#27492;&#26356;&#25509;&#36817;&#30340;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we utilize the high-fidelity generation abilities of diffusion models to solve blind JPEG restoration at high compression levels. We propose an elegant modification of the forward stochastic differential equation of diffusion models to adapt them to this restoration task and name our method DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same network architecture and two state-of-the-art techniques for JPEG restoration, we show that our approach can escape the tendency of other methods to generate blurry images, and recovers the distribution of clean images significantly more faithfully. For this, only a dataset of clean/corrupted image pairs and no knowledge about the corruption operation is required, enabling wider applicability to other restoration tasks. In contrast to other conditional and unconditional diffusion models, we utilize the idea that the distributions of clean and corrupted images are much closer to each other than each is to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2211.01407</link><description>&lt;p&gt;
&#20851;&#20110;&#30417;&#30563;&#20449;&#21495;&#30340;&#20449;&#24687;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20391;&#37325;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20016;&#23500;&#30340;&#27880;&#37322;&#65288;&#22914;&#36719;&#26631;&#31614;&#65289;&#27604;&#31232;&#30095;&#30340;&#27880;&#37322;&#65288;&#22914;&#30828;&#26631;&#31614;&#65289;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#38598;&#25104;&#26412;&#20063;&#26356;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#33021;&#21147;&#22914;&#20309;&#21463;&#21040;&#26631;&#31614;&#25968;&#12289;&#31867;&#21035;&#12289;&#32500;&#24230;&#21644;&#22122;&#22768;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;ODE&#20013;&#35757;&#32451;&#21644;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.16940</link><description>&lt;p&gt;
FI-ODE: &#31070;&#32463;ODE&#20013;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
FI-ODE: Certifiably Robust Forward Invariance in Neural ODEs. (arXiv:2210.16940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;ODE&#20013;&#35757;&#32451;&#21644;&#35777;&#26126;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#19981;&#21464;&#24615;&#26159;&#25511;&#21046;&#29702;&#35770;&#20013;&#38271;&#26399;&#30740;&#31350;&#30340;&#24615;&#36136;&#65292;&#29992;&#20110;&#35777;&#26126;&#21160;&#24577;&#31995;&#32479;&#22312;&#25152;&#26377;&#26102;&#38388;&#20869;&#20445;&#25345;&#22312;&#19968;&#20123;&#39044;&#23450;&#29366;&#24577;&#38598;&#21512;&#20869;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#22312;&#25200;&#21160;&#19979;&#20445;&#25345;&#35777;&#20070;&#26377;&#25928;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#21487;&#35777;&#26126;&#35777;&#23454;&#31070;&#32463;ODE&#20013;&#30340;&#40065;&#26834;&#21069;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22330;&#26223;&#20013;&#24212;&#29992;&#20102;&#36825;&#20010;&#26694;&#26550;&#65306;&#40065;&#26834;&#36830;&#32493;&#25511;&#21046;&#20013;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#24615;&#65292;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#38750;&#34394;&#20551;&#20445;&#35777;&#30340;&#35757;&#32451;NODE&#31574;&#30053;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forward invariance is a long-studied property in control theory that is used to certify that a dynamical system stays within some pre-specified set of states for all time, and also admits robustness guarantees (e.g., the certificate holds under perturbations). We propose a general framework for training and provably certifying robust forward invariance in Neural ODEs. We apply this framework in two settings: certified safety in robust continuous control, and certified adversarial robustness for image classification. To our knowledge, this is the first instance of training NODE policies with such non-vacuous certified guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25130;&#26029;&#39044;&#27979;&#25439;&#22833;&#21644;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#20013;&#30340;&#22122;&#22768;&#22788;&#29702;&#12289;&#27169;&#22411;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#20272;&#35745;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14816</link><description>&lt;p&gt;
&#28145;&#24230;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Subspace Encoders for Nonlinear System Identification. (arXiv:2210.14816v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25130;&#26029;&#39044;&#27979;&#25439;&#22833;&#21644;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#20013;&#30340;&#22122;&#22768;&#22788;&#29702;&#12289;&#27169;&#22411;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#20272;&#35745;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#35768;&#22810;&#23454;&#38469;&#21644;&#29702;&#35770;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22122;&#22768;&#22788;&#29702;&#21644;&#27169;&#22411;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#20272;&#35745;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#19979;&#26159;&#26368;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#21518;&#32773;&#20276;&#38543;&#30528;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#29190;&#28856;&#21644;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#25130;&#26029;&#39044;&#27979;&#25439;&#22833;&#21644;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#36873;&#25321;&#22810;&#20010;&#25130;&#26029;&#23376;&#27573;&#24182;&#35745;&#31639;&#24179;&#22343;&#39044;&#27979;&#25439;&#22833;&#26469;&#35745;&#31639;&#25130;&#26029;&#39044;&#27979;&#25439;&#22833;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#26368;&#23567;&#21270;&#25130;&#26029;&#39044;&#27979;&#25439;&#22833;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using Artificial Neural Networks (ANN) for nonlinear system identification has proven to be a promising approach, but despite of all recent research efforts, many practical and theoretical problems still remain open. Specifically, noise handling and models, issues of consistency and reliable estimation under minimisation of the prediction error are the most severe problems. The latter comes with numerous practical challenges such as explosion of the computational cost in terms of the number of data samples and the occurrence of instabilities during optimization. In this paper, we aim to overcome these issues by proposing a method which uses a truncated prediction loss and a subspace encoder for state estimation. The truncated prediction loss is computed by selecting multiple truncated subsections from the time series and computing the average prediction loss. To obtain a computationally efficient estimation method that minimizes the truncated prediction loss, a subspace encoder represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.10264</link><description>&lt;p&gt;
SignReLU&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36924;&#36817;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#20102;DNN&#20013;&#31070;&#32463;&#20803;&#22914;&#20309;&#22788;&#29702;&#36755;&#20837;&#20449;&#21495;&#12290;&#23427;&#20204;&#23545;&#20110;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#22312;&#36830;&#32493;&#31070;&#32463;&#20803;&#23618;&#20043;&#38388;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#30740;&#31350;DNN&#30340;&#36924;&#36817;&#33021;&#21147;&#26469;&#35299;&#37322;&#20854;&#24378;&#22823;&#21644;&#25104;&#21151;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#65292;&#25506;&#32034;&#20102;DNN&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;&#25968;&#20540;&#23454;&#39564;&#27604;&#36739;&#20102;SignReLU&#19982;&#29616;&#26377;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;Leaky ReLU&#21644;ELU&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;SignReLU&#30340;&#31454;&#20105;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10209</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20851;&#27880;&#22312;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#38543;&#30528;&#26102;&#38388;&#32047;&#31215;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;Wortsman&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;SupSup&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22266;&#23450;&#22522;&#30784;&#32593;&#32476;&#65292;&#24182;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#25214;&#21040;&#19968;&#20010;&#36229;&#25513;&#30721;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25110;&#31227;&#38500;&#27599;&#20010;&#26435;&#37325;&#20197;&#20135;&#29983;&#19968;&#20010;&#23376;&#32593;&#32476;&#12290;&#20182;&#20204;&#36890;&#36807;&#19981;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;&#34429;&#28982;&#27809;&#26377;&#36951;&#24536;&#65292;&#20294;SupSup&#30340;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#20869;&#37096;&#27809;&#26377;&#30693;&#35782;&#30340;&#31215;&#32047;&#25110;&#20256;&#36882;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExSSNeT&#65288;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#65289;&#65292;&#23427;&#36827;&#34892;&#20102;&#29420;&#26377;&#19988;&#19981;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#26435;&#37325;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#21518;&#32493;&#20219;&#21153;&#23545;&#20849;&#20139;&#26435;&#37325;&#30340;&#20914;&#31361;&#26356;&#26032;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20173;&#28982;&#38450;&#27490;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KNN&#30340;&#30693;&#35782;&#20256;&#36882;&#65288;KKT&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#25913;&#21892;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#20248;&#21270;&#26465;&#20214;&#65292;&#24182;&#24378;&#35843;&#20102;&#25552;&#39640;&#29983;&#25104;&#25968;&#25454;&#21487;&#21306;&#20998;&#24615;&#23545;&#20110;&#25913;&#21892;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09643</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#25913;&#21892;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness by Contrastive Guided Diffusion Process. (arXiv:2210.09643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#25913;&#21892;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#20248;&#21270;&#26465;&#20214;&#65292;&#24182;&#24378;&#35843;&#20102;&#25552;&#39640;&#29983;&#25104;&#25968;&#25454;&#21487;&#21306;&#20998;&#24615;&#23545;&#20110;&#25913;&#21892;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24050;&#25104;&#20026;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#20013;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26032;&#20852;&#24037;&#20855;&#65292;&#22240;&#20026;&#40065;&#26834;&#23398;&#20064;&#38656;&#35201;&#27604;&#26631;&#20934;&#20998;&#31867;&#20219;&#21153;&#26356;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#21508;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#33021;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#25913;&#21892;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#25955;&#22411;&#26041;&#27861;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#36890;&#24120;&#36739;&#24930;&#12290;&#36817;&#26399;&#34429;&#28982;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#21152;&#36895;&#25216;&#26415;&#65292;&#20294;&#30740;&#31350;&#22914;&#20309;&#25552;&#39640;&#29983;&#25104;&#25968;&#25454;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#21512;&#25104;&#20998;&#24067;&#36798;&#21040;&#38750;&#24179;&#20961;&#40065;&#26834;&#20934;&#30830;&#24615;&#30340;&#26368;&#20248;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#22686;&#24378;&#29983;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#21306;&#20998;&#24615;&#23545;&#20110;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#26469;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation has become an emerging tool to help improve the adversarial robustness in classification tasks since robust learning requires a significantly larger amount of training samples compared with standard classification tasks. Among various deep generative models, the diffusion model has been shown to produce high-quality synthetic images and has achieved good performance in improving the adversarial robustness. However, diffusion-type methods are typically slow in data generation as compared with other generative models. Although different acceleration techniques have been proposed recently, it is also of great importance to study how to improve the sample efficiency of generated data for the downstream task. In this paper, we first analyze the optimality condition of synthetic distribution for achieving non-trivial robust accuracy. We show that enhancing the distinguishability among the generated data is critical for improving adversarial robustness. Thus, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;UniTune&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#22270;&#20687;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23558;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#12290;UniTune&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#32534;&#36753;&#25513;&#30721;&#21644;&#20002;&#22833;&#32534;&#36753;&#37096;&#20998;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.09477</link><description>&lt;p&gt;
UniTune: &#36890;&#36807;&#22312;&#21333;&#20010;&#22270;&#20687;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;UniTune&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#22270;&#20687;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26469;&#23558;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36716;&#25442;&#20026;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#12290;UniTune&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#32534;&#36753;&#25513;&#30721;&#21644;&#20002;&#22833;&#32534;&#36753;&#37096;&#20998;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20351;&#26222;&#36890;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32534;&#36753;&#29616;&#26377;&#22270;&#20687;&#26469;&#35828;&#65292;&#31867;&#20284;&#30340;&#33021;&#21147;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#12290;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32534;&#36753;&#25513;&#30721;&#65292;&#23545;&#20110;&#38656;&#35201;&#36827;&#34892;&#26174;&#33879;&#35270;&#35273;&#21464;&#21270;&#30340;&#32534;&#36753;&#26377;&#22256;&#38590;&#65292;&#24182;&#19988;&#26080;&#27861;&#36731;&#26494;&#20445;&#30041;&#32534;&#36753;&#37096;&#20998;&#30340;&#29305;&#23450;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22312;&#21333;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#36716;&#25442;&#20026;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#37319;&#26679;&#21069;&#20351;&#29992;&#23545;&#22522;&#30784;&#22270;&#20687;&#28155;&#21152;&#22122;&#22768;&#30340;&#21021;&#22987;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24182;&#22312;&#37319;&#26679;&#21518;&#25554;&#20540;&#30456;&#20851;&#32454;&#33410;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#32534;&#36753;&#25805;&#20316;&#30340;&#36136;&#37327;&#12290;&#32467;&#21512;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;UniTune&#12290;UniTune&#25509;&#25910;&#20219;&#24847;&#22270;&#20687;&#21644;&#25991;&#26412;&#32534;&#36753;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25191;&#34892;&#32534;&#36753;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;Abu-Khzam&#65288;2017&#65289;&#20851;&#20110;(1,1)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#32422;&#31616;&#26041;&#27861;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07722</link><description>&lt;p&gt;
(1,1)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#26159;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#30340;
&lt;/p&gt;
&lt;p&gt;
(1,1)-Cluster Editing is Polynomial-time Solvable. (arXiv:2210.07722v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;Abu-Khzam&#65288;2017&#65289;&#20851;&#20110;(1,1)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#32422;&#31616;&#26041;&#27861;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#22270;H&#26159;&#19968;&#20010;&#22242;&#22270;&#65292;&#37027;&#20040;H&#26159;&#19968;&#20010;&#39030;&#28857;&#19981;&#20132;&#24182;&#30340;&#22242;&#30340;&#32852;&#21512;&#12290;&#22312;2017&#24180;&#65292;Abu-Khzam&#24341;&#20837;&#20102;(a,d)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#19968;&#20010;&#22270;G&#21644;&#39030;&#28857;&#26435;&#37325;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#23450;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#21024;&#38500;&#19981;&#36229;&#36807;&#27599;&#20010;&#39030;&#28857;v&#19978;&#30340;d^*(v)&#26465;&#36793;&#65292;&#24182;&#28155;&#21152;&#19981;&#36229;&#36807;&#27599;&#20010;&#39030;&#28857;v&#19978;&#30340;a^*(v)&#26465;&#36793;&#65292;&#23558;G&#21464;&#25104;&#19968;&#20010;&#38598;&#32676;&#22270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32473;&#20986;&#20102;(a,d)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#22312;P&#25110;NP&#23436;&#20840;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#24615;&#20998;&#30028;&#32447;&#65292;&#21807;&#19968;&#26410;&#35299;&#20915;&#30340;&#24773;&#20917;&#26159;&#24403;a=d=1&#12290;Abu-Khzam&#65288;2017&#65289;&#29468;&#27979;(1,1)-&#38598;&#32676;&#32534;&#36753;&#38382;&#39064;&#26159;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#31995;&#21015;&#22810;&#39033;&#24335;&#26102;&#38388;&#32422;&#31616;&#21040;&#24230;&#19981;&#36229;&#36807;3&#30340;C_3&#21644;C_4_free&#22270;&#30340;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35299;&#20915;Abu-Khzam&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
A graph $H$ is a clique graph if $H$ is a vertex-disjoin union of cliques. Abu-Khzam (2017) introduced the $(a,d)$-{Cluster Editing} problem, where for fixed natural numbers $a,d$, given a graph $G$ and vertex-weights $a^*:\ V(G)\rightarrow \{0,1,\dots, a\}$ and $d^*{}:\ V(G)\rightarrow \{0,1,\dots, d\}$, we are to decide whether $G$ can be turned into a cluster graph by deleting at most $d^*(v)$ edges incident to every $v\in V(G)$ and adding at most $a^*(v)$ edges incident to every $v\in V(G)$. Results by Komusiewicz and Uhlmann (2012) and Abu-Khzam (2017) provided a dichotomy of complexity (in P or NP-complete) of $(a,d)$-{Cluster Editing} for all pairs $a,d$ apart from $a=d=1.$ Abu-Khzam (2017) conjectured that $(1,1)$-{Cluster Editing} is in P. We resolve Abu-Khzam's conjecture in affirmative by (i) providing a serious of five polynomial-time reductions to $C_3$-free and $C_4$-free graphs of maximum degree at most 3, and (ii) designing a polynomial-time algorithm for solving $(1,1)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;IsoVec&#65292;&#19968;&#31181;&#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30456;&#23545;&#21516;&#26500;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#21152;&#20837;&#20840;&#23616;&#21516;&#26500;&#24230;&#24230;&#37327;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#21516;&#26500;&#24615;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05098</link><description>&lt;p&gt;
IsoVec: &#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;IsoVec&#65292;&#19968;&#31181;&#25511;&#21046;&#35789;&#21521;&#37327;&#31354;&#38388;&#30456;&#23545;&#21516;&#26500;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#21152;&#20837;&#20840;&#23616;&#21516;&#26500;&#24230;&#24230;&#37327;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#21516;&#26500;&#24615;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#35821;&#35328;&#35789;&#21521;&#37327;&#31354;&#38388;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#35789;&#20856;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#31354;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#8212;&#8212;&#23427;&#20204;&#30340;&#8220;&#21516;&#26500;&#24230;&#8221;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#26144;&#23556;&#20986;&#29616;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65306;&#21363;&#35789;&#21521;&#37327;&#35757;&#32451;&#23548;&#33268;&#24213;&#23618;&#31354;&#38388;&#19981;&#21516;&#26500;&#12290;&#25105;&#20204;&#23558;&#21516;&#26500;&#24230;&#30340;&#20840;&#23616;&#24230;&#37327;&#30452;&#25509;&#21512;&#24182;&#21040;Skip-gram&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25104;&#21151;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#21518;&#35789;&#21521;&#37327;&#31354;&#38388;&#30340;&#30456;&#23545;&#21516;&#26500;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#23427;&#20204;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#36328;&#35821;&#35328;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26159;&#22312;&#19968;&#33324;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#35757;&#32451;&#31639;&#27861;&#19981;&#30456;&#20284;&#24773;&#20917;&#19979;&#25913;&#21892;&#20102;&#21452;&#35821;&#35789;&#27719;&#34920;&#24402;&#32435;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;https://github.com/kellymarchisio/isovec &#19978;&#21457;&#24067;&#20102;IsoVec&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces -- their degree of "isomorphism." We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the Skip-gram loss function, successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions, under domain mismatch, and with training algorithm dissimilarities. We release IsoVec at https://github.com/kellymarchisio/isovec.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EraseNet&#30340;&#24490;&#29615;&#27531;&#24046;&#32593;&#32476;&#65292;&#29992;&#20110;&#30417;&#30563;&#25991;&#26723;&#28165;&#27927;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20840;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#28165;&#27927;&#33039;&#20081;&#25991;&#26723;&#65292;&#24182;&#33021;&#22815;&#24674;&#22797;&#20855;&#26377;&#32570;&#38519;&#30340;&#25991;&#26723;&#20197;&#25552;&#39640;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00708</link><description>&lt;p&gt;
EraseNet: &#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#25991;&#26723;&#28165;&#27927;&#30340;&#24490;&#29615;&#27531;&#24046;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EraseNet: A Recurrent Residual Network for Supervised Document Cleaning. (arXiv:2210.00708v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EraseNet&#30340;&#24490;&#29615;&#27531;&#24046;&#32593;&#32476;&#65292;&#29992;&#20110;&#30417;&#30563;&#25991;&#26723;&#28165;&#27927;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20840;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#28165;&#27927;&#33039;&#20081;&#25991;&#26723;&#65292;&#24182;&#33021;&#22815;&#24674;&#22797;&#20855;&#26377;&#32570;&#38519;&#30340;&#25991;&#26723;&#20197;&#25552;&#39640;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#21435;&#22122;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20173;&#26377;&#25968;&#30334;&#19975;&#20221;&#24453;&#25968;&#23383;&#21270;&#30340;&#25991;&#26723;&#65292;&#20294;&#26159;&#30001;&#20110;&#33258;&#28982;&#21644;&#20154;&#20026;&#22240;&#32032;&#24341;&#36215;&#30340;&#25991;&#26723;&#36864;&#21270;&#31561;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#26032;&#30340;&#20840;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#28165;&#27927;&#33039;&#20081;&#25991;&#26723;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#24674;&#22797;&#20855;&#26377;&#32570;&#38519;&#30340;&#25991;&#26723;&#65292;&#20363;&#22914;&#30001;&#20110;&#25991;&#26723;&#32769;&#21270;&#24341;&#36215;&#30340;&#21464;&#24418;&#12289;&#22797;&#21360;&#26102;&#30041;&#19979;&#30340;&#25240;&#30165;&#12289;&#38543;&#26426;&#30340;&#40657;&#33394;&#26001;&#28857;&#12289;&#36731;&#24494;&#21487;&#35265;&#30340;&#25991;&#26412;&#31561;&#65292;&#24182;&#19988;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#31995;&#32479;(OCR)&#24615;&#33021;&#12290;&#22312;&#25991;&#26723;&#25104;&#20026;OCR&#31995;&#32479;&#30340;&#36755;&#20837;&#20043;&#21069;&#65292;&#20174;&#25195;&#25551;&#25991;&#26723;&#20013;&#21435;&#38500;&#22122;&#22768;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#22122;&#22768;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;OCR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21508;&#31181;&#24120;&#35265;&#21644;&#19981;&#23547;&#24120;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document denoising is considered one of the most challenging tasks in computer vision. There exist millions of documents that are still to be digitized, but problems like document degradation due to natural and man-made factors make this task very difficult. This paper introduces a supervised approach for cleaning dirty documents using a new fully convolutional auto-encoder architecture. This paper focuses on restoring documents with discrepancies like deformities caused due to aging of a document, creases left on the pages that were xeroxed, random black patches, lightly visible text, etc., and also improving the quality of the image for better optical character recognition system (OCR) performance. Removing noise from scanned documents is a very important step before the documents as this noise can severely affect the performance of an OCR system. The experiments in this paper have shown promising results as the model is able to learn a variety of ordinary as well as unusual noises a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#30111;&#30142;&#35786;&#26029;&#20013;&#25351;&#23548;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#20102;&#29702;&#35299;&#20020;&#24202;&#38656;&#27714;&#21644;&#20219;&#21153;&#30456;&#20851;&#24230;&#37327;&#26631;&#20934;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#35299;&#20915;&#24403;&#21069;ML&#24037;&#20316;&#24573;&#35270;&#20020;&#24202;&#38656;&#27714;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.06947</link><description>&lt;p&gt;
&#29992;&#20110;&#25351;&#23548;&#30111;&#30142;&#35786;&#26029;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#30340;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Metrics to guide development of machine learning algorithms for malaria diagnosis. (arXiv:2209.06947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#30111;&#30142;&#35786;&#26029;&#20013;&#25351;&#23548;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#20102;&#29702;&#35299;&#20020;&#24202;&#38656;&#27714;&#21644;&#20219;&#21153;&#30456;&#20851;&#24230;&#37327;&#26631;&#20934;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#35299;&#20915;&#24403;&#21069;ML&#24037;&#20316;&#24573;&#35270;&#20020;&#24202;&#38656;&#27714;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#30111;&#30142;&#35786;&#26029;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32780;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#20294;&#39640;&#20215;&#20540;&#30340;&#30446;&#26631;&#65292;&#26377;&#25928;&#30340;&#31639;&#27861;&#21487;&#20197;&#25405;&#25937;&#35768;&#22810;&#20799;&#31461;&#30340;&#29983;&#21629;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ML&#30740;&#31350;&#22823;&#22810;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#29992;&#20363;&#32422;&#26463;&#65292;&#22240;&#27492;&#22312;&#20020;&#24202;&#19978;&#27809;&#26377;&#23454;&#29992;&#24615;&#12290;&#20004;&#20010;&#29305;&#21035;&#20851;&#38190;&#30340;&#22240;&#32032;&#26377;&#21161;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;&#20020;&#24202;&#29616;&#22330;&#29615;&#22659;&#30340;&#31639;&#27861;&#65306;&#65288;i&#65289;&#23545;ML&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#65307;&#65288;ii&#65289;&#29992;&#20110;&#25351;&#23548;&#21644;&#35780;&#20272;ML&#27169;&#22411;&#30340;&#20219;&#21153;&#30456;&#20851;&#24230;&#37327;&#26631;&#20934;&#12290;&#24573;&#35270;&#36825;&#20123;&#22240;&#32032;&#20005;&#37325;&#38459;&#30861;&#20102;&#36807;&#21435;&#22312;&#30111;&#30142;&#19978;&#30340;ML&#24037;&#20316;&#65292;&#22240;&#20026;&#30001;&#27492;&#20135;&#29983;&#30340;&#31639;&#27861;&#19982;&#20020;&#24202;&#38656;&#27714;&#19981;&#31526;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;Giemsa&#26579;&#33394;&#34880;&#28082;&#34180;&#33180;&#30340;&#33258;&#21160;&#21270;&#30111;&#30142;&#35786;&#26029;&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20026;&#20160;&#20040;&#19987;&#19994;&#30693;&#35782;&#23545;&#20110;&#26377;&#25928;&#24212;&#29992;ML&#20110;&#30111;&#30142;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#21015;&#20030;&#20102;&#25552;&#20379;&#35813;&#39046;&#22495;&#30693;&#35782;&#30340;&#25216;&#26415;&#25991;&#26723;&#21644;&#20854;&#20182;&#36164;&#28304;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25152;&#38656;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#25351;&#23548;&#21644;&#35780;&#20272;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated malaria diagnosis is a difficult but high-value target for machine learning (ML), and effective algorithms could save many thousands of children's lives. However, current ML efforts largely neglect crucial use case constraints and are thus not clinically useful. Two factors in particular are crucial to developing algorithms translatable to clinical field settings: (i) Clear understanding of the clinical needs that ML solutions must accommodate; and (ii) task-relevant metrics for guiding and evaluating ML models. Neglect of these factors has seriously hampered past ML work on malaria, because the resulting algorithms do not align with clinical needs.  In this paper we address these two issues in the context of automated malaria diagnosis via microscopy on Giemsa-stained blood films. First, we describe why domain expertise is crucial to effectively apply ML to malaria, and list technical documents and other resources that provide this domain knowledge. Second, we detail perform
&lt;/p&gt;</description></item><item><title>PlaStIL&#26159;&#19968;&#31181;&#26080;&#38656;&#20869;&#23384;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26368;&#26089;&#30340;&#22686;&#37327;&#29366;&#24577;&#20013;&#20351;&#29992;&#20923;&#32467;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#20445;&#35777;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#37096;&#20998;&#24494;&#35843;&#27169;&#22411;&#26469;&#24341;&#20837;&#21487;&#22609;&#24615;&#65292;&#20197;&#25214;&#21040;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2209.06606</link><description>&lt;p&gt;
PlaStIL&#65306;&#26080;&#38656;&#20869;&#23384;&#30340;&#31283;&#23450;&#21487;&#22609;&#24615;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06606
&lt;/p&gt;
&lt;p&gt;
PlaStIL&#26159;&#19968;&#31181;&#26080;&#38656;&#20869;&#23384;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26368;&#26089;&#30340;&#22686;&#37327;&#29366;&#24577;&#20013;&#20351;&#29992;&#20923;&#32467;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#20445;&#35777;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#37096;&#20998;&#24494;&#35843;&#27169;&#22411;&#26469;&#24341;&#20837;&#21487;&#22609;&#24615;&#65292;&#20197;&#25214;&#21040;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#38656;&#35201;&#21516;&#26102;&#20855;&#22791;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20197;&#20415;&#20174;&#26032;&#25968;&#25454;&#20013;&#23398;&#20064;&#21516;&#26102;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24403;&#27809;&#26377;&#20869;&#23384;&#32531;&#20914;&#21306;&#21487;&#29992;&#26102;&#65292;&#22312;&#36825;&#20004;&#20010;&#23646;&#24615;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#23588;&#20026;&#25361;&#25112;&#12290;&#20027;&#27969;&#26041;&#27861;&#38656;&#35201;&#23384;&#20648;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#20174;&#20043;&#21069;&#30340;&#22686;&#37327;&#29366;&#24577;&#36827;&#34892;&#24494;&#35843;&#24182;&#38598;&#25104;&#26032;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19982;&#36825;&#20123;&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#65292;&#20294;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20998;&#37197;&#21442;&#25968;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#22312;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#12290;&#22312;&#21021;&#22987;&#29366;&#24577;&#20043;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#24050;&#32463;&#34987;&#36716;&#31227;&#23398;&#20064;&#22686;&#37327;&#26041;&#27861;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#26368;&#26089;&#30340;&#22686;&#37327;&#29366;&#24577;&#20013;&#30340;&#31867;&#20351;&#29992;&#20923;&#32467;&#30340;&#25552;&#21462;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#26368;&#36817;&#30340;&#31867;&#21035;&#20351;&#29992;&#37096;&#20998;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#24341;&#20837;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21487;&#22609;&#24615;&#23618;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity and stability are needed in class-incremental learning in order to learn from new data while preserving past knowledge. Due to catastrophic forgetting, finding a compromise between these two properties is particularly challenging when no memory buffer is available. Mainstream methods need to store two deep models since they integrate new classes using fine-tuning with knowledge distillation from the previous incremental state. We propose a method which has similar number of parameters but distributes them differently in order to find a better balance between plasticity and stability. Following an approach already deployed by transfer-based incremental methods, we freeze the feature extractor after the initial state. Classes in the oldest incremental states are trained with this frozen extractor to ensure stability. Recent classes are predicted using partially fine-tuned models in order to introduce plasticity. Our proposed plasticity layer can be incorporated to any transfer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2209.06356</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#31616;&#21333;&#29366;&#24577;-&#21160;&#20316;&#25277;&#35937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#36805;&#36895;&#25512;&#26029;&#20986;&#31561;&#20215;&#22870;&#21169;&#21644;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#21453;&#22797;&#35797;&#38169;&#26469;&#23398;&#20064;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#30340;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#30340;&#35266;&#23519;MDP&#31616;&#21270;&#20026;&#25277;&#35937;MDP&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#24403;&#21487;&#20197;&#20107;&#20808;&#26500;&#24314;&#36866;&#24403;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#26102;&#65292;&#21487;&#20197;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#65292;&#36890;&#24120;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#21516;&#24577;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#21738;&#20123;&#29366;&#24577;&#21160;&#20316;&#23545;&#23548;&#33268;&#30456;&#21516;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#37319;&#29992;&#20915;&#31574;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#30340;&#20998;&#31867;&#22120;&#27604;&#36739;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#26131;&#22788;&#29702;&#30340;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#30340;&#20004;&#26679;&#26412;&#35266;&#23519;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2209.01857</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#32479;&#35745;&#27604;&#36739;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Statistical Comparisons of Classifiers by Generalized Stochastic Dominance. (arXiv:2209.01857v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#37319;&#29992;&#20915;&#31574;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#30340;&#20998;&#31867;&#22120;&#27604;&#36739;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#26131;&#22788;&#29702;&#30340;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#30340;&#20004;&#26679;&#26412;&#35266;&#23519;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21457;&#23637;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26681;&#25454;&#22810;&#20010;&#26631;&#20934;&#27604;&#36739;&#20998;&#31867;&#22120;&#20173;&#28982;&#27809;&#26377;&#20849;&#35782;&#12290;&#27599;&#20010;&#27604;&#36739;&#26694;&#26550;&#37117;&#38754;&#20020;&#33267;&#23569;&#19977;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;&#36136;&#37327;&#26631;&#20934;&#30340;&#22810;&#26679;&#24615;&#65292;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;&#36873;&#25321;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#26368;&#36817;&#22312;&#20915;&#31574;&#29702;&#35770;&#20013;&#30340;&#21457;&#23637;&#65292;&#20026;&#36825;&#22330;&#28608;&#28872;&#30340;&#36777;&#35770;&#22686;&#28155;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#22522;&#20110;&#25152;&#35859;&#30340;&#20559;&#22909;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#30340;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#24378;&#22823;&#22320;&#32469;&#36807;&#20102;&#32321;&#29712;&#19988;&#24448;&#24448;&#33258;&#30456;&#30683;&#30462;&#30340;&#32858;&#21512;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24191;&#20041;&#38543;&#26426;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#26131;&#22788;&#29702;&#30340;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#36866;&#24212;&#30340;&#20004;&#26679;&#26412;&#35266;&#23519;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#12290;&#36825;&#30830;&#23454;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although being a crucial question for the development of machine learning algorithms, there is still no consensus on how to compare classifiers over multiple data sets with respect to several criteria. Every comparison framework is confronted with (at least) three fundamental challenges: the multiplicity of quality criteria, the multiplicity of data sets and the randomness of the selection of data sets. In this paper, we add a fresh view to the vivid debate by adopting recent developments in decision theory. Based on so-called preference systems, our framework ranks classifiers by a generalized concept of stochastic dominance, which powerfully circumvents the cumbersome, and often even self-contradictory, reliance on aggregates. Moreover, we show that generalized stochastic dominance can be operationalized by solving easy-to-handle linear programs and moreover statistically tested employing an adapted two-sample observation-randomization test. This yields indeed a powerful framework fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#32463;&#20856;&#25968;&#25454;&#19978;&#25214;&#21040;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#27604;&#20219;&#20309;&#32463;&#20856;&#23398;&#20064;&#31639;&#27861;&#26356;&#24555;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#35782;&#21035;&#36825;&#26679;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#23450;&#20041;&#30340;&#24494;&#22937;&#21464;&#21270;&#21487;&#20197;&#23548;&#33268;&#27010;&#24565;&#19978;&#26174;&#30528;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31163;&#25110;&#26080;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30740;&#31350;&#24050;&#26377;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#37327;&#23376;&#21152;&#36895;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#28860;&#20986;&#20102;&#19968;&#32452;&#26356;&#36890;&#29992;&#21644;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20197;&#23637;&#31034;&#32463;&#20856;&#21644;&#37327;&#23376;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2208.06339</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#32463;&#20856;&#25968;&#25454;&#19978;&#24314;&#31435;&#32463;&#20856;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#23398;&#20064;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On establishing learning separations between classical and quantum machine learning with classical data. (arXiv:2208.06339v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#32463;&#20856;&#25968;&#25454;&#19978;&#25214;&#21040;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#27604;&#20219;&#20309;&#32463;&#20856;&#23398;&#20064;&#31639;&#27861;&#26356;&#24555;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#35782;&#21035;&#36825;&#26679;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#23450;&#20041;&#30340;&#24494;&#22937;&#21464;&#21270;&#21487;&#20197;&#23548;&#33268;&#27010;&#24565;&#19978;&#26174;&#30528;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31163;&#25110;&#26080;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30740;&#31350;&#24050;&#26377;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#37327;&#23376;&#21152;&#36895;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#28860;&#20986;&#20102;&#19968;&#32452;&#26356;&#36890;&#29992;&#21644;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20197;&#23637;&#31034;&#32463;&#20856;&#21644;&#37327;&#23376;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32463;&#36807;&#22810;&#24180;&#21162;&#21147;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20165;&#33021;&#22312;&#26576;&#20123;&#20570;&#20316;&#30340;&#23494;&#30721;&#23398;&#21551;&#21457;&#25968;&#25454;&#38598;&#30340;&#32463;&#20856;&#25968;&#25454;&#24773;&#20917;&#19979;&#23637;&#31034;&#37327;&#23376;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23547;&#25214;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#27604;&#20219;&#20309;&#32463;&#20856;&#23398;&#20064;&#31639;&#27861;&#26356;&#24555;&#23398;&#20064;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#35782;&#21035;&#27492;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21453;&#24605;&#20102;&#19982;&#27492;&#38382;&#39064;&#30456;&#20851;&#30340;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#23450;&#20041;&#19978;&#30340;&#24494;&#22937;&#21464;&#21270;&#22914;&#20309;&#24847;&#21619;&#30528;&#27010;&#24565;&#19978;&#26174;&#30528;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#23548;&#33268;&#20998;&#31163;&#25110;&#23436;&#20840;&#26080;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#37327;&#23376;&#21152;&#36895;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#25552;&#28860;&#20986;&#19968;&#32452;&#26356;&#36890;&#29992;&#21644;&#20805;&#20998;&#26465;&#20214;&#65288;&#21363;&#8220;&#26816;&#26597;&#28165;&#21333;&#8221;&#65289;&#65292;&#20197;&#23637;&#31034;&#23398;&#20064;&#38382;&#39064;&#22312;&#32463;&#20856;&#21644;&#37327;&#23376;&#23398;&#20064;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#26816;&#26597;&#28165;&#21333;&#26088;&#22312;&#31616;&#21270;&#23545;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite years of effort, the quantum machine learning community has only been able to show quantum learning advantages for certain contrived cryptography-inspired datasets in the case of classical data. In this note, we discuss the challenges of finding learning problems that quantum learning algorithms can learn much faster than any classical learning algorithm, and we study how to identify such learning problems. Specifically, we reflect on the main concepts in computational learning theory pertaining to this question, and we discuss how subtle changes in definitions can mean conceptually significantly different tasks, which can either lead to a separation or no separation at all. Moreover, we study existing learning problems with a provable quantum speedup to distill sets of more general and sufficient conditions (i.e., ``checklists'') for a learning problem to exhibit a separation between classical and quantum learners. These checklists are intended to streamline one's approach to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03341</link><description>&lt;p&gt;
&#26080;Softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#22312;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#20013;&#36215;&#21040;&#20102;&#25512;&#21160;&#20316;&#29992;&#12290;ViTs&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#22312;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#36924;&#36817;&#33258;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#30340;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#35201;&#20040;&#22312;&#29702;&#35770;&#19978;&#26377;&#32570;&#38519;&#65292;&#35201;&#20040;&#22312;&#23454;&#36341;&#20013;&#26080;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26469;&#28304;&#20110;&#22312;&#36924;&#36817;&#36807;&#31243;&#20013;&#32487;&#25215;&#20102;&#22522;&#20110;softmax&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#20351;&#29992;softmax&#20989;&#25968;&#23545;&#20196;&#29260;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;&#28857;&#31215;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#20010;softmax&#25805;&#20316;&#65292;&#25361;&#25112;&#20102;&#20219;&#20309;&#21518;&#32493;&#30340;&#32447;&#24615;&#21270;&#24037;&#20316;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;softmax&#30340;&#21464;&#25442;&#22120;(SOFT)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#26367;&#20195;&#28857;&#31215;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#33258;&#27880;&#24847;&#30697;&#38453;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#38543;&#26426;&#31614;&#21040;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#25918;&#22823;&#25552;&#39640;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#38544;&#31169;&#35745;&#31639;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#27861;&#35780;&#20272;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#36890;&#29992;&#30340;&#27927;&#29260;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.03151</link><description>&lt;p&gt;
&#38543;&#26426;&#31614;&#21040;&#30340;&#38544;&#31169;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Shuffled Check-Ins. (arXiv:2206.03151v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#38543;&#26426;&#31614;&#21040;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#25918;&#22823;&#25552;&#39640;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#38544;&#31169;&#35745;&#31639;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#27861;&#35780;&#20272;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#36890;&#29992;&#30340;&#27927;&#29260;&#26426;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#38543;&#26426;&#31614;&#21040;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21327;&#35758;&#65292;&#23427;&#22312;&#27809;&#26377;&#20219;&#20309;&#20854;&#20182;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#65292;&#38543;&#26426;&#31614;&#21040;&#20801;&#35768;&#23458;&#25143;&#31471;&#29420;&#31435;&#21644;&#38543;&#26426;&#22320;&#20915;&#23450;&#21442;&#19982;&#35745;&#31639;&#65292;&#28040;&#38500;&#20102;&#26381;&#21153;&#22120;&#21457;&#36215;&#30340;&#23376;&#25277;&#26679;&#30340;&#38656;&#35201;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#35777;&#26126;&#38543;&#26426;&#31614;&#21040;&#36890;&#36807;&#38544;&#31169;&#25918;&#22823;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#38544;&#31169;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#27861;&#26469;&#36319;&#36394;&#36890;&#29992;&#27927;&#29260;&#26426;&#21046;&#30340;&#38544;&#31169;&#24615;&#33021;&#65292;&#21253;&#25324;&#39640;&#26031;&#26426;&#21046;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23616;&#37096;/&#27927;&#29260;&#27169;&#22411;&#19979;&#30340;&#36890;&#29992;&#26426;&#21046;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#35777;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a protocol for distributed computation called shuffled check-in, which achieves strong privacy guarantees without requiring any further trust assumptions beyond a trusted shuffler. Unlike most existing work, shuffled check-in allows clients to make independent and random decisions to participate in the computation, removing the need for server-initiated subsampling. Leveraging differential privacy, we show that shuffled check-in achieves tight privacy guarantees through privacy amplification, with a novel analysis based on R{\'e}nyi differential privacy that improves privacy accounting over existing work. We also introduce a numerical approach to track the privacy of generic shuffling mechanisms, including Gaussian mechanism, which is the first evaluation of a generic mechanism under the distributed setting within the local/shuffle model in the literature. Empirical studies are also given to demonstrate the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30456;&#23218;&#32654;&#65292;&#26263;&#31034;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.10652</link><description>&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30495;&#30340;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30456;&#23218;&#32654;&#65292;&#26263;&#31034;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#21019;&#24314;&#21644;&#32500;&#25252;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;KG&#20063;&#36828;&#26410;&#23436;&#22791;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#24050;&#25104;&#20026;KG&#30740;&#31350;&#20013;&#26368;&#20851;&#38190;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#65288;&#22270;&#65289;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#33258;&#28982;&#24402;&#22240;&#20110;&#30456;&#27604;&#20110;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#32452;&#20214;&#30340;MPNNs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#31616;&#21333;&#30340;MLP&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;MPNNs&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;MP&#21487;&#33021;&#24182;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20180;&#32454;&#30340;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;KGC&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#12290;&#36825;&#34920;&#26126;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#23545;&#35780;&#20998;&#20989;&#25968;&#35774;&#35745;&#12289;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;MP&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#39564;&#23460;&#21512;&#25104;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#21306;&#22359;&#38142;&#33410;&#28857;&#20849;&#20139;&#23398;&#20064;&#30693;&#35782;&#20197;&#26816;&#27979;&#25915;&#20987;&#65292;&#24182;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.11076</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning for Cyberattack Detection in Blockchain Networks. (arXiv:2203.11076v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#36890;&#36807;&#23454;&#39564;&#23460;&#21512;&#25104;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#20801;&#35768;&#21306;&#22359;&#38142;&#33410;&#28857;&#20849;&#20139;&#23398;&#20064;&#30693;&#35782;&#20197;&#26816;&#27979;&#25915;&#20987;&#65292;&#24182;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20837;&#20405;&#25915;&#20987;&#65292;&#24182;&#20026;&#21306;&#22359;&#38142;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#22312;&#23454;&#39564;&#23460;&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#19968;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23558;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#30340;&#27969;&#37327;&#25968;&#25454;&#65288;&#21253;&#25324;&#27491;&#24120;&#25968;&#25454;&#21644;&#25915;&#20987;&#25968;&#25454;&#65289;&#65292;&#20197;&#29992;&#20110;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#23454;&#26102;&#23454;&#39564;&#65292;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#20837;&#20405;&#26816;&#27979;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#23454;&#39564;&#23460;&#20013;&#21512;&#25104;&#30340;&#29992;&#20110;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21306;&#22359;&#38142;&#32593;&#32476;&#20013;&#23454;&#29616;&#39640;&#25928;&#37096;&#32626;&#20197;&#26816;&#27979;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#21306;&#22359;&#38142;&#33410;&#28857;&#33021;&#22815;&#20027;&#21160;&#25910;&#38598;&#25968;&#25454;&#65292;&#20849;&#20139;&#20174;&#20854;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#19982;&#32593;&#32476;&#20013;&#30340;&#20854;&#20182;&#21306;&#22359;&#38142;&#33410;&#28857;&#20132;&#25442;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#19981;&#20165;&#33021;&#22815;&#21033;&#29992;&#20854;&#20182;&#33410;&#28857;&#30340;&#30693;&#35782;&#36827;&#34892;&#25915;&#20987;&#26816;&#27979;&#65292;&#36824;&#33021;&#22815;&#22686;&#24378;&#25972;&#20010;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to study intrusion attacks and then develop a novel cyberattack detection framework for blockchain networks. Specifically, we first design and implement a blockchain network in our laboratory. This blockchain network will serve two purposes, i.e., to generate the real traffic data (including both normal data and attack data) for our learning models and implement real-time experiments to evaluate the performance of our proposed intrusion detection framework. To the best of our knowledge, this is the first dataset that is synthesized in a laboratory for cyberattacks in a blockchain network. We then propose a novel collaborative learning model that allows efficient deployment in the blockchain network to detect attacks. The main idea of the proposed learning model is to enable blockchain nodes to actively collect data, share the knowledge learned from its data, and then exchange the knowledge with other blockchain nodes in the network. In this way, we can not only levera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#65292;&#32780;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.12888</link><description>&lt;p&gt;
&#29992;&#20110;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#65292;&#32780;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#19968;&#31995;&#21015;&#36172;&#21338;&#26426;&#20219;&#21153;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#20174;&#19968;&#20010;&#26410;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#20013;&#29420;&#31435;&#37319;&#26679;&#30340;&#65292;&#24182;&#23398;&#20064;&#20854;&#20803;&#21442;&#25968;&#20197;&#22312;&#26410;&#26469;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#35774;&#32622;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#20803;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20854;&#22312;$m$&#20010;&#36172;&#21338;&#26426;&#20219;&#21153;&#20013;&#65292;&#26102;&#38388;&#30028;&#20026;$n$&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#20165;&#20026;$\tilde{O}(m / \sqrt{n})$&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#29575;&#27966;&#31639;&#27861;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#20026;$\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$&#12290;&#23613;&#31649;&#36951;&#25022;&#26356;&#22823;&#65292;&#20294;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20803;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#20960;&#31867;&#36172;&#21338;&#26426;&#38382;&#39064;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25913;&#21892;&#25345;&#32493;&#24615;&#33021;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.09826</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25913;&#21892;&#25345;&#32493;&#24615;&#33021;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#19978;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#26041;&#27861;&#35797;&#22270;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20551;&#35774;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21482;&#26377;&#19968;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#25345;&#32493;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#38598;&#25104;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290;&#21463;&#21040;&#36825;&#20010;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20811;&#26381;&#38598;&#25104;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23376;&#31354;&#38388;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#20294;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, ensembles' training and inference costs can increase significantly as the number of models grows. Motivated by this limitation, we study different ensemble models to understand their benefits and drawbacks in continual learning scenarios. Finally, to overcome the high compute cost of ensembles, we leverage recent advances in neural network subspace to propose a computationally cheap algorithm with similar runtime to a single model yet enjoying the performance benefits of ensembles.
&lt;/p&gt;</description></item><item><title>SAITS&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.08516</link><description>&lt;p&gt;
SAITS: &#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08516
&lt;/p&gt;
&lt;p&gt;
SAITS&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#24120;&#24120;&#25104;&#20026;&#28145;&#20837;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;&#25554;&#20540;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#30830;&#23450;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32570;&#22833;&#20540;&#25554;&#20540;&#26041;&#27861;&#8212;&#8212;SAITS&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;SAITS&#36890;&#36807;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#23398;&#20064;&#32570;&#22833;&#20540;&#12290;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#21487;&#20197;&#26126;&#30830;&#22320;&#25429;&#25417;&#26102;&#38388;&#27493;&#38271;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25554;&#20540;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#21516;&#26102;&#65292;&#21152;&#26435;&#32452;&#21512;&#35774;&#35745;&#20351;&#24471;SAITS&#33021;&#22815;&#26681;&#25454;&#27880;&#24847;&#21147;&#22270;&#21644;&#32570;&#22833;&#20449;&#24687;&#21160;&#24577;&#22320;&#20998;&#37197;&#26469;&#33258;&#20004;&#20010;&#23545;&#35282;&#32447;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#22359;&#30340;&#23398;&#20064;&#34920;&#31034;&#30340;&#26435;&#37325;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SAITS&#22312;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32452;&#21512;&#24615;&#23450;&#20041;&#20026;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#24182;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2201.12926</link><description>&lt;p&gt;
&#12298;&#32452;&#21512;&#24615;&#20316;&#20026;&#35789;&#27719;&#23545;&#31216;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32452;&#21512;&#24615;&#23450;&#20041;&#20026;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#36716;&#25442;&#24182;&#24212;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#35299;&#26512;&#12289;&#25351;&#20196;&#36981;&#24490;&#21644;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#20013;&#65292;&#26631;&#20934;&#30340;&#28145;&#24230;&#32593;&#32476;&#22312;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#26102;&#20250;&#22833;&#36133;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#21477;&#23376;&#35299;&#37322;&#30340;&#32452;&#21512;&#36807;&#31243;&#30340;&#27169;&#22411;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#36890;&#29992;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#32452;&#21512;&#24615;&#24418;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#25968;&#25454;&#20998;&#24067;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#32780;&#19981;&#26159;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#26080;&#35770;&#20309;&#26102;&#19968;&#20010;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#32452;&#21512;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#8212;&#8212;&#23558;&#31034;&#20363;&#36716;&#25442;&#20026;&#20854;&#20182;&#21512;&#36866;&#31034;&#20363;&#30340;&#36807;&#31243;&#8212;&#8212;&#21487;&#20197;&#20026;&#35299;&#20915;&#30456;&#21516;&#20219;&#21153;&#30340;&#20219;&#20309;&#35757;&#32451;&#27169;&#22411;&#36171;&#20104;&#32452;&#21512;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#36716;&#25442;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26222;&#36890;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#36807;&#31243;&#65292;&#31216;&#20026;LEXSYM&#12290;&#19982;&#29616;&#26377;&#30340;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#19981;&#21516;&#65292;LEXSYM&#21487;&#20197;&#34987;&#24555;&#36895;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26679;&#26412;&#30697;&#26041;&#27861;&#36827;&#34892;&#38750;&#20256;&#32479;&#21442;&#25968;&#21270;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#26041;Hellinger&#36317;&#31163;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#20984;&#20248;&#21270;&#24471;&#21040;&#21807;&#19968;&#35299;&#12290;&#36890;&#36807;&#24130;&#30697;&#20272;&#35745;&#25552;&#20986;&#20102;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#36136;&#21644;&#28176;&#36817;&#35823;&#24046;&#19978;&#30028;&#65292;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.04786</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#26679;&#26412;&#30697;&#26041;&#27861;&#30340;&#38750;&#20256;&#32479;&#21442;&#25968;&#21270;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Non-Classical Parameterization for Density Estimation Using Sample Moments. (arXiv:2201.04786v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26679;&#26412;&#30697;&#26041;&#27861;&#36827;&#34892;&#38750;&#20256;&#32479;&#21442;&#25968;&#21270;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#26041;Hellinger&#36317;&#31163;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#20984;&#20248;&#21270;&#24471;&#21040;&#21807;&#19968;&#35299;&#12290;&#36890;&#36807;&#24130;&#30697;&#20272;&#35745;&#25552;&#20986;&#20102;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#36136;&#21644;&#28176;&#36817;&#35823;&#24046;&#19978;&#30028;&#65292;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#26159;&#32479;&#35745;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#30697;&#26041;&#27861;&#26159;&#23494;&#24230;&#20272;&#35745;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#24378;&#28872;&#20381;&#36182;&#20110;&#21487;&#34892;&#20989;&#25968;&#30340;&#36873;&#25321;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26679;&#26412;&#30697;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#30340;&#38750;&#20256;&#32479;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36873;&#25321;&#36825;&#26679;&#30340;&#20989;&#25968;&#12290;&#35813;&#21442;&#25968;&#21270;&#26159;&#30001;&#24179;&#26041;Hellinger&#36317;&#31163;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#23427;&#30340;&#35299;&#34987;&#35777;&#26126;&#22312;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#31616;&#21333;&#20808;&#39564;&#26465;&#20214;&#19979;&#23384;&#22312;&#24182;&#19988;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20984;&#20248;&#21270;&#24471;&#21040;&#12290;&#36890;&#36807;&#24130;&#30697;&#20272;&#35745;&#25552;&#20986;&#20102;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#20197;&#21450;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#35823;&#24046;&#19978;&#30028;&#12290;&#32473;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#23494;&#24230;&#20272;&#35745;&#22120;&#22312;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#19982;&#20960;&#31181;&#27969;&#34892;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#27169;&#25311;&#32467;&#26524;&#39564;&#35777;&#20102;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability density estimation is a core problem of statistics and signal processing. Moment methods are an important means of density estimation, but they are generally strongly dependent on the choice of feasible functions, which severely affects the performance. In this paper, we propose a non-classical parametrization for density estimation using sample moments, which does not require the choice of such functions. The parametrization is induced by the squared Hellinger distance, and the solution of it, which is proved to exist and be unique subject to a simple prior that does not depend on data, and can be obtained by convex optimization. Statistical properties of the density estimator, together with an asymptotic error upper bound are proposed for the estimator by power moments. Applications of the proposed density estimator in signal processing tasks are given. Simulation results validate the performance of the estimator by a comparison to several prevailing methods. To the best 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.08645</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23398;&#20064;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Models Through Multi-Objective Neural Architecture Search. (arXiv:2112.08645v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#26080;&#21487;&#32622;&#30097;&#65292;&#20294;&#20854;&#26550;&#26500;&#35774;&#35745;&#21644;&#21487;&#35299;&#37322;&#24615;&#21364;&#24182;&#38750;&#26131;&#20107;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#20351;&#36825;&#20123;&#26041;&#27861;&#26356;&#20026;&#23454;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;NAS&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#8220;&#21487;&#35270;&#35782;&#21035;&#24615;&#8221;&#65292;&#36825;&#26159;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#20195;&#29702;&#24230;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;&#35813;&#26694;&#26550;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monumental advances in deep learning have led to unprecedented achievements across various domains. While the performance of deep neural networks is indubitable, the architectural design and interpretability of such models are nontrivial. Research has been introduced to automate the design of neural network architectures through neural architecture search (NAS). Recent progress has made these methods more pragmatic by exploiting distributed computation and novel optimization algorithms. However, there is little work in optimizing architectures for interpretability. To this end, we propose a multi-objective distributed NAS framework that optimizes for both task performance and "introspectability," a surrogate metric for aspects of interpretability. We leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable AI (XAI) techniques to reward architectures that can be better comprehended by domain experts. The framework is evaluated on several image classification datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26126;&#30830;&#20559;&#35265;&#26631;&#31614;&#25110;&#26080;&#20559;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20559;&#35265;&#36716;&#21270;&#65292;&#20351;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#23558;&#19968;&#20010;&#20559;&#35265;&#27169;&#24335;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#65292;&#21516;&#26102;&#20445;&#30041;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#23545;&#27604;&#21435;&#20559;&#25191;&#12290;</title><link>http://arxiv.org/abs/2112.01021</link><description>&lt;p&gt;
&#20197;&#28779;&#25915;&#25932;&#65306;&#36890;&#36807;&#29983;&#25104;&#20559;&#35265;&#36716;&#21270;&#36827;&#34892;&#23545;&#27604;&#21435;&#20559;&#25191;&#65292;&#26080;&#38656;&#26080;&#20559;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation. (arXiv:2112.01021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26126;&#30830;&#20559;&#35265;&#26631;&#31614;&#25110;&#26080;&#20559;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20559;&#35265;&#36716;&#21270;&#65292;&#20351;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#23558;&#19968;&#20010;&#20559;&#35265;&#27169;&#24335;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#65292;&#21516;&#26102;&#20445;&#30041;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#23545;&#27604;&#21435;&#20559;&#25191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34429;&#28982;&#33021;&#22815;&#20197;&#36229;&#36807;&#23481;&#37327;&#30340;&#32593;&#32476;&#36827;&#34892;&#27867;&#21270;&#65292;&#20294;&#22312;&#21028;&#21035;&#20219;&#21153;&#20013;&#24120;&#24120;&#20381;&#36182;&#24694;&#24615;&#20559;&#35265;&#20316;&#20026;&#36817;&#36947;&#65292;&#32780;&#19981;&#26159;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#19982;&#20559;&#35265;&#30456;&#20851;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#20294;&#36825;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#33719;&#24471;&#65292;&#25110;&#32773;&#31579;&#36873;&#20986;&#23569;&#37327;&#30340;&#26080;&#20559;&#26679;&#26412;&#36827;&#34892;&#21435;&#20559;&#25191;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#24182;&#19981;&#24635;&#26159;&#33021;&#24471;&#21040;&#20445;&#35777;&#65292;&#22240;&#20026;&#20551;&#35774;&#19981;&#19968;&#23450;&#33021;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#29983;&#25104;&#20559;&#35265;&#36716;&#25442;&#30340;&#23545;&#27604;&#21435;&#20559;&#25191;&#65288;CDvG&#65289;&#65292;&#23427;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#20559;&#35265;&#26631;&#31614;&#25110;&#26080;&#20559;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#19981;&#20165;&#21028;&#21035;&#27169;&#22411;&#65292;&#32780;&#19988;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#20063;&#20542;&#21521;&#20110;&#20851;&#27880;&#24694;&#24615;&#20559;&#35265;&#65292;CDvG&#21033;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#23558;&#19968;&#20010;&#20559;&#35265;&#27169;&#24335;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#20559;&#35265;&#27169;&#24335;&#65292;&#21516;&#26102;&#20445;&#30041;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20559;&#35265;&#36716;&#25442;&#30340;&#35270;&#22270;&#30456;&#20114;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), despite their impressive ability to generalize over-capacity networks, often rely heavily on malignant bias as shortcuts instead of task-related information for discriminative tasks. To address this problem, recent studies utilize auxiliary information related to the bias, which is rarely obtainable in practice, or sift through a handful of bias-free samples for debiasing. However, the success of these methods is not always guaranteed due to the unfulfilled presumptions. In this paper, we propose a novel method, Contrastive Debiasing via Generative Bias-transformation (CDvG), which works without explicit bias labels or bias-free samples. Motivated by our observation that not only discriminative models but also image translation models tend to focus on the malignant bias, CDvG employs an image translation model to transform one bias mode into another while preserving the task-relevant information. Additionally, the bias-transformed views are set against each
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20250;&#23548;&#33268;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#29109;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.15430</link><description>&lt;p&gt;
&#36793;&#38469;&#30340;&#39764;&#39740;&#65306;&#22522;&#20110;&#36793;&#38469;&#30340;&#26631;&#31614;&#24179;&#28369;&#29992;&#20110;&#32593;&#32476;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.15430
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20250;&#23548;&#33268;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#29109;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#30528;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#24448;&#24448;&#26657;&#20934;&#19981;&#33391;&#65292;&#23548;&#33268;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36807;&#25311;&#21512;&#20250;&#21152;&#21095;&#26657;&#20934;&#19981;&#33391;&#29616;&#35937;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20132;&#21449;&#29109;&#30340;&#26368;&#23567;&#21270;&#20250;&#20419;&#20351;&#39044;&#27979;&#30340; softmax &#27010;&#29575;&#19982;&#29420;&#28909;&#26631;&#31614;&#20998;&#37197;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#20351;&#24471;&#27491;&#30830;&#31867;&#21035;&#30340;&#39044; softmax &#28608;&#27963;&#36828;&#22823;&#20110;&#20854;&#20182;&#28608;&#27963;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35777;&#25454;&#34920;&#26126;&#65292;&#34164;&#21547;&#38544;&#24335;&#25110;&#26174;&#24335;&#26368;&#22823;&#29109;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#33021;&#22815;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#26657;&#20934;&#25439;&#22833;&#30340;&#32479;&#19968;&#32422;&#26463;&#26368;&#20248;&#21270;&#35266;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#25439;&#22833;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#20301;&#21183;&#36317;&#31163;&#26045;&#21152;&#30456;&#31561;&#32422;&#26463;&#30340;&#32447;&#24615;&#24809;&#32602;&#65288;&#25110;&#25289;&#26684;&#26391;&#26085;&#20989;&#25968;&#65289;&#30340;&#36817;&#20284;&#12290;&#36825;&#25351;&#20986;&#20102;&#36825;&#31181;&#24213;&#23618;&#30456;&#31561;&#32422;&#26463;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36127;&#24863;&#30693;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#20010;&#38543;&#26426;&#27169;&#22411;&#30740;&#31350;&#20102;&#22312;&#22823;&#25968;&#25454;&#26465;&#20214;&#19979;&#26368;&#22823;&#36127;&#38388;&#38548;&#30340;&#19978;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2110.15824</link><description>&lt;p&gt;
&#20174;&#36807;&#21442;&#25968;&#21270;&#21040;&#21487;&#22788;&#29702;&#24615;: &#36127;&#24863;&#30693;&#26426;&#30340;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
Tractability from overparametrization: The example of the negative perceptron. (arXiv:2110.15824v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36127;&#24863;&#30693;&#26426;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#20010;&#38543;&#26426;&#27169;&#22411;&#30740;&#31350;&#20102;&#22312;&#22823;&#25968;&#25454;&#26465;&#20214;&#19979;&#26368;&#22823;&#36127;&#38388;&#38548;&#30340;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36127;&#24863;&#30693;&#26426;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#23450;&#20102;$n$&#20010;&#25968;&#25454;&#28857;$({\boldsymbol x}_i,y_i)$&#65292;&#20854;&#20013;${\boldsymbol x}_i$&#26159;&#19968;&#20010;$d$&#32500;&#21521;&#37327;&#65292;$y_i\in\{+1,-1\}$&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#26631;&#31614;&#12290;&#25968;&#25454;&#19981;&#26159;&#32447;&#24615;&#21487;&#20998;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#28385;&#36275;&#20110;&#25214;&#21040;&#20855;&#26377;&#26368;&#22823;\emph{&#36127;}&#38388;&#38548;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19968;&#20010;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;${\boldsymbol \theta}$&#65292;&#20351;&#24471;$\min_{i\le n}y_i\langle {\boldsymbol \theta},{\boldsymbol x}_i\rangle$&#26368;&#22823;&#21270;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;(&#30456;&#24403;&#20110;&#22312;&#19968;&#20010;&#22810;&#38754;&#20307;&#20013;&#25214;&#21040;&#26368;&#22823;&#33539;&#25968;&#21521;&#37327;)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20004;&#20010;&#38543;&#26426;&#27169;&#22411;&#19979;&#35813;&#38382;&#39064;&#30340;&#20856;&#22411;&#24615;&#36136;&#12290;&#25105;&#20204;&#32771;&#34385;&#24403;$n,d\to \infty$&#19988;$n/d\to\delta$&#26102;&#30340;&#27604;&#20363;&#28176;&#36817;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#22823;&#38388;&#38548;$\kappa_{\text{s}}(\delta)$&#25110;&#31561;&#20215;&#22320;&#20854;&#36870;&#20989;&#25968;$\delta_{\text{s}}(\kappa)$&#30340;&#19978;&#19979;&#30028;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;$\delta_{\text{s}}(\kappa)$&#26159;&#36807;&#21442;&#25968;&#21270;&#38408;&#20540;&#30340;&#19968;&#20010;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the negative perceptron problem we are given $n$ data points $({\boldsymbol x}_i,y_i)$, where ${\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\in\{+1,-1\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \emph{negative} margin. In other words, we want to find a unit norm vector ${\boldsymbol \theta}$ that maximizes $\min_{i\le n}y_i\langle {\boldsymbol \theta},{\boldsymbol x}_i\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data.  We consider the proportional asymptotics in which $n,d\to \infty$ with $n/d\to\delta$, and prove upper and lower bounds on the maximum margin $\kappa_{\text{s}}(\delta)$ or -- equivalently -- on its inverse function $\delta_{\text{s}}(\kappa)$. In other words, $\delta_{\text{s}}(\kappa)$ is the overparametrization thresho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#36873;&#25321;&#30340;&#920;&#21644;&#957;&#19979;&#65292;&#33539;&#25968;&#27979;&#35797;&#21644;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#26159;&#31561;&#20215;&#30340;&#65292;&#21516;&#26102;&#25351;&#20986;&#22312;&#26368;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#21487;&#20197;&#20687;&#33539;&#25968;&#27979;&#35797;&#19968;&#26679;&#24265;&#20215;&#12290;</title><link>http://arxiv.org/abs/2109.10933</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#19981;&#21516;&#33258;&#36866;&#24212;&#25209;&#37327;&#22823;&#23567;&#36873;&#25321;&#31574;&#30053;&#30340;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods. (arXiv:2109.10933v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#36873;&#25321;&#30340;&#920;&#21644;&#957;&#19979;&#65292;&#33539;&#25968;&#27979;&#35797;&#21644;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#26159;&#31561;&#20215;&#30340;&#65292;&#21516;&#26102;&#25351;&#20986;&#22312;&#26368;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#21487;&#20197;&#20687;&#33539;&#25968;&#27979;&#35797;&#19968;&#26679;&#24265;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#36873;&#25321;&#30340;&#920;&#21644;&#957;&#19979;&#65292;\cite{Bol18}&#20013;&#25552;&#20986;&#30340;&#33539;&#25968;&#27979;&#35797;&#21644;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#26159;&#31561;&#20215;&#30340;&#65292;&#20854;&#20013;&#1013;&#178;=&#952;&#178;+&#957;&#178;&#12290;&#36825;&#37324;&#65292;&#1013;&#25511;&#21046;&#26799;&#24230;&#33539;&#25968;&#30340;&#30456;&#23545;&#32479;&#35745;&#35823;&#24046;&#65292;&#32780;&#952;&#21644;&#957;&#20998;&#21035;&#25511;&#21046;&#26799;&#24230;&#22312;&#26799;&#24230;&#26041;&#21521;&#21644;&#26799;&#24230;&#27491;&#20132;&#26041;&#21521;&#19978;&#30340;&#30456;&#23545;&#32479;&#35745;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#21487;&#20197;&#20687;&#33539;&#25968;&#27979;&#35797;&#19968;&#26679;&#24265;&#20215;&#65292;&#22914;&#26524;&#952;&#21644;&#957;&#34987;&#26368;&#20248;&#36873;&#25321;&#65292;&#20294;&#26159;&#22914;&#26524;&#1013;&#178;=&#952;&#178;+&#957;&#178;&#65292;&#20869;&#31215;/&#27491;&#20132;&#24615;&#27979;&#35797;&#27704;&#36828;&#19981;&#20250;&#27604;&#33539;&#25968;&#27979;&#35797;&#26356;&#20855;&#35745;&#31639;&#21487;&#25215;&#21463;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we demonstrate that the norm test and inner product/orthogonality test presented in \cite{Bol18} are equivalent in terms of the convergence rates associated with Stochastic Gradient Descent (SGD) methods if $\epsilon^2=\theta^2+\nu^2$ with specific choices of $\theta$ and $\nu$. Here, $\epsilon$ controls the relative statistical error of the norm of the gradient while $\theta$ and $\nu$ control the relative statistical error of the gradient in the direction of the gradient and in the direction orthogonal to the gradient, respectively. Furthermore, we demonstrate that the inner product/orthogonality test can be as inexpensive as the norm test in the best case scenario if $\theta$ and $\nu$ are optimally selected, but the inner product/orthogonality test will never be more computationally affordable than the norm test if $\epsilon^2=\theta^2+\nu^2$. Finally, we present two stochastic optimization problems to illustrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#20195;&#20215;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#34987;&#21160;&#37319;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2106.09973</link><description>&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#20195;&#20215;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#34987;&#21160;&#37319;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23454;&#39564;&#21487;&#33021;&#34987;&#35748;&#20026;&#39118;&#38505;&#22826;&#22823;&#65292;&#22240;&#27492;&#36890;&#24120;&#20250;&#34987;&#21160;&#37319;&#38598;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#31616;&#21333;&#24773;&#20917;&#19979;&#65292;&#22914;&#22312;&#36172;&#21338;&#26426;&#20013;&#65292;&#34987;&#21160;&#21644;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#25928;&#26524;&#30456;&#20284;&#65292;&#20294;&#22312;&#20174;&#24102;&#26377;&#21487;&#25511;&#29366;&#24577;&#30340;&#31995;&#32479;&#20013;&#25910;&#38598;&#25968;&#25454;&#26102;&#65292;&#34987;&#21160;&#37319;&#26679;&#30340;&#20195;&#20215;&#21487;&#33021;&#20250;&#26356;&#39640;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#23545;&#36825;&#31181;&#20195;&#20215;&#30340;&#29305;&#24449;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#20855;&#26377;$\mathrm{S}$&#20010;&#29366;&#24577;&#21644;$\mathrm{A}$&#20010;&#21160;&#20316;&#30340;&#31163;&#25955;&#29366;&#24577;-&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#20351;&#29992;&#26368;&#20339;&#65288;&#20294;&#34987;&#21160;&#36873;&#25321;&#30340;&#65289;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#65292;&#20063;&#38656;&#35201;&#65288;&#19988;&#36275;&#22815;&#65289;&#33719;&#24471;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$&#20010;&#22238;&#21512;&#65292;&#20854;&#20013;$H$&#26159;&#22238;&#21512;&#38271;&#24230;&#12290;&#35831;&#27880;&#24847;&#65292;&#36825;&#34920;&#26126;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#36825;&#20010;&#32467;&#26524;&#26159;&#21487;&#20197;&#39044;&#26009;&#30340;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#21457;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with $\mathrm{S}$ states and $\mathrm{A}$ actions, we show that even with the best (but passively chosen) logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy, where $H$ is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26131;&#22788;&#29702;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#32852;&#21512;&#24191;&#20041;&#27169;&#22411;&#20013;&#22522;&#20110;&#22343;&#20540;&#22870;&#21169;&#20272;&#35745;&#36880;&#27493;&#28040;&#38500;&#27425;&#20248;&#33218;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;GLM&#20013;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;BAI&#20998;&#26512;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.04763</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Best-Arm Identification in Structured Bandits. (arXiv:2106.04763v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26131;&#22788;&#29702;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#32852;&#21512;&#24191;&#20041;&#27169;&#22411;&#20013;&#22522;&#20110;&#22343;&#20540;&#22870;&#21169;&#20272;&#35745;&#36880;&#27493;&#28040;&#38500;&#27425;&#20248;&#33218;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;GLM&#20013;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;BAI&#20998;&#26512;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#26159;&#19968;&#20010;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#23398;&#20064;&#20195;&#29702;&#22312;&#22266;&#23450;&#35266;&#27979;&#27425;&#25968;&#21518;&#26368;&#22823;&#21270;&#35782;&#21035;&#20986;&#26368;&#20248;&#65288;&#26368;&#20339;&#65289;&#33218;&#30340;&#27010;&#29575;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#30740;&#31350;&#37117;&#26159;&#38024;&#23545;&#38750;&#32467;&#26500;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33218;&#30340;&#25968;&#37327;&#36739;&#23569;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26131;&#22788;&#29702;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#32852;&#21512;&#24191;&#20041;&#27169;&#22411;&#20013;&#22522;&#20110;&#22343;&#20540;&#22870;&#21169;&#20272;&#35745;&#36880;&#27493;&#28040;&#38500;&#27425;&#20248;&#33218;&#26469;&#24341;&#20837;&#32467;&#26500;&#21270;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#20013;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;G-&#26368;&#20248;&#35774;&#35745;&#30340;&#23454;&#38469;&#23454;&#29616;&#26041;&#27861;&#12290;&#22312;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#38169;&#35823;&#20445;&#35777;&#26041;&#38754;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#33267;&#23569;&#34920;&#29616;&#24471;&#19981;&#38169;&#12290;&#22312;GLM&#20013;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#39044;&#31639;BAI&#20998;&#26512;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best-arm identification (BAI) in a fixed-budget setting is a bandit problem where the learning agent maximizes the probability of identifying the optimal (best) arm after a fixed number of observations. Most works on this topic study unstructured problems with a small number of arms, which limits their applicability. We propose a general tractable algorithm that incorporates the structure, by successively eliminating suboptimal arms based on their mean reward estimates from a joint generalization model. We analyze our algorithm in linear and generalized linear models (GLMs), and propose a practical implementation based on a G-optimal design. In linear models, our algorithm has competitive error guarantees to prior works and performs at least as well empirically. In GLMs, this is the first practical algorithm with analysis for fixed-budget BAI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#36890;&#36807;&#37197;&#32622;&#22823;&#37096;&#20998;&#27531;&#24046;&#20989;&#25968;&#20026;1D&#26102;&#22495;&#21367;&#31215;&#65292;&#24182;&#20351;&#29992;&#24191;&#25773;&#27531;&#24046;&#36830;&#25509;&#23558;&#26102;&#22495;&#36755;&#20986;&#25193;&#23637;&#21040;&#39057;&#29575;-&#26102;&#22495;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#36127;&#36733;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#30340;&#24191;&#25773;&#27531;&#24046;&#32593;&#32476;&#65288;BC-ResNet&#65289;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26681;&#25454;&#30446;&#26631;&#35774;&#22791;&#30340;&#36164;&#28304;&#26469;&#25193;&#23637;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2106.04140</link><description>&lt;p&gt;
&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#36890;&#36807;&#37197;&#32622;&#22823;&#37096;&#20998;&#27531;&#24046;&#20989;&#25968;&#20026;1D&#26102;&#22495;&#21367;&#31215;&#65292;&#24182;&#20351;&#29992;&#24191;&#25773;&#27531;&#24046;&#36830;&#25509;&#23558;&#26102;&#22495;&#36755;&#20986;&#25193;&#23637;&#21040;&#39057;&#29575;-&#26102;&#22495;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#36127;&#36733;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#30340;&#24191;&#25773;&#27531;&#24046;&#32593;&#32476;&#65288;BC-ResNet&#65289;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26681;&#25454;&#30446;&#26631;&#35774;&#22791;&#30340;&#36164;&#28304;&#26469;&#25193;&#23637;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#22312;&#26234;&#33021;&#35774;&#22791;&#30340;&#21796;&#37266;&#21644;&#29992;&#25143;&#20132;&#20114;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;&#25163;&#26426;&#65289;&#19978;&#39640;&#25928;&#36816;&#34892;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#38169;&#35823;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#36127;&#36733;&#19979;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;&#27531;&#24046;&#20989;&#25968;&#37197;&#32622;&#20026;1D&#26102;&#22495;&#21367;&#31215;&#65292;&#21516;&#26102;&#20351;&#29992;&#24191;&#25773;&#27531;&#24046;&#36830;&#25509;&#23558;&#26102;&#22495;&#36755;&#20986;&#25193;&#23637;&#21040;&#39057;&#29575;-&#26102;&#22495;&#32500;&#24230;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#26377;&#29992;&#30340;&#38899;&#39057;&#29305;&#24449;&#65292;&#35745;&#31639;&#37327;&#27604;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24191;&#25773;&#27531;&#24046;&#32593;&#32476;&#65288;BC-ResNet&#65289;&#65292;&#22522;&#20110;&#24191;&#25773;&#27531;&#24046;&#23398;&#20064;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#26681;&#25454;&#30446;&#26631;&#35774;&#22791;&#30340;&#36164;&#28304;&#26469;&#25193;&#23637;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword spotting is an important research field because it plays a key role in device wake-up and user interaction on smart devices. However, it is challenging to minimize errors while operating efficiently in devices with limited resources such as mobile phones. We present a broadcasted residual learning method to achieve high accuracy with small model size and computational load. Our method configures most of the residual functions as 1D temporal convolution while still allows 2D convolution together using a broadcasted-residual connection that expands temporal output to frequency-temporal dimension. This residual mapping enables the network to effectively represent useful audio features with much less computation than conventional convolutional neural networks. We also propose a novel network architecture, Broadcasting-residual network (BC-ResNet), based on broadcasted residual learning and describe how to scale up the model according to the target device's resources. BC-ResNets ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#21512;&#32422;&#39537;&#21160;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;iid&#25968;&#25454;&#38598;&#21644;&#20449;&#20219;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#12290;&#21019;&#26032;&#30340;&#26234;&#33021;&#21512;&#32422;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#36798;&#25104;&#19968;&#33268;&#65292;&#30830;&#23450;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#26368;&#20339;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2103.07050</link><description>&lt;p&gt;
SCEI: &#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#26234;&#33021;&#21512;&#32422;&#39537;&#21160;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SCEI: A Smart-Contract Driven Edge Intelligence Framework for IoT Systems. (arXiv:2103.07050v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#21512;&#32422;&#39537;&#21160;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;iid&#25968;&#25454;&#38598;&#21644;&#20449;&#20219;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#12290;&#21019;&#26032;&#30340;&#26234;&#33021;&#21512;&#32422;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#36798;&#25104;&#19968;&#33268;&#65292;&#30830;&#23450;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#26368;&#20339;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#12290;FL&#22312;&#22788;&#29702;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;iid&#65289;&#25968;&#25454;&#38598;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#22788;&#29702;&#38750;iid&#25968;&#25454;&#38598;&#26102;&#24456;&#22256;&#38590;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#20559;&#26012;&#31561;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#20986;&#29616;&#65288;&#20363;&#22914;&#65292;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#39550;&#39542;&#34892;&#20026;&#38543;&#26102;&#38388;&#21644;&#20301;&#32622;&#30340;&#21464;&#21270;&#65289;&#12290;&#27492;&#22806;&#65292;&#38476;&#29983;&#35774;&#22791;&#20043;&#38388;&#30340;&#20449;&#20219;&#38382;&#39064;&#21644;&#38598;&#20013;&#24335;&#32858;&#21512;&#22120;&#30340;&#23433;&#20840;&#38382;&#39064;&#20063;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21160;&#24577;&#20248;&#21270;&#20010;&#24615;&#21270;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21306;&#22359;&#38142;&#20013;&#23454;&#29616;&#30340;&#21019;&#26032;&#26234;&#33021;&#21512;&#32422;&#20351;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#23601;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#26368;&#20339;&#26435;&#37325;&#36798;&#25104;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables collaborative training of a shared model on edge devices while maintaining data privacy. FL is effective when dealing with independent and identically distributed (iid) datasets, but struggles with non-iid datasets. Various personalized approaches have been proposed, but such approaches fail to handle underlying shifts in data distribution, such as data distribution skew commonly observed in real-world scenarios (e.g., driver behavior in smart transportation systems changing across time and location). Additionally, trust concerns among unacquainted devices and security concerns with the centralized aggregator pose additional challenges. To address these challenges, this paper presents a dynamically optimized personal deep learning scheme based on blockchain and federated learning. Specifically, the innovative smart contract implemented in the blockchain allows distributed edge devices to reach a consensus on the optimal weights of personalized models. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.14956</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21450;&#20854;&#22312;&#24189;&#38376;&#34746;&#26438;&#33740;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#32570;&#20047;&#20934;&#30830;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#22240;&#27492;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#39318;&#20808;&#23545;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#36827;&#34892;&#19968;&#20123;&#32416;&#27491;&#65292;&#28982;&#21518;&#29992;&#32416;&#27491;&#20449;&#24687;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#65288;MHWSIA&#65289;&#31561;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#19987;&#23478;&#24448;&#24448;&#38590;&#20197;&#25110;&#29978;&#33267;&#26080;&#27861;&#25163;&#21160;&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#23548;&#33268;&#26631;&#31614;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#12290;&#36825;&#31181;&#24773;&#20917;&#24341;&#21457;&#20102;&#20004;&#20010;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#26631;&#31614;&#20013;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#65292;&#20808;&#21069;&#26041;&#27861;&#32416;&#27491;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#30340;&#26041;&#27861;&#23398;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;2&#65289;&#30001;&#20110;&#25910;&#38598;&#26080;&#22122;&#22768;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#38750;&#24120;&#22256;&#38590;&#65292;&#39564;&#35777;/&#27979;&#35797;&#30340;&#36866;&#24403;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;CTR&#39044;&#27979;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#23558;&#30693;&#35782;&#20256;&#36755;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2011.04106</link><description>&lt;p&gt;
&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ensemble Knowledge Distillation for CTR Prediction. (arXiv:2011.04106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.04106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;CTR&#39044;&#27979;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#23558;&#30693;&#35782;&#20256;&#36755;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26500;&#24314;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#29305;&#24449;&#20132;&#20114;&#21644;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;&#22686;&#21152;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#21487;&#33021;&#20250;&#20943;&#24930;&#22312;&#32447;&#25512;&#26029;&#36895;&#24230;&#65292;&#24182;&#38459;&#30861;&#20854;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#26032;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#12290;KD&#26159;&#19968;&#31181;&#23558;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#26694;&#26550;&#12290;&#30693;&#35782;&#33976;&#39311;&#31574;&#30053;&#19981;&#20165;&#20801;&#35768;&#25105;&#20204;&#23558;&#23398;&#29983;&#27169;&#22411;&#31616;&#21270;&#20026;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#19988;&#36824;&#22312;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#20123;&#20248;&#28857;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;&#25945;&#24072;&#38598;&#21512;&#26469;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#23398;&#29983;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning-based models have been widely studied for click-through rate (CTR) prediction and lead to improved prediction accuracy in many industrial applications. However, current research focuses primarily on building complex network architectures to better capture sophisticated feature interactions and dynamic user behaviors. The increased model complexity may slow down online inference and hinder its adoption in real-time applications. Instead, our work targets at a new model training strategy based on knowledge distillation (KD). KD is a teacher-student learning framework to transfer knowledge learned from a teacher model to a student model. The KD strategy not only allows us to simplify the student model as a vanilla DNN model but also achieves significant accuracy improvements over the state-of-the-art teacher models. The benefits thus motivate us to further explore the use of a powerful ensemble of teachers for more accurate student model training. We also propose s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2009.07888</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38500;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#19987;&#19994;&#30693;&#35782;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#36890;&#36807;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23454;&#29616;&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#65292;&#36890;&#36807;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#21644;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#26469;&#25552;&#21319;&#32467;&#26524;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2003.09053</link><description>&lt;p&gt;
&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#30340;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.09053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#36890;&#36807;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23454;&#29616;&#19977;&#32500;&#28857;&#20113;&#20998;&#21106;&#65292;&#36890;&#36807;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#21644;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#26469;&#25552;&#21319;&#32467;&#26524;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24418;&#29366;&#38598;&#21512;&#20013;&#20256;&#36882;&#36880;&#28857;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#19977;&#32500;&#24418;&#29366;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20351;&#19968;&#20010;&#24418;&#29366;&#30340;&#36880;&#28857;&#29305;&#24449;&#19982;&#20854;&#20182;&#24418;&#29366;&#30340;&#36880;&#28857;&#29305;&#24449;&#20135;&#29983;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26426;&#21046;&#35780;&#20272;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#31243;&#24230;&#24182;&#22312;&#24418;&#29366;&#20043;&#38388;&#20171;&#23548;&#29305;&#24449;&#20256;&#25773;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#29992;&#20110;&#24418;&#29366;&#20998;&#21106;&#30340;&#32467;&#26524;&#30340;&#28857;&#36880;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#29366;&#26816;&#32034;&#24230;&#37327;&#65292;&#20197;&#36873;&#25321;&#36866;&#21512;&#27599;&#20010;&#27979;&#35797;&#24418;&#29366;&#30340;&#20132;&#21449;&#24418;&#29366;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;PartNet&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method that propagates point-wise feature representations across shapes within a collection for the purpose of 3D shape segmentation. We propose a cross-shape attention mechanism to enable interactions between a shape's point-wise features and those of other shapes. The mechanism assesses both the degree of interaction between points and also mediates feature propagation across shapes, improving the accuracy and consistency of the resulting point-wise feature representations for shape segmentation. Our method also proposes a shape retrieval measure to select suitable shapes for cross-shape attention operations for each test shape. Our experiments demonstrate that our approach yields state-of-the-art results in the popular PartNet dataset.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#36890;&#36807;&#28145;&#20837;&#20102;&#35299;&#20219;&#21153;&#38656;&#27714;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/1802.00810</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;: &#19968;&#20010;&#31616;&#27905;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Genomics: A Concise Overview. (arXiv:1802.00810v3 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1802.00810
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20294;&#36890;&#36807;&#28145;&#20837;&#20102;&#35299;&#20219;&#21153;&#38656;&#27714;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#27979;&#24207;&#31561;&#22522;&#22240;&#32452;&#30740;&#31350;&#30340;&#36827;&#23637;&#24050;&#23558;&#29616;&#20195;&#22522;&#22240;&#32452;&#23398;&#25512;&#21521;&#20102;"&#22823;&#25968;&#25454;"&#23398;&#31185;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#28856;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#22522;&#22240;&#32452;&#23398;&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;&#19982;&#24378;&#22823;&#31639;&#27861;&#30340;&#32039;&#24613;&#38656;&#27714;&#30456;&#24179;&#34892;&#30340;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#26469;&#35828;&#65292;&#28145;&#24230;&#23398;&#20064;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#36229;&#20986;&#25105;&#20204;&#30340;&#30693;&#35782;&#25506;&#32034;&#22522;&#22240;&#32452;&#30340;&#35299;&#35835;&#12290;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#35813;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#20174;&#22522;&#22240;&#32452;&#23398;&#35282;&#24230;&#30475;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20415;&#23558;&#27599;&#20010;&#29305;&#23450;&#20219;&#21153;&#19982;&#36866;&#24403;&#30340;&#28145;&#24230;&#26550;&#26500;&#30456;&#21305;&#37197;&#65292;&#24182;&#23545;&#21457;&#23637;&#29616;&#20195;&#22522;&#22240;&#32452;&#23398;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#23454;&#36341;&#32771;&#34385;&#36827;&#34892;&#20102;&#35780;&#36848;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#31616;&#26126;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into "big data" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#38450;&#24481;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#20197;&#27450;&#39575;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#21516;&#26102;&#20998;&#31867;&#22120;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#27491;&#30830;&#20998;&#31867;&#21407;&#22987;&#21644;&#23545;&#25239;&#22270;&#20687;&#65292;&#36825;&#19968;&#36807;&#31243;&#20351;&#20998;&#31867;&#22120;&#32593;&#32476;&#23545;&#23545;&#25239;&#25200;&#21160;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#36824;&#33021;&#26377;&#25928;&#22320;&#38477;&#20302;&#32593;&#32476;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/1705.03387</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#22120;&#65306;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#38450;&#24481;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. (arXiv:1705.03387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1705.03387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#38450;&#24481;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#20197;&#27450;&#39575;&#20998;&#31867;&#22120;&#32593;&#32476;&#65292;&#21516;&#26102;&#20998;&#31867;&#22120;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#27491;&#30830;&#20998;&#31867;&#21407;&#22987;&#21644;&#23545;&#25239;&#22270;&#20687;&#65292;&#36825;&#19968;&#36807;&#31243;&#20351;&#20998;&#31867;&#22120;&#32593;&#32476;&#23545;&#23545;&#25239;&#25200;&#21160;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#36824;&#33021;&#26377;&#25928;&#22320;&#38477;&#20302;&#32593;&#32476;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20351;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20132;&#26367;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#12290;&#29983;&#25104;&#22120;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#27599;&#20010;&#22270;&#20687;&#30340;&#26799;&#24230;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#65292;&#20174;&#32780;&#36731;&#26494;&#27450;&#39575;&#20998;&#31867;&#22120;&#32593;&#32476;&#12290;&#21516;&#26102;&#65292;&#20998;&#31867;&#22120;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#27491;&#30830;&#20998;&#31867;&#30001;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#21407;&#22987;&#21644;&#23545;&#25239;&#22270;&#20687;&#12290;&#36825;&#20123;&#36807;&#31243;&#26377;&#21161;&#20110;&#20351;&#20998;&#31867;&#22120;&#32593;&#32476;&#23545;&#23545;&#25239;&#25200;&#21160;&#26356;&#21152;&#40065;&#26834;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22914;Dropout&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;CIFAR&#25968;&#25454;&#38598;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25913;&#36827;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.
&lt;/p&gt;</description></item></channel></rss>