<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2401.18079</link><description>&lt;p&gt;
KVQuant: &#20197;KV&#32531;&#23384;&#37327;&#21270;&#23454;&#29616;1000&#19975;&#19978;&#19979;&#25991;&#38271;&#24230;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18079
&lt;/p&gt;
&lt;p&gt;
KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25688;&#35201;&#31561;&#38656;&#35201;&#22823;&#31383;&#21475;&#19978;&#19979;&#25991;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;KV&#32531;&#23384;&#28608;&#27963;&#25104;&#20026;&#35760;&#24518;&#28040;&#32791;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21387;&#32553;KV&#32531;&#23384;&#28608;&#27963;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#65288;&#22914;&#20302;&#20110;4&#20301;&#65289;&#30340;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KVQuant&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26041;&#27861;&#37327;&#21270;&#32531;&#23384;&#30340;KV&#28608;&#27963;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(i)&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#65292;&#22312;&#37327;&#21270;&#38190;&#28608;&#27963;&#26102;&#35843;&#25972;&#32500;&#24230;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20998;&#24067;&#65307;(ii)RoPE&#21069;&#37327;&#21270;&#38190;&#65292;&#22312;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#20043;&#21069;&#37327;&#21270;&#38190;&#28608;&#27963;&#20197;&#20943;&#36731;&#20854;&#23545;&#37327;&#21270;&#30340;&#24433;&#21709;&#65307;(iii)&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#22312;&#27599;&#23618;&#25512;&#23548;&#20986;&#26435;&#37325;&#24863;&#30693;&#30340;&#38750;&#22343;&#21248;&#25968;&#25454;&#31867;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#65292;&#20174;&#19979;&#24448;&#19978;&#25972;&#21512;&#24182;&#26816;&#32034;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#65292;&#23545;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2401.18059</link><description>&lt;p&gt;
RAPTOR: &#36882;&#24402;&#25277;&#35937;&#22788;&#29702;&#29992;&#20110;&#26641;&#29366;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#65292;&#20174;&#19979;&#24448;&#19978;&#25972;&#21512;&#24182;&#26816;&#32034;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#65292;&#23545;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19990;&#30028;&#29366;&#24577;&#30340;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#38271;&#23614;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#20174;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30701;&#36830;&#32493;&#22359;&#65292;&#38480;&#21046;&#20102;&#23545;&#25972;&#20307;&#25991;&#26723;&#19978;&#19979;&#25991;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36882;&#24402;&#23884;&#20837;&#12289;&#32858;&#31867;&#21644;&#25688;&#35201;&#25991;&#26412;&#22359;&#65292;&#20174;&#19979;&#24448;&#19978;&#26500;&#24314;&#20855;&#26377;&#19981;&#21516;&#25688;&#35201;&#32423;&#21035;&#30340;&#26641;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#25105;&#20204;&#30340;RAPTOR&#27169;&#22411;&#20174;&#36825;&#26869;&#26641;&#20013;&#26816;&#32034;&#65292;&#23558;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#38271;&#24230;&#36739;&#38271;&#30340;&#25991;&#26723;&#20013;&#12290;&#25511;&#21046;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36882;&#24402;&#25688;&#35201;&#30340;&#26816;&#32034;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27604;&#20256;&#32479;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#28041;&#21450;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#20363;&#22914;&#65292;&#36890;&#36807;&#23558;RAPTOR&#26816;&#32034;&#19982;GPT-4&#30340;&#20351;&#29992;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;QuALITY&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#24615;&#33021;&#25552;&#39640;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20%
&lt;/p&gt;</description></item><item><title>LongAlign&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#21644;&#20351;&#29992;&#25171;&#21253;&#12289;&#25490;&#24207;&#21644;&#25439;&#22833;&#21152;&#26435;&#31574;&#30053;&#65292;&#23427;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.18058</link><description>&lt;p&gt;
LongAlign&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LongAlign: A Recipe for Long Context Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18058
&lt;/p&gt;
&lt;p&gt;
LongAlign&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#21644;&#20351;&#29992;&#25171;&#21253;&#12289;&#25490;&#24207;&#21644;&#25439;&#22833;&#21152;&#26435;&#31574;&#30053;&#65292;&#23427;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26377;&#25928;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#38656;&#35201;&#23545;&#30456;&#20284;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongAlign - &#19968;&#31181;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#25351;&#23548;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#26500;&#24314;&#38271;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#23427;&#28085;&#30422;&#20102;&#26469;&#33258;&#19981;&#21516;&#38271;&#19978;&#19979;&#25991;&#26469;&#28304;&#30340;&#24191;&#27867;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#25171;&#21253;&#21644;&#25490;&#24207;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#21152;&#36895;&#22312;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#30340;&#21463;&#30417;&#30563;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#25439;&#22833;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#25171;&#21253;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#25439;&#22833;&#23545;&#19981;&#21516;&#24207;&#21015;&#30340;&#36129;&#29486;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LongBench-Chat&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;10k-100k&#38271;&#24230;&#30340;&#26597;&#35810;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LongAlign&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#30340;LLMs&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#22810;&#36798;30\%&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#20102;&#20854;&#29087;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\%, while also maintaining their proficienc
&lt;/p&gt;</description></item><item><title>Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information.</title><link>https://arxiv.org/abs/2401.18057</link><description>&lt;p&gt;
&#25490;&#21517;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Rank Supervised Contrastive Learning for Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18057
&lt;/p&gt;
&lt;p&gt;
Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#24182;&#23637;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#26159;&#21033;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26500;&#24314;&#21487;&#34892;&#30340;&#27491;&#26679;&#26412;&#65292;&#20351;&#24471;&#32534;&#30721;&#22120;&#33021;&#22815;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23558;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#26144;&#23556;&#24471;&#26356;&#36817;&#65292;&#23558;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#26144;&#23556;&#24471;&#26356;&#36828;&#65292;&#20174;&#32780;&#20135;&#29983;&#31283;&#20581;&#32780;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#26631;&#35760;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#27491;&#26679;&#26412;&#30340;&#32454;&#31890;&#24230;&#30456;&#23545;&#30456;&#20284;&#24615;&#20449;&#24687;&#65288;&#20363;&#22914;&#25490;&#21517;&#65289;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank Supervised Contrastive Learning&#65288;RankSCL&#65289;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19981;&#21516;&#65292;RankSCL&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20197;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#29305;&#23450;&#30340;&#36807;&#28388;&#35268;&#21017;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#23545;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#65292;&#26469;&#20026;&#19981;&#21516;&#30340;&#26679;&#26412;&#36171;&#20104;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#36830;&#32493;&#22270;&#23398;&#20064;&#20013;&#30340;GNN&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#22522;&#20934;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CGL&#26041;&#27861;&#22312;&#31867;&#21035;&#21644;&#20219;&#21153;&#39034;&#24207;&#19978;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#19981;&#21516;&#26550;&#26500;&#19979;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18054</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#35780;&#20272;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#36830;&#32493;&#22270;&#23398;&#20064;&#20013;&#30340;GNN&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#22522;&#20934;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CGL&#26041;&#27861;&#22312;&#31867;&#21035;&#21644;&#20219;&#21153;&#39034;&#24207;&#19978;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#19981;&#21516;&#26550;&#26500;&#19979;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22312;&#19981;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36830;&#32493;&#22320;&#31215;&#32047;&#19981;&#21516;&#20219;&#21153;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24494;&#35843;&#21518;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#36801;&#31227;&#65288;Hu&#31561;&#65292;2020&#65289;&#65292;&#36825;&#19982;CL&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;&#30740;&#31350;CGL&#35774;&#32622;&#19979;&#30340;GNN&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31354;&#26102;&#22270;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#22312;&#36825;&#31181;&#26032;&#39062;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#26469;&#35780;&#20272;&#30693;&#21517;&#30340;CGL&#26041;&#27861;&#12290;&#35813;&#22522;&#20934;&#22522;&#20110;N-UCLA&#21644;NTU-RGB+D&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39592;&#39612;&#21160;&#20316;&#35782;&#21035;&#12290;&#38500;&#20102;&#23545;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;CGL&#26041;&#27861;&#30340;&#31867;&#21035;&#21644;&#20219;&#21153;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#21363;&#23398;&#20064;&#39034;&#24207;&#23545;&#27599;&#20010;&#31867;/&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#23485;&#24230;&#21644;&#28145;&#24230;&#19979;&#20351;&#29992;&#39592;&#24178;GNN&#30340;CGL&#26041;&#27861;&#30340;&#26550;&#26500;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#27969;&#34892;&#30149;&#24314;&#27169;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#28145;&#24230;&#23398;&#20064;&#20110;&#19968;&#20307;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#31283;&#24577;&#27169;&#24335;&#21644;&#27969;&#34892;&#30149;&#30340;&#22810;&#20010;&#27874;&#21160;&#12290;&#27169;&#22411;&#20027;&#35201;&#28385;&#36275;&#19977;&#20010;&#30446;&#26631;&#65306;&#21608;&#26399;&#24615;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12289;&#32771;&#34385;&#25152;&#26377;&#26041;&#38754;&#30340;&#24433;&#21709;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2401.18047</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#21464;SIRD&#65292;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#27969;&#34892;&#30149;&#24314;&#27169;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#28145;&#24230;&#23398;&#20064;&#20110;&#19968;&#20307;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#38750;&#31283;&#24577;&#27169;&#24335;&#21644;&#27969;&#34892;&#30149;&#30340;&#22810;&#20010;&#27874;&#21160;&#12290;&#27169;&#22411;&#20027;&#35201;&#28385;&#36275;&#19977;&#20010;&#30446;&#26631;&#65306;&#21608;&#26399;&#24615;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12289;&#32771;&#34385;&#25152;&#26377;&#26041;&#38754;&#30340;&#24433;&#21709;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20256;&#25773;&#27169;&#24335;&#26159;&#31283;&#23450;&#30340;&#35805;&#65292;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#26159;&#26368;&#36866;&#21512;&#29992;&#20110;&#24314;&#27169;&#27969;&#34892;&#30149;&#30340;&#12290;&#20026;&#20102;&#22788;&#29702;&#38750;&#31283;&#24577;&#27169;&#24335;&#21644;&#27969;&#34892;&#30149;&#30340;&#22810;&#20010;&#27874;&#21160;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#27969;&#34892;&#30149;&#24314;&#27169;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20027;&#35201;&#28385;&#36275;&#19977;&#20010;&#30446;&#26631;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39044;&#27979;&#65306;1. &#21608;&#26399;&#24615;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;2. &#21033;&#29992;&#25968;&#25454;&#25311;&#21512;&#21644;&#21442;&#25968;&#20248;&#21270;&#26469;&#32771;&#34385;&#25152;&#26377;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;3. &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#21442;&#25968;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#26469;&#36827;&#34892;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;-&#27515;&#20129;&#65288;SIRD&#65289;&#27969;&#34892;&#30149;&#24314;&#27169;&#65292;&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20248;&#21270;&#65292;&#20351;&#29992;&#22534;&#21472;LSTM&#39044;&#27979;&#27169;&#22411;&#21442;&#25968;&#12290;&#21333;&#27425;&#25110;&#39318;&#27425;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26080;&#27861;&#24314;&#27169;&#27969;&#34892;&#30149;&#30340;&#22810;&#20010;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23450;&#26399;&#65288;&#27599;&#21608;&#65289;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;PSO&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#21442;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary. To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning. The model mainly caters to three objectives for better prediction: 1. Periodic estimation of the model parameters. 2. Incorporating impact of all the aspects using data fitting and parameter optimization 3. Deep learning based prediction of the model parameters. In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters. Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic. So, we estimate the model parameters periodically (weekly). We use PSO to identify the optimum v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#29256;&#26412;&#30340;Na\"ive Bayes&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24615;&#33021;&#24230;&#37327;&#26469;&#25351;&#23548;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2401.18039</link><description>&lt;p&gt;
Na\"ive Bayes&#20998;&#31867;&#30340;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Variable selection for Na\"ive Bayes classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#29256;&#26412;&#30340;Na\"ive Bayes&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24615;&#33021;&#24230;&#37327;&#26469;&#25351;&#23548;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#21464;&#37327;&#20998;&#26512;&#20013;&#65292;&#32463;&#20856;&#30340;Na\"ive Bayes&#20998;&#31867;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26131;&#20110;&#22788;&#29702;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29305;&#24449;&#36890;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#36829;&#21453;&#20102;Na\"ive Bayes&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#22823;&#37327;&#30340;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#32467;&#26524;&#30340;&#35299;&#37322;&#21464;&#24471;&#22797;&#26434;&#65292;&#24182;&#20943;&#24930;&#35813;&#26041;&#27861;&#30340;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#29256;&#26412;&#30340;Na\"ive Bayes&#20998;&#31867;&#22120;&#65292;&#23427;&#20855;&#26377;&#19977;&#20010;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#32771;&#34385;&#21327;&#21464;&#37327;&#30340;&#30456;&#20851;&#32467;&#26500;&#23454;&#29616;&#20102;&#31232;&#30095;&#24615;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#24615;&#33021;&#24230;&#37327;&#26469;&#25351;&#23548;&#29305;&#24449;&#30340;&#36873;&#25321;&#12290;&#31532;&#19977;&#65292;&#21487;&#20197;&#21253;&#25324;&#23545;&#26356;&#24863;&#20852;&#36259;&#30340;&#32452;&#21035;&#30340;&#24615;&#33021;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#25552;&#26696;&#21487;&#20197;&#23454;&#29616;&#26234;&#33021;&#25628;&#32034;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#20998;&#31867;&#30340;&#24615;&#33021;&#24230;&#37327;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Na\"ive Bayes has proven to be a tractable and efficient method for classification in multivariate analysis. However, features are usually correlated, a fact that violates the Na\"ive Bayes' assumption of conditional independence, and may deteriorate the method's performance. Moreover, datasets are often characterized by a large number of features, which may complicate the interpretation of the results as well as slow down the method's execution.   In this paper we propose a sparse version of the Na\"ive Bayes classifier that is characterized by three properties. First, the sparsity is achieved taking into account the correlation structure of the covariates. Second, different performance measures can be used to guide the selection of features. Third, performance constraints on groups of higher interest can be included. Our proposal leads to a smart search, which yields competitive running times, whereas the flexibility in terms of performance measure for classification is integrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#39635;&#26679;&#30382;&#23618;&#30340;&#25240;&#21472;&#27169;&#24335;&#65292;&#20026;&#35299;&#20915;&#30382;&#23618;&#25240;&#21472;&#30340;&#21464;&#24322;&#24615;&#21644;&#19982;&#20010;&#20307;&#34892;&#20026;&#29305;&#24449;&#20197;&#21450;&#30149;&#29702;&#23398;&#30340;&#20851;&#31995;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.18035</link><description>&lt;p&gt;
&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#20197;&#26816;&#27979;&#39635;&#26679;&#30382;&#23618;&#25240;&#21472;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Optimizing contrastive learning for cortical folding pattern detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#39635;&#26679;&#30382;&#23618;&#30340;&#25240;&#21472;&#27169;&#24335;&#65292;&#20026;&#35299;&#20915;&#30382;&#23618;&#25240;&#21472;&#30340;&#21464;&#24322;&#24615;&#21644;&#19982;&#20010;&#20307;&#34892;&#20026;&#29305;&#24449;&#20197;&#21450;&#30149;&#29702;&#23398;&#30340;&#20851;&#31995;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#30340;&#30382;&#23618;&#26377;&#35768;&#22810;&#20984;&#36215;&#21644;&#27807;&#27133;&#65292;&#31216;&#20026;&#22238;&#29366;&#33041;&#22238;&#21644;&#27807;&#29366;&#33041;&#27807;&#12290;&#34429;&#28982;&#20027;&#35201;&#30382;&#23618;&#35126;&#30385;&#23384;&#22312;&#30528;&#39640;&#24230;&#20010;&#20307;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#24403;&#25105;&#20204;&#26816;&#26597;&#20855;&#20307;&#30340;&#24418;&#29366;&#21644;&#25240;&#21472;&#27169;&#24335;&#30340;&#32454;&#33410;&#26102;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#30001;&#20110;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#25551;&#36848;&#30382;&#23618;&#25240;&#21472;&#30340;&#21464;&#24322;&#24615;&#24182;&#23558;&#20854;&#19982;&#21463;&#35797;&#32773;&#30340;&#34892;&#20026;&#29305;&#24449;&#25110;&#30149;&#29702;&#23398;&#32852;&#31995;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#20960;&#20309;&#36317;&#31163;&#26631;&#35760;&#19968;&#20123;&#29305;&#23450;&#30340;&#27169;&#24335;&#65292;&#35201;&#20040;&#25163;&#21160;&#26631;&#35760;&#65292;&#35201;&#20040;&#21322;&#33258;&#21160;&#26631;&#35760;&#65292;&#20294;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#25968;&#20197;&#19975;&#35745;&#30340;&#21463;&#35797;&#32773;&#30340;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#26368;&#36817;&#21487;&#29992;&#24615;&#20351;&#20854;&#29305;&#21035;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#25187;&#24102;&#21306;&#22495;&#30340;&#25240;&#21472;&#27169;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25299;&#25169;&#30340;&#22686;&#24378;&#35757;&#32451;&#20102;&#19968;&#20010;&#23545;&#27604;&#33258;&#30417;&#30563;&#27169;&#22411;&#65288;SimCLR&#65289;&#65292;&#20351;&#29992;&#20102;&#20154;&#20307;&#36830;&#36890;&#35745;&#21010;&#65288;1101&#20010;&#21463;&#35797;&#32773;&#65289;&#21644;UKBioBank&#65288;21070&#20010;&#21463;&#35797;&#32773;&#65289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human cerebral cortex has many bumps and grooves called gyri and sulci. Even though there is a high inter-individual consistency for the main cortical folds, this is not the case when we examine the exact shapes and details of the folding patterns. Because of this complexity, characterizing the cortical folding variability and relating them to subjects' behavioral characteristics or pathologies is still an open scientific problem. Classical approaches include labeling a few specific patterns, either manually or semi-automatically, based on geometric distances, but the recent availability of MRI image datasets of tens of thousands of subjects makes modern deep-learning techniques particularly attractive. Here, we build a self-supervised deep-learning model to detect folding patterns in the cingulate region. We train a contrastive self-supervised model (SimCLR) on both Human Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with topological-based augmentation
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24322;&#26500;&#21464;&#25442;&#30340;&#26680;&#20559;&#24046;&#27979;&#37327;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#20013;&#30340;&#25361;&#25112;&#65292;&#20197;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#25928;&#24212;&#20043;&#38388;&#30340;&#39640;&#38454;&#32467;&#26500;&#21464;&#24322;&#30340;&#20027;&#35201;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2401.18017</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#21464;&#25442;&#30340;&#26680;&#20559;&#24046;&#27979;&#37327;&#26469;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24322;&#26500;&#21464;&#25442;&#30340;&#26680;&#20559;&#24046;&#27979;&#37327;&#26469;&#35299;&#20915;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#20013;&#30340;&#25361;&#25112;&#65292;&#20197;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#25928;&#24212;&#20043;&#38388;&#30340;&#39640;&#38454;&#32467;&#26500;&#21464;&#24322;&#30340;&#20027;&#35201;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26159;&#31185;&#23398;&#30340;&#22522;&#26412;&#30446;&#26631;&#65292;&#24182;&#19988;&#26368;&#36817;&#20063;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#30495;&#27491;&#26426;&#22120;&#26234;&#33021;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#20013;&#19968;&#31867;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#25216;&#26415;&#22522;&#20110;&#36825;&#26679;&#19968;&#31181;&#35770;&#28857;&#65306;&#22240;&#26524;&#26041;&#21521;&#21644;&#21453;&#22240;&#26524;&#26041;&#21521;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#32467;&#26500;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#25928;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#35768;&#22810;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#27604;&#36739;&#26465;&#20214;&#20998;&#24067;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#33539;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22522;&#20110;RKHS&#23884;&#20837;&#30340;&#36825;&#31181;&#26041;&#27861;&#19981;&#36275;&#20197;&#25429;&#25417;&#28041;&#21450;&#26465;&#20214;&#20998;&#24067;&#30340;&#39640;&#38454;&#32467;&#26500;&#21464;&#24322;&#30340;&#20027;&#35201;&#26631;&#35760;&#30340;&#22240;&#26524;&#25928;&#24212;&#19981;&#23545;&#31216;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#20102;&#24322;&#26500;&#21464;&#25442;&#30340;&#26680;&#20869;&#22312;&#19981;&#21464;&#24615;&#27979;&#37327;&#65288;KIIM-HT&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of causal relationships in a set of random variables is a fundamental objective of science and has also recently been argued as being an essential component towards real machine intelligence. One class of causal discovery techniques are founded based on the argument that there are inherent structural asymmetries between the causal and anti-causal direction which could be leveraged in determining the direction of causation. To go about capturing these discrepancies between cause and effect remains to be a challenge and many current state-of-the-art algorithms propose to compare the norms of the kernel mean embeddings of the conditional distributions. In this work, we argue that such approaches based on RKHS embeddings are insufficient in capturing principal markers of cause-effect asymmetry involving higher-order structural variabilities of the conditional distributions. We propose Kernel Intrinsic Invariance Measure with Heterogeneous Transform (KIIM-HT) which introduces 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24182;&#21457;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#21516;&#25506;&#32034;&#26469;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#34920;&#29616;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#31639;&#27861;&#20013;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#25552;&#21462;&#25511;&#21046;&#20010;&#20307;&#24046;&#24322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#25968;&#25454;&#20849;&#20139;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#22810;&#26679;&#21270;&#21160;&#20316;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18012</link><description>&lt;p&gt;
&#22240;&#26524;&#21327;&#21516;&#24182;&#21457;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Coordinated Concurrent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24182;&#21457;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#21516;&#25506;&#32034;&#26469;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#34920;&#29616;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#31639;&#27861;&#20013;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#25552;&#21462;&#25511;&#21046;&#20010;&#20307;&#24046;&#24322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#25968;&#25454;&#20849;&#20139;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#22810;&#26679;&#21270;&#21160;&#20316;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#21516;&#25506;&#32034;&#65292;&#20197;&#22312;&#24182;&#21457;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#29615;&#22659;&#19979;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#34920;&#29616;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#20854;&#20182;&#20551;&#35774;&#25152;&#26377;&#20195;&#29702;&#37117;&#22312;&#30456;&#21516;&#29615;&#22659;&#19979;&#34892;&#21160;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#32780;&#26159;&#32771;&#34385;&#27599;&#20010;&#20195;&#29702;&#22312;&#20849;&#20139;&#20840;&#23616;&#32467;&#26500;&#20294;&#20063;&#23384;&#22312;&#20010;&#20307;&#24046;&#24322;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#65292;&#21363;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411; - &#28151;&#21512;&#27169;&#22411;&#65288;ANM-MM&#65289;&#65292;&#36890;&#36807;&#29420;&#31435;&#24615;&#24378;&#21270;&#25552;&#21462;&#25511;&#21046;&#20010;&#20307;&#24046;&#24322;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#21462;&#30340;&#27169;&#22411;&#21442;&#25968;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#25968;&#25454;&#20849;&#20139;&#26041;&#26696;&#65292;&#24182;&#22312;&#19968;&#32452;&#33258;&#22238;&#24402;&#12289;&#25670;&#26438;&#21644;&#20498;&#31435;&#25670;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#26368;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#21160;&#20316;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel algorithmic framework for data sharing and coordinated exploration for the purpose of learning more data-efficient and better performing policies under a concurrent reinforcement learning (CRL) setting. In contrast to other work which make the assumption that all agents act under identical environments, we relax this restriction and instead consider the formulation where each agent acts within an environment which shares a global structure but also exhibits individual variations. Our algorithm leverages a causal inference algorithm in the form of Additive Noise Model - Mixture Model (ANM-MM) in extracting model parameters governing individual differentials via independence enforcement. We propose a new data sharing scheme based on a similarity measure of the extracted model parameters and demonstrate superior learning speeds on a set of autoregressive, pendulum and cart-pole swing-up tasks and finally, we show the effectiveness of diverse action selecti
&lt;/p&gt;</description></item><item><title>EEG-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#21644;&#35299;&#35835;EEG&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#65292;&#19988;&#22312;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2401.18006</link><description>&lt;p&gt;
EEG-GPT: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;EEG&#20998;&#31867;&#21644;&#35299;&#35835;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18006
&lt;/p&gt;
&lt;p&gt;
EEG-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#21644;&#35299;&#35835;EEG&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#65292;&#19988;&#22312;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#20013;&#65292;&#24448;&#24448;&#26159;&#26377;&#38480;&#30340;&#32858;&#28966;&#65292;&#20165;&#20165;&#23396;&#31435;&#22320;&#20851;&#27880;&#36328;&#36234;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#65288;&#20174;&#27627;&#31186;&#30340;&#30636;&#26102;&#23574;&#23792;&#21040;&#25345;&#32493;&#20960;&#20998;&#38047;&#30340;&#30315;&#30187;&#21457;&#20316;&#65289;&#21644;&#31354;&#38388;&#23610;&#24230;&#65288;&#20174;&#23616;&#37096;&#39640;&#39057;&#25391;&#33633;&#21040;&#20840;&#23616;&#30561;&#30496;&#27963;&#21160;&#65289;&#30340;&#29305;&#23450;&#33041;&#27963;&#21160;&#12290;&#36825;&#31181;&#23396;&#31435;&#30340;&#26041;&#27861;&#38480;&#21046;&#20102;&#21457;&#23637;&#20986;&#20855;&#26377;&#22810;&#23610;&#24230;&#30005;&#29983;&#29702;&#29702;&#35299;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;EEG ML&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20856;&#22411;&#30340;ML EEG&#26041;&#27861;&#37319;&#29992;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EEG-GPT&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;EEG&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#12290;EEG-GPT&#22312;&#20165;&#21033;&#29992;2&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;few-shot&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#23545;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;EEG&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity). This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities. Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts. Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM). EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data. Furthermore, it off
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MONet&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#20381;&#36182;&#22810;&#32447;&#24615;&#31639;&#23376;&#26469;&#36827;&#34892;&#22270;&#20687;&#35782;&#21035;&#65292;&#36890;&#36807;&#25429;&#25417;&#36755;&#20837;&#20803;&#32032;&#30340;&#39640;&#27425;&#20132;&#20114;&#65292;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#65292;&#24182;&#19982;&#29616;&#20195;&#26550;&#26500;&#24615;&#33021;&#30456;&#36817;&#12290;&#36825;&#19968;&#30740;&#31350;&#21487;&#20197;&#28608;&#21457;&#23545;&#23436;&#20840;&#20351;&#29992;&#22810;&#32447;&#24615;&#25805;&#20316;&#30340;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2401.17992</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multilinear Operator Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MONet&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#20381;&#36182;&#22810;&#32447;&#24615;&#31639;&#23376;&#26469;&#36827;&#34892;&#22270;&#20687;&#35782;&#21035;&#65292;&#36890;&#36807;&#25429;&#25417;&#36755;&#20837;&#20803;&#32032;&#30340;&#39640;&#27425;&#20132;&#20114;&#65292;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#65292;&#24182;&#19982;&#29616;&#20195;&#26550;&#26500;&#24615;&#33021;&#30456;&#36817;&#12290;&#36825;&#19968;&#30740;&#31350;&#21487;&#20197;&#28608;&#21457;&#23545;&#23436;&#20840;&#20351;&#29992;&#22810;&#32447;&#24615;&#25805;&#20316;&#30340;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#22522;&#26412;&#26410;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#26377;&#24453;&#28040;&#38500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#39033;&#24335;&#32593;&#32476;&#26159;&#19968;&#31867;&#19981;&#38656;&#35201;&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#26410;&#19982;&#29616;&#20195;&#26550;&#26500;&#30456;&#23218;&#32654;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#20165;&#20381;&#36182;&#22810;&#32447;&#24615;&#31639;&#23376;&#30340;MONet&#27169;&#22411;&#12290;MONet&#30340;&#26680;&#24515;&#23618;&#31216;&#20026;Mu-Layer&#65292;&#25429;&#25417;&#20102;&#36755;&#20837;&#20803;&#32032;&#30340;&#20056;&#27861;&#20132;&#20114;&#12290;MONet&#25429;&#25417;&#20102;&#36755;&#20837;&#20803;&#32032;&#30340;&#39640;&#27425;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#22270;&#20687;&#35782;&#21035;&#21644;&#31185;&#23398;&#35745;&#31639;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#22810;&#39033;&#24335;&#32593;&#32476;&#65292;&#24182;&#19982;&#29616;&#20195;&#26550;&#26500;&#24615;&#33021;&#30456;&#36817;&#12290;&#25105;&#20204;&#30456;&#20449;MONet&#21487;&#20197;&#28608;&#21457;&#23545;&#23436;&#20840;&#20351;&#29992;&#22810;&#32447;&#24615;&#25805;&#20316;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#32534;&#30721;&#29702;&#35770;&#21644;&#31070;&#32463;&#31185;&#23398;&#24037;&#20855;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2401.17975</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding polysemanticity in neural networks through coding theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17975
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#32534;&#30721;&#29702;&#35770;&#21644;&#31070;&#32463;&#31185;&#23398;&#24037;&#20855;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#20197;&#25417;&#25720;&#30340;&#30446;&#26631;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#23545;&#22823;&#22810;&#25968;&#21333;&#20010;&#31070;&#32463;&#20803;&#23545;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#25552;&#20379;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;&#36825;&#20010;&#38480;&#21046;&#26159;&#30001;&#20110;&#22823;&#22810;&#25968;&#31070;&#32463;&#20803;&#30340;&#22810;&#35821;&#20041;&#24615;&#65292;&#21363;&#19968;&#20010;&#32473;&#23450;&#30340;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#19981;&#30456;&#20851;&#30340;&#32593;&#32476;&#29366;&#24577;&#65292;&#20351;&#35299;&#37322;&#35813;&#31070;&#32463;&#20803;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#31070;&#32463;&#31185;&#23398;&#21644;&#20449;&#24687;&#35770;&#20013;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#23545;&#22810;&#35821;&#20041;&#24615;&#21644;&#32534;&#30721;&#23494;&#24230;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#28608;&#27963;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#35889;&#26469;&#25512;&#26029;&#32593;&#32476;&#20195;&#30721;&#30340;&#20887;&#20313;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#25237;&#24433;&#22914;&#20309;&#25581;&#31034;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#24179;&#28369;&#30340;&#25110;&#19981;&#21487;&#24494;&#30340;&#20195;&#30721;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20195;&#30721;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20010;&#30456;&#21516;&#30340;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#35821;&#20041;&#31070;&#32463;&#20803;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#20248;&#21183;&#24182;&#35299;&#37322;&#20102;
&lt;/p&gt;
&lt;p&gt;
Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains
&lt;/p&gt;</description></item><item><title>MelNet&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23450;&#21046;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17972</link><description>&lt;p&gt;
MelNet:&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MelNet: A Real-Time Deep Learning Algorithm for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17972
&lt;/p&gt;
&lt;p&gt;
MelNet&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#65292;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23450;&#21046;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MelNet&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;MelNet&#21033;&#29992;KITTI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#35757;&#32451;&#12290;&#32463;&#36807;300&#20010;&#35757;&#32451;&#36718;&#27425;&#65292;MelNet&#36798;&#21040;&#20102;0.732&#30340;mAP&#65288;&#24179;&#22343;&#31934;&#24230;&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#19977;&#20010;&#22791;&#36873;&#27169;&#22411;&#65288;YOLOv5&#12289;EfficientDet&#21644;Faster-RCNN-MobileNetv3&#65289;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#19982;MelNet&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#26159;&#26377;&#25928;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#20808;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#12289;COCO&#21644;Pascal VOC&#65289;&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21478;&#19968;&#20010;&#21457;&#29616;&#24378;&#35843;&#20102;&#26681;&#25454;&#29305;&#23450;&#24773;&#26223;&#21019;&#24314;&#26032;&#27169;&#22411;&#24182;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#21487;&#34892;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;MelNet&#22312;150&#20010;&#36718;&#27425;&#21518;&#20063;&#36229;&#36807;&#20102;EfficientDet&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#65292;MelNet&#30340;&#24615;&#33021;&#19982;EfficientDet&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that
&lt;/p&gt;</description></item><item><title>CONCORD&#26159;&#19968;&#31181;&#38754;&#21521;&#21487;&#37197;&#32622;&#22270;&#24418;&#20195;&#30721;&#34920;&#31034;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#23454;&#29616;&#20943;&#23569;&#22270;&#24418;&#22823;&#23567;&#22797;&#26434;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#34920;&#31034;&#65292;&#24182;&#22312;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#20013;&#23637;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17967</link><description>&lt;p&gt;
CONCORD: &#38754;&#21521;&#21487;&#37197;&#32622;&#22270;&#24418;&#20195;&#30721;&#34920;&#31034;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
CONCORD: Towards a DSL for Configurable Graph Code Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17967
&lt;/p&gt;
&lt;p&gt;
CONCORD&#26159;&#19968;&#31181;&#38754;&#21521;&#21487;&#37197;&#32622;&#22270;&#24418;&#20195;&#30721;&#34920;&#31034;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#23454;&#29616;&#20943;&#23569;&#22270;&#24418;&#22823;&#23567;&#22797;&#26434;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#34920;&#31034;&#65292;&#24182;&#22312;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#20013;&#23637;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25366;&#25496;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#28304;&#20195;&#30721;&#30340;&#30456;&#20851;&#29305;&#24449;&#21644;&#29305;&#24615;&#30340;&#26684;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#22240;&#20854;&#33021;&#22815;&#27169;&#25311;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20855;&#22312;&#36328;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#26500;&#24314;&#22270;&#24418;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#36755;&#20986;&#36890;&#24120;&#32570;&#20047;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#23548;&#33268;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CONCORD&#65292;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#21487;&#23450;&#21046;&#22270;&#24418;&#34920;&#31034;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#12290;&#23427;&#23454;&#29616;&#20102;&#20943;&#23569;&#22270;&#24418;&#22823;&#23567;&#22797;&#26434;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#65306;&#39318;&#20808;&#65292;CONCORD&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#37197;&#32622;&#33258;&#21160;&#20135;&#29983;&#20195;&#30721;&#34920;&#31034;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#22270;&#24418;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is widely used to uncover hidden patterns in large code corpora. To achieve this, constructing a format that captures the relevant characteristics and features of source code is essential. Graph-based representations have gained attention for their ability to model structural and semantic information. However, existing tools lack flexibility in constructing graphs across different programming languages, limiting their use. Additionally, the output of these tools often lacks interoperability and results in excessively large graphs, making graph-based neural networks training slower and less scalable.   We introduce CONCORD, a domain-specific language to build customizable graph representations. It implements reduction heuristics to reduce graphs' size complexity. We demonstrate its effectiveness in code smell detection as an illustrative use case and show that: first, CONCORD can produce code representations automatically per the specified configuration, and second, our he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20551;&#35774;&#24471;&#20998;&#20272;&#35745;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2401.17958</link><description>&lt;p&gt;
&#22312;Wasserstein&#36317;&#31163;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#33324;&#27010;&#29575;&#27969;ODE&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20551;&#35774;&#24471;&#20998;&#20272;&#35745;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24555;&#36895;&#30340;&#22522;&#20110;ODE&#30340;&#25277;&#26679;&#22120;&#24182;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#65292;&#20294;&#23545;&#27010;&#29575;&#27969;ODE&#30340;&#25910;&#25947;&#24615;&#23646;&#24615;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#39318;&#20010;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#32467;&#26524;&#65292;&#20551;&#35774;&#20934;&#30830;&#30340;&#24471;&#20998;&#20272;&#35745;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#31034;&#20363;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#24212;&#22522;&#20110;ODE&#30340;&#25277;&#26679;&#22120;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications. While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited. In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates. We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers.
&lt;/p&gt;</description></item><item><title>LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2401.17919</link><description>&lt;p&gt;
LOCOST: &#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LOCOST: State-Space Models for Long Document Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17919
&lt;/p&gt;
&lt;p&gt;
LOCOST&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#12290;&#19982;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;LOCOST&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;LOCOST&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;93-96%&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#32534;&#30721;&#38271;&#24207;&#21015;&#21644;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#30340;&#20302;&#22797;&#26434;&#24230;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCOST&#65306;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O&#65288;L log L&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#27604;&#22522;&#20110;&#31232;&#30095;&#27880;&#24847;&#27169;&#24335;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38271;&#25991;&#26723;&#25277;&#35937;&#25688;&#35201;&#21270;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#27700;&#24179;&#19978;&#36798;&#21040;&#20102;&#19982;&#30456;&#21516;&#22823;&#23567;&#30340;&#26368;&#20248;&#31232;&#30095;&#21464;&#21387;&#22120;&#30456;&#24403;&#30340;93-96%&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;50%&#30340;&#20869;&#23384;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#33410;&#30465;&#20102;&#39640;&#36798;87%&#30340;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;LOCOST&#26377;&#25928;&#22320;&#22788;&#29702;&#36229;&#36807;600K&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#20026;&#23436;&#25972;&#20070;&#25688;&#35201;&#21270;&#35774;&#23450;&#20102;&#26032;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#24182;&#20026;&#38271;&#36755;&#20837;&#22788;&#29702;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#22810;&#26234;&#33021;&#20307;&#21487;&#20449;&#21306;&#38388;&#65288;GA-MATR&#65289;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26080;&#20154;&#26426;&#36741;&#21161;&#36890;&#20449;&#20013;&#30340;&#36335;&#24452;&#35774;&#35745;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#22270;&#24490;&#29615;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#36890;&#20449;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#25552;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2401.17880</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#22810;&#26080;&#20154;&#26426;&#36741;&#21161;&#36890;&#20449;&#20013;&#36335;&#24452;&#35774;&#35745;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#22810;&#26234;&#33021;&#20307;&#21487;&#20449;&#21306;&#38388;&#65288;GA-MATR&#65289;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26080;&#20154;&#26426;&#36741;&#21161;&#36890;&#20449;&#20013;&#30340;&#36335;&#24452;&#35774;&#35745;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#22270;&#24490;&#29615;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#36890;&#20449;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#25552;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#26080;&#20154;&#26426;&#36741;&#21161;&#19979;&#34892;&#36890;&#20449;&#20013;&#65292;&#26080;&#20154;&#26426;&#22522;&#31449;&#65288;UAV BSs&#65289;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23454;&#29616;&#36335;&#24452;&#35774;&#35745;&#21644;&#36164;&#28304;&#20998;&#37197;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#20449;&#32593;&#32476;&#20013;&#26080;&#20154;&#26426;&#22522;&#31449;&#20043;&#38388;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#23548;&#33268;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#19978;&#36848;&#20915;&#31574;&#38382;&#39064;&#30340;&#37325;&#35201;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#20849;&#21516;&#38382;&#39064;&#65292;&#22914;&#31995;&#32479;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#29575;&#20302;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#22810;&#26234;&#33021;&#20307;&#21487;&#20449;&#21306;&#38388;&#65288;GA-MATR&#65289;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#26080;&#20154;&#26426;&#36741;&#21161;&#36890;&#20449;&#38382;&#39064;&#12290;&#24341;&#20837;&#22270;&#24490;&#29615;&#32593;&#32476;&#26469;&#22788;&#29702;&#21644;&#20998;&#26512;&#36890;&#20449;&#32593;&#32476;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#65292;&#20174;&#35266;&#27979;&#20449;&#24687;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#27169;&#24335;&#12290;&#27880;&#24847;&#26426;&#21046;&#20026;&#20256;&#36882;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments. The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem. Multi-agent reinforcement learning is a significant solution for the above decision-making. However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application. In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem. Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information. The attention mechanism provides additional weighting for conveyed informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17870</link><description>&lt;p&gt;
&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#36827;&#34892;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#27425;&#23395;&#33410;&#22825;&#27668;&#39044;&#25253;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38598;&#25104;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#26469;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27425;&#23395;&#33410;&#39044;&#25253;&#23545;&#20892;&#19994;&#12289;&#27700;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#39044;&#35686;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22823;&#27668;&#30340;&#28151;&#27788;&#24615;&#65292;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#36890;&#36807;&#23454;&#29616;&#19982;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#38761;&#26032;&#20102;&#22825;&#27668;&#39044;&#25253;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#36825;&#23548;&#33268;&#30456;&#24403;&#22810;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#20135;&#29983;&#24179;&#28369;&#30340;&#32467;&#26524;&#26469;&#24858;&#24324;&#20687;&#32032;&#35823;&#24046;&#35780;&#20998;&#65292;&#36825;&#20123;&#32467;&#26524;&#32570;&#20047;&#29289;&#29702;&#19968;&#33268;&#24615;&#21644;&#27668;&#35937;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;Transformer&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Pangu&#27169;&#22411;&#26469;&#33719;&#24471;&#33391;&#22909;&#30340;&#21021;&#22987;&#26435;&#37325;&#65292;&#24182;&#38598;&#25104;&#20102;&#19968;&#20010;&#36828;&#31243;&#36830;&#25509;&#36890;&#30693;&#30340;&#26102;&#38388;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#22312;&#24310;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#35843;&#25972;Pangu&#27169;&#22411;&#30340;1.1%&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Conv-LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;SAM&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#36731;&#37327;&#32423;&#21367;&#31215;&#21442;&#25968;&#21644;&#20302;&#31209;&#35843;&#25972;&#65292;&#23558;&#22270;&#20687;&#30456;&#20851;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;ViT&#32534;&#30721;&#22120;&#65292;&#20197;&#25552;&#39640;SAM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#39640;&#32423;&#22270;&#20687;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17868</link><description>&lt;p&gt;
&#21367;&#31215;&#19982;LoRA&#30456;&#36935;: &#29992;&#20110;Segment Anything&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Conv-LoRA&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;SAM&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#36731;&#37327;&#32423;&#21367;&#31215;&#21442;&#25968;&#21644;&#20302;&#31209;&#35843;&#25972;&#65292;&#23558;&#22270;&#20687;&#30456;&#20851;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;ViT&#32534;&#30721;&#22120;&#65292;&#20197;&#25552;&#39640;SAM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#39640;&#32423;&#22270;&#20687;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything&#27169;&#22411;&#65288;SAM&#65289;&#26159;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#26694;&#26550;&#12290;&#22312;&#20856;&#22411;&#22330;&#26223;&#20013;&#65292;&#23427;&#23637;&#29616;&#20986;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#21644;&#36965;&#24863;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#20854;&#20248;&#21183;&#20943;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conv-LoRA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#36229;&#36731;&#37327;&#32423;&#21367;&#31215;&#21442;&#25968;&#19982;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#30456;&#32467;&#21512;&#65292;Conv-LoRA&#33021;&#22815;&#23558;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#24402;&#32435;&#20559;&#35265;&#27880;&#20837;&#26222;&#36890;&#30340;ViT&#32534;&#30721;&#22120;&#20013;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;SAM&#30340;&#23616;&#37096;&#20808;&#39564;&#20551;&#35774;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Conv-LoRA&#19981;&#20165;&#20445;&#30041;&#20102;SAM&#30340;&#24191;&#27867;&#20998;&#21106;&#30693;&#35782;&#65292;&#36824;&#24674;&#22797;&#20102;&#20854;&#23398;&#20064;&#39640;&#32423;&#22270;&#20687;&#35821;&#20041;&#33021;&#21147;&#65292;&#36825;&#19968;&#33021;&#21147;&#21463;&#21040;SAM&#30340;&#21069;&#26223;-&#32972;&#26223;&#20998;&#21106;&#39044;&#35757;&#32451;&#30340;&#38480;&#21046;&#12290;&#22312;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#24378;&#35843;&#20102;Conv-LoRA&#22312;&#23558;SAM&#36866;&#24212;&#21040;&#23454;&#38469;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#22312;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#30699;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#23454;&#29616;&#20010;&#20154;&#21033;&#30410;&#12290;</title><link>https://arxiv.org/abs/2401.17865</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#25945;&#23398;&#20013;&#25805;&#32437;&#31163;&#25955;&#36755;&#20837;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Manipulating Predictions over Discrete Inputs in Machine Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#22312;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#30699;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#23454;&#29616;&#20010;&#20154;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#25945;&#23398;&#36890;&#24120;&#28041;&#21450;&#21019;&#24314;&#19968;&#20010;&#26368;&#20248;&#65288;&#36890;&#24120;&#26159;&#26368;&#23567;&#30340;&#65289;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#65288;&#34987;&#31216;&#20026;&#8220;&#23398;&#29983;&#8221;&#65289;&#26681;&#25454;&#25945;&#24072;&#32473;&#20986;&#30340;&#29305;&#23450;&#30446;&#26631;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#12290;&#23613;&#31649;&#22312;&#36830;&#32493;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#20294;&#22312;&#31163;&#25955;&#22495;&#20013;&#23545;&#26426;&#22120;&#25945;&#23398;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#31163;&#25955;&#22495;&#20013;&#30340;&#26426;&#22120;&#25945;&#23398;&#65292;&#20855;&#20307;&#22320;&#35828;&#65292;&#26159;&#36890;&#36807;&#26377;&#25928;&#22320;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#26469;&#25805;&#32437;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#25945;&#24072;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#36845;&#20195;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25945;&#24072;&#35797;&#22270;&#32416;&#27491;&#38169;&#35823;&#30340;&#39044;&#27979;&#20197;&#25913;&#21892;&#23398;&#29983;&#27169;&#22411;&#65292;&#25110;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#20197;&#38169;&#35823;&#20998;&#31867;&#26576;&#20123;&#29305;&#23450;&#26679;&#26412;&#21040;&#19982;&#20854;&#20010;&#20154;&#21033;&#30410;&#19968;&#33268;&#30340;&#30446;&#26631;&#31867;&#21035;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#20540;&#20248;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36229;&#32423;
&lt;/p&gt;
&lt;p&gt;
Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#12290;&#26694;&#26550;&#21253;&#25324;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#12289;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#21644;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#38656;&#27714;&#20379;&#24212;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2401.17838</link><description>&lt;p&gt;
&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#29992;&#20110;&#25216;&#33021;&#38656;&#27714;&#20379;&#24212;&#32852;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#12290;&#26694;&#26550;&#21253;&#25324;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#12289;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#21644;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#38656;&#27714;&#20379;&#24212;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21644;&#20135;&#19994;&#30340;&#36805;&#36895;&#21464;&#21270;&#23548;&#33268;&#25216;&#33021;&#38656;&#27714;&#21160;&#24577;&#21464;&#21270;&#65292;&#22240;&#27492;&#21592;&#24037;&#21644;&#38599;&#20027;&#39044;&#27979;&#36825;&#31181;&#21464;&#21270;&#20197;&#22312;&#21171;&#21160;&#24066;&#22330;&#20445;&#25345;&#31454;&#20105;&#20248;&#21183;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#65292;&#35201;&#20040;&#23558;&#25216;&#33021;&#28436;&#21464;&#35270;&#20026;&#31616;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#20197;&#21450;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#21464;&#21270;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#39044;&#27979;&#30340;&#36328;&#35270;&#35282;&#20998;&#23618;&#22270;&#23398;&#20064;&#36229;&#32593;&#32476;&#65288;CHGH&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CHGH&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21253;&#25324;&#65306;i) &#19968;&#20010;&#36328;&#35270;&#35282;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#25429;&#25417;&#25216;&#33021;&#38656;&#27714;&#21644;&#20379;&#24212;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#65292;ii) &#19968;&#20010;&#23618;&#27425;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#20174;&#38598;&#32676;&#35282;&#24230;&#24314;&#27169;&#25216;&#33021;&#30340;&#20849;&#21516;&#28436;&#21464;&#65292;iii) &#19968;&#20010;&#26465;&#20214;&#36229;&#35299;&#30721;&#22120;&#29992;&#20110;&#20849;&#21516;&#39044;&#27979;&#38656;&#27714;&#21644;&#20379;&#24212;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply va
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#30340;&#28508;&#22312;&#21160;&#24577;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#26356;&#21152;&#21487;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#28508;&#22312;&#29366;&#24577;&#39044;&#27979;&#12289;&#35270;&#39057;&#39044;&#27979;&#21644;&#35268;&#21010;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17835</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#19990;&#30028;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Predicting the Future with Simple World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#30340;&#28508;&#22312;&#21160;&#24577;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#26356;&#21152;&#21487;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#28508;&#22312;&#29366;&#24577;&#39044;&#27979;&#12289;&#35270;&#39057;&#39044;&#27979;&#21644;&#35268;&#21010;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#29992;&#32039;&#20945;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#28508;&#22312;&#39640;&#32500;&#24230;&#20687;&#32032;&#35266;&#23519;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#29615;&#22659;&#21160;&#24577;&#24314;&#27169;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25512;&#23548;&#20986;&#30340;&#28508;&#22312;&#21160;&#24577;&#21487;&#33021;&#20173;&#28982;&#38750;&#24120;&#22797;&#26434;&#12290;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#26469;&#25277;&#35937;&#29615;&#22659;&#30340;&#21160;&#24577;&#21487;&#20197;&#24102;&#26469;&#20960;&#20010;&#22909;&#22788;&#12290;&#22914;&#26524;&#28508;&#22312;&#21160;&#24577;&#31616;&#21333;&#65292;&#27169;&#22411;&#21487;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#30340;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#26377;&#29992;&#30340;&#29615;&#22659;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#19990;&#30028;&#27169;&#22411;&#28508;&#22312;&#21160;&#24577;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#31616;&#32422;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65288;PLSM&#65289;&#65292;&#26368;&#23567;&#21270;&#20102;&#28508;&#22312;&#29366;&#24577;&#19982;&#20854;&#20043;&#38388;&#20135;&#29983;&#30340;&#21160;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#21160;&#24577;&#22312;&#29366;&#24577;&#19978;&#20855;&#26377;&#36719;&#24615;&#19981;&#21464;&#24615;&#65292;&#24182;&#20351;&#24471;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#24433;&#21709;&#26356;&#21152;&#21487;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;PLSM&#19982;&#29992;&#20110;i)&#26410;&#26469;&#28508;&#22312;&#29366;&#24577;&#39044;&#27979;&#12289;ii)&#35270;&#39057;&#39044;&#27979;&#21644;iii)&#35268;&#21010;&#30340;&#19977;&#31181;&#19981;&#21516;&#27169;&#22411;&#31867;&#32467;&#21512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment. However, the latent dynamics inferred by these models may still be highly complex. Abstracting the dynamics of the environment with simple models can have several benefits. If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states. We propose a regularization scheme that simplifies the world model's latent dynamics. Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them. This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable. We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning. We find that our regulariz
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2401.17823</link><description>&lt;p&gt;
&#37319;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21457;&#24067;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving data release leveraging optimal transport and particle gradient descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25919;&#24220;&#65289;&#20013;&#38544;&#31169;&#30340;&#34920;&#26684;&#25968;&#25454;&#24046;&#20998;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#65292;&#20174;&#31169;&#26377;&#36793;&#38469;&#20272;&#35745;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PrivPGD&#65292;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#26032;&#19968;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22823;&#33539;&#22260;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#32467;&#21512;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.17802</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#30340;&#33976;&#39311;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#24615;&#20197;&#21450;&#30417;&#30563;&#20449;&#21495;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36890;&#24120;&#32858;&#28966;&#20110;&#26102;&#38388;&#20869;&#37096;&#29305;&#24449;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DE-TSMCL&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26159;&#21542;&#23631;&#34109;&#26102;&#38388;&#25139;&#20197;&#33719;&#24471;&#20248;&#21270;&#30340;&#23376;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#25506;&#32034;&#26102;&#38388;&#24207;&#21015;&#30340;&#26679;&#26412;&#38388;&#21644;&#26102;&#38388;&#20869;&#37096;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23398;&#20064;&#26410;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#32467;&#26500;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30417;&#30563;&#20219;&#21153;&#65292;&#20197;&#23398;&#20064;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#24182;&#20419;&#36827;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#19978;&#36848;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADIN&#30340;&#20302;&#25104;&#26412;&#38598;&#25104;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#38598;&#21512;logit&#24615;&#33021;&#26469;&#36817;&#20284;soup&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#27169;&#22411;soup&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17790</link><description>&lt;p&gt;
RADIN: &#20302;&#25104;&#26412;&#38598;&#25104;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RADIN: Souping on a Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADIN&#30340;&#20302;&#25104;&#26412;&#38598;&#25104;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#38598;&#21512;logit&#24615;&#33021;&#26469;&#36817;&#20284;soup&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#27169;&#22411;soup&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;Soup&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#19981;&#21516;&#36229;&#21442;&#25968;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#25193;&#23637;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#65288;SWA&#65289;&#65292;&#20294;&#26159;&#20854;&#37319;&#29992;&#21463;&#21040;&#20102;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35745;&#31639;&#25361;&#25112;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#38598;&#21512;logit&#24615;&#33021;&#26469;&#36817;&#20284;soup&#24615;&#33021;&#20174;&#32780;&#21152;&#36895;soup&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#19978;&#30340;&#27934;&#23519;&#39564;&#35777;&#20102;&#22312;&#20219;&#20309;&#28151;&#21512;&#27604;&#20363;&#19979;&#65292;&#38598;&#21512;logit&#19982;&#26435;&#37325;&#24179;&#22343;soup&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#25972;&#36164;&#28304;soup&#26500;&#36896;&#65288;RADIN&#65289;&#36807;&#31243;&#36890;&#36807;&#20801;&#35768;&#28789;&#27963;&#30340;&#35780;&#20272;&#39044;&#31639;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#36164;&#28304;&#35843;&#25972;&#25506;&#32034;&#39044;&#31639;&#65292;&#21516;&#26102;&#25552;&#39640;&#20302;&#39044;&#31639;&#19979;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#36138;&#24515;&#26041;&#27861;&#25552;&#39640;&#20102;4&#65285;&#65288;&#22312;ImageNet&#19978;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17789</link><description>&lt;p&gt;
&#24377;&#24615;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Robustly overfitting latents for flexible neural image compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#27169;&#22411;&#12290;&#31070;&#32463;&#21387;&#32553;&#27169;&#22411;&#23398;&#20250;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39640;&#25928;&#22320;&#21457;&#36865;&#32473;&#35299;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#20877;&#23558;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#35299;&#30721;&#20026;&#37325;&#24314;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20248;&#21270;&#19981;&#23436;&#32654;&#20197;&#21450;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23481;&#37327;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#23548;&#33268;&#20102;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;Gumbel&#36864;&#28779;&#65288;SGA&#65289;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SGA+&#25193;&#23637;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;SGA+&#21253;&#21547;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;SGA&#30340;&#22522;&#30784;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27599;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#19977;&#20010;&#32780;&#19981;&#26159;&#20004;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#25968;&#23383;&#23402;&#29983;&#21019;&#24314;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#27627;&#31859;&#27874;&#31995;&#32479;&#20013;3D&#25968;&#23383;&#23402;&#29983;&#30340;&#31934;&#24230;&#35201;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27874;&#26463;&#33719;&#21462;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2401.17781</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#27627;&#31859;&#27874;&#27874;&#26463;&#31649;&#29702;&#25968;&#23383;&#23402;&#29983;&#21019;&#24314;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vision-Assisted Digital Twin Creation for mmWave Beam Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#25968;&#23383;&#23402;&#29983;&#21019;&#24314;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#27627;&#31859;&#27874;&#31995;&#32479;&#20013;3D&#25968;&#23383;&#23402;&#29983;&#30340;&#31934;&#24230;&#35201;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27874;&#26463;&#33719;&#21462;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#22797;&#26080;&#32447;&#30005;&#39057;&#29575;&#65288;RF&#65289;&#20256;&#25773;&#29615;&#22659;&#20197;&#21450;&#31995;&#32479;&#34892;&#20026;&#30340;&#26041;&#24335;&#65292;&#20801;&#35768;&#22522;&#20110;&#27169;&#25311;&#26469;&#20248;&#21270;&#37096;&#32626;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#24212;&#29992;&#20110;&#27627;&#31859;&#27874;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#36890;&#36947;&#27169;&#25311;&#22120;&#23545;3D&#25968;&#23383;&#23402;&#29983;&#31934;&#24230;&#30340;&#20005;&#26684;&#35201;&#27714;&#65292;&#38477;&#20302;&#20102;&#25216;&#26415;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#25968;&#23383;&#23402;&#29983;&#21019;&#24314;&#27969;&#31243;&#21644;&#36890;&#36947;&#27169;&#25311;&#22120;&#65292;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#23433;&#35013;&#30340;&#25668;&#20687;&#22836;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;DeepSense6G&#25361;&#25112;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#22312;&#27874;&#26463;&#33719;&#21462;&#30340;&#19979;&#28216;&#23376;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#20102;&#19982;&#19981;&#26126;&#30830;&#24314;&#27169;3D&#29615;&#22659;&#30340;&#26041;&#27861;&#30456;&#27604;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of communication networks, digital twin technology provides a means to replicate the radio frequency (RF) propagation environment as well as the system behaviour, allowing for a way to optimize the performance of a deployed system based on simulations. One of the key challenges in the application of Digital Twin technology to mmWave systems is the prevalent channel simulators' stringent requirements on the accuracy of the 3D Digital Twin, reducing the feasibility of the technology in real applications. We propose a practical Digital Twin creation pipeline and a channel simulator, that relies only on a single mounted camera and position information. We demonstrate the performance benefits compared to methods that do not explicitly model the 3D environment, on downstream sub-tasks in beam acquisition, using the real-world dataset of the DeepSense6G challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17780</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#32422;&#26463;MDPs&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#25506;&#32034;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#28385;&#36275;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22238;&#25253;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#25991;&#29486;&#20165;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#26410;&#33021;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#22343;&#21248;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24615;&#65288;Uniform-PAC&#65289;&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#20197;&#23454;&#29616;&#20219;&#20309;&#30446;&#26631;&#31934;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#32447;CMDP&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;Uniform-PAC&#31639;&#27861;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;CMDP&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#25391;&#33633;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#39640;&#20110;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#26102;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17760</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#39640;&#20110;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#26102;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#31867;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#20998;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#26465;&#20214;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#39640;&#20110;&#25110;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#32447;&#24615;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;LDA&#65288;RLDA&#65289;&#26041;&#27861;&#12290;RLDA&#26041;&#27861;&#30340;&#24615;&#33021;&#24050;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24182;&#24050;&#25552;&#20986;&#20102;&#26368;&#20248;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#38750;&#32447;&#24615;&#65288;NL&#65289;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30456;&#19968;&#33268;&#30340;&#27491;&#21322;&#23450; Ridge &#22411;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#21033;&#29992;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#24471;&#21040;&#20102;&#35813;&#20272;&#35745;&#22120;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#25152;&#25552;&#20986;&#30340;NL-RLDA&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear discriminant analysis (LDA) is a widely used technique for data classification. The method offers adequate performance in many classification problems, but it becomes inefficient when the data covariance matrix is ill-conditioned. This often occurs when the feature space's dimensionality is higher than or comparable to the training data size. Regularized LDA (RLDA) methods based on regularized linear estimators of the data covariance matrix have been proposed to cope with such a situation. The performance of RLDA methods is well studied, with optimal regularization schemes already proposed. In this paper, we investigate the capability of a positive semidefinite ridge-type estimator of the inverse covariance matrix that coincides with a nonlinear (NL) covariance matrix estimator. The estimator is derived by reformulating the score function of the optimal classifier utilizing linear estimation methods, which eventually results in the proposed NL-RLDA classifier. We derive asymptot
&lt;/p&gt;</description></item><item><title>PF-GNN&#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17752</link><description>&lt;p&gt;
PF-GNN: &#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
PF-GNN: Differentiable particle filtering based approximation of universal graph representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17752
&lt;/p&gt;
&lt;p&gt;
PF-GNN&#26159;&#19968;&#31181;&#21487;&#24494;&#30340;&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#30340;&#36890;&#29992;&#22270;&#34920;&#31034;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#34920;&#31034;&#22270;&#21516;&#26500;&#24615;&#30340;1-WL&#39068;&#33394;&#31934;&#28860;&#27979;&#35797;&#26041;&#38754;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20854;&#20182;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#38656;&#35201;&#39044;&#22788;&#29702;&#26469;&#25552;&#21462;&#32467;&#26500;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#31934;&#30830;&#21516;&#26500;&#27714;&#35299;&#25216;&#26415;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#20351;GNN&#25104;&#20026;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#25216;&#26415;&#22312;&#20010;&#20307;&#21270;&#21644;&#31934;&#28860;&#65288;IR&#65289;&#33539;&#24335;&#19979;&#25805;&#20316;&#65292;&#36890;&#36807;&#20154;&#20026;&#24341;&#20837;&#19981;&#23545;&#31216;&#24615;&#24182;&#36827;&#19968;&#27493;&#31934;&#28860;&#30528;&#33394;&#65292;&#24403;1-WL&#20572;&#27490;&#26102;&#12290;&#21516;&#26500;&#27714;&#35299;&#22120;&#29983;&#25104;&#19968;&#20010;&#39068;&#33394;&#30528;&#33394;&#30340;&#25628;&#32034;&#26641;&#65292;&#20854;&#21494;&#23376;&#33410;&#28857;&#33021;&#21807;&#19968;&#26631;&#35782;&#22270;&#12290;&#28982;&#32780;&#65292;&#25628;&#32034;&#26641;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#30340;&#20462;&#21098;&#25216;&#26415;&#65292;&#36825;&#22312;&#23398;&#20064;&#35282;&#24230;&#19978;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#20174;&#26681;&#33410;&#28857;&#21040;&#21494;&#23376;&#33410;&#28857;&#30340;&#25628;&#32034;&#26641;&#20013;&#37319;&#26679;&#22810;&#26465;&#36335;&#24452;&#65292;&#26469;&#36817;&#20284;&#39068;&#33394;&#30528;&#33394;&#30340;&#25628;&#32034;&#26641;&#65288;&#21363;&#23884;&#20837;&#65289;&#12290;&#20197;&#23398;&#20064;&#26356;&#26377;&#36776;&#21035;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of Individualization and Refinement (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings (i.e. embeddings) by sampling multiple paths from root to leaves of the search tree. To learn more discriminative represe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#31639;&#27861;&#40065;&#26834;&#30340;&#39044;&#27979;&#32858;&#21512;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#20840;&#30693;&#32858;&#21512;&#30456;&#27604;&#20855;&#26377;&#26368;&#23567;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30340;&#32858;&#21512;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17743</link><description>&lt;p&gt;
&#31639;&#27861;&#40065;&#26834;&#30340;&#39044;&#27979;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Robust Forecast Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#31639;&#27861;&#40065;&#26834;&#30340;&#39044;&#27979;&#32858;&#21512;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#20840;&#30693;&#32858;&#21512;&#30456;&#27604;&#20855;&#26377;&#26368;&#23567;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30340;&#32858;&#21512;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32858;&#21512;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39044;&#27979;&#22120;&#20449;&#24687;&#32467;&#26500;&#30340;&#32570;&#20047;&#20102;&#35299;&#38459;&#30861;&#20102;&#26368;&#20248;&#32858;&#21512;&#30340;&#23454;&#29616;&#12290;&#22312;&#32473;&#23450;&#20449;&#24687;&#32467;&#26500;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#40065;&#26834;&#30340;&#39044;&#27979;&#32858;&#21512;&#26088;&#22312;&#25214;&#21040;&#19982;&#20840;&#30693;&#32858;&#21512;&#30456;&#27604;&#20855;&#26377;&#26368;&#23567;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30340;&#32858;&#21512;&#22120;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#40065;&#26834;&#39044;&#27979;&#32858;&#21512;&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#39564;&#35266;&#23519;&#21644;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#31639;&#27861;&#40065;&#26834;&#30340;&#39044;&#27979;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38024;&#23545;&#20855;&#26377;&#26377;&#38480;&#20449;&#24687;&#32467;&#26500;&#26063;&#30340;&#26222;&#36941;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36924;&#36817;&#26041;&#26696;&#12290;&#22312;Arieli&#31561;&#20154;&#65288;2018&#65289;&#30740;&#31350;&#30340;&#24773;&#22659;&#19979;&#65292;&#20854;&#20013;&#20004;&#20010;&#20195;&#29702;&#25509;&#25910;&#19982;&#20108;&#36827;&#21046;&#29366;&#24577;&#26377;&#20851;&#30340;&#29420;&#31435;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#23545;&#32858;&#21512;&#22120;&#26045;&#21152;Lipschitz&#26465;&#20214;&#25110;&#23545;&#20195;&#29702;&#25253;&#21578;&#26045;&#21152;&#31163;&#25955;&#26465;&#20214;&#26469;&#25552;&#20379;&#39640;&#25928;&#30340;&#36924;&#36817;&#26041;&#26696;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecast aggregation combines the predictions of multiple forecasters to improve accuracy. However, the lack of knowledge about forecasters' information structure hinders optimal aggregation. Given a family of information structures, robust forecast aggregation aims to find the aggregator with minimal worst-case regret compared to the omniscient aggregator. Previous approaches for robust forecast aggregation rely on heuristic observations and parameter tuning. We propose an algorithmic framework for robust forecast aggregation. Our framework provides efficient approximation schemes for general information aggregation with a finite family of possible information structures. In the setting considered by Arieli et al. (2018) where two agents receive independent signals conditioned on a binary state, our framework also provides efficient approximation schemes by imposing Lipschitz conditions on the aggregator or discrete conditions on agents' reports. Numerical experiments demonstrate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2401.17739</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator learning without the adjoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#35868;&#22242;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#38750;&#33258;&#20276;&#38543;&#31639;&#23376;&#65311;&#30446;&#21069;&#30340;&#23454;&#38469;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#30001;&#31639;&#23376;&#30340;&#27491;&#21521;&#20316;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#31639;&#23376;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20276;&#38543;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#30475;&#65292;&#20284;&#20046;&#26377;&#24517;&#35201;&#37319;&#26679;&#20276;&#38543;&#31639;&#23376;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37096;&#20998;&#35299;&#37322;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#19981;&#26597;&#35810;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20294;&#25105;&#20204;&#30340;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#30340;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#21683;&#22013;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#22788;&#29702;&#21644;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;98.49%&#21644;98.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17738</link><description>&lt;p&gt;
&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#36827;&#34892;&#21683;&#22013;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#30340;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#21683;&#22013;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#22788;&#29702;&#21644;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;98.49%&#21644;98.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20869;&#32622;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30340;&#26234;&#33021;&#25163;&#34920;&#30417;&#27979;&#21683;&#22013;&#24182;&#26816;&#27979;&#21508;&#31181;&#21683;&#22013;&#31867;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;32&#21517;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#65292;&#24182;&#20197;&#21463;&#25511;&#26041;&#24335;&#25910;&#38598;&#20102;9&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;&#25968;&#25454;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#32467;&#26500;&#21270;&#26041;&#27861;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#24471;&#21040;&#20102;223&#20010;&#38451;&#24615;&#21683;&#22013;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#20102;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#38750;&#27493;&#34892;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;98.49%&#65292;&#22312;&#27493;&#34892;&#26102;&#20026;98.2%&#65292;&#34920;&#26126;&#26234;&#33021;&#25163;&#34920;&#21487;&#20197;&#26816;&#27979;&#21040;&#21683;&#22013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the potential of using smartwatches with built-in microphone sensors for monitoring coughs and detecting various cough types. We conducted a study involving 32 participants and collected 9 hours of audio data in a controlled manner. Afterward, we processed this data using a structured approach, resulting in 223 positive cough samples. We further improved the dataset through augmentation techniques and employed a specialized 1D CNN model. This model achieved an impressive accuracy rate of 98.49% while non-walking and 98.2% while walking, showing smartwatches can detect cough. Moreover, our research successfully identified four distinct types of coughs using clustering techniques.
&lt;/p&gt;</description></item><item><title>BICauseTree&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20559;&#24046;&#39537;&#21160;&#20998;&#23618;&#30340;&#21487;&#35299;&#37322;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#24179;&#34913;&#12289;&#20943;&#23569;&#20559;&#24046;&#21644;&#30830;&#23450;&#30446;&#26631;&#20154;&#32676;&#23450;&#20041;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17737</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#20559;&#24046;&#39537;&#21160;&#20998;&#23618;&#30340;&#21487;&#35299;&#37322;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17737
&lt;/p&gt;
&lt;p&gt;
BICauseTree&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20559;&#24046;&#39537;&#21160;&#20998;&#23618;&#30340;&#21487;&#35299;&#37322;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#24179;&#34913;&#12289;&#20943;&#23569;&#20559;&#24046;&#21644;&#30830;&#23450;&#30446;&#26631;&#20154;&#32676;&#23450;&#20041;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#23545;&#20110;&#23558;&#35266;&#23519;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#32435;&#20837;&#25919;&#31574;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36879;&#26126;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#23581;&#35797;&#21253;&#25324;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BICauseTree&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24179;&#34913;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23616;&#37096;&#21457;&#29983;&#33258;&#28982;&#23454;&#39564;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24102;&#26377;&#33258;&#23450;&#20041;&#30446;&#26631;&#20989;&#25968;&#30340;&#20915;&#31574;&#26641;&#65292;&#20197;&#25913;&#36827;&#24179;&#34913;&#21644;&#20943;&#23569;&#22788;&#29702;&#20998;&#37197;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23427;&#36824;&#21487;&#20197;&#26816;&#27979;&#20986;&#23384;&#22312;&#27491;&#24615;&#36829;&#35268;&#30340;&#23376;&#32676;&#20307;&#65292;&#25490;&#38500;&#23427;&#20204;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#21327;&#21464;&#37327;&#30340;&#30446;&#26631;&#20154;&#32676;&#23450;&#20041;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#25512;&#26029;&#24182;&#25512;&#24191;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#32034;&#20854;&#20559;&#24046;&#21487;&#35299;&#37322;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability and transparency are essential for incorporating causal effect models from observational data into policy decision-making. They can provide trust for the model in the absence of ground truth labels to evaluate the accuracy of such models. To date, attempts at transparent causal effect estimation consist of applying post hoc explanation methods to black-box models, which are not interpretable. Here, we present BICauseTree: an interpretable balancing method that identifies clusters where natural experiments occur locally. Our approach builds on decision trees with a customized objective function to improve balancing and reduce treatment allocation bias. Consequently, it can additionally detect subgroups presenting positivity violations, exclude them, and provide a covariate-based definition of the target population we can infer from and generalize to. We evaluate the method's performance using synthetic and realistic datasets, explore its bias-interpretability tradeoff, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#24615;&#21516;&#26102;&#26368;&#23567;&#21270;&#21151;&#32791;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17733</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#36808;&#21521;&#29289;&#29702;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Physical Plausibility in Neuroevolution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#31070;&#32463;&#36827;&#21270;&#31995;&#32479;&#20013;&#29289;&#29702;&#21512;&#29702;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#24615;&#21516;&#26102;&#26368;&#23567;&#21270;&#21151;&#32791;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#26469;&#20248;&#21270;&#27169;&#22411;&#34920;&#29616;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#65292;&#24341;&#21457;&#20102;&#23545;&#26356;&#21152;&#33410;&#33021;&#31639;&#27861;&#21644;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#36215;&#20102;&#29615;&#22659;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25512;&#29702;&#38454;&#27573;&#26085;&#30410;&#22686;&#38271;&#30340;&#33021;&#28304;&#28040;&#32791;&#38382;&#39064;&#12290;&#21363;&#20351;&#31245;&#24494;&#20943;&#23569;&#30005;&#21147;&#20351;&#29992;&#20063;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#28304;&#33410;&#32422;&#65292;&#20351;&#29992;&#25143;&#12289;&#20844;&#21496;&#21644;&#29615;&#22659;&#37117;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#22312;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#20013;&#26368;&#22823;&#21270;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#21151;&#32791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#36866;&#24212;&#24230;&#20989;&#25968;&#20013;&#32771;&#34385;&#20102;&#21151;&#32791;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31361;&#21464;&#31574;&#30053;&#65292;&#20197;&#38543;&#26426;&#26041;&#24335;&#37325;&#26032;&#24341;&#20837;&#23618;&#27169;&#22359;&#65292;&#20855;&#26377;&#33410;&#33021;&#27169;&#22359;&#30340;&#36873;&#25321;&#26426;&#20250;&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#20004;&#20010;&#29420;&#31435;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20799;&#31461;&#21019;&#20260;&#12289;&#24515;&#29702;&#20581;&#24247;&#21442;&#25968;&#21644;&#20854;&#20182;&#34892;&#20026;&#22240;&#32032;&#65292;&#39044;&#27979;&#21360;&#24230;&#25104;&#24180;&#20154;&#30340;&#33258;&#26432;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32423;&#32852;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#33021;&#26377;&#25928;&#20998;&#31867;&#21644;&#39044;&#27979;&#33258;&#26432;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2401.17705</link><description>&lt;p&gt;
&#20351;&#29992;&#20799;&#31461;&#21019;&#20260;&#12289;&#24515;&#29702;&#20581;&#24247;&#38382;&#21367;&#21644;&#26426;&#22120;&#23398;&#20064;&#32423;&#32852;&#38598;&#25104;&#39044;&#27979;&#21360;&#24230;&#25104;&#24180;&#20154;&#30340;&#33258;&#26432;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Predicting suicidal behavior among Indian adults using childhood trauma, mental health questionnaires and machine learning cascade ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20799;&#31461;&#21019;&#20260;&#12289;&#24515;&#29702;&#20581;&#24247;&#21442;&#25968;&#21644;&#20854;&#20182;&#34892;&#20026;&#22240;&#32032;&#65292;&#39044;&#27979;&#21360;&#24230;&#25104;&#24180;&#20154;&#30340;&#33258;&#26432;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32423;&#32852;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#33021;&#26377;&#25928;&#20998;&#31867;&#21644;&#39044;&#27979;&#33258;&#26432;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24180;&#36731;&#25104;&#24180;&#20154;&#20013;&#65292;&#33258;&#26432;&#26159;&#21360;&#24230;&#30340;&#20027;&#35201;&#27515;&#22240;&#65292;&#21344;&#21040;&#20102;&#24778;&#20154;&#30340;&#22269;&#23478;&#33258;&#26432;&#29575;&#32422;16%&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#24320;&#22987;&#21033;&#29992;&#21508;&#31181;&#34892;&#20026;&#29305;&#24449;&#26469;&#39044;&#27979;&#33258;&#26432;&#34892;&#20026;&#12290;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#21360;&#24230;&#32972;&#26223;&#19979;&#30340;&#33258;&#26432;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#20799;&#31461;&#21019;&#20260;&#12289;&#19981;&#21516;&#30340;&#24515;&#29702;&#20581;&#24247;&#25351;&#26631;&#21644;&#20854;&#20182;&#34892;&#20026;&#22240;&#32032;&#65292;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#38598;&#25104;&#26469;&#39044;&#27979;&#33258;&#26432;&#34892;&#20026;&#12290;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;&#21360;&#24230;&#26576;&#20010;&#20581;&#24247;&#20013;&#24515;&#30340;391&#21517;&#20010;&#20307;&#12290;&#36890;&#36807;&#26631;&#20934;&#21270;&#38382;&#21367;&#33719;&#21462;&#20102;&#20851;&#20110;&#20182;&#20204;&#30340;&#20799;&#31461;&#21019;&#20260;&#12289;&#24515;&#29702;&#20581;&#24247;&#21644;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#20449;&#24687;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#32423;&#32852;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#23558;&#33258;&#26432;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among young adults, suicide is India's leading cause of death, accounting for an alarming national suicide rate of around 16%. In recent years, machine learning algorithms have emerged to predict suicidal behavior using various behavioral traits. But to date, the efficacy of machine learning algorithms in predicting suicidal behavior in the Indian context has not been explored in literature. In this study, different machine learning algorithms and ensembles were developed to predict suicide behavior based on childhood trauma, different mental health parameters, and other behavioral factors. The dataset was acquired from 391 individuals from a wellness center in India. Information regarding their childhood trauma, psychological wellness, and other mental health issues was acquired through standardized questionnaires. Results revealed that cascade ensemble learning methods using a support vector machine, decision trees, and random forest were able to classify suicidal behavior with an ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2401.17695</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#20809;&#35889;&#32858;&#31867;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Datacube segmentation via Deep Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#35270;&#35273;&#25216;&#26415;&#22312;&#29289;&#29702;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#31867;&#20998;&#26512;&#20135;&#29983;&#30340;&#25968;&#25454;&#31435;&#26041;&#20307;&#22312;&#35299;&#37322;&#19978;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#65292;&#22240;&#20026;&#24456;&#38590;&#20174;&#32452;&#25104;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#20809;&#35889;&#20013;&#36776;&#21035;&#20986;&#30456;&#20851;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#30340;&#24040;&#22823;&#32500;&#24230;&#23545;&#20110;&#32479;&#35745;&#35299;&#37322;&#26469;&#35828;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#21253;&#21547;&#20102;&#22823;&#37327;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21033;&#29992;&#65292;&#20197;&#25551;&#32472;&#20986;&#25152;&#30740;&#31350;&#26696;&#20363;&#30340;&#19968;&#20123;&#22522;&#26412;&#29305;&#24615;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#36866;&#24403;&#23450;&#20041;&#30340;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#36827;&#34892;&#65288;&#28145;&#24230;&#65289;&#32858;&#31867;&#26469;&#33719;&#24471;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#32534;&#30721;&#31354;&#38388;&#20013;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#19987;&#38376;&#35757;&#32451;&#30340;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;&#22120;&#36827;&#34892;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#22312;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;t-SNE&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;t-SNE&#29983;&#25104;&#30340;&#28857;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#24471;&#20986;&#20102;KL&#25955;&#24230;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17675</link><description>&lt;p&gt;
t-SNE&#20316;&#20026;&#27969;&#24418;&#19978;&#28857;&#20113;&#30340;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence analysis of t-SNE as a gradient flow for point cloud on a manifold
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17675
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;t-SNE&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;t-SNE&#29983;&#25104;&#30340;&#28857;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#24471;&#20986;&#20102;KL&#25955;&#24230;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;t-SNE&#31639;&#27861;&#26377;&#30028;&#24615;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;t-SNE&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#21644;KL&#25955;&#24230;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#19968;&#32452;&#28857;&#65292;&#20351;&#20854;&#19982;&#21407;&#22987;&#25968;&#25454;&#28857;&#30340;&#30456;&#20284;&#24230;&#36739;&#39640;&#65292;&#26368;&#23567;&#21270;KL&#25955;&#24230;&#12290;&#22312;&#23545;&#37319;&#26679;&#25968;&#25454;&#38598;&#36827;&#34892;&#24369;&#25910;&#25947;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;t-SNE&#30340;&#23646;&#24615;&#65292;&#22914;&#22256;&#24785;&#24230;&#21644;&#30456;&#20284;&#24230;&#65292;&#25506;&#31350;&#20102;t-SNE&#29983;&#25104;&#30340;&#28857;&#22312;&#36830;&#32493;&#26799;&#24230;&#27969;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#35777;&#26126;t-SNE&#29983;&#25104;&#30340;&#28857;&#20445;&#25345;&#26377;&#30028;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#35770;&#26469;&#24314;&#31435;KL&#25955;&#24230;&#30340;&#26368;&#23567;&#20540;&#23384;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical foundation regarding the boundedness of the t-SNE algorithm. t-SNE employs gradient descent iteration with Kullback-Leibler (KL) divergence as the objective function, aiming to identify a set of points that closely resemble the original data points in a high-dimensional space, minimizing KL divergence. Investigating t-SNE properties such as perplexity and affinity under a weak convergence assumption on the sampled dataset, we examine the behavior of points generated by t-SNE under continuous gradient flow. Demonstrating that points generated by t-SNE remain bounded, we leverage this insight to establish the existence of a minimizer for KL divergence.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17657</link><description>&lt;p&gt;
&#20174;&#33021;&#37327;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of energy-based model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17657
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#12290;&#36890;&#36807;&#21338;&#24328;&#35770;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#36923;&#36753;&#28165;&#26224;&#65292;&#20844;&#24335;&#31616;&#21333;&#26126;&#20102;&#12290;&#22240;&#27492;&#36991;&#20813;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#35299;&#37322;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#28040;&#38500;&#20102;&#35299;&#20915;&#26631;&#20934;&#21270;&#20998;&#27597;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20551;&#35774;&#26725;&#26753;&#31867;&#22411;&#30340;&#31181;&#32676;&#31526;&#21512;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#37327;&#20989;&#25968;&#12290;&#21033;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#25216;&#26415;&#29983;&#25104;&#33021;&#37327;&#20540;&#36739;&#20302;&#30340;&#26032;&#26679;&#26412;&#65292;&#20174;&#32780;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#26725;&#26753;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#33021;&#37327;&#20989;&#25968;&#35757;&#32451;&#65292;&#31934;&#30830;&#35745;&#31639;&#30495;&#23454;&#21644;&#20266;&#36896;&#26679;&#26412;&#30340;&#33021;&#37327;&#20540;&#12290;&#20174;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#33021;&#37327;&#20989;&#25968;&#23558;&#37319;&#26679;&#28857;&#36716;&#21270;&#20026;&#33021;&#37327;&#24471;&#20998;&#20302;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use energy-based model for bridge-type innovation. The loss function is explained by the game theory, the logic is clear and the formula is simple and clear. Thus avoid the use of maximum likelihood estimation to explain the loss function and eliminate the need for Monte Carlo methods to solve the normalized denominator. Assuming that the bridge-type population follows a Boltzmann distribution, a neural network is constructed to represent the energy function. Use Langevin dynamics technology to generate a new sample with low energy value, thus a generative model of bridge-type based on energy is established. Train energy function on symmetric structured image dataset of three span beam bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately calculate the energy values of real and fake samples. Sampling from latent space, using gradient descent algorithm, the energy function transforms the sampling points into low energy score samples, thereby generating new bridge
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#19981;&#20844;&#24320;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#20513;&#35758;&#21644;&#39033;&#30446;&#26500;&#24605;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#35299;&#20915;&#38544;&#31169;&#21644;&#27861;&#35268;&#38382;&#39064;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17653</link><description>&lt;p&gt;
&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A primer on synthetic health data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17653
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#19981;&#20844;&#24320;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#20513;&#35758;&#21644;&#39033;&#30446;&#26500;&#24605;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#35299;&#20915;&#38544;&#31169;&#21644;&#27861;&#35268;&#38382;&#39064;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#26088;&#22312;&#22312;&#19981;&#20844;&#24320;&#30149;&#20154;&#36523;&#20221;&#25110;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#20174;&#25935;&#24863;&#20581;&#24247;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#12289;&#27169;&#24335;&#21644;&#24635;&#20307;&#31185;&#23398;&#32467;&#35770;&#12290;&#22240;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20419;&#36827;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#25903;&#25345;&#19968;&#31995;&#21015;&#20513;&#35758;&#65292;&#21253;&#25324;&#24320;&#21457;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#12289;&#20808;&#36827;&#30340;&#20581;&#24247;IT&#24179;&#21488;&#20197;&#21450;&#19968;&#33324;&#39033;&#30446;&#26500;&#24605;&#21644;&#20551;&#35774;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#38382;&#39064;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#22914;&#20309;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#33268;&#30340;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#20998;&#20139;&#26102;&#23545;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#39069;&#22806;&#30340;&#27861;&#35268;&#21644;&#27835;&#29702;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35299;&#20915;&#12290;&#22312;&#36825;&#20010;&#21021;&#25506;&#20013;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#26803;&#29702;&#65292;&#21253;&#25324;&#29983;&#25104;&#21644;&#35780;&#20272;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#21450;&#29616;&#26377;&#37096;&#32626;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets. These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information. Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development. However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared. Additional regulatory and governance issues have not been widely addressed. In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17629</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#26694;&#26550;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;IR&#20013;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20165;&#32771;&#34385;&#20687;&#32032;&#32423;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;IR&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#40723;&#21169;&#22270;&#20687;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#20445;&#25345;&#25968;&#25454;&#20445;&#30495;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20462;&#22797;&#12289;&#38477;&#22122;&#21644;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#32454;&#33268;&#35780;&#20272;&#34920;&#26126;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20197;LPIPS&#21644;FID&#25351;&#26631;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;IR&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2401.17626</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative AI to Generate Test Data Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20551;&#25968;&#25454;&#26159;&#29616;&#20195;&#36719;&#20214;&#27979;&#35797;&#30340;&#37325;&#35201;&#32500;&#24230;&#20043;&#19968;&#65292;&#20247;&#22810;&#25968;&#25454;&#20266;&#36896;&#24211;&#30340;&#25968;&#37327;&#21644;&#37325;&#35201;&#24615;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#20266;&#36896;&#24211;&#30340;&#24320;&#21457;&#32773;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#21644;&#39046;&#22495;&#25152;&#38656;&#29983;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#33539;&#22260;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#25191;&#34892;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#65306;1&#65289;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#65292;2&#65289;&#21512;&#25104;&#29305;&#23450;&#35821;&#35328;&#30340;&#31243;&#24207;&#20197;&#29983;&#25104;&#26377;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;3&#65289;&#29983;&#25104;&#20351;&#29992;&#23574;&#31471;&#20266;&#36896;&#24211;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;LLMs&#20026;11&#20010;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#38598;&#25104;&#32423;&#21035;&#19978;&#65292;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064; (GraphMSL)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#23610;&#24230;&#30340;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#21270;&#23398;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17615</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Multi-Similarity Learning for Molecular Property Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064; (GraphMSL)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#23610;&#24230;&#30340;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#21270;&#23398;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25928;&#26524;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26126;&#26174;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#27491;&#36127;&#23545;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20108;&#36827;&#21046;&#30456;&#20284;&#24615;&#20998;&#31867;&#36807;&#20110;&#31616;&#21270;&#20102;&#22797;&#26434;&#20998;&#23376;&#20851;&#31995;&#30340;&#24615;&#36136;&#65292;&#24182;&#24573;&#35270;&#20102;&#20998;&#23376;&#20043;&#38388;&#30456;&#23545;&#30456;&#20284;&#24615;&#30340;&#31243;&#24230;&#65292;&#32473;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#22810;&#30456;&#20284;&#24615;&#23398;&#20064;(GraphMSL)&#26694;&#26550;&#12290;GraphMSL&#23558;&#24191;&#20041;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#34701;&#20837;&#21040;&#36830;&#32493;&#23610;&#24230;&#20013;&#65292;&#21253;&#25324;&#33258;&#30456;&#20284;&#24230;&#21644;&#30456;&#23545;&#30456;&#20284;&#24615;&#12290;&#21333;&#27169;&#22810;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#33258;&#20110;&#21508;&#31181;&#21270;&#23398;&#27169;&#24577;&#65292;&#32780;&#36825;&#20123;&#24230;&#37327;&#30340;&#34701;&#21512;&#24418;&#24335;&#21017;&#26174;&#33879;&#22686;&#24378;&#20102;GraphMSL&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#34701;&#21512;&#30340;&#28789;&#27963;&#24615;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#38656;&#27714;&#65292;&#20351;&#24471;GraphMSL&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#22791;&#26356;&#24191;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective molecular representation learning is essential for molecular property prediction. Contrastive learning, a prominent self-supervised approach for molecular representation learning, relies on establishing positive and negative pairs. However, this binary similarity categorization oversimplifies the nature of complex molecular relationships and overlooks the degree of relative similarities among molecules, posing challenges to the effectiveness and generality of representation learning. In response to this challenge, we propose the Graph Multi-Similarity Learning for Molecular Property Prediction (GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity metric in a continuous scale, capturing self-similarity and relative similarities. The unimodal multi-similarity metrics are derived from various chemical modalities, and the fusion of these metrics into a multimodal form significantly enhances the effectiveness of GraphMSL. In addition, the flexibility of fusion
&lt;/p&gt;</description></item><item><title>IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17612</link><description>&lt;p&gt;
IGCN&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17612
&lt;/p&gt;
&lt;p&gt;
IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#29992;&#20110;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32508;&#21512;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#23545;&#20110;&#28041;&#21450;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#26576;&#20123;&#25968;&#25454;&#27169;&#24577;&#22312;&#39044;&#27979;&#19968;&#20010;&#31867;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#21487;&#33021;&#22312;&#39044;&#27979;&#19981;&#21516;&#31867;&#21035;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#32508;&#21512;&#24037;&#20855;&#32570;&#20047;&#23545;&#20854;&#29305;&#23450;&#39044;&#27979;&#32972;&#21518;&#21407;&#29702;&#30340;&#20840;&#38754;&#21644;&#36830;&#36143;&#29702;&#35299;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#32593;&#32476;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;IGCN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.17585</link><description>&lt;p&gt;
&#20256;&#25773;&#19982;&#38519;&#38449;&#65306;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#35780;&#20272;&#22522;&#20110;&#25512;&#29702;&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;ReCoE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#25152;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#23588;&#20854;&#22312;&#29305;&#23450;&#30340;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#21644;&#36830;&#36143;&#24615;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#26377;&#25928;&#20256;&#25773;&#26356;&#26032;&#30340;&#30456;&#20114;&#20851;&#32852;&#20107;&#23454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#38459;&#30861;&#20934;&#30830;&#25512;&#29702;&#27169;&#22411;&#20013;&#26356;&#26032;&#30693;&#35782;&#36866;&#24403;&#20256;&#25773;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25512;&#29702;&#30340;&#22522;&#20934;&#8212;&#8212;ReCoE&#65288;&#22522;&#20110;&#25512;&#29702;&#30340;&#21453;&#20107;&#23454;&#32534;&#36753;&#25968;&#25454;&#38598;&#65289;&#65292;&#28085;&#30422;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20845;&#31181;&#24120;&#35265;&#25512;&#29702;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#36755;&#20837;&#22686;&#24378;&#12289;&#24494;&#35843;&#21644;&#23450;&#20301;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#37117;&#26126;&#26174;&#36739;&#20302;&#65292;&#23588;&#20854;&#26159;&#22312;&#26576;&#20123;&#25512;&#29702;&#26041;&#26696;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#32534;&#36753;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#29983;&#25104;&#30340;&#20998;&#26512;&#65292;&#20174;&#25512;&#29702;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#29616;&#26377;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19981;&#36275;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#32534;&#36753;&#12289;&#20107;&#23454;&#22238;&#24518;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17580</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning with Cohesive Subgraph Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#21508;&#31181;&#22270;&#34920;&#24449;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#31038;&#20132;&#21644;&#29983;&#29289;&#21307;&#23398;&#32593;&#32476;&#12290;GCL&#24191;&#27867;&#20351;&#29992;&#38543;&#26426;&#22270;&#25299;&#25169;&#22686;&#24378;&#65292;&#22914;&#22343;&#21248;&#33410;&#28857;&#20002;&#22833;&#65292;&#29983;&#25104;&#22686;&#24378;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#26426;&#22686;&#24378;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#24182;&#24694;&#21270;&#21518;&#32493;&#30340;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#22270;&#22686;&#24378;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26377;&#21487;&#33021;&#25552;&#39640;GCL&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CTAug&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#20869;&#32858;&#24847;&#35782;&#25972;&#21512;&#21040;&#21508;&#31181;&#29616;&#26377;&#30340;GCL&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CTAug&#21253;&#25324;&#20004;&#20010;&#19987;&#38376;&#30340;&#27169;&#22359;&#65306;&#25299;&#25169;&#22686;&#24378;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#22686;&#24378;&#12290;&#21069;&#32773;&#29983;&#25104;&#35880;&#24910;&#20445;&#30041;&#20869;&#32858;&#24615;&#36136;&#30340;&#22686;&#24378;&#22270;&#65292;&#32780;&#21518;&#32773;&#22686;&#24378;&#20102;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#26367;&#25442;&#20026;Hyena&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#24182;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#25216;&#26415;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2401.17574</link><description>&lt;p&gt;
Scavenging Hyena: &#23558;Transformer&#27169;&#22411;&#31934;&#28860;&#20026;&#38271;&#21367;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scavenging Hyena: Distilling Transformers into Long Convolution Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#23558;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#26367;&#25442;&#20026;Hyena&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#24182;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#36825;&#19968;&#25216;&#26415;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;GPT-4&#31561;&#26550;&#26500;&#20026;&#20856;&#33539;&#65292;&#37325;&#22609;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#36328;&#26550;&#26500;&#36801;&#31227;&#30340;&#26041;&#27861;&#12290;&#20511;&#37492;&#39640;&#25928;&#30340;Hyena&#26426;&#21046;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;Hyena&#26469;&#26367;&#25442;Transformer&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26367;&#20195;&#20256;&#32479;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35201;&#38754;&#23545;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#20108;&#27425;&#27880;&#24847;&#21147;&#26426;&#21046;&#22266;&#26377;&#30340;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;LLM&#19981;&#26029;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36861;&#27714;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33021;&#21147;&#21644;&#29615;&#22659;&#24433;&#21709;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#30340;&#24037;&#33402;&#25511;&#21046;&#21644;&#30417;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#20013;&#39640;&#32500;&#24230;&#22522;&#20110;&#22270;&#20687;&#30340;&#21472;&#21152;&#35823;&#24046;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26377;&#38480;&#30340;&#25511;&#21046;&#37197;&#26041;&#20943;&#23567;&#21472;&#21152;&#35823;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#31283;&#23450;&#30340;&#24352;&#37327;&#25968;&#25454;&#25511;&#21046;&#22120;&#26469;&#22788;&#29702;&#39640;&#32500;&#24230;&#25200;&#21160;&#12290;</title><link>https://arxiv.org/abs/2401.17573</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#25511;&#21046;&#19982;&#30417;&#25511;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tensor-based process control and monitoring for semiconductor manufacturing with unstable disturbances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#30340;&#24037;&#33402;&#25511;&#21046;&#21644;&#30417;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#20013;&#39640;&#32500;&#24230;&#22522;&#20110;&#22270;&#20687;&#30340;&#21472;&#21152;&#35823;&#24046;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26377;&#38480;&#30340;&#25511;&#21046;&#37197;&#26041;&#20943;&#23567;&#21472;&#21152;&#35823;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#31283;&#23450;&#30340;&#24352;&#37327;&#25968;&#25454;&#25511;&#21046;&#22120;&#26469;&#22788;&#29702;&#39640;&#32500;&#24230;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21046;&#36896;&#31995;&#32479;&#20013;&#23433;&#35013;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#21644;&#26222;&#21450;&#65292;&#21046;&#36896;&#36807;&#31243;&#20013;&#25910;&#38598;&#21040;&#20102;&#22797;&#26434;&#30340;&#25968;&#25454;&#65292;&#32473;&#20256;&#32479;&#30340;&#36807;&#31243;&#25511;&#21046;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#33402;&#25511;&#21046;&#21644;&#30417;&#25511;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#20013;&#39640;&#32500;&#24230;&#22522;&#20110;&#22270;&#20687;&#30340;&#21472;&#21152;&#35823;&#24046;&#65288;&#20197;&#24352;&#37327;&#24418;&#24335;&#24314;&#27169;&#65289;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#25511;&#21046;&#37197;&#26041;&#20943;&#23567;&#21472;&#21152;&#35823;&#24046;&#12290;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#24352;&#37327;-&#21521;&#37327;&#22238;&#24402;&#31639;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#65292;&#20197;&#20943;&#36731;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#24352;&#37327;&#21442;&#25968;&#30340;&#20272;&#35745;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#31283;&#23450;&#24615;&#30340;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#65288;EWMA&#65289;&#24352;&#37327;&#25968;&#25454;&#25511;&#21046;&#22120;&#65292;&#20854;&#31283;&#23450;&#24615;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;&#32771;&#34385;&#21040;&#20302;&#32500;&#24230;&#30340;&#25511;&#21046;&#37197;&#26041;&#19981;&#33021;&#24357;&#34917;&#25152;&#26377;&#39640;&#32500;&#24230;&#25200;&#21160;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development and popularity of sensors installed in manufacturing systems, complex data are collected during manufacturing processes, which brings challenges for traditional process control methods. This paper proposes a novel process control and monitoring method for the complex structure of high-dimensional image-based overlay errors (modeled in tensor form), which are collected in semiconductor manufacturing processes. The proposed method aims to reduce overlay errors using limited control recipes. We first build a high-dimensional process model and propose different tensor-on-vector regression algorithms to estimate parameters in the model to alleviate the curse of dimensionality. Then, based on the estimate of tensor parameters, the exponentially weighted moving average (EWMA) controller for tensor data is designed whose stability is theoretically guaranteed. Considering the fact that low-dimensional control recipes cannot compensate for all high-dimensional disturbances o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#22312;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#22810;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#21098;&#26525;&#26041;&#27861;&#21644;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17546</link><description>&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#22312;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#22810;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Stage Training Model For Edge Computing Devices In Intrusion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#22312;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#22810;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#21098;&#26525;&#26041;&#27861;&#21644;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#25193;&#24352;&#21644;&#20114;&#32852;&#29615;&#22659;&#20013;&#65292;&#20837;&#20405;&#26816;&#27979;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#24694;&#24847;&#20195;&#30721;&#30340;&#19981;&#26029;&#36827;&#21270;&#21644;&#25915;&#20987;&#26041;&#27861;&#30340;&#22797;&#26434;&#21270;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#20934;&#30830;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#20197;&#20351;&#20854;&#26356;&#36866;&#24212;&#21508;&#31181;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#65292;&#29305;&#21035;&#26159;&#23884;&#20837;&#22312;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#35774;&#22791;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#65292;&#21152;&#19978;&#22686;&#24378;&#30340;&#21098;&#26525;&#26041;&#27861;&#21644;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#30446;&#26631;&#26159;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20837;&#20405;&#26816;&#27979;&#30340;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#22312;UNSW-NB15&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#30340;&#35268;&#27169;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#31867;&#20284;&#25552;&#26696;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrusion detection poses a significant challenge within expansive and persistently interconnected environments. As malicious code continues to advance and sophisticated attack methodologies proliferate, various advanced deep learning-based detection approaches have been proposed. Nevertheless, the complexity and accuracy of intrusion detection models still need further enhancement to render them more adaptable to diverse system categories, particularly within resource-constrained devices, such as those embedded in edge computing systems. This research introduces a three-stage training paradigm, augmented by an enhanced pruning methodology and model compression techniques. The objective is to elevate the system's effectiveness, concurrently maintaining a high level of accuracy for intrusion detection. Empirical assessments conducted on the UNSW-NB15 dataset evince that this solution notably reduces the model's dimensions, while upholding accuracy levels equivalent to similar proposals.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22266;&#23450;&#28857;&#37327;&#21270;&#26041;&#27861;QFX&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;FPGA&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#12290;QFX&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#20108;&#36827;&#21046;&#28857;&#20301;&#32622;&#65292;&#24182;&#24341;&#20837;&#20102;&#26080;&#20056;&#27861;&#30340;&#37327;&#21270;&#31574;&#30053;&#20197;&#26368;&#23567;&#21270;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17544</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22266;&#23450;&#28857;&#37327;&#21270;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#22312;FPGA&#19978;&#30340;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22266;&#23450;&#28857;&#37327;&#21270;&#26041;&#27861;QFX&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;FPGA&#30340;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#12290;QFX&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#20108;&#36827;&#21046;&#28857;&#20301;&#32622;&#65292;&#24182;&#24341;&#20837;&#20102;&#26080;&#20056;&#27861;&#30340;&#37327;&#21270;&#31574;&#30053;&#20197;&#26368;&#23567;&#21270;&#30828;&#20214;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#65288;&#22914;&#23884;&#20837;&#24335;FPGA&#65289;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#30697;&#38453;&#20056;&#27861;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#23558;BatchNorm&#25110;shortcut&#31561;&#20854;&#20182;&#23618;&#20445;&#30041;&#20026;&#28014;&#28857;&#24418;&#24335;&#65292;&#23613;&#31649;&#22312;FPGA&#19978;&#22266;&#23450;&#28857;&#31639;&#26415;&#26356;&#39640;&#25928;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;FPGA&#37096;&#32626;&#65292;&#20294;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35757;&#32451;&#22266;&#23450;&#28857;&#37327;&#21270;&#26041;&#27861;QFX&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#23398;&#20064;&#20108;&#36827;&#21046;&#28857;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#20056;&#27861;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;DSP&#30340;&#20351;&#29992;&#12290;QFX&#26159;&#22522;&#20110;PyTorch&#30340;&#24211;&#65292;&#21487;&#20197;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#27169;&#25311;&#22266;&#23450;&#28857;&#31639;&#26415;&#65292;&#24182;&#24471;&#21040;FPGA HLS&#25903;&#25345;&#12290;&#36890;&#36807;&#20351;&#29992;QFX&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;HLS&#37096;&#32626;&#65292;&#24182;&#20135;&#29983;&#19982;&#28014;&#28857;&#27169;&#22411;&#30456;&#21516;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2401.17541</link><description>&lt;p&gt;
&#36879;&#36807;&#26657;&#20934;&#30340;&#35270;&#35282;&#29702;&#35299;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36817;&#20284;&#26041;&#27861;&#30340;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#25216;&#26415;&#65292;&#20197;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#65292;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20851;&#38190;&#29305;&#24449;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27979;&#35797;&#20998;&#24067;&#24448;&#24448;&#19982;&#35757;&#32451;&#19981;&#21516;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#36234;&#22495;&#27867;&#21270;&#65292;&#22312;&#24120;&#35268;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#20316;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#26088;&#22312;&#35782;&#21035;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#36234;&#22495;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;IRM&#30340;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;&#21452;&#23618;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#36817;&#20284;IRM&#25216;&#26415;&#65292;&#20351;&#29992;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#20316;&#20026;&#20851;&#38190;&#25351;&#26631;&#12290;ECE&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#23427;&#26159;&#34913;&#37327;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#25429;&#25417;&#21040;&#29615;&#22659;&#19981;&#21464;&#29305;&#24449;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;IRM&#22312;&#21387;&#32553;&#20102;...&#65288;&#25509;&#19979;&#37096;&#20998;&#25688;&#35201;&#36229;&#36807;200&#23383;&#65292;&#25552;&#21462;&#21069;200&#23383;&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38598;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#20998;&#25968;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#25104;&#21151;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#31890;&#23376;&#38598;&#21512;&#21160;&#24577;&#35745;&#31639;&#36817;&#20284;&#21453;&#25193;&#25955;&#28418;&#31227;&#30340;&#26080;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23545;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#24314;&#27169;&#21644;&#22312;&#22320;&#29699;&#29289;&#29702;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17539</link><description>&lt;p&gt;
&#29992;&#38598;&#25104;&#26041;&#27861;&#22686;&#24378;&#22522;&#20110;&#20998;&#25968;&#30340;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Score-Based Sampling Methods with Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17539
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38598;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#20998;&#25968;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#25104;&#21151;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#31890;&#23376;&#38598;&#21512;&#21160;&#24577;&#35745;&#31639;&#36817;&#20284;&#21453;&#25193;&#25955;&#28418;&#31227;&#30340;&#26080;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23545;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#24314;&#27169;&#21644;&#22312;&#22320;&#29699;&#29289;&#29702;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#25277;&#26679;&#26041;&#27861;&#20013;&#24341;&#20837;&#20102;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#31890;&#23376;&#38598;&#21512;&#21160;&#24577;&#35745;&#31639;&#36817;&#20284;&#21453;&#25193;&#25955;&#28418;&#31227;&#30340;&#26080;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24213;&#23618;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#23427;&#19982;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21644;&#20808;&#21069;&#20171;&#32461;&#30340;F&#246;llmer&#25277;&#26679;&#22120;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#31034;&#20363;&#35777;&#26126;&#20102;&#38598;&#25104;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#28085;&#30422;&#20102;&#20174;&#20302;&#32500;&#21040;&#20013;&#31561;&#32500;&#24230;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#23792;&#21644;&#39640;&#24230;&#38750;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;NUTS&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#22312;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#38598;&#25104;&#31574;&#30053;&#22312;&#24314;&#27169;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22320;&#29699;&#29289;&#29702;&#31185;&#23398;&#30340;&#36125;&#21494;&#26031;&#21453;&#28436;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ensembles within score-based sampling methods to develop gradient-free approximate sampling techniques that leverage the collective dynamics of particle ensembles to compute approximate reverse diffusion drifts. We introduce the underlying methodology, emphasizing its relationship with generative diffusion models and the previously introduced F\"ollmer sampler. We demonstrate the efficacy of ensemble strategies through various examples, ranging from low- to medium-dimensionality sampling problems, including multi-modal and highly non-Gaussian probability distributions, and provide comparisons to traditional methods like NUTS. Our findings highlight the potential of ensemble strategies for modeling complex probability distributions in situations where gradients are unavailable. Finally, we showcase its application in the context of Bayesian inversion problems within the geophysical sciences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#28216;&#25103;&#35770;&#22495;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21338;&#24328;&#22343;&#34913;&#32473;&#20986;&#20102;&#26368;&#24378;&#22823;&#30340;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28216;&#25103;&#35770;&#22495;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#65288;GUE&#65289;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.17523</link><description>&lt;p&gt;
&#28216;&#25103;&#35770;&#22495;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Unlearnable Example Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#28216;&#25103;&#35770;&#22495;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21338;&#24328;&#22343;&#34913;&#32473;&#20986;&#20102;&#26368;&#24378;&#22823;&#30340;&#27602;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28216;&#25103;&#35770;&#22495;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#65288;GUE&#65289;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#25915;&#20987;&#26159;&#19968;&#31181;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#26088;&#22312;&#36890;&#36807;&#21521;&#35757;&#32451;&#26679;&#26412;&#28155;&#21152;&#24494;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#30340;&#24178;&#20928;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#36825;&#21487;&#20197;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#20108;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#23545;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23558;&#25915;&#20987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#38750;&#38646;&#21644;Stackelberg&#21338;&#24328;&#12290;&#39318;&#20808;&#65292;&#22312;&#27491;&#24120;&#35774;&#32622;&#21644;&#23545;&#25239;&#35757;&#32451;&#35774;&#32622;&#19979;&#35777;&#26126;&#20102;&#21338;&#24328;&#22343;&#34913;&#30340;&#23384;&#22312;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#26102;&#65292;&#21338;&#24328;&#22343;&#34913;&#32473;&#20986;&#20102;&#26368;&#24378;&#22823;&#30340;&#27602;&#25915;&#20987;&#65292;&#21363;&#22312;&#30456;&#21516;&#20551;&#35774;&#31354;&#38388;&#20869;&#65292;&#21463;&#23475;&#32773;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#26368;&#20302;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#28216;&#25103;&#35770;&#22495;&#19981;&#21487;&#23398;&#20064;&#31034;&#20363;&#65288;GUE&#65289;&#65292;&#23427;&#20027;&#35201;&#21253;&#25324;&#19977;&#20010;&#26799;&#24230;&#12290;&#65288;1&#65289;&#36890;&#36807;&#30452;&#25509;&#27714;&#35299;&#22343;&#34913;&#33719;&#24471;&#27602;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting. It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients. (1) The poisons are obtained by directly solving the equilibrium of the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#27169;&#22411;&#30340;&#8220;&#36951;&#24536;&#8221;&#36827;&#34892;&#20102;&#22240;&#26524;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31227;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#26102;&#23545;&#21097;&#20313;&#25968;&#25454;&#20449;&#24687;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17504</link><description>&lt;p&gt;
CaMU&#65306;&#28145;&#24230;&#27169;&#22411;&#8220;&#36951;&#24536;&#8221;&#30340;&#22240;&#26524;&#25928;&#24212;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
CaMU: Disentangling Causal Effects in Deep Model Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#27169;&#22411;&#30340;&#8220;&#36951;&#24536;&#8221;&#36827;&#34892;&#20102;&#22240;&#26524;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31227;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#26102;&#23545;&#21097;&#20313;&#25968;&#25454;&#20449;&#24687;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#8220;&#36951;&#24536;&#8221;&#38656;&#35201;&#22312;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#30340;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#20445;&#30041;&#21097;&#20313;&#25968;&#25454;&#30340;&#24517;&#35201;&#20449;&#24687;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#30340;&#25928;&#26524;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#21487;&#33021;&#23545;&#21097;&#20313;&#25968;&#25454;&#30340;&#20449;&#24687;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#25968;&#25454;&#31227;&#38500;&#21518;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#31227;&#38500;&#21518;&#20462;&#22797;&#21097;&#20313;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#34987;&#36951;&#24536;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#22312;&#20462;&#22797;&#21518;&#20877;&#27425;&#20986;&#29616;&#12290;&#36825;&#20010;&#38382;&#39064;&#26159;&#30001;&#20110;&#8220;&#36951;&#24536;&#8221;&#21644;&#21097;&#20313;&#25968;&#25454;&#30340;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20132;&#32455;&#36896;&#25104;&#30340;&#12290;&#22312;&#20805;&#20998;&#21306;&#20998;&#36825;&#20004;&#31867;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#20043;&#21069;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#20882;&#30528;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#30340;&#39118;&#38505;&#65292;&#35201;&#20040;&#25439;&#22833;&#21097;&#20313;&#25968;&#25454;&#30340;&#23453;&#36149;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#19981;&#36275;&#65292;&#26412;&#30740;&#31350;&#23545;&#8220;&#36951;&#24536;&#8221;&#36827;&#34892;&#20102;&#22240;&#26524;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning requires removing the information of forgetting data while keeping the necessary information of remaining data. Despite recent advancements in this area, existing methodologies mainly focus on the effect of removing forgetting data without considering the negative impact this can have on the information of the remaining data, resulting in significant performance degradation after data removal. Although some methods try to repair the performance of remaining data after removal, the forgotten information can also return after repair. Such an issue is due to the intricate intertwining of the forgetting and remaining data. Without adequately differentiating the influence of these two kinds of data on the model, existing algorithms take the risk of either inadequate removal of the forgetting data or unnecessary loss of valuable information from the remaining data. To address this shortcoming, the present study undertakes a causal analysis of the unlearning and introduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17484</link><description>&lt;p&gt;
&#20687;&#32032;&#21040;&#39640;&#31243;&#65306;&#20351;&#29992;&#22270;&#20687;&#23398;&#20064;&#39044;&#27979;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#30340;&#38271;&#36317;&#31163;&#39640;&#31243;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#38271;&#36317;&#31163;&#29702;&#35299;&#22320;&#24418;&#25299;&#25169;&#23545;&#20110;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#36895;&#23548;&#33322;&#26102;&#12290;&#30446;&#21069;&#65292;&#20960;&#20309;&#26144;&#23556;&#20027;&#35201;&#20381;&#36182;&#20110;LiDAR&#20256;&#24863;&#22120;&#65292;&#20294;&#22312;&#26356;&#36828;&#36317;&#31163;&#30340;&#26144;&#23556;&#26102;&#25552;&#20379;&#30340;&#27979;&#37327;&#25968;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20165;&#20351;&#29992;&#23454;&#26102;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#20803;&#32032;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#19982;&#20808;&#21069;&#30340;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#36328;&#35270;&#22270;&#20851;&#32852;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#23558;3D&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#19982;&#22810;&#35270;&#35282;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#22320;&#24418;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#65292;&#20197;&#23454;&#29616;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#26356;&#22909;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding terrain topology at long-range is crucial for the success of off-road robotic missions, especially when navigating at high-speeds. LiDAR sensors, which are currently heavily relied upon for geometric mapping, provide sparse measurements when mapping at greater distances. To address this challenge, we present a novel learning-based approach capable of predicting terrain elevation maps at long-range using only onboard egocentric images in real-time. Our proposed method is comprised of three main elements. First, a transformer-based encoder is introduced that learns cross-view associations between the egocentric views and prior bird-eye-view elevation map predictions. Second, an orientation-aware positional encoding is proposed to incorporate the 3D vehicle pose information over complex unstructured terrain with multi-view visual image features. Lastly, a history-augmented learn-able map embedding is proposed to achieve better temporal consistency between elevation map predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2401.17477</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#24515;&#29702;&#38556;&#30861;&#65306;&#22522;&#20110;ChatGPT&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#23545;&#35805;&#20195;&#29702;&#22120;ChatGPT&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#24182;&#20511;&#21161;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#35835;&#30340;&#35780;&#35770;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#34920;&#36798;&#30340;&#25233;&#37057;&#30151;&#29366;&#30340;&#39057;&#29575;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#26041;&#27861;&#26469;&#21450;&#26102;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;ChatGPT&#31561;&#23545;&#35805;&#20195;&#29702;&#22120;&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24212;&#23545;&#21487;&#35299;&#37322;&#24615;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;Twitter&#29305;&#23450;&#21464;&#20307;BERTweet&#19982;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#35299;&#37322;&#27169;&#22411;BERT-XDD&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35299;&#37322;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#21147;&#25552;&#20379;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;ChatGPT&#23558;&#25216;&#26415;&#35299;&#37322;&#36716;&#21270;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#35780;&#35770;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#21487;&#35299;&#37322;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#21457;&#23637;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#25968;&#23383;&#24179;&#21488;&#20570;&#20986;&#36129;&#29486;&#65292;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17460</link><description>&lt;p&gt;
&#20351;&#26080;&#32447;&#29615;&#22659;&#23545;&#26799;&#24230;&#20272;&#35745;&#22120;&#26377;&#29992;&#65306;&#19968;&#31181;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#36890;&#20449;&#26102;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#28857;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#30693;&#36947;&#36890;&#36947;&#29366;&#24577;&#31995;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#26080;&#32447;&#36890;&#36947;&#21253;&#21547;&#22312;&#23398;&#20064;&#31639;&#27861;&#26412;&#36523;&#20013;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#28010;&#36153;&#36164;&#28304;&#26469;&#20998;&#26512;&#21644;&#28040;&#38500;&#20854;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#26159;&#65292;&#22312;FL&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#19981;&#26159;&#20984;&#30340;&#65292;&#36825;&#20351;&#24471;&#23558;FL&#25193;&#23637;&#21040;ZO&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20197;&#21450;&#21253;&#25324;&#24433;&#21709;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a novel approach to machine learning that allows multiple edge devices to collaboratively train a model without disclosing their raw data. However, several challenges hinder the practical implementation of this approach, especially when devices and the server communicate over wireless channels, as it suffers from communication and computation bottlenecks in this case. By utilizing a communication-efficient framework, we propose a novel zero-order (ZO) method with a one-point gradient estimator that harnesses the nature of the wireless communication channel without requiring the knowledge of the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. The two main difficulties of this work are that in FL, the objective function is usually not convex, which makes the extension of FL to ZO methods challenging, and that including the impa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28082;&#24577;&#27665;&#20027;&#23454;&#29616;&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#22996;&#27966;&#26426;&#21046;&#35782;&#21035;&#21644;&#31227;&#38500;&#20887;&#20313;&#20998;&#31867;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38598;&#21512;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#27604;&#26576;&#20123;&#22686;&#24378;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#26694;&#26550;&#22312;&#38750;&#20256;&#32479;&#39046;&#22495;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.17443</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#28082;&#24577;&#27665;&#20027;
&lt;/p&gt;
&lt;p&gt;
Liquid Democracy for Low-Cost Ensemble Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28082;&#24577;&#27665;&#20027;&#23454;&#29616;&#20302;&#25104;&#26412;&#38598;&#21512;&#21098;&#26525;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#22996;&#27966;&#26426;&#21046;&#35782;&#21035;&#21644;&#31227;&#38500;&#20887;&#20313;&#20998;&#31867;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#38598;&#21512;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#27604;&#26576;&#20123;&#22686;&#24378;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#26694;&#26550;&#22312;&#38750;&#20256;&#32479;&#39046;&#22495;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#38598;&#21512;&#23398;&#20064;&#21644;&#19968;&#31181;&#20195;&#29702;&#25237;&#31080;&#27169;&#24335;&#8212;&#8212;&#28082;&#24577;&#27665;&#20027;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#28872;&#30340;&#32852;&#31995;&#65292;&#21487;&#20197;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#26469;&#38477;&#20302;&#38598;&#21512;&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#28082;&#24577;&#27665;&#20027;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#22996;&#27966;&#26426;&#21046;&#26469;&#35782;&#21035;&#21644;&#31227;&#38500;&#38598;&#21512;&#20013;&#30340;&#20887;&#20313;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#36807;&#31243;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#19968;&#20010;&#23436;&#25972;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#31934;&#36873;&#24213;&#23618;&#30340;&#22996;&#27966;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#32676;&#20307;&#30340;&#26435;&#37325;&#38598;&#20013;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#65292;&#36825;&#39033;&#24037;&#20316;&#20063;&#23637;&#31034;&#20102;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#25991;&#29486;&#20013;&#30340;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#38750;&#20256;&#32479;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that there is a strong connection between ensemble learning and a delegative voting paradigm -- liquid democracy -- that can be leveraged to reduce ensemble training costs. We present an incremental training procedure that identifies and removes redundant classifiers from an ensemble via delegation mechanisms inspired by liquid democracy. Through both analysis and extensive experiments we show that this process greatly reduces the computational cost of training compared to training a full ensemble. By carefully selecting the underlying delegation mechanism, weight centralization in the classifier population is avoided, leading to higher accuracy than some boosting methods. Furthermore, this work serves as an exemplar of how frameworks from computational social choice literature can be applied to problems in nontraditional domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#21463;&#21040;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#30340;&#21327;&#26041;&#24046;&#35745;&#31639;&#23545;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17441</link><description>&lt;p&gt;
&#25581;&#31034;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining Predictive Uncertainty by Exposing Second-Order Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#21463;&#21040;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#30340;&#21327;&#26041;&#24046;&#35745;&#31639;&#23545;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;Explainable AI&#65289;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#40657;&#31665;&#21464;&#24471;&#36879;&#26126;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#29992;&#26469;&#36827;&#34892;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#20026;&#20160;&#20040;&#27169;&#22411;&#8220;&#19981;&#30830;&#23450;&#8221;&#65292;&#30446;&#21069;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20027;&#35201;&#30001;&#28041;&#21450;&#21333;&#20010;&#29305;&#24449;&#25110;&#29305;&#24449;&#20043;&#38388;&#30340;&#20056;&#31215;&#30456;&#20114;&#20316;&#29992;&#30340;&#20108;&#38454;&#24433;&#21709;&#25152;&#20027;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36825;&#20123;&#20108;&#38454;&#24433;&#21709;&#26469;&#35299;&#37322;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#35745;&#31639;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#25104;&#23545;&#19968;&#32452;&#19968;&#38454;&#35299;&#37322;&#36827;&#34892;&#31616;&#21333;&#21327;&#26041;&#24046;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#21487;&#20197;&#23558;&#24120;&#35265;&#30340;&#24402;&#22240;&#25216;&#26415;&#65288;LRP&#65292;Gradient x Input&#31561;&#65289;&#36716;&#21270;&#20026;&#24378;&#22823;&#30340;&#20108;&#38454;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#22120;&#65292;&#31216;&#20026;CovLRP&#65292;CovGI&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#37327;&#21270;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#20135;&#29983;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25972;&#20307;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#22823;&#23884;&#20837;&#32500;&#24230;&#24773;&#20917;&#19979;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#19979;&#37117;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2401.17426</link><description>&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Superiority of Multi-Head Attention in In-Context Linear Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#22823;&#23884;&#20837;&#32500;&#24230;&#24773;&#20917;&#19979;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#19979;&#37117;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#22312;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;softmax&#27880;&#24847;&#21147;&#30340;transformer&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#21333;&#22836;/&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25910;&#25947;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#36739;&#22823;&#23884;&#20837;&#32500;&#24230;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#27604;&#21333;&#22836;&#27880;&#24847;&#21147;&#34920;&#29616;&#26356;&#22909;&#12290;&#24403;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;D&#22686;&#21152;&#26102;&#65292;&#20351;&#29992;&#21333;&#22836;/&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#39044;&#27979;&#25439;&#22833;&#20026;O(1/D)&#65292;&#32780;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#20056;&#27861;&#24120;&#25968;&#36739;&#23567;&#12290;&#38500;&#20102;&#26368;&#31616;&#21333;&#30340;&#25968;&#25454;&#20998;&#24067;&#35774;&#32622;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26356;&#22810;&#24773;&#26223;&#65292;&#20363;&#22914;&#22122;&#22768;&#26631;&#31614;&#65292;&#23616;&#37096;&#31034;&#20363;&#65292;&#30456;&#20851;&#29305;&#24449;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24635;&#30340;&#26469;&#35828;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#20248;&#20110;&#21333;&#22836;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#22522;&#20110;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#39044;&#27979;&#20102;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#32467;&#26524;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2401.17424</link><description>&lt;p&gt;
&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#24555;&#36895;&#20013;&#24494;&#23376;&#25442; flavor &#21518;&#30340;&#33021;&#35889;
&lt;/p&gt;
&lt;p&gt;
Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#22522;&#20110;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#39044;&#27979;&#20102;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#32467;&#26524;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24494;&#23376;&#22312;&#26497;&#23494;&#24230;&#30340;&#22825;&#20307;&#29615;&#22659;&#20013;&#65292;&#22914;&#26680;&#24515;&#22349;&#32553;&#36229;&#26032;&#26143;&#21644;&#20013;&#23376;&#26143;&#21512;&#24182;&#20013;&#65292;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#30340; flavor &#36716;&#25442;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#30340;&#24555;&#36895; flavor &#36716;&#25442;&#65292;&#21457;&#29616;&#24403;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#22686;&#38271;&#29575;&#26174;&#33879;&#36229;&#36807;&#30495;&#31354;&#21704;&#23494;&#39039;&#37327;&#30340;&#22686;&#38271;&#29575;&#26102;&#65292;&#25152;&#26377;&#20013;&#24494;&#23376;&#65288;&#26080;&#35770;&#20854;&#33021;&#37327;&#22914;&#20309;&#65289;&#37117;&#20849;&#20139;&#19968;&#20010;&#30001;&#33021;&#37327;&#31215;&#20998;&#20013;&#24494;&#23376;&#35889;&#20915;&#23450;&#30340;&#23384;&#27963;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#39044;&#27979;&#22312;&#36825;&#31181;&#22810;&#33021;&#37327;&#20013;&#24494;&#23376;&#27668;&#20307;&#20013;&#24555;&#36895; flavor &#36716;&#25442;&#30340;&#28176;&#36817;&#32467;&#26524;&#12290;&#36825;&#20123;&#39044;&#27979;&#22522;&#20110;&#27599;&#20010;&#33021;&#37327; bin &#30340;&#20013;&#24494;&#23376;&#35282;&#20998;&#24067;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#36890;&#24120;&#21487;&#20197;&#20174;&#26368;&#20808;&#36827;&#30340;&#36229;&#26032;&#26143;&#21644;&#20013;&#23376;&#26143;&#27169;&#25311;&#20013;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340; PINNs &#23545;&#20110;&#39044;&#27979;&#30005;&#23376;&#36890;&#36947;&#20013;&#30340;&#20013;&#24494;&#23376;&#25968;&#37327;&#21644;&#20013;&#24494;&#23376;&#30697;&#30340;&#30456;&#23545;&#32477;&#23545;&#35823;&#24046;&#20998;&#21035;&#36798;&#21040;&#19981;&#21040;6&#65285;&#21644;&#19981;&#21040;18&#65285;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense astrophysical environments such as core-collapse supernovae (CCSNe) and neutron star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy} neutrino gas, revealing that when the FFC growth rate significantly exceeds that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a common survival probability dictated by the energy-integrated neutrino spectrum. We then employ physics-informed neural networks (PINNs) to predict the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These predictions are based on the first two moments of neutrino angular distributions for each energy bin, typically available in state-of-the-art CCSN and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and $\lesssim 18\%$ for predicting the number of neutrinos in the electron channel and the relative absolute error in the neutrino moments, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17417</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31359;&#22681;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Through-Wall Imaging based on WiFi Channel State Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#22312;&#31359;&#22681;&#22330;&#26223;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#21033;&#29992;WiFi&#30340;&#20248;&#21183;&#65292;&#22914;&#25104;&#26412;&#25928;&#30410;&#65292;&#20809;&#29031;&#19981;&#21464;&#24615;&#21644;&#31359;&#22681;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#29615;&#22659;&#30340;&#21487;&#35270;&#21270;&#30417;&#27979;&#65292;&#36234;&#36807;&#25151;&#38388;&#36793;&#30028;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23427;&#36890;&#36807;&#35299;&#38145;&#25191;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#27963;&#21160;&#35782;&#21035;&#65289;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;WiFi CSI&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20174;WiFi CSI&#21040;&#22270;&#20687;&#30340;&#36328;&#27169;&#24577;&#36716;&#25442;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36866;&#24212;&#25105;&#20204;&#38382;&#39064;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26550;&#26500;&#37197;&#32622;&#30340;&#21076;&#38500;&#30740;&#31350;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#23450;&#37327;/&#23450;&#24615;&#35780;&#20272;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#20234;&#36763;&#30828;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17408</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#29627;&#23572;&#20857;&#26364;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Boltzmann Optimization Problems with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#20234;&#36763;&#30828;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#25928;&#29575;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#20960;&#21313;&#24180;&#21518;&#65292;&#21363;&#23558;&#32467;&#26463;&#12290;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#25216;&#26415;&#24050;&#25509;&#36817;&#29289;&#29702;&#26497;&#38480;&#65292;&#36827;&#19968;&#27493;&#24494;&#22411;&#21270;&#23558;&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#26410;&#26469;&#30340;HPC&#25928;&#29575;&#25552;&#21319;&#23558;&#24517;&#28982;&#20381;&#36182;&#20110;&#26032;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;&#20234;&#36763;&#27169;&#22411;&#26174;&#31034;&#20986;&#20316;&#20026;&#39640;&#33021;&#25928;&#35745;&#31639;&#26410;&#26469;&#26694;&#26550;&#30340;&#29305;&#27530;&#28508;&#21147;&#12290;&#20234;&#36763;&#31995;&#32479;&#33021;&#22815;&#22312;&#25509;&#36817;&#28909;&#21147;&#23398;&#26497;&#38480;&#30340;&#33021;&#37327;&#28040;&#32791;&#19979;&#36816;&#34892;&#35745;&#31639;&#12290;&#20234;&#36763;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#20805;&#24403;&#36923;&#36753;&#21644;&#23384;&#20648;&#22120;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#28040;&#38500;&#26114;&#36149;&#30340;&#25968;&#25454;&#31227;&#21160;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#26174;&#33879;&#38477;&#20302; CMOS &#35745;&#31639;&#25152;&#22266;&#26377;&#30340;&#33021;&#28304;&#25104;&#26412;&#12290;&#21019;&#24314;&#22522;&#20110;&#20234;&#36763;&#30340;&#30828;&#20214;&#30340;&#25361;&#25112;&#22312;&#20110;&#20248;&#21270;&#33021;&#22815;&#22312;&#26681;&#26412;&#19978;&#19981;&#30830;&#23450;&#30340;&#30828;&#20214;&#19978;&#20135;&#29983;&#27491;&#30830;&#32467;&#26524;&#30340;&#26377;&#29992;&#30005;&#36335;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#19968;&#31181;&#32508;&#21512;&#20102;&#32463;&#20856;&#35745;&#31639;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Ising&#30828;&#20214;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades of exponential scaling in high performance computing (HPC) efficiency is coming to an end. Transistor based logic in complementary metal-oxide semiconductor (CMOS) technology is approaching physical limits beyond which further miniaturization will be impossible. Future HPC efficiency gains will necessarily rely on new technologies and paradigms of compute. The Ising model shows particular promise as a future framework for highly energy efficient computation. Ising systems are able to operate at energies approaching thermodynamic limits for energy consumption of computation. Ising systems can function as both logic and memory. Thus, they have the potential to significantly reduce energy costs inherent to CMOS computing by eliminating costly data movement. The challenge in creating Ising-based hardware is in optimizing useful circuits that produce correct results on fundamentally nondeterministic hardware. The contribution of this paper is a novel machine learning approach, a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#27493;&#38271;&#20248;&#21270;&#38382;&#39064;&#65292;&#25351;&#20986;&#20256;&#32479;&#31639;&#27861;&#24573;&#35270;&#20102;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#32780;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#26126;&#30830;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2401.17401</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#27493;&#38271;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Step-size Optimization for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#27493;&#38271;&#20248;&#21270;&#38382;&#39064;&#65292;&#25351;&#20986;&#20256;&#32479;&#31639;&#27861;&#24573;&#35270;&#20102;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#32780;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#33021;&#22815;&#26126;&#30830;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#38656;&#35201;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#23398;&#20064;&#25968;&#25454;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20915;&#23450;&#35201;&#20445;&#30041;&#20160;&#20040;&#30693;&#35782;&#21644;&#25918;&#24323;&#20160;&#20040;&#30693;&#35782;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27493;&#38271;&#21521;&#37327;&#26469;&#32553;&#25918;&#26799;&#24230;&#26679;&#26412;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#25913;&#21464;&#31243;&#24230;&#26469;&#23454;&#29616;&#12290;&#24120;&#35265;&#30340;&#31639;&#27861;&#65292;&#22914;RMSProp&#21644;Adam&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26631;&#20934;&#21270;&#65292;&#26469;&#36866;&#24212;&#36825;&#20010;&#27493;&#38271;&#21521;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#24573;&#35270;&#20102;&#23427;&#20204;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#30340;&#36866;&#24212;&#25928;&#26524;&#65292;&#20363;&#22914;&#23558;&#27493;&#38271;&#21521;&#37327;&#36828;&#31163;&#26356;&#22909;&#30340;&#27493;&#38271;&#21521;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;IDBD&#65288;Sutton&#65292;1992&#65289;&#36825;&#26679;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#26126;&#30830;&#22320;&#38024;&#23545;&#25972;&#20307;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#27493;&#38271;&#21521;&#37327;&#12290;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IDBD&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#27493;&#38271;&#21521;&#37327;&#65292;&#32780;RMSProp&#21644;Adam&#21017;&#19981;&#34892;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2401.17350</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#20379;&#24212;&#21830;&#20998;&#37197;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BL&#27169;&#22411;&#21644;Perspective&#30697;&#38453;&#65292;&#20197;&#20248;&#21270;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#20379;&#24212;&#21830;&#20851;&#31995;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#23545;&#22797;&#26434;&#20379;&#24212;&#21830;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;Masked Ranking&#26426;&#21046;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20379;&#24212;&#21830;&#25490;&#24207;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;DBLM&#22312;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#21644;&#31934;&#30830;&#32622;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#24773;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#22312;&#32479;&#35745;&#36136;&#37327;&#12289;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#26041;&#38754;&#19982;&#21407;&#22987;C&#23454;&#29616;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2401.17345</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#33021;&#25928;&#21644;&#24615;&#33021;&#65306;&#23545;Python&#12289;NumPy&#12289;TensorFlow&#21644;PyTorch&#23454;&#29616;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#22312;&#32479;&#35745;&#36136;&#37327;&#12289;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#26041;&#38754;&#19982;&#21407;&#22987;C&#23454;&#29616;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;(PRNGs)&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#20247;&#22810;&#26041;&#27861;&#20013;&#37117;&#38750;&#24120;&#26377;&#24847;&#24605;&#12290;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#27604;&#22914;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25345;&#32493;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19982;&#21487;&#37325;&#22797;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#31185;&#23398;&#30740;&#31350;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#33021;&#25928;&#21017;&#24378;&#35843;&#20102;&#20445;&#25252;&#26377;&#38480;&#20840;&#29699;&#36164;&#28304;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#12289;&#24211;&#21644;&#26694;&#26550;&#20013;&#26368;&#20027;&#35201;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;(PRNGs)&#19982;&#21508;&#33258;&#31639;&#27861;&#21407;&#22987;C&#23454;&#29616;&#30456;&#27604;&#65292;&#22312;&#32479;&#35745;&#36136;&#37327;&#21644;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26088;&#22312;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17342</link><description>&lt;p&gt;
&#25552;&#39640;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17342
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#37325;&#28857;&#20851;&#27880;&#34442;&#23376;&#31181;&#32676;&#65288;MA&#65289;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;EO&#25968;&#25454;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#24314;&#31435;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;MA&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#65288;AE&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;EO&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#22320;&#21306;&#21463;&#34442;&#23376;&#31181;&#32676;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#26159;MA&#39044;&#27979;&#30340;AE&#19982;&#25152;&#25552;&#20986;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#20043;&#38388;&#23384;&#22312;0.46&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#36825;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#12289;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#22312;&#35813;&#32972;&#26223;&#19979;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
&lt;/p&gt;</description></item><item><title>&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20801;&#35768;&#20445;&#25252;&#38544;&#31169;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#35813;&#32508;&#36848;&#23545;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#23545;&#25163;&#21644;&#38450;&#24481;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2401.17319</link><description>&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning: A Survey on Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17319
&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20801;&#35768;&#20445;&#25252;&#38544;&#31169;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#35813;&#32508;&#36848;&#23545;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#23545;&#25163;&#21644;&#38450;&#24481;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#31561;&#20248;&#21183;&#65292;&#22312;&#36817;&#24180;&#26469;&#36805;&#36895;&#21457;&#23637;&#24182;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26550;&#26500;&#20013;&#65292;&#27169;&#22411;&#26356;&#26032;&#21644;&#26799;&#24230;&#30340;&#20132;&#25442;&#20026;&#32593;&#32476;&#20013;&#30340;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#21361;&#21450;&#27169;&#22411;&#24615;&#33021;&#20197;&#21450;&#29992;&#25143;&#21644;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#36890;&#36807;&#21435;&#38500;&#26381;&#21153;&#22120;&#24182;&#36890;&#36807;&#21306;&#22359;&#38142;&#31561;&#25216;&#26415;&#36827;&#34892;&#34917;&#20607;&#26469;&#28040;&#38500;&#19982;&#26381;&#21153;&#22120;&#30456;&#20851;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20248;&#21183;&#21364;&#20197;&#25361;&#25112;&#31995;&#32479;&#38754;&#20020;&#26032;&#30340;&#38544;&#31169;&#23041;&#32961;&#20026;&#20195;&#20215;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#31181;&#26032;&#33539; paradigm&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#23041;&#32961;&#21644;&#23545;&#25163;&#21464;&#21270;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#36824;&#32771;&#34385;&#20102;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#24230;&#21644;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has been rapidly evolving and gaining popularity in recent years due to its privacy-preserving features, among other advantages. Nevertheless, the exchange of model updates and gradients in this architecture provides new attack surfaces for malicious users of the network which may jeopardize the model performance and user and data privacy. For this reason, one of the main motivations for decentralized federated learning is to eliminate server-related threats by removing the server from the network and compensating for it through technologies such as blockchain. However, this advantage comes at the cost of challenging the system with new privacy threats. Thus, performing a thorough security analysis in this new paradigm is necessary. This survey studies possible variations of threats and adversaries in decentralized federated learning and overviews the potential defense mechanisms. Trustability and verifiability of decentralized federated learning are also considered 
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.16092</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25918;&#22823;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#20462;&#27491;&#24037;&#31243;&#21487;&#33021;&#26080;&#27861;&#24110;&#21161;&#24744;
&lt;/p&gt;
&lt;p&gt;
Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16092
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#36890;&#36807;&#25913;&#21892;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#26356;&#22810;&#30340;&#31038;&#32676;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#19968;&#26679;&#21463;&#21040;(&#24615;&#21035;)&#20559;&#35265;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#33258;&#28982;&#26399;&#26395;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#25552;&#20379;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20294;&#20107;&#23454;&#24182;&#38750;&#22914;&#27492;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#27809;&#26377;&#24615;&#21035;&#20559;&#35265;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;MAGBIG&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;T2I&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;MAGBIG&#25918;&#22823;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#35831;&#27714;&#29305;&#23450;&#32844;&#19994;&#25110;&#29305;&#36136;&#30340;&#20154;&#20687;&#22270;&#20687;(&#20351;&#29992;&#24418;&#23481;&#35789;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;&#27169;&#22411;&#20559;&#31163;&#20102;&#35268;&#33539;&#30340;&#20551;&#35774;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2401.15469</link><description>&lt;p&gt;
&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#19982;&#39564;&#35777;&#65306;&#20174;ERA5&#21040;CERRA&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#39118;&#36895;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copernicus&#21306;&#22495;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;CERRA&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#27431;&#27954;&#21306;&#22495;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#21508;&#31181;&#19982;&#27668;&#20505;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23427;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#22825;&#27668;&#39044;&#25253;&#12289;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#12289;&#31354;&#27668;&#36136;&#37327;&#39118;&#38505;&#35780;&#20272;&#20197;&#21450;&#32597;&#35265;&#20107;&#20214;&#30340;&#39044;&#27979;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#33719;&#21462;&#25152;&#38656;&#22806;&#37096;&#25968;&#25454;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#35745;&#31639;&#37327;&#22823;&#65292;CERRA&#30340;&#21487;&#29992;&#24615;&#28382;&#21518;&#20110;&#24403;&#21069;&#26085;&#26399;&#20004;&#24180;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36817;&#20284;CERRA&#30340;&#38477;&#23610;&#24230;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#12290;&#36890;&#36807;&#21033;&#29992;&#36793;&#30028;&#26465;&#20214;&#30001;&#20302;&#20998;&#36776;&#29575;ERA5&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;CERRA&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#12290;&#20197;&#24847;&#22823;&#21033;&#21608;&#22260;&#30340;&#39118;&#36895;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29616;&#26377;CERRA&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution regional reanalysis dataset for the European domain. In recent years it has shown significant utility across various climate-related tasks, ranging from forecasting and climate change research to renewable energy prediction, resource management, air quality risk assessment, and the forecasting of rare events, among others. Unfortunately, the availability of CERRA is lagging two years behind the current date, due to constraints in acquiring the requisite external data and the intensive computational demands inherent in its generation. As a solution, this paper introduces a novel method using diffusion models to approximate CERRA downscaling in a data-driven manner, without additional informations. By leveraging the lower resolution ERA5 dataset, which provides boundary conditions for CERRA, we approach this as a super-resolution task. Focusing on wind speed around Italy, our model, trained on existing CERRA data,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;nnU-Net&#21644;DeepMedic&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#20799;&#31461;&#33041;&#32959;&#30244;&#33258;&#21160;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20799;&#31461;&#29305;&#23450;&#30340;&#22810;&#26426;&#26500;&#33041;&#32959;&#30244;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08404</link><description>&lt;p&gt;
nnU-Net&#21644;DeepMedic&#26041;&#27861;&#22312;&#20799;&#31461;&#33041;&#32959;&#30244;&#33258;&#21160;&#20998;&#21106;&#20013;&#30340;&#35757;&#32451;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Training and Comparison of nnU-Net and DeepMedic Methods for Autosegmentation of Pediatric Brain Tumors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;nnU-Net&#21644;DeepMedic&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#20799;&#31461;&#33041;&#32959;&#30244;&#33258;&#21160;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20799;&#31461;&#29305;&#23450;&#30340;&#22810;&#26426;&#26500;&#33041;&#32959;&#30244;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#26368;&#24120;&#35265;&#30340;&#22266;&#20307;&#32959;&#30244;&#65292;&#20063;&#26159;&#20799;&#31461;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#25163;&#26415;&#21644;&#27835;&#30103;&#35745;&#21010;&#12289;&#21453;&#24212;&#35780;&#20272;&#21644;&#30417;&#27979;&#20013;&#65292;&#32959;&#30244;&#20998;&#21106;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#20998;&#21106;&#32791;&#26102;&#19988;&#25805;&#20316;&#32773;&#38388;&#30340;&#21464;&#24322;&#24615;&#39640;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20799;&#31461;&#29305;&#23450;&#30340;&#22810;&#26426;&#26500;&#33041;&#32959;&#30244;&#25968;&#25454;&#36890;&#36807;&#22810;&#21442;&#25968;MRI&#25195;&#25551;&#23545;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#20998;&#21106;&#27169;&#22411;DeepMedic&#21644;nnU-Net&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23545;339&#21517;&#20799;&#31461;&#24739;&#32773;&#65288;&#20869;&#37096;&#32452;&#21644;&#22806;&#37096;&#32452;&#20998;&#21035;&#20026;293&#20363;&#21644;46&#20363;&#65289;&#30340;&#22810;&#21442;&#25968;&#26415;&#21069;MRI&#25195;&#25551;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#25163;&#21160;&#20998;&#21106;&#65292;&#21010;&#20998;&#20026;&#22686;&#24378;&#32959;&#30244;&#65288;ET&#65289;&#12289;&#38750;&#22686;&#24378;&#32959;&#30244;&#65288;NET&#65289;&#12289;&#22218;&#24615;&#32452;&#20998;&#65288;CC&#65289;&#21644;&#21608;&#22260;&#27700;&#32959;&#65288;ED&#65289;&#22235;&#20010;&#32959;&#30244;&#20122;&#21306;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#36890;&#36807;Dice&#20998;&#25968;&#12289;&#25935;&#24863;&#24230;&#31561;&#25351;&#26631;&#35780;&#20272;&#20102;&#20004;&#31181;&#27169;&#22411;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumors are the most common solid tumors and the leading cause of cancer-related death among children. Tumor segmentation is essential in surgical and treatment planning, and response assessment and monitoring. However, manual segmentation is time-consuming and has high inter-operator variability, underscoring the need for more efficient methods. We compared two deep learning-based 3D segmentation models, DeepMedic and nnU-Net, after training with pediatric-specific multi-institutional brain tumor data using based on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of 339 pediatric patients (n=293 internal and n=46 external cohorts) with a variety of tumor subtypes, were preprocessed and manually segmented into four tumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic components (CC), and peritumoral edema (ED). After training, performance of the two models on internal and external test sets was evaluated using Dice scores, sensitivity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#22522;&#24577;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38271;&#31243;&#21644;&#31561;&#21464;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#35823;&#24046;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.17019</link><description>&lt;p&gt;
&#39640;&#25928;&#23398;&#20064;&#38271;&#31243;&#21644;&#31561;&#21464;&#37327;&#37327;&#23376;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Long-Range and Equivariant Quantum Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#22522;&#24577;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38271;&#31243;&#21644;&#31561;&#21464;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#35823;&#24046;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;-&#25214;&#21040;&#21644;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#26469;&#39044;&#27979;&#20960;&#20309;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#30340;&#22522;&#24577;&#26399;&#26395;&#20540;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#30701;&#31243;&#32570;&#38519;&#21704;&#23494;&#39039;&#37327;&#65292;&#24471;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#37327;&#23376;&#20301;&#25968;&#30340;&#23545;&#25968;&#21644;&#35823;&#24046;&#30340;&#20934;&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#36229;&#20986;&#21704;&#23494;&#39039;&#37327;&#21644;&#35266;&#27979;&#37327;&#30340;&#23616;&#37096;&#35201;&#27714;&#65292;&#36825;&#26159;&#30001;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#20851;&#24615;&#25152;&#39537;&#21160;&#30340;&#12290;&#23545;&#20110;&#25351;&#25968;&#22823;&#20110;&#31995;&#32479;&#32500;&#25968;&#20004;&#20493;&#30340;&#24130;&#24459;&#34928;&#20943;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#39640;&#25928;&#23545;&#25968;&#26631;&#24230;&#20851;&#20110;&#37327;&#23376;&#20301;&#25968;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;&#35823;&#24046;&#30340;&#20381;&#36182;&#24615;&#24694;&#21270;&#21040;&#20102;&#25351;&#25968;&#32423;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#30456;&#20114;&#20316;&#29992;&#36229;&#22270;&#30340;&#33258;&#21516;&#26500;&#32676;&#19979;&#31561;&#21464;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider a fundamental task in quantum many-body physics - finding and learning ground states of quantum Hamiltonians and their properties. Recent works have studied the task of predicting the ground state expectation value of sums of geometrically local observables by learning from data. For short-range gapped Hamiltonians, a sample complexity that is logarithmic in the number of qubits and quasipolynomial in the error was obtained. Here we extend these results beyond the local requirements on both Hamiltonians and observables, motivated by the relevance of long-range interactions in molecular and atomic systems. For interactions decaying as a power law with exponent greater than twice the dimension of the system, we recover the same efficient logarithmic scaling with respect to the number of qubits, but the dependence on the error worsens to exponential. Further, we show that learning algorithms equivariant under the automorphism group of the interaction hypergraph a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;ReLU&#32593;&#32476;&#20013;&#20351;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#30028;&#38480;&#25910;&#32039;&#30340;&#35745;&#31639;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#12289;&#27491;&#21017;&#21270;&#21644;&#33293;&#20837;&#30340;&#23454;&#26045;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2312.16699</link><description>&lt;p&gt;
&#35745;&#31639;&#20013;&#22522;&#20110;&#20248;&#21270;&#30340;&#30028;&#38480;&#25910;&#32039;&#22312;ReLU&#32593;&#32476;&#20013;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Computational Tradeoffs of Optimization-Based Bound Tightening in ReLU Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;ReLU&#32593;&#32476;&#20013;&#20351;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#30028;&#38480;&#25910;&#32039;&#30340;&#35745;&#31639;&#26435;&#34913;&#65292;&#25552;&#20379;&#20102;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#12289;&#27491;&#21017;&#21270;&#21644;&#33293;&#20837;&#30340;&#23454;&#26045;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#27169;&#22411;&#26469;&#34920;&#31034;&#20855;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;MILP&#25216;&#26415;&#26469;&#27979;&#35797;&#25110;&#21387;&#21147;&#27979;&#35797;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#20026;&#20102;&#23545;&#23427;&#20204;&#30340;&#35757;&#32451;&#36827;&#34892;&#25932;&#23545;&#25913;&#36827;&#65292;&#24182;&#23558;&#23427;&#20204;&#23884;&#20837;&#21040;&#21033;&#29992;&#23427;&#20204;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#12290;&#20854;&#20013;&#35768;&#22810;MILP&#27169;&#22411;&#20381;&#36182;&#20110;&#28608;&#27963;&#30028;&#38480;&#65292;&#21363;&#27599;&#20010;&#31070;&#32463;&#20803;&#36755;&#20837;&#20540;&#30340;&#30028;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#30028;&#38480;&#30340;&#32039;&#23494;&#24230;&#19982;&#35299;&#20915;&#32467;&#26524;MILP&#27169;&#22411;&#30340;&#35745;&#31639;&#24037;&#20316;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#12289;&#27491;&#21017;&#21270;&#21644;&#33293;&#20837;&#30340;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Mixed-Integer Linear Programming (MILP) models to represent neural networks with Rectified Linear Unit (ReLU) activations has become increasingly widespread in the last decade. This has enabled the use of MILP technology to test-or stress-their behavior, to adversarially improve their training, and to embed them in optimization models leveraging their predictive power. Many of these MILP models rely on activation bounds. That is, bounds on the input values of each neuron. In this work, we explore the tradeoff between the tightness of these bounds and the computational effort of solving the resulting MILP models. We provide guidelines for implementing these models based on the impact of network structure, regularization, and rounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.06353</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#20449;&#25104;&#26412;&#20302;&#20110;18&#21315;&#23383;&#33410;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedKSeed&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#21644;&#26377;&#38480;&#30340;&#38543;&#26426;&#31181;&#23376;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25104;&#26412;&#36739;&#20302;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#21487;&#20197;&#36827;&#34892;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#36890;&#36807;&#32454;&#21270;&#35843;&#25972;&#26469;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#19981;&#29306;&#29298;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#32456;&#31471;&#35774;&#22791;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#32454;&#21270;&#35843;&#25972;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;LLM&#32852;&#37030;&#32454;&#21270;&#35843;&#25972;&#26041;&#27861;&#20381;&#36182;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32454;&#21270;&#35843;&#25972;&#25216;&#26415;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#20840;&#21442;&#25968;&#35843;&#25972;&#21487;&#33021;&#36798;&#21040;&#30340;&#24615;&#33021;&#39640;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;FedKSeed&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#31181;&#23376;&#30340;&#26377;&#38480;&#38598;&#21512;&#36827;&#34892;&#38646;&#38454;&#20248;&#21270;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#26381;&#21153;&#22120;&#21644;&#32456;&#31471;&#20043;&#38388;&#30340;&#20256;&#36755;&#35201;&#27714;&#65292;&#20165;&#38656;&#20256;&#36755;&#20960;&#20010;&#38543;&#26426;&#31181;&#23376;&#21644;&#26631;&#37327;&#26799;&#24230;&#65292;&#20165;&#21344;&#29992;&#20960;&#21315;&#23383;&#33410;&#30340;&#31354;&#38388;&#65292;&#20351;&#24471;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#33021;&#22815;&#36827;&#34892;&#20159;&#32423;LLM&#30340;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#27010;&#29575;&#24046;&#24322;&#21270;&#31181;&#23376;&#37319;&#26679;&#65292;&#20248;&#20808;&#32771;&#34385;&#19968;&#20123;&#31181;&#23376;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#32852;&#37030;&#20840;&#21442;&#25968;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>ECNR&#26159;&#19968;&#31181;&#38024;&#23545;&#26102;&#21464;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#21644;&#22810;&#20010;&#23567;&#22411;MLP&#65292;&#20197;&#21450;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.12831</link><description>&lt;p&gt;
ECNR: &#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#30340;&#26102;&#21464;&#20307;&#31215;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12831
&lt;/p&gt;
&lt;p&gt;
ECNR&#26159;&#19968;&#31181;&#38024;&#23545;&#26102;&#21464;&#25968;&#25454;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#21644;&#22810;&#20010;&#23567;&#22411;MLP&#65292;&#20197;&#21450;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#27010;&#24565;&#31616;&#21333;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#31649;&#29702;&#22823;&#35268;&#27169;&#20307;&#31215;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#21387;&#32553;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#24403;&#21069;&#30340;&#31070;&#32463;&#21387;&#32553;&#23454;&#36341;&#21033;&#29992;&#21333;&#20010;&#22823;&#22411;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26469;&#23545;&#20840;&#23616;&#20307;&#31215;&#36827;&#34892;&#32534;&#30721;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#21464;&#25968;&#25454;&#21387;&#32553;&#30340;&#39640;&#25928;&#21387;&#32553;&#31070;&#32463;&#34920;&#31034;&#65288;ECNR&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#36827;&#34892;&#33258;&#36866;&#24212;&#20449;&#21495;&#25311;&#21512;&#12290;&#36981;&#24490;&#22810;&#23610;&#24230;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23610;&#24230;&#19978;&#21033;&#29992;&#22810;&#20010;&#23567;&#22411;MLP&#26469;&#25311;&#21512;&#26412;&#22320;&#20869;&#23481;&#25110;&#27531;&#24046;&#22359;&#12290;&#36890;&#36807;&#23558;&#30456;&#20284;&#30340;&#22359;&#20998;&#37197;&#32473;&#30456;&#21516;&#22823;&#23567;&#30340;MLP&#65292;&#36890;&#36807;&#22823;&#23567;&#32479;&#19968;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;MLP&#20043;&#38388;&#30340;&#24179;&#34913;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#19982;&#22810;&#23610;&#24230;&#32467;&#26500;&#21327;&#21516;&#24037;&#20316;&#30340;&#26159;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#31181;&#28145;&#24230;&#21387;&#32553;&#31574;&#30053;&#26469;&#21387;&#32553;&#32467;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ECNR&#30340;&#25928;&#26524;&#65292;&#26377;&#22810;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25968;&#25454;&#38544;&#31169;&#39118;&#38505;&#20998;&#26512;&#21644;FL&#20013;&#32531;&#35299;&#31574;&#30053;&#30340;&#20840;&#38754;&#26694;&#26550;&#65288;MedPFL&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;FL&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#23384;&#22312;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20934;&#30830;&#37325;&#26500;&#31169;&#23494;&#21307;&#23398;&#22270;&#20687;&#12290;&#20026;&#20102;&#32531;&#35299;&#38544;&#31169;&#25915;&#20987;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.06643</link><description>&lt;p&gt;
&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#39118;&#38505;&#20998;&#26512;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25968;&#25454;&#38544;&#31169;&#39118;&#38505;&#20998;&#26512;&#21644;FL&#20013;&#32531;&#35299;&#31574;&#30053;&#30340;&#20840;&#38754;&#26694;&#26550;&#65288;MedPFL&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;FL&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#23384;&#22312;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20934;&#30830;&#37325;&#26500;&#31169;&#23494;&#21307;&#23398;&#22270;&#20687;&#12290;&#20026;&#20102;&#32531;&#35299;&#38544;&#31169;&#25915;&#20987;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#29992;&#20110;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20445;&#25252;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#21644;&#36981;&#23432;&#38544;&#31169;&#27861;&#35268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;FL&#30340;&#40664;&#35748;&#35774;&#32622;&#21487;&#33021;&#22312;&#38544;&#31169;&#25915;&#20987;&#19979;&#27844;&#38706;&#31169;&#23494;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;FL&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#21542;&#23384;&#22312;&#27492;&#31867;&#38544;&#31169;&#39118;&#38505;&#65292;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#25968;&#25454;&#38544;&#31169;&#39118;&#38505;&#20998;&#26512;&#21644;FL&#20013;&#32531;&#35299;&#31574;&#30053;&#30340;&#20840;&#38754;&#26694;&#26550;&#65288;MedPFL&#65289;&#65292;&#20197;&#20998;&#26512;&#38544;&#31169;&#39118;&#38505;&#24182;&#21046;&#23450;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#20197;&#20445;&#25252;&#31169;&#23494;&#21307;&#23398;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FL&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#23384;&#22312;&#30340;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#38544;&#31169;&#25915;&#20987;&#20197;&#20934;&#30830;&#37325;&#24314;&#31169;&#23494;&#21307;&#23398;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#20197;&#32531;&#35299;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is gaining increasing popularity in the medical domain for analyzing medical images, which is considered an effective technique to safeguard sensitive patient data and comply with privacy regulations. However, several recent studies have revealed that the default settings of FL may leak private training data under privacy attacks. Thus, it is still unclear whether and to what extent such privacy risks of FL exist in the medical domain, and if so, "how to mitigate such risks?". In this paper, first, we propose a holistic framework for Medical data Privacy risk analysis and mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop effective mitigation strategies in FL for protecting private medical data. Second, we demonstrate the substantial privacy risks of using FL to process medical images, where adversaries can easily perform privacy attacks to reconstruct private medical images accurately. Third, we show that the defense approach of addi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#21322;&#20809;&#28369;&#29275;&#39039;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26680;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#26679;&#26412;&#37327;&#12290;</title><link>https://arxiv.org/abs/2310.14087</link><description>&lt;p&gt;
&#19968;&#31181;&#19987;&#29992;&#30340;&#21322;&#20809;&#28369;&#29275;&#39039;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#26680;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14087
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#21322;&#20809;&#28369;&#29275;&#39039;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26680;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#26679;&#26412;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#21151;&#33021;&#20272;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#26469;&#33258;&#26679;&#26412;&#30340;OT&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#25554;&#20540;&#65288;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#65289;OT&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#39640;&#32500;&#24230;&#19979;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#32479;&#35745;&#20248;&#21183;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#65306;&#22240;&#20026;&#20854;&#35745;&#31639;&#20381;&#36182;&#20110;&#30701;&#27493;&#38271;&#20869;&#28857;&#26041;&#27861;&#65288;SSIPM&#65289;&#65292;&#22312;&#23454;&#36341;&#20013;&#36845;&#20195;&#27425;&#25968;&#24456;&#22823;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#24456;&#24555;&#22312;&#26679;&#26412;&#37327;$n$&#26041;&#38754;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#20272;&#35745;&#22120;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;$n$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20809;&#28369;&#22266;&#23450;&#28857;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#26680;&#30340;OT&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#19987;&#38376;&#30340;&#21322;&#20809;&#28369;&#29275;&#39039;&#65288;SSN&#65289;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#65306;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#38382;&#39064;&#30340;&#32467;&#26500;&#34920;&#26126;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25191;&#34892;&#19968;&#20010;SSN&#27493;&#39588;&#30340;&#27599;&#27425;&#36845;&#20195;&#25104;&#26412;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel-based optimal transport (OT) estimators offer an alternative, functional estimation procedure to address OT problems from samples. Recent works suggest that these estimators are more statistically efficient than plug-in (linear programming-based) OT estimators when comparing probability measures in high-dimensions~\citep{Vacher-2021-Dimension}. Unfortunately, that statistical benefit comes at a very steep computational price: because their computation relies on the short-step interior-point method (SSIPM), which comes with a large iteration count in practice, these estimators quickly become intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we propose a nonsmooth fixed-point model for the kernel-based OT problem, and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method: We show, exploring the problem's structure, that the per-iteration cost of performing one SSN step can be significantly reduced in practice. We prove t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22635;&#34917;&#65292;&#20197;&#23545;&#23454;&#20307;&#23545;&#26681;&#25454;&#20854;&#28385;&#36275;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#21644;&#22810;&#20010;LLM&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2305.15002</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A RelEntLess Benchmark for Modelling Graded Relations between Named Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#20998;&#32423;&#20851;&#31995;&#30340;&#26080;&#24773;&#22522;&#20934;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22635;&#34917;&#65292;&#20197;&#23545;&#23454;&#20307;&#23545;&#26681;&#25454;&#20854;&#28385;&#36275;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#21644;&#22810;&#20010;LLM&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#8220;&#21463;&#24433;&#21709;&#20110;&#8221;&#12289;&#8220;&#20197;...&#38395;&#21517;&#8221;&#25110;&#8220;&#19982;...&#31454;&#20105;&#8221;&#20043;&#31867;&#30340;&#20851;&#31995;&#26412;&#36136;&#19978;&#26159;&#20998;&#32423;&#30340;&#65306;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#23454;&#20307;&#23545;&#28385;&#36275;&#36825;&#20123;&#20851;&#31995;&#30340;&#31243;&#24230;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#65292;&#20294;&#24456;&#38590;&#23558;&#28385;&#36275;&#21644;&#19981;&#28385;&#36275;&#36825;&#20123;&#20851;&#31995;&#30340;&#23454;&#20307;&#23545;&#21010;&#20998;&#24320;&#12290;&#36825;&#26679;&#30340;&#20998;&#32423;&#20851;&#31995;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#19981;&#21253;&#21547;&#27492;&#31867;&#20851;&#31995;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#23454;&#20307;&#23545;&#24517;&#39035;&#26681;&#25454;&#20854;&#28385;&#36275;&#32473;&#23450;&#20998;&#32423;&#20851;&#31995;&#30340;&#31243;&#24230;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#20219;&#21153;&#34987;&#23450;&#20041;&#20026;&#23569;&#26679;&#26412;&#25490;&#24207;&#38382;&#39064;&#65292;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#20851;&#31995;&#30340;&#25551;&#36848;&#21644;&#20116;&#20010;&#21407;&#22411;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#23884;&#20837;&#31574;&#30053;&#20197;&#21450;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#21644;&#23553;&#38381;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relations such as "is influenced by", "is known for" or "is a competitor of" are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several recent LLMs, covering both publicly available LLMs and closed models such as GPT-4. Overall, we find a stro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#36890;&#29992;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#33258;&#21161;&#39044;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#30340;&#22810;&#22836;&#32858;&#31867;&#39044;&#27979;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.13530</link><description>&lt;p&gt;
&#36890;&#29992;&#22810;&#39046;&#22495;&#32858;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain-Generalizable Multiple-Domain Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#36890;&#29992;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#33258;&#21161;&#39044;&#35757;&#32451;&#21644;&#20266;&#26631;&#31614;&#30340;&#22810;&#22836;&#32858;&#31867;&#39044;&#27979;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26080;&#30417;&#30563;&#39046;&#22495;&#36890;&#29992;&#21270;&#38382;&#39064;&#25512;&#24191;&#21040;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#24773;&#20917;&#65288;&#23436;&#20840;&#26080;&#30417;&#30563;&#65289;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#26469;&#33258;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#30340;&#39044;&#27979;&#22120;&#65292;&#23558;&#31034;&#20363;&#20998;&#37197;&#21040;&#35821;&#20041;&#30456;&#20851;&#30340;&#32858;&#31867;&#20013;&#12290;&#36890;&#36807;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#39046;&#22495;&#20013;&#39044;&#27979;&#32858;&#31867;&#20998;&#37197;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#26694;&#26550;&#65306;&#65288;1&#65289;&#33258;&#21161;&#39044;&#35757;&#32451;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#65288;2&#65289;&#20351;&#29992;&#20266;&#26631;&#31614;&#30340;&#22810;&#22836;&#32858;&#31867;&#39044;&#27979;&#65292;&#35813;&#20266;&#26631;&#31614;&#20381;&#36182;&#20110;&#29305;&#24449;&#31354;&#38388;&#21644;&#32858;&#31867;&#22836;&#39044;&#27979;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#27979;&#30340;&#26631;&#31614;&#24179;&#28369;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#38656;&#35201;&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#25110;&#26576;&#31181;&#31243;&#24230;&#30340;&#30417;&#30563;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#21387;&#21147;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2210.06225</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalizability of ECG-based Stress Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#21387;&#21147;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#21387;&#21147;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#21147;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#24456;&#26222;&#36941;&#65292;&#21253;&#25324;&#24037;&#20316;&#12289;&#21307;&#30103;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#21508;&#31181;&#29983;&#29289;&#20449;&#21495;&#30340;&#25163;&#24037;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#21387;&#21147;&#30340;&#25351;&#26631;&#12290;&#26368;&#36817;&#65292;&#20063;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21387;&#21147;&#12290;&#36890;&#24120;&#65292;&#21387;&#21147;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#20010;&#21387;&#21147;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#22330;&#26223;&#25910;&#38598;&#21387;&#21147;&#25968;&#25454;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22312;&#20854;&#20182;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#31243;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#24515;&#30005;&#22270;(ECG)&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#24515;&#30005;&#22270;&#29305;&#24449;&#65288;&#21363;&#24515;&#29575;&#21464;&#24322;&#24615;(HRV)&#29305;&#24449;&#65289;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19977;&#20010;HRV&#27169;&#22411;&#21644;&#20004;&#20010;&#20351;&#29992;ECG&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#21387;&#21147;&#25968;&#25454;&#38598;&#65288;WESAD&#21644;SWELL-KW&#65289;&#30340;ECG&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stress is prevalent in many aspects of everyday life including work, healthcare, and social interactions. Many works have studied handcrafted features from various bio-signals that are indicators of stress. Recently, deep learning models have also been proposed to detect stress. Typically, stress models are trained and validated on the same dataset, often involving one stressful scenario. However, it is not practical to collect stress data for every scenario. So, it is crucial to study the generalizability of these models and determine to what extent they can be used in other scenarios. In this paper, we explore the generalization capabilities of Electrocardiogram (ECG)-based deep learning models and models based on handcrafted ECG features, i.e., Heart Rate Variability (HRV) features. To this end, we train three HRV models and two deep learning models that use ECG signals as input. We use ECG signals from two popular stress datasets - WESAD and SWELL-KW - differing in terms of stresso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2209.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Empathetic AI Coach for Self-Attachment Therapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#36848;&#30103;&#27861;&#30340;&#20849;&#24773;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#35268;&#21017;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#21644;&#29983;&#25104;&#27969;&#30021;&#12289;&#20849;&#24773;&#30340;&#23545;&#35805;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#38750;&#20020;&#24202;&#35797;&#39564;&#39564;&#35777;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#36848;&#30103;&#27861;&#30340;&#25968;&#23383;&#36741;&#23548;&#31995;&#32479;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#23545;&#35805;&#20195;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#35782;&#21035;&#29992;&#25143;&#25991;&#26412;&#22238;&#22797;&#20013;&#30340;&#28508;&#22312;&#24773;&#32490;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#26816;&#32034;&#26041;&#27861;&#29983;&#25104;&#26032;&#39062;&#12289;&#27969;&#30021;&#21644;&#20849;&#24773;&#30340;&#35805;&#35821;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#32452;&#31867;&#20284;&#20154;&#31867;&#30340;&#35282;&#33394;&#20379;&#29992;&#25143;&#36873;&#25321;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#34394;&#25311;&#30103;&#27861;&#20250;&#35805;&#20013;&#23454;&#29616;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#39033;&#38750;&#20020;&#24202;&#35797;&#39564;&#20013;&#23545;N=16&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#22312;&#20116;&#22825;&#20869;&#33267;&#23569;&#19982;&#20195;&#29702;&#36827;&#34892;&#20102;&#22235;&#27425;&#20114;&#21160;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#22312;&#20849;&#24773;&#24230;&#12289;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#34987;&#35780;&#20215;&#24471;&#26356;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35774;&#35745;&#21644;&#24615;&#33021;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#20174;&#19968;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#24182;&#24433;&#21709;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#22810;&#20010;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2205.15523</link><description>&lt;p&gt;
&#21464;&#20998;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Variational Transfer Learning using Cross-Domain Latent Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.15523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#20174;&#19968;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#24182;&#24433;&#21709;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#22810;&#20010;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#22320;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24212;&#29992;&#21040;&#26032;&#39046;&#22495;&#65292;&#24378;&#22823;&#30340;&#36716;&#31227;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#28508;&#22312;&#35843;&#21046;&#26426;&#21046;&#65292;&#23558;&#20854;&#24341;&#20837;&#21040;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;&#19968;&#20010;&#25968;&#25454;&#39046;&#22495;&#33719;&#21462;&#28145;&#23618;&#34920;&#31034;&#65292;&#24182;&#29992;&#23427;&#26469;&#24433;&#21709;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#25512;&#29702;&#27169;&#22411;&#26469;&#25552;&#21462;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#28145;&#23618;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#21453;&#36716;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#28145;&#23618;&#34920;&#31034;&#36328;&#35843;&#21046;&#21040;&#21478;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#32534;&#30721;&#20013;&#65292;&#24182;&#24212;&#29992;&#19968;&#33268;&#24615;&#32422;&#26463;&#12290;&#22312;&#21253;&#25324;&#19968;&#20123;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#36716;&#31227;&#23398;&#20064;&#22522;&#20934;&#20219;&#21153;&#30340;&#23454;&#35777;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#25903;&#25345;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
To successfully apply trained neural network models to new domains, powerful transfer learning solutions are essential. We propose to introduce a novel cross-domain latent modulation mechanism to a variational autoencoder framework so as to achieve effective transfer learning. Our key idea is to procure deep representations from one data domain and use it to influence the reparameterization of the latent variable of another domain. Specifically, deep representations of the source and target domains are first extracted by a unified inference model and aligned by employing gradient reversal. The learned deep representations are then cross-modulated to the latent encoding of the alternative domain, where consistency constraints are also applied. In the empirical validation that includes a number of transfer learning benchmark tasks for unsupervised domain adaptation and image-to-image translation, our model demonstrates competitive performance, which is also supported by evidence obtained
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#26799;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#26377;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2201.09196</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to Predict Gradients for Semi-Supervised Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.09196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#26799;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#26377;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#26234;&#33021;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#19981;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#36830;&#32493;&#23398;&#20064;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#65292;&#20154;&#31867;&#33021;&#22815;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#26410;&#26631;&#35760;&#25968;&#25454;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#33267;&#20170;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#29992;&#22320;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#23398;&#20064;&#22120;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#27979;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#20197;&#36866;&#24212;&#26377;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#28508;&#22312;&#31867;&#21035;&#23545;&#23398;&#20064;&#36807;&#31243;&#26159;&#24050;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for machine intelligence is to learn new visual concepts without forgetting the previously acquired knowledge. Continual learning is aimed towards addressing this challenge. However, there is a gap between existing supervised continual learning and human-like intelligence, where human is able to learn from both labeled and unlabeled data. How unlabeled data affects learning and catastrophic forgetting in the continual learning process remains unknown. To explore these issues, we formulate a new semi-supervised continual learning method, which can be generically applied to existing continual learning models. Specifically, a novel gradient learner learns from labeled data to predict gradients on unlabeled data. Hence, the unlabeled data could fit into the supervised continual learning method. Different from conventional semi-supervised settings, we do not hypothesize that the underlying classes, which are associated to the unlabeled data, are known to the learning process
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#20998;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#38454;&#21644;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#23454;&#35777;&#35797;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2105.11309</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24335;&#27604;&#22810;&#39033;&#24335;&#39640;&#25928;&#35299;&#20915;&#39640;&#38454;&#21644;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#30740;&#31350;: &#20998;&#25968;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficiently Solving High-Order and Nonlinear ODEs with Rational Fraction Polynomial: the Ratio Net
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.11309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#20998;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#38454;&#21644;&#38750;&#32447;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#23454;&#35777;&#35797;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#36827;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#20316;&#20026;&#35797;&#25506;&#20989;&#25968;&#20197;&#21450;&#22312;&#20989;&#25968;&#31354;&#38388;&#20869;&#36924;&#36817;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21463;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;ODE&#30340;&#36807;&#31243;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#38454;&#21644;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24378;&#35843;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#24050;&#26377;&#30340;&#30693;&#35782;&#38598;&#25104;&#26469;&#25552;&#39640;&#38382;&#39064;&#27714;&#35299;&#25928;&#29575;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#26500;&#24314;&#35797;&#25506;&#20989;&#25968;&#65292;&#31216;&#20026;&#20998;&#25968;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#32467;&#26500;&#20511;&#37492;&#20102;&#20998;&#24335;&#27604;&#22810;&#39033;&#24335;&#36924;&#36817;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;Pade&#36924;&#36817;&#12290;&#36890;&#36807;&#23454;&#35777;&#35797;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#22810;&#39033;&#24335;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in solving ordinary differential equations (ODEs) with neural networks have been remarkable. Neural networks excel at serving as trial functions and approximating solutions within functional spaces, aided by gradient backpropagation algorithms. However, challenges remain in solving complex ODEs, including high-order and nonlinear cases, emphasizing the need for improved efficiency and effectiveness. Traditional methods have typically relied on established knowledge integration to improve problem-solving efficiency. In contrast, this study takes a different approach by introducing a new neural network architecture for constructing trial functions, known as ratio net. This architecture draws inspiration from rational fraction polynomial approximation functions, specifically the Pade approximant. Through empirical trials, it demonstrated that the proposed method exhibits higher efficiency compared to existing approaches, including polynomial-based and multilayer perceptron
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33016;&#37096;X&#20809;&#22270;&#20687;&#36827;&#34892;COVID-19&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2006.02570</link><description>&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#33016;&#37096;X&#20809;&#22270;&#20687;&#30340;&#28145;&#24230;COVID-19&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33016;&#37096;X&#20809;&#22270;&#20687;&#36827;&#34892;COVID-19&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30340;&#29190;&#21457;&#20197;&#20854;&#30456;&#23545;&#24555;&#36895;&#30340;&#20256;&#25773;&#32780;&#38663;&#24778;&#20102;&#25972;&#20010;&#19990;&#30028;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#39046;&#22495;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#38480;&#21046;&#20854;&#20256;&#25773;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#23545;&#24863;&#26579;&#32773;&#36827;&#34892;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#21307;&#23398;&#24433;&#20687;&#65292;&#22914;X&#20809;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#28508;&#21147;&#65292;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#36215;&#30528;&#25903;&#25345;&#21307;&#21153;&#20154;&#21592;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;(ResNet18&#12289;ResNet34&#12289;InceptionV3&#12289;InceptionResNetV2&#21644;DenseNet161)&#21450;&#20854;&#38598;&#25104;&#65292;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#33016;&#37096;X&#20809;&#22270;&#20687;&#23545;COVID-19&#12289;&#32954;&#28814;&#21644;&#20581;&#24247;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#27599;&#20010;&#24739;&#32773;&#65292;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#20197;&#39044;&#27979;&#20854;&#23384;&#22312;&#30340;&#22810;&#20010;&#30149;&#29702;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;-&#36974;&#25377;&#12289;&#26174;&#33879;&#24615;&#12289;&#36755;&#20837;X&#26799;&#24230;&#12289;&#24341;&#23548;&#24615;&#21453;&#20256;&#12289;&#38598;&#25104;&#26799;&#24230;&#26469;&#23545;&#27599;&#20010;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The outbreak of COVID-19 has shocked the entire world with its fairly rapid spread and has challenged different sectors. One of the most effective ways to limit its spread is the early and accurate diagnosing infected patients. Medical imaging, such as X-ray and Computed Tomography (CT), combined with the potential of Artificial Intelligence (AI), plays an essential role in supporting medical personnel in the diagnosis process. Thus, in this article five different deep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2 and DenseNet161) and their ensemble, using majority voting have been used to classify COVID-19, pneumoni{\ae} and healthy subjects using chest X-ray images. Multilabel classification was performed to predict multiple pathologies for each patient, if present. Firstly, the interpretability of each of the networks was thoroughly studied using local interpretability methods - occlusion, saliency, input X gradient, guided backpropagation, integrated gradients
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.16986</link><description>&lt;p&gt;
&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#21457;&#23637;&#25588;&#21161;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Causal Machine Learning for Cost-Effective Allocation of Development Aid. (arXiv:2401.16986v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#22269;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#25552;&#20379;&#20102;&#8220;&#26080;&#20154;&#34987;&#36951;&#24323;&#8221;&#30340;&#26356;&#32654;&#22909;&#26410;&#26469;&#34013;&#22270;&#65292;&#20026;&#20102;&#22312;2030&#24180;&#20043;&#21069;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#36139;&#31351;&#22269;&#23478;&#38656;&#35201;&#22823;&#37327;&#30340;&#21457;&#23637;&#25588;&#21161;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#19968;&#20010;&#24179;&#34913;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23558;&#39640;&#32500;&#22269;&#23478;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#35299;&#20915;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#35745;&#31639;&#22312;&#19981;&#21516;&#25588;&#21161;&#35268;&#27169;&#19979;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#19968;&#20010;&#25512;&#26029;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#21270;&#30340;&#27835;&#30103;&#25928;&#26524;&#26354;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;105&#20010;&#22269;&#23478;&#25112;&#30053;&#24615;&#21457;&#23637;&#25588;&#21161;&#25968;&#25454;&#65288;&#24635;&#39069;&#36229;&#36807;52&#20159;&#32654;&#20803;&#65289;&#65292;&#20197;&#32467;&#26463;HIV/AIDS&#20026;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#33539;&#20363;&#26469;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#20540;&#25968;&#25454;&#20013;&#21305;&#37197;&#28388;&#27874;&#30340;&#38590;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30456;&#27604;&#65292;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16729</link><description>&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65306;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#21487;&#35299;&#37322;&#24615;&#20043;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs. (arXiv:2401.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#33539;&#20363;&#26469;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#20540;&#25968;&#25454;&#20013;&#21305;&#37197;&#28388;&#27874;&#30340;&#38590;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30456;&#27604;&#65292;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23454;&#20540;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36890;&#36807;&#21305;&#37197;&#28388;&#27874;&#22120;&#22312;&#25968;&#25454;&#20013;&#25214;&#21040;&#29305;&#24449;&#30340;&#20219;&#21153;&#19982;&#20854;&#30452;&#25509;&#21644;&#26377;&#29289;&#29702;&#21547;&#20041;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#19968;&#33539;&#24335;&#24212;&#29992;&#20110;&#25581;&#31034;&#22797;&#26434;&#20540;CNNs&#21487;&#35299;&#37322;&#24615;&#36935;&#21040;&#20102;&#19968;&#20010;&#24040;&#22823;&#30340;&#38556;&#30861;&#65306;&#24191;&#20041;&#38750;&#24490;&#29615;&#22797;&#26434;&#20540;&#25968;&#25454;&#30340;&#21305;&#37197;&#28388;&#27874;&#22120;&#25193;&#23637;&#65292;&#21363;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#65292;&#22312;&#25991;&#29486;&#20013;&#20165;&#20165;&#26159;&#38544;&#21547;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#22797;&#26434;&#20540;CNNs&#25805;&#20316;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;WLMF&#33539;&#20363;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#20445;&#35777;&#20005;&#35880;&#24615;&#65292;&#25105;&#20204;&#30340;WLMF&#35299;&#20915;&#26041;&#26696;&#19981;&#23545;&#22122;&#22768;&#30340;&#27010;&#29575;&#23494;&#24230;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#26631;&#20934;&#20005;&#26684;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30340;&#29702;&#35770;&#20248;&#21183;&#34987;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters. However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature. To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance. For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise. The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.16655</link><description>&lt;p&gt;
&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#26500;&#24314;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#20351;&#29992;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#36755;&#20986;&#30340;&#8220;&#26435;&#37325;&#8221;&#26469;&#33258;&#25511;&#21046;&#36755;&#20837;&#30340;&#29305;&#24449;&#24207;&#21015;&#65292;&#23427;&#30001;&#25511;&#21046;&#36755;&#20837;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;&#36845;&#20195;&#31215;&#20998;&#26500;&#25104;&#12290;&#32780;&#8220;&#29305;&#24449;&#8221;&#21017;&#22522;&#20110;&#21463;&#25511;ODE&#27169;&#22411;&#20013;&#36755;&#20986;&#20989;&#25968;&#30456;&#23545;&#20110;&#21521;&#37327;&#22330;&#30340;&#36845;&#20195;&#26446;&#23548;&#25968;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24212;&#29992;&#36825;&#20010;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;&#36825;&#19968;&#32467;&#26524;&#21033;&#29992;&#20102;&#21333;&#23618;&#32467;&#26500;&#25152;&#24102;&#26469;&#30340;&#30452;&#25509;&#20998;&#26512;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20855;&#20307;&#31995;&#32479;&#30340;&#20363;&#23376;&#23454;&#20363;&#21270;&#35813;&#30028;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#21518;&#32493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.15294</link><description>&lt;p&gt;
&#29699;&#38754;&#19978;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#30340;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#65288;&#21253;&#25324;Tikhonov&#27491;&#21017;&#21270;&#12289;Landaweber&#36845;&#20195;&#12289;&#35889;&#25130;&#26029;&#21644;&#36845;&#20195;Tikhonov&#65289;&#22312;&#25311;&#21512;&#21487;&#33021;&#23384;&#22312;&#30340;&#26080;&#30028;&#38543;&#26426;&#22122;&#22768;&#30340;&#22024;&#26434;&#25968;&#25454;&#26102;&#30340;&#36924;&#36817;&#24615;&#33021;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#37319;&#26679;&#19981;&#31561;&#24335;&#26041;&#27861;&#21644;&#35268;&#33539;&#38598;&#26041;&#27861;&#30340;&#24310;&#20280;&#12290;&#36890;&#36807;&#25552;&#20379;&#31639;&#23376;&#24046;&#24322;&#21644;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;Sobolev&#31867;&#22411;&#35823;&#24046;&#20272;&#35745;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#20272;&#35745;&#19981;&#21463;&#25991;&#29486;&#20013;Tikhonov&#27491;&#21017;&#21270;&#30340;&#39281;&#21644;&#29616;&#35937;&#12289;&#29616;&#26377;&#35823;&#24046;&#20998;&#26512;&#20013;&#30340;&#26412;&#22320;&#31354;&#38388;&#23631;&#38556;&#21644;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15292</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#33258;&#36866;&#24212;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15292
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#30693;&#22359;&#32467;&#26500;&#19979;&#30340;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#22359;&#31232;&#30095;&#20449;&#21495;&#37325;&#26500;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26159;&#29616;&#26377;&#26041;&#27861;LOP-$\ell_2$/$\ell_1$&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#22312;&#38750;&#21487;&#36870;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#65292;&#32780;LOP-$\ell_2$/$\ell_1$&#19981;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#22823;&#20102;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26356;&#21152;&#28789;&#27963;&#21644;&#24378;&#22823;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>PrivStream&#26159;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#32447;&#24212;&#29992;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#38382;&#39064;&#12290;&#31639;&#27861;&#21487;&#20197;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#26694;&#26550;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14577</link><description>&lt;p&gt;
PrivStream&#65306;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PrivStream: An Algorithm for Streaming Differentially Private Data. (arXiv:2401.14577v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14577
&lt;/p&gt;
&lt;p&gt;
PrivStream&#26159;&#19968;&#31181;&#29992;&#20110;&#27969;&#24335;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#31163;&#32447;&#24212;&#29992;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#25928;&#29992;&#38382;&#39064;&#12290;&#31639;&#27861;&#21487;&#20197;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#26694;&#26550;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#30740;&#31350;&#37117;&#30528;&#37325;&#20110;&#20551;&#35774;&#25152;&#26377;&#25968;&#25454;&#19968;&#27425;&#24615;&#21487;&#29992;&#30340;&#31163;&#32447;&#24212;&#29992;&#12290;&#20294;&#24403;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#20013;&#30340;&#25968;&#25454;&#27969;&#65292;&#35201;&#20040;&#36829;&#21453;&#20102;&#38544;&#31169;&#20445;&#35777;&#65292;&#35201;&#20040;&#23548;&#33268;&#20102;&#31967;&#31957;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31354;&#38388;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#27969;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#36873;&#25321;&#24615;&#35745;&#25968;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#26597;&#35810;&#24212;&#31572;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the research in differential privacy has focused on offline applications with the assumption that all data is available at once. When these algorithms are applied in practice to streams where data is collected over time, this either violates the privacy guarantees or results in poor utility. We derive an algorithm for differentially private synthetic streaming data generation, especially curated towards spatial datasets. Furthermore, we provide a general framework for online selective counting among a collection of queries which forms a basis for many tasks such as query answering and synthetic data generation. The utility of our algorithm is verified on both real-world and simulated datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.14442</link><description>&lt;p&gt;
&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Antibody Humanness Prediction using Patent Data. (arXiv:2401.14442v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#25552;&#39640;&#20102;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#20197;&#21450;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#39044;&#27979;&#20102;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19987;&#21033;&#25968;&#25454;&#26469;&#25552;&#39640;&#25239;&#20307;&#20154;&#24615;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#20102;&#22810;&#38454;&#27573;&#12289;&#22810;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25239;&#20307;&#20154;&#24615;&#20316;&#20026;&#23545;&#25239;&#20307;&#27835;&#30103;&#30340;&#20813;&#30123;&#21453;&#24212;&#30340;&#20195;&#29702;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;&#25239;&#20307;&#27835;&#30103;&#38754;&#20020;&#30528;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#35270;&#20026;&#19968;&#20010;&#24369;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#25239;&#20307;&#24207;&#21015;&#19982;&#21487;&#33021;&#26377;&#22810;&#20010;&#21151;&#33021;&#26631;&#35782;&#31526;&#30456;&#20851;&#32852;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#20854;&#19987;&#21033;&#23646;&#24615;&#23558;&#23427;&#20204;&#20998;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#23545;&#27604;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#32487;&#32493;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#19987;&#21033;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25239;&#20307;&#24207;&#21015;&#30340;&#20154;&#24615;&#35780;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#30340;&#20813;&#30123;&#21407;&#24615;&#25968;&#25454;&#38598;&#36827;&#34892;&#25512;&#29702;&#65292;&#23637;&#31034;&#20102;&#19987;&#21033;&#25968;&#25454;&#21644;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;l
&lt;/p&gt;
&lt;p&gt;
We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the l
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#20855;&#26377;&#26222;&#36866;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#19979;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.04286</link><description>&lt;p&gt;
&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#19968;&#33268;&#24615;&#20197;&#21450;Kolmogorov-Donoho&#26368;&#20248;&#20989;&#25968;&#31867;&#30340;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#20855;&#26377;&#26222;&#36866;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#19979;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#25193;&#23637;&#20102;FL93&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#30340;&#26222;&#36866;&#19968;&#33268;&#24615;&#12290;&#19982;FL93&#20013;&#20998;&#35299;&#20272;&#35745;&#21644;&#32463;&#39564;&#35823;&#24046;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25554;&#20540;&#20219;&#24847;&#25968;&#37327;&#30340;&#28857;&#30340;&#35266;&#23519;&#65292;&#30452;&#25509;&#20998;&#26512;&#20998;&#31867;&#39118;&#38505;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#28304;&#20110;&#23454;&#36341;&#32773;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34987;&#35757;&#32451;&#25104;&#36798;&#21040;0&#35757;&#32451;&#35823;&#24046;&#30340;&#20107;&#23454;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#26368;&#36817;&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#28145;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#36895;&#29575;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#20989;&#25968;&#31867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas
&lt;/p&gt;</description></item><item><title>HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01145</link><description>&lt;p&gt;
HAAQI-Net: &#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01145
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAAQI-Net&#65292;&#19968;&#31181;&#38024;&#23545;&#21161;&#21548;&#22120;&#29992;&#25143;&#23450;&#21046;&#30340;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Hearing Aid Audio Quality Index (HAAQI) &#19981;&#21516;&#65292;HAAQI-Net&#37319;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(BLSTM)&#12290;&#35813;&#27169;&#22411;&#20197;&#35780;&#20272;&#30340;&#38899;&#20048;&#26679;&#26412;&#21644;&#21548;&#21147;&#25439;&#22833;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;HAAQI&#24471;&#20998;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26469;&#33258;&#38899;&#39057;&#21464;&#25442;&#22120;(BEATs)&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#25968;&#19982;&#30495;&#23454;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;HAAQI-Net&#36798;&#21040;&#20102;0.9257&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#30456;&#20851;(LCC)&#65292;0.9394&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;(SRCC)&#65292;&#21644;0.0080&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#20276;&#38543;&#30528;&#25512;&#29702;&#26102;&#38388;&#30340;&#22823;&#24133;&#20943;&#23569;&#65306;&#20174;62.52&#31186;(HAAQI)&#20943;&#23569;&#21040;2.71&#31186;(HAAQI-Net)&#65292;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.12784</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#29992;&#20110;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks. (arXiv:2312.12784v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#20934;&#30830;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33455;&#29255;&#32467;&#26500;&#65292;&#22312;&#21508;&#31181;&#24037;&#33402;&#21442;&#25968;&#19979;&#39044;&#27979;&#31934;&#24230;&#39640;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;100&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#22312;&#20808;&#36827;&#21322;&#23548;&#20307;&#24037;&#33402;&#24320;&#21457;&#20013;&#23454;&#29616;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#26368;&#20339;&#21270;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#22312;&#35774;&#35745;&#25216;&#26415;&#20849;&#20248;&#21270;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#33455;&#29255;&#24211;&#29305;&#24449;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#33455;&#29255;&#32467;&#26500;&#65292;&#24182;&#22312;&#21508;&#31181;&#24037;&#33402;-&#30005;&#21387;-&#28201;&#24230;&#65288;PVT&#65289;&#35282;&#21644;&#25216;&#26415;&#21442;&#25968;&#19978;&#23637;&#31034;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;&#22312;512&#20010;&#26410;&#35265;&#36807;&#30340;&#24037;&#33402;&#35282;&#21644;&#19968;&#30334;&#19975;&#20010;&#27979;&#35797;&#25968;&#25454;&#28857;&#30340;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;33&#31181;&#31867;&#22411;&#30340;&#21333;&#20803;&#30340;&#24310;&#36831;&#12289;&#21151;&#29575;&#21644;&#36755;&#20837;&#24341;&#33050;&#30005;&#23481;&#20855;&#26377;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#22343;&#26041;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#8804; 0.95%&#65292;&#19982;SPICE&#20223;&#30495;&#30456;&#27604;&#21152;&#36895;&#20102;100&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31995;&#32479;&#32423;&#25351;&#26631;&#65292;&#22914;&#26368;&#24046;&#36127;&#26494;&#24347;&#65288;WNS&#65289;&#12289;&#28431;&#30005;&#21151;&#32791;&#21644;&#21160;&#24577;...
&lt;/p&gt;
&lt;p&gt;
Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16592</link><description>&lt;p&gt;
&#26080;&#32447;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#30340;&#36807;&#31354;&#20013;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#23454;&#29616;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#65292;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36807;&#31354;&#20013;&#32858;&#21512;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#12289;&#20248;&#21270;&#21644;&#24863;&#30693;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#31354;&#20013;&#32852;&#21512;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#30340;&#26234;&#33021;&#20307;&#21516;&#26102;&#21521;&#20849;&#20139;&#30340;&#26080;&#32447;&#20449;&#36947;&#24191;&#25773;&#25658;&#24102;&#26412;&#22320;&#20449;&#24687;&#30340;&#27169;&#25311;&#20449;&#21495;&#65292;&#20013;&#22830;&#25511;&#21046;&#22120;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#27719;&#24635;&#27874;&#24418;&#26469;&#26356;&#26032;&#31574;&#30053;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21644;&#20449;&#36947;&#22833;&#30495;&#23545;&#25152;&#25552;&#20986;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#36890;&#20449;&#21644;&#37319;&#26679;&#30340;&#22797;&#26434;&#24230;&#26469;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#36817;&#20284;&#30340;&#31283;&#23450;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13139</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#23436;&#20840;&#30001;&#36866;&#24403;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#26469;&#25551;&#36848;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20219;&#20309;&#22312;&#26631;&#35760;&#22270;&#19978;&#35299;&#37322;&#30340;&#20851;&#20110;&#20108;&#20803;&#36923;&#36753;&#29255;&#27573;&#65288;GC2&#65289;&#30340;&#26597;&#35810;&#37117;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#22823;&#23567;&#20165;&#21462;&#20915;&#20110;&#26597;&#35810;&#28145;&#24230;&#30340;GNN&#26469;&#34920;&#31034;&#12290;&#27491;&#22914;[Barcelo&#65286;Al&#12290;&#65292;2020&#65292;Grohe&#65292;2021]&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#25551;&#36848;&#36866;&#29992;&#20110;&#19968;&#32452;&#28608;&#27963;&#20989;&#25968;&#30340;&#23478;&#26063;&#65292;&#36825;&#34920;&#26126;GNN&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#26469;&#34920;&#36798;&#19981;&#21516;&#30340;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#26080;&#27861;&#34920;&#31034;GC2&#26597;&#35810;&#12290;&#36825;&#24847;&#21619;&#30528;&#22810;&#39033;&#24335;&#21644;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;sigmoid&#12289;&#21452;&#26354;&#27491;&#20999;&#31561;&#65289;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20998;&#31163;&#65292;&#24182;&#22238;&#31572;&#20102;[Grohe&#65292;2021]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15375</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#21495;&#22270;&#65288;ECG&#25110;EKG&#65289;&#26159;&#19968;&#31181;&#27979;&#37327;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#21307;&#23398;&#27979;&#35797;&#12290;ECG&#24120;&#29992;&#20110;&#35786;&#26029;&#21644;&#30417;&#27979;&#21508;&#31181;&#24515;&#33039;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#24459;&#22833;&#24120;&#12289;&#24515;&#32908;&#26775;&#22622;&#21644;&#24515;&#21147;&#34928;&#31469;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ECG&#38656;&#35201;&#20020;&#24202;&#27979;&#37327;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21307;&#30103;&#26426;&#26500;&#30340;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21333;&#23548;&#32852;ECG&#24050;&#32463;&#22312;&#20329;&#25140;&#24335;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#12290;&#21478;&#19968;&#31181;ECG&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20809;&#27978;&#24230;&#33033;&#25615;&#26816;&#27979;&#65288;PPG&#65289;&#65292;&#23427;&#37319;&#29992;&#38750;&#20405;&#20837;&#24615;&#12289;&#20302;&#25104;&#26412;&#30340;&#20809;&#23398;&#26041;&#27861;&#26469;&#27979;&#37327;&#24515;&#33039;&#29983;&#29702;&#23398;&#65292;&#20351;&#20854;&#25104;&#20026;&#25429;&#25417;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#24515;&#33039;&#20449;&#21495;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#34429;&#28982;ECG&#21644;PPG&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#21518;&#32773;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#26174;&#30340;&#20020;&#24202;&#35786;&#26029;&#20215;&#20540;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
&lt;/p&gt;</description></item><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#26524;&#65292;&#20174;&#32780;&#20351;&#20854;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.12612</link><description>&lt;p&gt;
&#23581;&#35797;&#26356;&#31616;&#21333;-&#25913;&#36827;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#20256;&#32479;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#26524;&#65292;&#20174;&#32780;&#20351;&#20854;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#28608;&#21457;&#20102;&#23545;&#22686;&#24378;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#20852;&#36259;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#26085;&#24535;&#20107;&#20214;&#65288;&#26085;&#24535;&#28040;&#24687;&#27169;&#26495;&#65289;&#20013;&#25552;&#21462;&#24847;&#20041;&#65292;&#24182;&#24320;&#21457;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#22797;&#26434;&#24615;&#36739;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#34429;&#28982;&#19981;&#22826;&#20381;&#36182;&#25968;&#25454;&#65292;&#26356;&#39640;&#25928;&#65292;&#20294;&#27604;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20351;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#26356;&#23454;&#29992;&#65292;&#30446;&#26631;&#26159;&#22686;&#24378;&#20256;&#32479;&#25216;&#26415;&#20197;&#36798;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#20197;&#21069;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#38142;&#25509;Stack Overflow&#19978;&#30340;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#20248;&#21270;&#30340;&#20256;&#32479;&#25216;&#26415;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#21463;&#21040;&#36825;&#19968;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#36731;&#37327;&#32423;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#26080;&#30417;&#30563;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#19968;&#31181;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#19968;&#33268;&#22320;&#37325;&#24314;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290; (arXiv:2308.12459v1 [eess.SP])</title><link>http://arxiv.org/abs/2308.12459</link><description>&lt;p&gt;
&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#23454;&#29616;&#38646;&#24310;&#36831;&#19968;&#33268;&#20449;&#21495;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series. (arXiv:2308.12459v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#19968;&#33268;&#22320;&#37325;&#24314;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290; (arXiv:2308.12459v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#29616;&#23454;&#19990;&#30028;&#30340;&#27169;&#25311;&#20449;&#21495;&#36890;&#24120;&#28041;&#21450;&#26102;&#38388;&#37319;&#26679;&#21644;&#24133;&#24230;&#31163;&#25955;&#21270;&#12290;&#21518;&#32493;&#30340;&#20449;&#21495;&#37325;&#24314;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20135;&#29983;&#19968;&#20010;&#19982;&#24133;&#24230;&#20998;&#36776;&#29575;&#21644;&#33719;&#21462;&#26679;&#26412;&#30340;&#26102;&#38388;&#23494;&#24230;&#26377;&#20851;&#30340;&#35823;&#24046;&#12290;&#20174;&#23454;&#26045;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#19968;&#33268;&#30340;&#20449;&#21495;&#37325;&#24314;&#26041;&#27861;&#22312;&#37319;&#26679;&#29575;&#22686;&#21152;&#26102;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26377;&#30410;&#30340;&#35823;&#24046;&#34928;&#20943;&#25928;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#33719;&#24471;&#30340;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#20174;&#25968;&#25454;&#27969;&#20013;&#36827;&#34892;&#19968;&#33268;&#20449;&#21495;&#37325;&#24314;&#30340;&#26041;&#27861;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#24310;&#36831;&#21709;&#24212;&#35201;&#27714;&#19979;&#19968;&#33268;&#22320;&#37325;&#24314;&#37327;&#21270;&#38388;&#38548;&#30340;&#27969;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21487;&#20197;&#20943;&#23569;&#38646;&#24310;&#36831;&#20449;&#21495;&#37325;&#24314;&#30340;&#31895;&#31961;&#24230;&#12290;&#26412;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30528;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.11199</link><description>&lt;p&gt;
ConcatPlexer&#65306;&#36890;&#36807;&#38468;&#21152;Dim1&#25209;&#22788;&#29702;&#20197;&#21152;&#24555;ViTs&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConcatPlexer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#20351;&#29992;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21463;&#21040;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36824;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;Transformer&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#24102;&#26469;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#20005;&#37325;&#22686;&#21152;&#65292;&#22240;&#27492;&#26377;&#20960;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20943;&#23569;&#36825;&#31181;&#36127;&#25285;&#30340;&#26041;&#27861;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#20943;&#23569;&#25104;&#26412;&#30340;&#26041;&#27861;Data Multiplexing (DataMUX)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35270;&#35273;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38468;&#21152;&#30340;Dim1&#25209;&#22788;&#29702;&#65288;&#21363;&#36830;&#25509;&#65289;&#65292;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#22522;&#30784;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#35270;&#35273;&#27169;&#22411;&#24341;&#20837;&#20102;DataMux&#30340;&#19968;&#31181;&#22825;&#28982;&#36866;&#24212;&#26041;&#27861;&#65292;&#22270;&#20687;&#22810;&#36335;&#22797;&#29992;&#22120;&#65288;Image Multiplexer&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#32452;&#20214;&#26469;&#20811;&#26381;&#20854;&#32570;&#28857;&#65292;&#36827;&#32780;&#24418;&#25104;&#20102;&#25105;&#20204;&#26368;&#32456;&#30340;&#27169;&#22411;ConcatPlexer&#65292;&#22312;&#25512;&#29702;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#28857;&#12290;ConcatPlexer&#22312;ImageNet1K&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#30340;&#22522;&#30784;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;&#24847;&#22806;&#35760;&#24518;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04836</link><description>&lt;p&gt;
&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#23454;&#29616;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#24847;&#22806;&#35760;&#24518;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#30340;&#22522;&#30784;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;&#24847;&#22806;&#35760;&#24518;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#24847;&#22806;&#30340;&#25506;&#32034;&#30340;&#23616;&#38480;&#24615;&#12290;&#22870;&#21169;&#26159;&#24847;&#22806;&#30340;&#26032;&#39062;&#24615;&#65292;&#32780;&#19981;&#26159;&#24847;&#22806;&#30340;&#35268;&#33539;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35760;&#24518;&#32593;&#32476;&#20013;&#30340;&#26816;&#32034;&#38169;&#35823;&#26469;&#20272;&#35745;&#24847;&#22806;&#30340;&#26032;&#39062;&#24615;&#65292;&#35760;&#24518;&#23384;&#20648;&#21644;&#37325;&#24314;&#24847;&#22806;&#12290;&#25105;&#20204;&#30340;&#24847;&#22806;&#35760;&#24518;&#65288;SM&#65289;&#22686;&#21152;&#20102;&#22522;&#20110;&#24847;&#22806;&#30340;&#20869;&#22312;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#65292;&#20445;&#25345;&#20102;&#23545;&#28608;&#21160;&#20154;&#24515;&#30340;&#25506;&#32034;&#30340;&#20852;&#36259;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#19981;&#21487;&#39044;&#27979;&#25110;&#22122;&#22768;&#35266;&#23519;&#30340;&#19981;&#24517;&#35201;&#30340;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32467;&#21512;&#21508;&#31181;&#24847;&#22806;&#39044;&#27979;&#22120;&#30340;SM&#23637;&#31034;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#26368;&#32456;&#24615;&#33021;&#65292;&#21253;&#25324;&#22122;&#22768;&#30005;&#35270;&#12289;&#23548;&#33322;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Atari&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.10895</link><description>&lt;p&gt;
&#29273;&#31185;&#28857;&#20113;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#29273;&#31185;&#23398;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FDI 16&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#29273;&#40831;&#32593;&#26684;&#21644;&#28857;&#20113;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#21464;&#20998;FoldingNet&#65288;VF-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28857;&#20113;&#35774;&#35745;&#30340;&#23436;&#20840;&#27010;&#29575;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#28857;&#20113;&#28508;&#21464;&#37327;&#27169;&#22411;&#32570;&#20047;&#36755;&#20837;&#21644;&#36755;&#20986;&#28857;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20248;&#21270;Chamfer&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#32570;&#20047;&#24402;&#19968;&#21270;&#20998;&#24067;&#23545;&#24212;&#30340;&#24230;&#37327;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#29992;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#21462;&#20195;&#20102;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;Chamfer&#36317;&#31163;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#31616;&#21270;&#20102;&#27010;&#29575;&#25193;&#23637;&#12290;&#36825;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29273;&#40831;&#37325;&#24314;&#20013;&#36739;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital dentistry has made significant advancements, yet numerous challenges remain. This paper introduces the FDI 16 dataset, an extensive collection of tooth meshes and point clouds. Additionally, we present a novel approach: Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder designed for point clouds. Notably, prior latent variable models for point clouds lack a one-to-one correspondence between input and output points. Instead, they rely on optimizing Chamfer distances, a metric that lacks a normalized distributional counterpart, rendering it unsuitable for probabilistic modeling. We replace the explicit minimization of Chamfer distances with a suitable encoder, increasing computational efficiency while simplifying the probabilistic extension. This allows for straightforward application in various tasks, including mesh generation, shape completion, and representation learning. Empirically, we provide evidence of lower reconstruction error in dental recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.06555</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36924;&#36817;&#65306;&#20174;ReLU&#21040;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#38598;&#21512;A&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#12289;LeakyReLU&#12289;ReLU^2&#12289;ELU&#12289;SELU&#12289;Softplus&#12289;GELU&#12289;SiLU&#12289;Swish&#12289;Mish&#12289;Sigmoid&#12289;Tanh&#12289;Arctan&#12289;Softsign&#12289;dSiLU&#21644;SRS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;varrho&#8712;A&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#24471;&#22823;&#37096;&#20998;&#23545;&#20110;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65292;&#23613;&#31649;&#38656;&#35201;&#31245;&#22823;&#30340;&#24120;&#25968;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05318</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#28342;&#35299;&#24230;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#20294;&#38590;&#20197;&#39044;&#27979;&#30340;&#24615;&#36136;&#12290;&#20351;&#29992;&#19968;&#32423;&#21407;&#29702;&#26041;&#27861;&#35745;&#31639;&#28342;&#35299;&#24230;&#38656;&#35201;&#32771;&#34385;&#29109;&#21644;&#28947;&#30340;&#31454;&#20105;&#25928;&#24212;&#65292;&#23548;&#33268;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#19988;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#20219;&#20309;&#35745;&#31639;&#25216;&#26415;&#30340;&#26131;&#29992;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#23548;&#33268;&#32676;&#20307;&#36129;&#29486;&#26041;&#27861;&#30340;&#25345;&#32493;&#27969;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20855;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#38745;&#24577;&#32593;&#31449;&#19978;&#36816;&#34892;&#65288;&#26080;&#38656;&#26381;&#21153;&#22120;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35745;&#31639;&#38656;&#27714;&#36716;&#31227;&#21040;&#32593;&#31449;&#35775;&#38382;&#32773;&#36523;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23433;&#35013;&#65292;&#28040;&#38500;&#20102;&#25903;&#20184;&#21644;&#32500;&#25252;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28342;&#35299;&#24230;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21019;&#24314;&#24179;&#34913;&#28342;&#35299;&#24230;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2307.00162</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#23545;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00162
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;S3Ms&#65289;&#34987;&#24341;&#20837;&#65292;&#20026;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#25913;&#36827;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;S3Ms&#22312;&#19981;&#21516;&#30340;&#23618;&#20013;&#32534;&#30721;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19988;&#19968;&#20123;S3Ms&#20284;&#20046;&#23398;&#20064;&#20102;&#31867;&#20284;&#20110;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65288;&#22914;&#21333;&#35789;&#65289;&#30340;&#31243;&#24230;&#20197;&#21450;&#21333;&#35789;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#30721;&#20301;&#32622;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;S3Ms&#30340;&#19981;&#21516;&#23618;&#30340;&#21333;&#35789;&#29255;&#27573;&#34920;&#31034;&#36827;&#34892;&#20102;&#22810;&#31181;&#20998;&#26512;&#65306;wav2vec2&#12289;HuBERT&#21644;WavLM&#12290;&#25105;&#20204;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#26469;&#34913;&#37327;&#36825;&#20123;&#34920;&#31034;&#19982;&#21333;&#35789;&#32423;&#35821;&#35328;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#21333;&#35789;&#32423;&#35821;&#35328;&#20869;&#23481;&#24448;&#24448;&#20986;&#29616;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#65292;&#32780;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#65288;&#22914;&#21457;&#38899;&#65289;&#20063;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.09882</link><description>&lt;p&gt;
&#26102;&#31354;Tweedie&#27169;&#22411;&#22312;&#39044;&#27979;&#23384;&#22312;&#38646;&#33192;&#32960;&#21644;&#38271;&#23614;&#26053;&#34892;&#38656;&#27714;&#20013;&#30340;&#24212;&#29992;&#21450;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#31354;Tweedie&#27169;&#22411;STTD&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#35299;&#20915;&#39640;&#20998;&#36776;&#29575;OD&#30697;&#38453;&#20013;&#31232;&#30095;&#21644;&#38271;&#23614;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#38590;&#20197;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#31354;&#38388;-Tweedie&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STTD&#65289;&#12290;STTD&#23558;Tweedie&#20998;&#24067;&#20316;&#20026;&#20256;&#32479;&#30340;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#30340;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#23884;&#20837;&#26469;&#21442;&#25968;&#21270;&#26053;&#34892;&#38656;&#27714;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;STTD&#22312;&#39640;&#20998;&#36776;&#29575;&#22330;&#26223;&#19979;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17323</link><description>&lt;p&gt;
&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24378;&#20984;&#20294;&#28508;&#22312;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20248;&#21270;&#30340;&#65288;&#38543;&#26426;&#65289;&#27425;&#26799;&#24230;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31561;&#20215;&#23545;&#20598;&#25551;&#36848;&#65288;&#31867;&#20284;&#20110;&#23545;&#20598;&#24179;&#22343;&#65289;&#26469;&#25551;&#36848;&#32463;&#20856;&#30340;&#27425;&#26799;&#24230;&#27861;&#65292;&#36817;&#31471;&#27425;&#26799;&#24230;&#27861;&#21644;&#20999;&#25442;&#27425;&#26799;&#24230;&#27861;&#12290;&#36825;&#20123;&#31561;&#20215;&#24615;&#33021;&#22815;&#20197; $O(1/T)$ &#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#20998;&#21035;&#36824;&#25552;&#20379;&#20102;&#32463;&#20856;&#21407;&#22987;&#38388;&#38553;&#21644;&#21069;&#20154;&#26410;&#26366;&#20998;&#26512;&#30340;&#23545;&#20598;&#38388;&#38553;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#36825;&#20123;&#32463;&#20856;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#36817;&#20046;&#25152;&#26377;&#30340;&#27493;&#38271;&#36873;&#25321;&#21644;&#19968;&#31995;&#21015;&#30340;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#23545;&#20110;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27425;&#26799;&#24230;&#27861;&#30340;&#26089;&#26399;&#36845;&#20195;&#21487;&#33021;&#20250;&#20986;&#29616;&#25351;&#25968;&#32423;&#30340;&#21457;&#25955;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#22788;&#29702;&#36807;&#36825;&#31181;&#38382;&#39064;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#19981;&#33391;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20173;&#28982;&#30830;&#20445;&#21644; bounds &#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12095</link><description>&lt;p&gt;
&#20351;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20877;&#27425;&#21331;&#36234;&#65306;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer
&lt;/p&gt;
&lt;p&gt;
Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Transformer&#21644;MLP&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;NLP&#21644;CV&#26041;&#38754;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;MLP&#30456;&#27604;&#65292;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Transformer&#65292;&#21363;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#65288;CARD&#65289;&#65292;&#20197;&#35299;&#20915;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;CARD&#24341;&#20837;&#20102;&#21452;Transformer&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#22810;&#20010;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#20381;&#36182;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#28508;&#22312;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#39044;&#27979;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;CARD&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#39044;&#27979;&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#33647;&#20998;&#23376;&#12290;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#65292;&#32780;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17615</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Utilizing Reinforcement Learning for de novo Drug Design. (arXiv:2303.17615v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#39044;&#27979;&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#33647;&#20998;&#23376;&#12290;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#65292;&#32780;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#26032;&#33647;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#23383;&#31526;&#20018;&#29983;&#25104;&#26032;&#20998;&#23376;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;de novo&#33647;&#29289;&#35774;&#35745;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21508;&#31181;on-policy&#21644;off-policy &#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#37325;&#25773;&#32531;&#20914;&#21306;&#65292;&#23398;&#20064;&#22522;&#20110;RNN&#30340;&#31574;&#30053;&#65292;&#29983;&#25104;&#39044;&#27979;&#23545;&#20110;&#22810;&#24052;&#33018;&#21463;&#20307;DRD2&#20855;&#26377;&#27963;&#24615;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#20998;&#21644;&#20302;&#20998;&#20998;&#23376;&#26469;&#26356;&#26032;&#31574;&#30053;&#26159;&#26377;&#21033;&#30340;&#12290;&#23545;&#20110;on-policy&#31639;&#27861;&#65292;&#20351;&#29992;&#25152;&#26377;&#29983;&#25104;&#30340;&#20998;&#23376;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#37325;&#25918;&#39640;&#12289;&#20013;&#21644;&#20302;&#20998;&#23376;&#26102;&#65292;off-policy&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#29983;&#25104;&#20998;&#23376;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based approaches for generating novel drug molecules with specific properties have gained a lot of interest in the last years. Recent studies have demonstrated promising performance for string-based generation of novel molecules utilizing reinforcement learning. In this paper, we develop a unified framework for using reinforcement learning for de novo drug design, wherein we systematically study various on- and off-policy reinforcement learning algorithms and replay buffers to learn an RNN-based policy to generate novel molecules predicted to be active against the dopamine receptor DRD2. Our findings suggest that it is advantageous to use at least both top-scoring and low-scoring molecules for updating the policy when structural diversity is essential. Using all generated molecules at an iteration seems to enhance performance stability for on-policy algorithms. In addition, when replaying high, intermediate, and low-scoring molecules, off-policy algorithms display the pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#65292;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#65292;&#20462;&#27491;&#20266;&#36127;&#26679;&#26412;&#65292;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11673</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#33258;&#23398;&#20064;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Self-Supervised Contrastive Learning. (arXiv:2301.11673v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#65292;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#65292;&#20462;&#27491;&#20266;&#36127;&#26679;&#26412;&#65292;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24212;&#29992;&#65292;&#28982;&#32780;&#20854;&#33258;&#30417;&#30563;&#29256;&#26412;&#20173;&#23384;&#22312;&#35768;&#22810;&#28608;&#21160;&#20154;&#24515;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#36127;&#26679;&#26412;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#65292;&#22240;&#27492;&#38543;&#26426;&#36873;&#25321;&#30340;&#26679;&#26412;&#21487;&#33021;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#20266;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#35757;&#32451;&#19981;&#27491;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#23427;&#20173;&#28982;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#65292;&#20174;&#32780;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#12290;&#31361;&#20986;&#20248;&#28857;&#22312;&#20110;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26159;&#19968;&#20010;&#21442;&#25968;&#32467;&#26500;&#65292;&#20854;&#20013;&#20855;&#26377;&#20301;&#32622;&#21442;&#25968;&#20197;&#32416;&#27491;&#20266;&#36127;&#26679;&#26412;&#20197;&#21450;&#20855;&#26377;&#27987;&#24230;&#21442;&#25968;&#20197;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;BCL&#25439;&#22833;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;AST&#21644;DFG&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#38754;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#26102;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.05239</link><description>&lt;p&gt;
StructCoder: &#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;AST&#21644;DFG&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#38754;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#26102;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20195;&#30721;&#29983;&#25104;&#38382;&#39064;&#65292;&#30446;&#26631;&#22312;&#20110;&#22312;&#32473;&#23450;&#19981;&#21516;&#35821;&#35328;&#25110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#28304;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30446;&#26631;&#20195;&#30721;&#12290;&#38024;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#20005;&#26684;&#29702;&#35299;&#21644;&#29983;&#25104;&#38656;&#35201;&#19968;&#31181;&#26356;&#20026;&#20005;&#35880;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#65292;&#22312;&#27492;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#37117;&#26126;&#30830;&#22320;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#28304;&#20195;&#30721;&#21644;&#30446;&#26631;&#20195;&#30721;&#30340;&#35821;&#27861;&#21644;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#19981;&#20165;&#36890;&#36807;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#35821;&#27861;&#26641;&#21644;&#25968;&#25454;&#27969;&#22270;&#20351;&#32534;&#30721;&#22120;&#32467;&#26500;&#24863;&#30693;&#65292;&#36824;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#36741;&#21161;&#20219;&#21153;&#8212;&#8212;AST&#65288;&#25277;&#35937;&#35821;&#27861;&#65289;&#21644;DFG&#65288;&#25968;&#25454;&#27969;&#22270;&#65289;&#24110;&#21161;&#35299;&#30721;&#22120;&#20445;&#30041;&#30446;&#26631;&#20195;&#30721;&#30340;&#35821;&#27861;&#21644;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.01327</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#20809;&#35889;&#20687;&#32032;&#35299;&#28151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20551;&#35774;(1)&#20016;&#24230;&#21487;&#20197;&#32534;&#30721;&#20026;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;(2)&#25104;&#20998;&#30340;&#20809;&#35889;&#21487;&#20197;&#34920;&#31034;&#20026;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#19979;&#35299;&#20915;&#20102;&#20016;&#24230;&#20272;&#35745;&#21644;&#25104;&#20998;&#25552;&#21462;&#38382;&#39064;&#65292;&#20854;&#20013;&#29380;&#21033;&#20811;&#38647;&#29942;&#39048;&#23618;&#24314;&#27169;&#20016;&#24230;&#65292;&#35299;&#30721;&#22120;&#25191;&#34892;&#25104;&#20998;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#27169;&#22411;&#20165;&#22312;&#21253;&#21547;&#24863;&#20852;&#36259;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#25104;&#20998;&#30340;&#32447;&#24615;&#32452;&#21512;&#20687;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20174;&#32654;&#22269;&#22320;&#36136;&#35843;&#26597;&#23616;&#20809;&#35889;&#24211;&#20013;&#26816;&#32034;&#20986;&#25104;&#20998;(&#20809;&#35889;)&#12290;&#28982;&#21518;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#21253;&#21547;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#25152;&#20351;&#29992;&#30340;&#19968;&#37096;&#20998;&#25104;&#20998;&#30340;&#8220;&#23454;&#38469;&#25968;&#25454;&#8221;&#19978;&#36827;&#34892;&#20687;&#32032;&#35299;&#28151;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for hyperspectral pixel {\it unmixing}. The proposed method assumes that (1) {\it abundances} can be encoded as Dirichlet distributions and (2) spectra of {\it endmembers} can be represented as multivariate Normal distributions. The method solves the problem of abundance estimation and endmember extraction within a variational autoencoder setting where a Dirichlet bottleneck layer models the abundances, and the decoder performs endmember extraction. The proposed method can also leverage transfer learning paradigm, where the model is only trained on synthetic data containing pixels that are linear combinations of one or more endmembers of interest. In this case, we retrieve endmembers (spectra) from the United States Geological Survey Spectral Library. The model thus trained can be subsequently used to perform pixel unmixing on "real data" that contains a subset of the endmembers used to generated the synthetic data. The model achieves state-of-the-art results on sev
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.10953</link><description>&lt;p&gt;
&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#30340;InfoMap&#31639;&#27861;&#22312;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs. (arXiv:2112.10953v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10953
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
InfoMap&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#8220;&#31038;&#21306;&#8221;&#33410;&#28857;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20854;&#24212;&#29992;&#19982;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#26469;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#12290;&#25913;&#36827;&#21518;&#30340;InfoMap&#31639;&#27861;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#32467;&#26500;&#21487;&#33021;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
InfoMap is a popular approach for detecting densely connected "communities" of nodes in networks. To detect such communities, InfoMap uses random walks and ideas from information theory. Motivated by the dynamics of disease spread on networks, whose nodes may have heterogeneous disease-removal rates, we adapt InfoMap to absorbing random walks. To do this, we use absorption-scaled graphs, in which the edge weights are scaled according to absorption rates, along with Markov time sweeping. One of our adaptations of InfoMap converges to the standard version of InfoMap in the limit in which the node-absorption rates approach $0$. The community structure that we obtain using our adaptations of InfoMap can differ markedly from the community structure that one detects using methods that do not take node-absorption rates into account. Additionally, we demonstrate that the community structure that is induced by local dynamics can have important implications for susceptible-infected-recovered (SI
&lt;/p&gt;</description></item></channel></rss>