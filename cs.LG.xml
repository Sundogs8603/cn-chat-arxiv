<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#29616;&#23454;&#23039;&#24577;&#30340;&#20154;&#65292;&#20197;&#21450;&#21512;&#25104;&#33021;&#22815;&#22312;&#22330;&#26223;&#20013;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14406</link><description>&lt;p&gt;
&#23558;&#20154;&#32622;&#20110;&#20854;&#22330;&#26223;&#20013;&#65306;&#32771;&#34385;&#21487;&#20379;&#24615;&#30340;&#20154;&#31867;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
Putting People in Their Place: Affordance-Aware Human Insertion into Scenes. (arXiv:2304.14406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#29616;&#23454;&#23039;&#24577;&#30340;&#20154;&#65292;&#20197;&#21450;&#21512;&#25104;&#33021;&#22815;&#22312;&#22330;&#26223;&#20013;&#19982;&#20154;&#31867;&#36827;&#34892;&#33258;&#28982;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25512;&#26029;&#22330;&#26223;&#21487;&#20379;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#22312;&#22330;&#26223;&#20013;&#25554;&#20837;&#20154;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#26377;&#26631;&#35760;&#21306;&#22495;&#30340;&#22330;&#26223;&#22270;&#20687;&#21644;&#19968;&#20010;&#20154;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20154;&#25554;&#20837;&#21040;&#22330;&#26223;&#20013;&#65292;&#24182;&#23562;&#37325;&#22330;&#26223;&#21487;&#20379;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#19978;&#19979;&#25991;&#25512;&#26029;&#20986;&#19968;&#32452;&#29616;&#23454;&#30340;&#23039;&#24577;&#65292;&#37325;&#26032;&#35843;&#25972;&#21442;&#32771;&#20154;&#30340;&#23039;&#24577;&#65292;&#24182;&#35843;&#21644;&#26500;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#22312;&#35270;&#39057;&#21098;&#36753;&#20013;&#37325;&#26032;&#23039;&#24577;&#20154;&#31867;&#26469;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#35774;&#32622;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#21253;&#21547; 240 &#19975;&#20010;&#35270;&#39057;&#21098;&#36753;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20250;&#20135;&#29983;&#22810;&#26679;&#30340;&#21512;&#29702;&#23039;&#24577;&#65292;&#21516;&#26102;&#23562;&#37325;&#22330;&#26223;&#19978;&#19979;&#25991;&#12290;&#37492;&#20110;&#23398;&#20064;&#21040;&#30340;&#20154; - &#22330;&#26223;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#22312;&#27809;&#26377;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24863;&#24212;&#21040;&#30495;&#23454;&#30340;&#20154;&#21644;&#22330;&#26223;&#65292;&#20197;&#21450;&#21551;&#29992;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;&#23450;&#37327;&#35780;&#20272;&#26174;&#31034;&#20986;&#65292;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#25104;&#20102;&#26356;&#30495;&#23454;&#30340;&#20154;&#31867;&#22806;&#35266;&#21644;&#26356;&#33258;&#28982;&#30340;&#20154; - &#22330;&#26223;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re-pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. A quantitative evaluation shows that our method synthesizes more realistic human appearance and more natural human-scene interactions than prior work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#21453;&#28436;&#26041;&#27861;Make It So&#65292;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#25805;&#20316;&#20351;&#24471;&#20445;&#30041;&#20102;&#32534;&#36753;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#22495;&#22806;&#22270;&#20687;&#26041;&#38754;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#21453;&#28436;&#31934;&#24230;&#25552;&#39640;&#20116;&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#22797;&#26434;&#30340;&#23460;&#20869;&#22330;&#26223;&#65292;&#23454;&#29616;&#20102;&#21313;&#20493;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14403</link><description>&lt;p&gt;
&#35753;&#23427;&#21464;&#25104;&#29616;&#23454;: &#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#21453;&#36716;&#21644;&#32534;&#36753;&#30340;Steering StyleGAN
&lt;/p&gt;
&lt;p&gt;
Make It So: Steering StyleGAN for Any Image Inversion and Editing. (arXiv:2304.14403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#21453;&#28436;&#26041;&#27861;Make It So&#65292;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#25805;&#20316;&#20351;&#24471;&#20445;&#30041;&#20102;&#32534;&#36753;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#22495;&#22806;&#22270;&#20687;&#26041;&#38754;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#21453;&#28436;&#31934;&#24230;&#25552;&#39640;&#20116;&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#22797;&#26434;&#30340;&#23460;&#20869;&#22330;&#26223;&#65292;&#23454;&#29616;&#20102;&#21313;&#20493;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StyleGAN&#30340;&#20998;&#31163;&#39118;&#26684;&#34920;&#31034;&#20351;&#24471;&#36890;&#36807;&#25805;&#20316;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#24378;&#22823;&#30340;&#22270;&#20687;&#32534;&#36753;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20934;&#30830;&#22320;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#23427;&#20204;&#30340;&#28508;&#22312;&#21464;&#37327;&#65288;GAN&#21453;&#28436;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#22312;&#32500;&#25345;&#32534;&#36753;&#26041;&#21521;&#21644;&#29983;&#25104;&#36924;&#30495;&#32467;&#26524;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Make It So&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#65292;&#23427;&#22312;$\mathcal{Z}$&#65288;&#22122;&#22768;&#65289;&#31354;&#38388;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;$\mathcal{W}$&#65288;&#28508;&#22312;&#39118;&#26684;&#65289;&#31354;&#38388;&#20013;&#36816;&#34892;&#12290;Make It So&#20445;&#30041;&#20102;&#32534;&#36753;&#33021;&#21147;&#65292;&#21363;&#20351;&#26159;&#22312;&#22495;&#22806;&#22270;&#20687;&#26041;&#38754;&#12290;&#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;Make It So&#22312;&#21453;&#28436;&#31934;&#24230;&#26041;&#38754;&#27604;&#26368;&#20808;&#36827;&#30340;PTI&#26041;&#27861;~\cite{roich2021pivotal}&#25552;&#39640;&#20102;&#20116;&#20493;&#65292;&#24182;&#19988;&#23545;&#20110;&#22797;&#26434;&#30340;&#23460;&#20869;&#22330;&#26223;&#65292;&#23454;&#29616;&#20102;&#21313;&#20493;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
StyleGAN's disentangled style representation enables powerful image editing by manipulating the latent variables, but accurately mapping real-world images to their latent variables (GAN inversion) remains a challenge. Existing GAN inversion methods struggle to maintain editing directions and produce realistic results.  To address these limitations, we propose Make It So, a novel GAN inversion method that operates in the $\mathcal{Z}$ (noise) space rather than the typical $\mathcal{W}$ (latent style) space. Make It So preserves editing capabilities, even for out-of-domain images. This is a crucial property that was overlooked in prior methods. Our quantitative evaluations demonstrate that Make It So outperforms the state-of-the-art method PTI~\cite{roich2021pivotal} by a factor of five in inversion accuracy and achieves ten times better edit quality for complex indoor scenes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#25552;&#39640;&#21046;&#36896;&#19994;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#26410;&#26631;&#35760;&#30340;&#21644;&#26377;&#38480;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#39046;&#22495;&#31227;&#20301;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14398</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21046;&#36896;&#19994;&#27169;&#22411;&#27867;&#21270;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning. (arXiv:2304.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#25552;&#39640;&#21046;&#36896;&#19994;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#26410;&#26631;&#35760;&#30340;&#21644;&#26377;&#38480;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#39046;&#22495;&#31227;&#20301;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#22312;&#27809;&#26377;&#25163;&#21160;&#35774;&#35745;&#30340;&#32479;&#35745;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21407;&#22987;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20013;&#35786;&#26029;&#25925;&#38556;&#21644;&#35780;&#20272;&#26426;&#22120;&#20581;&#24247;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26497;&#20854;&#22256;&#38590;&#36866;&#29992;&#20110;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#12290;&#26426;&#22120;&#25968;&#25454;&#36890;&#24120;&#26159;&#26410;&#26631;&#35760;&#30340;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#20581;&#24247;&#26465;&#20214;&#65288;&#20363;&#22914;&#65292;&#20165;&#26377;&#27491;&#24120;&#25805;&#20316;&#25968;&#25454;&#65289;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24037;&#33402;&#21442;&#25968;&#30340;&#21464;&#21270;&#21644;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#22495;&#30340;&#31227;&#20301;&#12290;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#21487;&#33021;&#38590;&#20197;&#23398;&#20064;&#32039;&#20945;&#12289;&#26377;&#21306;&#21035;&#21147;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#19981;&#33021;&#25512;&#24191;&#21040;&#36825;&#20123;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22495;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#25317;&#26377;&#20016;&#23500;&#30340;&#31867;&#26469;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#23581;&#35797;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#65292;&#20294;&#20551;&#23450;&#20102;&#31867;&#20284;&#30340;&#23376;&#32467;&#26500;&#65292;&#22312;&#26032;&#30340;&#25925;&#38556;&#20986;&#29616;&#26102;&#21487;&#33021;&#19981;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#28304;&#22495;&#29305;&#24449;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#26469;&#25913;&#21892;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#21644;&#26426;&#22120;&#20581;&#24247;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#26410;&#26631;&#35760;&#21644;&#26377;&#38480;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#22495;&#31227;&#20301;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) can diagnose faults and assess machine health from raw condition monitoring data without manually designed statistical features. However, practical manufacturing applications remain extremely difficult for existing DL methods. Machine data is often unlabeled and from very few health conditions (e.g., only normal operating data). Furthermore, models often encounter shifts in domain as process parameters change and new categories of faults emerge. Traditional supervised learning may struggle to learn compact, discriminative representations that generalize to these unseen target domains since it depends on having plentiful classes to partition the feature space with decision boundaries. Transfer Learning (TL) with domain adaptation attempts to adapt these models to unlabeled target domains but assumes similar underlying structure that may not be present if new faults emerge. This study proposes focusing on maximizing the feature generality on the source domain and apply
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#21487;&#24494;&#20998;AIS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31867;&#20284;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#30340;&#37325;&#37319;&#26679;&#27493;&#39588;&#26469;&#36991;&#20813;&#31890;&#23376;&#28388;&#27874;&#20013;&#30340;&#26799;&#24230;&#26041;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14390</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#20013;&#30340;&#37325;&#37319;&#26679;&#26799;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers. (arXiv:2304.14390v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#21487;&#24494;&#20998;AIS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31867;&#20284;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#30340;&#37325;&#37319;&#26679;&#27493;&#39588;&#26469;&#36991;&#20813;&#31890;&#23376;&#28388;&#27874;&#20013;&#30340;&#26799;&#24230;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36864;&#28779;&#37325;&#35201;&#24615;&#37319;&#26679;&#65288;AIS&#65289;&#26159;&#23558;&#31890;&#23376;&#27839;&#30528;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#38142;&#20174;&#21487;&#35745;&#31639;&#30340;&#21021;&#22987;&#20998;&#24067;&#31227;&#21160;&#21040;&#19981;&#21487;&#35745;&#31639;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;AIS&#65288;DAIS&#65289;&#20801;&#35768;&#23545;AIS&#30340;&#36716;&#31227;&#26680;&#21644;&#20998;&#24067;&#36827;&#34892;&#39640;&#25928;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DAIS&#20013;&#23384;&#22312;&#20302;&#26377;&#25928;&#26679;&#26412;&#37327;&#65292;&#34920;&#26126;&#20998;&#24067;&#36864;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20511;&#37492;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#37319;&#26679;&#27493;&#39588;&#26469;&#25193;&#23637;DAIS&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#20063;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#35299;&#37322;&#65292;&#26080;&#38656;&#36890;&#36807;&#37325;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#24494;&#20998;&#65292;&#36825;&#36991;&#20813;&#20102;&#31890;&#23376;&#28388;&#27874;&#20013;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annealed Importance Sampling (AIS) moves particles along a Markov chain from a tractable initial distribution to an intractable target distribution. The recently proposed Differentiable AIS (DAIS) (Geffner and Domke, 2021; Zhang et al., 2021) enables efficient optimization of the transition kernels of AIS and of the distributions. However, we observe a low effective sample size in DAIS, indicating degenerate distributions. We thus propose to extend DAIS by a resampling step inspired by Sequential Monte Carlo. Surprisingly, we find empirically-and can explain theoretically-that it is not necessary to differentiate through the resampling step which avoids gradient variance issues observed in similar approaches for Particle Filters (Maddison et al., 2017; Naesseth et al., 2018; Le et al., 2018).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#65292;&#36798;&#21040;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2304.14385</link><description>&lt;p&gt;
&#24102;&#26377;&#36125;&#21494;&#26031;&#35828;&#26381;&#30340;&#21160;&#24577;&#23450;&#20215;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing and Learning with Bayesian Persuasion. (arXiv:2304.14385v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#65292;&#36798;&#21040;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#23450;&#20215;&#21644;&#23398;&#20064;&#35774;&#32622;&#65292;&#22312;&#25353;&#39034;&#24207;&#35774;&#32622;&#20135;&#21697;&#20215;&#26684;&#30340;&#21516;&#26102;&#65292;&#21334;&#23478;&#36824;&#39044;&#20808;&#25215;&#35834;&#8220;&#24191;&#21578;&#26041;&#26696;&#8221;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27599;&#36718;&#24320;&#22987;&#26102;&#65292;&#21334;&#23478;&#21487;&#20197;&#20915;&#23450;&#25552;&#20379;&#20160;&#20040;&#26679;&#30340;&#20449;&#21495;&#26469;&#21578;&#30693;&#20080;&#23478;&#20135;&#21697;&#23454;&#38469;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#26694;&#26550;&#26469;&#27169;&#25311;&#36825;&#20123;&#20449;&#21495;&#23545;&#20080;&#23478;&#30340;&#35780;&#20272;&#21644;&#36141;&#20080;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22312;&#26368;&#22823;&#21270;&#21334;&#26041;&#39044;&#26399;&#25910;&#20837;&#30340;&#21516;&#26102;&#25214;&#21040;&#24191;&#21578;&#26041;&#26696;&#21644;&#23450;&#20215;&#26041;&#26696;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#36807;&#21435;&#30340;&#36141;&#20080;&#21453;&#24212;&#26469;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26368;&#20248;&#23450;&#20215;&#21644;&#24191;&#21578;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31639;&#27861;&#30340;&#21518;&#24724;&#65292;&#19982;&#26368;&#20248;&#30340;&#21315;&#37324;&#20043;&#22564;&#20215;&#26684;&#21644;&#24191;&#21578;&#35745;&#21010;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21363;&#20351;&#21334;&#23478;&#27809;&#26377;&#20080;&#23478;&#38656;&#27714;&#20989;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20339;&#22266;&#23450;&#20215;&#26684;&#21644;&#24191;&#21578;&#26041;&#26696;&#30456;&#20851;&#30340;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a novel dynamic pricing and learning setting where in addition to setting prices of products in sequential rounds, the seller also ex-ante commits to 'advertising schemes'. That is, in the beginning of each round the seller can decide what kind of signal they will provide to the buyer about the product's quality upon realization. Using the popular Bayesian persuasion framework to model the effect of these signals on the buyers' valuation and purchase responses, we formulate the problem of finding an optimal design of the advertising scheme along with a pricing scheme that maximizes the seller's expected revenue. Without any apriori knowledge of the buyers' demand function, our goal is to design an online algorithm that can use past purchase responses to adaptively learn the optimal pricing and advertising strategy. We study the regret of the algorithm when compared to the optimal clairvoyant price and advertising scheme.  Our main result is a computationally efficient onlin
&lt;/p&gt;</description></item><item><title>"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2304.14382</link><description>&lt;p&gt;
&#27169;&#25311;&#24418;&#24335;&#36716;&#25442;&#22120;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14382
&lt;/p&gt;
&lt;p&gt;
"&#27169;&#25311;&#32593;&#32476;"&#27169;&#22411;&#22312;3D&#29289;&#20307;&#22330;&#26223;&#20998;&#21106;&#20013;&#37319;&#29992;&#31867;&#27604;&#25512;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#24182;&#39044;&#27979;&#31867;&#20284;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#33021;&#22815;&#22312;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#20013;&#24471;&#20986;&#30456;&#20284;&#30340;&#35299;&#26512;&#65292;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#19968;&#32452;&#26377;&#26631;&#35760;&#30340;&#32467;&#26500;&#21270;3D&#22330;&#26223;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#39046;&#22495;&#30693;&#35782;&#65288;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#37096;&#20998;&#65289;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#23545;3D&#29289;&#20307;&#22330;&#26223;&#36827;&#34892;&#20998;&#21106;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#30456;&#20851;&#22330;&#26223;&#21450;&#20854;&#30456;&#24212;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#35843;&#21046;&#26426;&#21046;&#20026;&#36755;&#20837;&#22330;&#26223;&#39044;&#27979;&#31867;&#20284;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#22330;&#26223;&#26144;&#23556;&#21040;&#37096;&#20998;&#20998;&#21106;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26816;&#32034;&#30340;&#35760;&#24518;&#36827;&#34892;&#26465;&#20214;&#25511;&#21046;&#65292;&#39044;&#27979;&#28151;&#21512;&#21305;&#37197;&#26816;&#32034;&#35760;&#24518;&#30340;&#32467;&#26500;&#21512;&#25104;&#12290;&#22312;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#20013;&#65292;&#19968;&#21457;&#12289;&#23569;&#21457;&#25110;&#22810;&#21457;&#23398;&#20064;&#34987;&#19968;&#33268;&#22320;&#22788;&#29702;&#65292;&#36890;&#36807;&#23545;&#36866;&#24403;&#30340;&#35760;&#24518;&#38598;&#36827;&#34892;&#26465;&#20214;&#35859;&#35789;&#65292;&#26080;&#35770;&#26159;&#20174;&#21333;&#20010;&#12289;&#23569;&#25968;&#36824;&#26159;&#35768;&#22810;&#23384;&#20648;&#23454;&#20363;&#20013;&#32487;&#25215;&#30456;&#20284;&#30340;&#35299;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27169;&#25311;&#32593;&#32476;&#8221;&#22312;&#35768;&#22810;&#26679;&#26412;&#24773;&#20917;&#19979;&#19982;&#26368;&#26032;&#30340;3D&#20998;&#21106;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#38416;&#36848;&#22914;&#20309;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.14378</link><description>&lt;p&gt;
&#21151;&#33021;&#25193;&#25955;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Functional Diffusion Maps. (arXiv:2304.14378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#38416;&#36848;&#22914;&#20309;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#34987;&#35270;&#20026;&#26159;&#21151;&#33021;&#24615;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#29983;&#25104;&#23427;&#20204;&#30340;&#36807;&#31243;&#26159;&#36830;&#32493;&#30340;&#12290;&#36825;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24615;&#26159;&#65292;&#29702;&#35770;&#19978;&#23427;&#20204;&#23646;&#20110;&#26080;&#38480;&#32500;&#31354;&#38388;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#21482;&#33021;&#24471;&#21040;&#26377;&#38480;&#25968;&#37327;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#20204;&#20173;&#28982;&#26159;&#39640;&#32500;&#30340;&#65292;&#22240;&#27492;&#38477;&#32500;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#29616;&#26377;&#26041;&#27861;&#26159;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#31181;&#32463;&#20856;&#25216;&#26415;&#20551;&#35774;&#25968;&#25454;&#20301;&#20110;&#19968;&#20010;&#32447;&#24615;&#27969;&#24418;&#20013;&#65292;&#22240;&#27492;&#24403;&#36825;&#20010;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65306;&#25193;&#25955;&#26144;&#23556;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#36825;&#31181;&#22810;&#21464;&#37327;&#26041;&#27861;&#25193;&#23637;&#21040;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#34892;&#20026;&#19982;&#21151;&#33021;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#19981;&#21516;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#20363;&#23376;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many real-world datasets can be considered as functional, in the sense that the processes which generate them are continuous. A fundamental property of this type of data is that in theory they belong to an infinite-dimensional space. Although in practice we usually receive finite observations, they are still high-dimensional and hence dimensionality reduction methods are crucial. In this vein, the main state-of-the-art method for functional data analysis is Functional PCA. Nevertheless, this classic technique assumes that the data lie in a linear manifold, and hence it could have problems when this hypothesis is not fulfilled. In this research, attention has been placed on a non-linear manifold learning method: Diffusion Maps. The article explains how to extend this multivariate method to functional data and compares its behavior against Functional PCA over different simulated and real examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.14374</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#26469;&#23398;&#20064;&#21487;&#20197;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25152;&#24471;&#27169;&#22411;&#30001;&#39640;&#36798;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#27169;&#25311;&#20195;&#34920;&#23432;&#24658;&#12289;&#32791;&#25955;&#21644;&#22806;&#21147;&#30340;&#39033;&#20197;&#21450;&#21487;&#20197;&#23398;&#20064;&#25110;&#20026;&#20808;&#21069;&#30693;&#35782;&#30340;&#31163;&#25955;&#21367;&#31215;&#31639;&#23376;&#26500;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;PHNN&#30456;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PHNN&#27169;&#22411;&#30001;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#32452;&#25104;&#65292;&#21487;&#20197;&#20998;&#21035;&#30740;&#31350;&#36825;&#20123;&#37096;&#20998;&#20197;&#33719;&#24471;&#23545;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#19988;&#21363;&#20351;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#65292;&#25152;&#23398;&#24471;&#30340;&#27169;&#22411;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22330;&#20316;&#20026;&#20108;&#32500;&#35821;&#20041;&#20998;&#21106;&#35299;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#35843;&#33410;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#36827;&#34892;&#35843;&#33410;&#30340;&#26041;&#27861;&#26368;&#20339;&#65292;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2304.14371</link><description>&lt;p&gt;
&#20108;&#32500;&#35821;&#20041;&#20998;&#21106;&#31070;&#32463;&#22330;&#35843;&#33410;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Field Conditioning Strategies for 2D Semantic Segmentation. (arXiv:2304.14371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22330;&#20316;&#20026;&#20108;&#32500;&#35821;&#20041;&#20998;&#21106;&#35299;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#35843;&#33410;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#36827;&#34892;&#35843;&#33410;&#30340;&#26041;&#27861;&#26368;&#20339;&#65292;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#26159;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#26399;&#26395;&#20449;&#21495;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22914;&#26524;&#31070;&#32463;&#22330;&#24212;&#35813;&#20849;&#21516;&#27169;&#25311;&#22810;&#20010;&#20449;&#21495;&#32780;&#19981;&#26159;&#21482;&#35760;&#24518;&#19968;&#20010;&#65292;&#21017;&#38656;&#35201;&#22312;&#25551;&#36848;&#25152;&#22788;&#29702;&#20449;&#21495;&#30340;&#28508;&#22312;&#20195;&#30721;&#19978;&#36827;&#34892;&#35843;&#33410;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#31070;&#32463;&#22330;&#20316;&#20026;&#20108;&#32500;&#35821;&#20041;&#20998;&#21106;&#35299;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35843;&#33410;&#26041;&#27861;&#65306;&#28508;&#22312;&#20195;&#30721;&#30340;&#31616;&#21333;&#36830;&#25509;&#65292;&#29305;&#24449;&#36880;&#20803;&#32032;&#32447;&#24615;&#35843;&#33410;&#20197;&#21450;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#25551;&#36848;&#23436;&#25972;&#22270;&#20687;&#25110;&#22270;&#20687;&#23616;&#37096;&#21306;&#22495;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35843;&#33410;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#36827;&#34892;&#35843;&#33410;&#30340;&#32467;&#26524;&#26368;&#20339;&#65292;&#19982;&#22522;&#20110;CNN&#30340;&#35821;&#20041;&#20998;&#21106;&#35299;&#30721;&#22120;&#30456;&#24403;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural fields are neural networks which map coordinates to a desired signal. When a neural field should jointly model multiple signals, and not memorize only one, it needs to be conditioned on a latent code which describes the signal at hand. Despite being an important aspect, there has been little research on conditioning strategies for neural fields. In this work, we explore the use of neural fields as decoders for 2D semantic segmentation. For this task, we compare three conditioning methods, simple concatenation of the latent code, Feature Wise Linear Modulation (FiLM), and Cross-Attention, in conjunction with latent codes which either describe the full image or only a local region of the image. Our results show a considerable difference in performance between the examined conditioning strategies. Furthermore, we show that conditioning via Cross-Attention achieves the best results and is competitive with a CNN-based decoder for semantic segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"&#31070;&#32463;&#26412;&#26500;&#27861;"&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#27492;&#26469;&#23398;&#20064;&#26412;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14369</link><description>&lt;p&gt;
&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#31070;&#32463;&#26412;&#26500;&#27861;&#26469;&#23454;&#29616;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics. (arXiv:2304.14369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"&#31070;&#32463;&#26412;&#26500;&#27861;"&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#27492;&#26469;&#23398;&#20064;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;(NN)&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#26041;&#27861;,&#29992;&#20110;&#20174;&#36816;&#21160;&#35266;&#27979;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;PDE&#21160;&#21147;&#23398;&#12290;&#35768;&#22810;NN&#26041;&#27861;&#23398;&#20064;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;,&#38544;&#21547;&#22320;&#24314;&#27169;&#20102;&#25152;&#24314;&#31435;&#30340;PDE&#21644;&#26412;&#26500;&#27169;&#22411;(&#25110;&#26448;&#26009;&#27169;&#22411;)&#12290;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#30340;PDE&#30693;&#35782;,&#19981;&#33021;&#20445;&#35777;&#29289;&#29702;&#27491;&#30830;&#24615;,&#20855;&#26377;&#26377;&#38480;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;,&#25152;&#24314;&#31435;&#30340;PDEs&#36890;&#24120;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;,&#24212;&#35813;&#26126;&#30830;&#24378;&#21046;&#25191;&#34892;,&#32780;&#19981;&#26159;&#23398;&#20064;&#12290;&#30456;&#21453;,&#26412;&#26500;&#27169;&#22411;&#30001;&#20110;&#20854;&#25968;&#25454;&#25311;&#21512;&#24615;&#36136;,&#29305;&#21035;&#36866;&#21512;&#20110;&#23398;&#20064;&#12290;&#20026;&#27492;,&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#31070;&#32463;&#26412;&#26500;&#27861;&#8221;(NCLaw)&#30340;&#26032;&#26694;&#26550;,&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#20005;&#26684;&#20445;&#35777;&#26631;&#20934;&#26412;&#26500;&#20808;&#39564;&#30340;&#32593;&#32476;&#26550;&#26500;,&#21253;&#25324;&#26059;&#36716;&#31561;&#21464;&#24615;&#21644;&#26410;&#21464;&#24418;&#29366;&#24577;&#24179;&#34913;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#32593;&#32476;&#23884;&#20837;&#21487;&#24494;&#20998;&#30340;&#27169;&#25311;&#20013;,&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#27169;&#25311;&#21644;m&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed "Neural Constitutive Laws" (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item><item><title>ChatGPT&#24102;&#26469;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#20294;&#26159;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.14347</link><description>&lt;p&gt;
ChatGPT&#30340;&#40657;&#26263;&#38754;&#65306;&#26469;&#33258;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination. (arXiv:2304.14347v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14347
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#26377;&#24456;&#22810;&#20248;&#21183;&#65292;&#20294;&#26159;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21160;&#25671;&#25105;&#20204;&#25972;&#20010;&#31038;&#20250;&#65292;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#24605;&#32500;&#12289;&#21019;&#36896;&#21644;&#29983;&#27963;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38543;&#26426;&#40550;&#40521;&#21644;&#24187;&#35273;&#31561;&#26032;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#20986;&#29616;&#65292;&#26032;&#20852;LLMs&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#27431;&#30431;&#26159;&#31532;&#19968;&#20010;&#23558;&#37325;&#28857;&#25918;&#22312;AI&#27169;&#22411;&#30417;&#31649;&#19978;&#30340;&#21496;&#27861;&#31649;&#36758;&#21306;&#12290;&#28982;&#32780;&#65292;&#26032;LLMs&#24102;&#26469;&#30340;&#39118;&#38505;&#21487;&#33021;&#20250;&#34987;&#26032;&#20852;&#30340;&#27431;&#30431;&#30417;&#31649;&#33539;&#24335;&#25152;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#26412;&#20989;&#21578;&#35686;&#31034;&#27431;&#27954;AI&#30417;&#31649;&#33539;&#24335;&#24517;&#39035;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our whole society, rapidly altering the way we think, create and live. For instance, the GPT integration in Bing has altered our approach to online searching. While nascent LLMs have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. The EU is the first and foremost jurisdiction that has focused on the regulation of AI models. However, the risks posed by the new LLMs are likely to be underestimated by the emerging EU regulatory paradigm. Therefore, this correspondence warns that the European AI regulatory paradigm must evolve further to mitigate such risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14339</link><description>&lt;p&gt;
MarsEclipse&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;&#26631;&#31614;&#26694;&#26550;&#26816;&#27979;&#21450;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#20351;&#29992;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#20219;&#21153;3&#30340;&#23376;&#20219;&#21153;2&#19978;&#36827;&#34892;&#26694;&#26550;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35774;&#32622;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#21644;&#23448;&#26041;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#23545;&#20110;&#25105;&#20204;&#26377;&#35757;&#32451;&#25968;&#25454;&#24182;&#33021;&#36827;&#34892;&#24494;&#35843;&#30340;&#20845;&#31181;&#35821;&#35328;&#20013;&#30340;&#20116;&#31181;&#35821;&#35328;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20197;&#21450;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20195;&#30721;&#21487;&#22312;https://github.com/QishengL/SemEval2023&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting, achieving very competitive results: our system was ranked first on the official test set and on the official shared task leaderboard for five of the six languages for which we had training data and for which we could perform fine-tuning. Here, we describe our experimental setup, as well as various ablation studies. The code of our system is available at https://github.com/QishengL/SemEval2023
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.14333</link><description>&lt;p&gt;
&#25104;&#35821;&#12289;&#25506;&#27979;&#21644;&#21361;&#38505;&#30340;&#20107;&#29289;&#65306;&#22522;&#20110;&#32467;&#26500;&#25506;&#27979;&#30340;&#35789;&#21521;&#37327;&#25104;&#35821;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#20102;&#25104;&#35821;&#22312;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#20013;&#30340;&#32534;&#30721;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#20173;&#26410;&#30830;&#23450;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#32467;&#26500;&#25506;&#27979;&#26041;&#27861;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25104;&#35821;&#20449;&#24687;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#35789;&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#33521;&#35821;&#21160;&#35789;&#22797;&#21512;&#35789;&#35821;&#65288;MWE&#65289;&#25968;&#25454;&#38598;&#26469;&#36866;&#24212;&#25506;&#27979;&#26694;&#26550;&#65292;&#24182;&#23545;&#38745;&#24577;&#65288;GloVe&#65289;&#21644;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#23884;&#20837;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20197;&#19981;&#21516;&#31243;&#24230;&#32534;&#30721;&#20102;&#19968;&#20123;&#25104;&#35821;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#25104;&#35821;&#24615;&#26159;&#21542;&#22312;&#21521;&#37327;&#33539;&#25968;&#20013;&#32534;&#30721;&#32473;&#20986;&#20102;&#20914;&#31361;&#30340;&#35777;&#25454;&#65292;&#36825;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25913;&#36827;&#20854;&#36866;&#29992;&#24615;&#36827;&#34892;&#25506;&#27979;&#20998;&#26512;&#30340;&#37325;&#35201;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Gibbs&#31639;&#27861;&#26469;&#20998;&#26512;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#20803;&#27867;&#21270;&#35823;&#24046;&#21051;&#30011;&#21644;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.14332</link><description>&lt;p&gt;
&#20851;&#20110;Gibbs&#31639;&#27861;&#20803;&#23398;&#20064;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Error of Meta Learning for the Gibbs Algorithm. (arXiv:2304.14332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Gibbs&#31639;&#27861;&#26469;&#20998;&#26512;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#20803;&#27867;&#21270;&#35823;&#24046;&#21051;&#30011;&#21644;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Gibbs&#31639;&#27861;&#26469;&#20998;&#26512;&#32852;&#21512;&#35757;&#32451;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20803;Gibbs&#31639;&#27861;&#30340;&#26399;&#26395;&#20803;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#21051;&#30011;&#65292;&#22522;&#20110;&#23545;&#31216;&#21270;KL&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#24230;&#37327;&#20102;&#25152;&#26377;&#20803;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#36755;&#20986;&#21442;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#21644;&#20803;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Steinke&#21644;Zakynthinou (2020)&#20197;&#21450;Hellstrom &#21644; Durisi (2022)&#20998;&#21035;&#24341;&#20837;&#30340;&#36229;&#26679;&#26412;&#21644;&#36229;&#20219;&#21153;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#26465;&#20214;&#23545;&#31216;KL&#20449;&#24687;&#65292;&#20026;&#36229;&#65306;&#20219;&#21153;Gibbs&#31639;&#27861;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#20803;&#27867;&#21270;&#35823;&#24046;&#21051;&#30011;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#36825;&#20123;Gibbs&#31639;&#27861;&#25552;&#20379;&#26032;&#30340;&#20998;&#24067;&#26080;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#65292;&#36866;&#29992;&#20110;&#20803;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the generalization ability of joint-training meta learning algorithms via the Gibbs algorithm. Our exact characterization of the expected meta generalization error for the meta Gibbs algorithm is based on symmetrized KL information, which measures the dependence between all meta-training datasets and the output parameters, including task-specific and meta parameters. Additionally, we derive an exact characterization of the meta generalization error for the super-task Gibbs algorithm, in terms of conditional symmetrized KL information within the super-sample and super-task framework introduced in Steinke and Zakynthinou (2020) and Hellstrom and Durisi (2022) respectively. Our results also enable us to provide novel distribution-free generalization error upper bounds for these Gibbs algorithms applicable to meta learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#25903;&#25345;&#22806;&#36827;&#34892;&#25512;&#24191;&#30340;&#38382;&#39064;&#30340;&#20256;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#8220;&#20256;&#23548;&#8221;&#37325;&#26032;&#21442;&#25968;&#21270;&#23558;&#25903;&#25345;&#22806;&#30340;&#22806;&#25512;&#38382;&#39064;&#36716;&#25442;&#20026;&#25903;&#25345;&#20869;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20445;&#30041;&#36807;&#24230;&#21442;&#25968;&#21270;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#36827;&#34892;&#22806;&#25512;&#12290;</title><link>http://arxiv.org/abs/2304.14329</link><description>&lt;p&gt;
&#23398;&#20064;&#22806;&#25512;&#65306;&#19968;&#31181;&#20256;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Extrapolate: A Transductive Approach. (arXiv:2304.14329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#25903;&#25345;&#22806;&#36827;&#34892;&#25512;&#24191;&#30340;&#38382;&#39064;&#30340;&#20256;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#8220;&#20256;&#23548;&#8221;&#37325;&#26032;&#21442;&#25968;&#21270;&#23558;&#25903;&#25345;&#22806;&#30340;&#22806;&#25512;&#38382;&#39064;&#36716;&#25442;&#20026;&#25903;&#25345;&#20869;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20445;&#30041;&#36807;&#24230;&#21442;&#25968;&#21270;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#36827;&#34892;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#25317;&#26377;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20174;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#20013;&#25552;&#21462;&#30340;&#26032;&#27979;&#35797;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#25903;&#25345;&#22806;&#30340;&#27979;&#35797;&#28857;&#19978;&#35780;&#20272;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20351;&#20854;&#20445;&#30041;&#36807;&#24230;&#21442;&#25968;&#21270;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#21487;&#33021;&#26102;&#20351;&#25512;&#24191;&#21040;&#25903;&#25345;&#22806;&#30340;&#27979;&#35797;&#28857;&#22806;&#25512;&#12290;&#36890;&#36807;&#27880;&#24847;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#8220;&#20256;&#23548;&#8221;&#37325;&#26032;&#21442;&#25968;&#21270;&#21487;&#20197;&#23558;&#25903;&#25345;&#22806;&#30340;&#22806;&#25512;&#38382;&#39064;&#36716;&#25442;&#20026;&#25903;&#25345;&#20869;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#32447;&#24615;&#23884;&#20837;&#30340;&#31616;&#21333;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#36825;&#31181;&#31867;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#20174;&#32780;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#35299;&#20915;&#20102;&#25903;&#25345;&#22806;&#25512;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#23454;&#20363;&#21270;&#19968;&#20010;&#36866;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#30340;&#31616;&#21333;&#23454;&#29992;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a "transductive" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.14326</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24102;&#38271;&#26399;&#32422;&#26463;&#30340;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints. (arXiv:2304.14326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29615;&#24418;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#23613;&#21487;&#33021;&#22810;&#30340;&#22870;&#21169;&#30340;&#21516;&#26102;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#35777;&#28385;&#36275;&#19968;&#20123;&#38271;&#26399;&#32422;&#26463;&#12290;&#22870;&#21169;&#21644;&#32422;&#26463;&#21487;&#20197;&#38543;&#26426;&#25110;&#25932;&#23545;&#22320;&#36873;&#25321;&#65292;&#24182;&#19988;&#36716;&#31227;&#20989;&#25968;&#23545;&#23398;&#20064;&#32773;&#26159;&#26410;&#30693;&#30340;&#12290;&#34429;&#28982;&#22312;&#32463;&#20856;&#30340;&#26080;&#32422;&#26463;MDPs&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21463;&#21040;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#20294;CMDP&#30340;&#35774;&#32622;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#19968;&#28857;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#33258;&#21160;&#25237;&#26631;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#39069;&#22806;&#30340;&#32422;&#26463;&#21644;&#35268;&#33539;&#65292;&#20195;&#29702;&#24517;&#39035;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36981;&#23432;&#36825;&#20123;&#35268;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#32422;&#26463;&#30340;CMDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#24211;&#29632;&#24102;&#20869;&#22825;&#20307;&#20998;&#31867;&#65292;&#27979;&#37327;&#20102;&#24211;&#29632;&#24102;&#30340;&#24179;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38750;&#20849;&#25391;&#24211;&#29632;&#24102;&#21644;&#21476;&#20856;&#24211;&#29632;&#24102;&#19982;&#22826;&#38451;&#31995;&#19981;&#21464;&#24179;&#38754;&#38750;&#24120;&#25509;&#36817;&#65292;&#20294;&#26377;&#21306;&#21035;&#65292;&#32622;&#20449;&#24230;&#22823;&#20110;99.7%&#12290;</title><link>http://arxiv.org/abs/2304.14312</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23545;&#24211;&#29632;&#24102;&#20869;&#22825;&#20307;&#20998;&#31867;&#30340;&#24179;&#38754;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Measurement of the Kuiper Belt's Mean Plane From Objects Classified By Machine Learning. (arXiv:2304.14312v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#24211;&#29632;&#24102;&#20869;&#22825;&#20307;&#20998;&#31867;&#65292;&#27979;&#37327;&#20102;&#24211;&#29632;&#24102;&#30340;&#24179;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38750;&#20849;&#25391;&#24211;&#29632;&#24102;&#21644;&#21476;&#20856;&#24211;&#29632;&#24102;&#19982;&#22826;&#38451;&#31995;&#19981;&#21464;&#24179;&#38754;&#38750;&#24120;&#25509;&#36817;&#65292;&#20294;&#26377;&#21306;&#21035;&#65292;&#32622;&#20449;&#24230;&#22823;&#20110;99.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#27979;&#25968;&#25454;&#23545;&#24211;&#29632;&#24102;&#30340;&#24179;&#38754;&#36827;&#34892;&#27979;&#37327;&#26159;&#20026;&#20102;&#27979;&#35797;&#22826;&#38451;&#31995;&#21160;&#21147;&#23398;&#27169;&#22411;&#28508;&#22312;&#24615;&#30340;&#19968;&#39033;&#28909;&#28857;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#27979;&#37327;&#32467;&#26524;&#23384;&#22312;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#39033;&#24211;&#29632;&#24102;&#24179;&#38754;&#30340;&#27979;&#37327;&#65292;&#20854;&#26679;&#26412;&#37327;&#26159;&#20043;&#21069;&#27979;&#37327;&#30340;&#20004;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#36712;&#36947;&#24050;&#30830;&#23450;&#30340;&#35266;&#27979;&#24211;&#29632;&#24102;&#20869;&#38750;&#20849;&#25391;&#22825;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;Monte Carlo&#36807;&#31243;&#20272;&#35745;&#27979;&#37327;&#35823;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#20849;&#25391;&#24211;&#29632;&#24102;&#65288;&#21322;&#38271;&#36724;&#33539;&#22260;35-150 au&#65289;&#21644;&#21476;&#20856;&#24211;&#29632;&#24102;&#65288;&#21322;&#38271;&#36724;&#33539;&#22260;42-48 au&#65289;&#30340;&#25972;&#20307;&#24179;&#38754;&#19982;&#22826;&#38451;&#31995;&#19981;&#21464;&#24179;&#38754;&#38750;&#24120;&#25509;&#36817;&#65288;&#35823;&#24046;&#32422;&#20026;0.7&#24230;&#65289;&#65292;&#20294;&#20004;&#32773;&#26377;&#21306;&#21035;&#65292;&#32622;&#20449;&#24230;&#22823;&#20110;99.7%&#12290;&#24403;&#25226;&#26679;&#26412;&#20998;&#25104;&#36739;&#23567;&#30340;&#21322;&#38271;&#36724;&#33539;&#22260;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#27979;&#24471;&#30340;&#24179;&#38754;&#22823;&#22810;&#19982;&#22826;&#38451;&#31995;&#19981;&#21464;&#24179;&#38754;&#30340;&#36317;&#31163;&#23567;&#20110;1.5&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean plane measurements of the Kuiper Belt from observational data are of interest for their potential to test dynamical models of the solar system. Recent measurements have yielded inconsistent results. Here we report a measurement of the Kuiper Belt's mean plane with a sample size more than twice as large as in previous measurements. The sample of interest is the non-resonant Kuiper belt objects, which we identify by using machine learning on the observed Kuiper Belt population whose orbits are well-determined. We estimate the measurement error with a Monte Carlo procedure. We find that the overall mean plane of the non-resonant Kuiper Belt (semimajor axis range 35-150 au) and also that of the classical Kuiper Belt (semimajor axis range 42-48 au) are both close to (within about 0.7 degrees) but distinguishable from the invariable plane of the solar system to greater than 99.7% confidence. When binning the sample into smaller semimajor axis bins, we find the measured mean plane mostly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28369;&#38634;&#26495;&#35013;&#32622;&#24212;&#21464;&#20256;&#24863;&#22120;&#23545;&#38634;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#27599;&#20010;10&#31186;&#30340;&#36712;&#36857;&#27573;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#29420;&#31435;&#20110;&#28369;&#38634;&#39118;&#26684;&#65292;&#20934;&#30830;&#22320;&#20998;&#37197;&#19977;&#20010;&#23450;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2304.14307</link><description>&lt;p&gt;
&#21033;&#29992;&#28369;&#38634;&#35013;&#32622;&#24212;&#21464;&#20256;&#24863;&#22120;&#23545;&#38634;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Classifying Snow Using Ski-Mounted Strain Sensors. (arXiv:2304.14307v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28369;&#38634;&#26495;&#35013;&#32622;&#24212;&#21464;&#20256;&#24863;&#22120;&#23545;&#38634;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#27599;&#20010;10&#31186;&#30340;&#36712;&#36857;&#27573;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#29420;&#31435;&#20110;&#28369;&#38634;&#39118;&#26684;&#65292;&#20934;&#30830;&#22320;&#20998;&#37197;&#19977;&#20010;&#23450;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23665;&#21306;&#26223;&#35266;&#20013;&#38634;&#30340;&#32467;&#26500;&#65292;&#25968;&#37327;&#21644;&#31867;&#22411;&#23545;&#20110;&#35780;&#20272;&#38634;&#23849;&#23433;&#20840;&#24615;&#12289;&#35299;&#37322;&#21355;&#26143;&#22270;&#20687;&#12289;&#26500;&#24314;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#22411;&#20197;&#21450;&#36873;&#25321;&#36866;&#21512;&#20320;&#21608;&#26411;&#26053;&#34892;&#30340;&#28369;&#38634;&#26495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#23433;&#35013;&#22312;&#38463;&#23572;&#21329;&#26031;&#28369;&#38634;&#26495;&#39030;&#37096;&#34920;&#38754;&#30340;&#24212;&#21464;&#20256;&#24863;&#22120;&#65292;&#22312;&#28369;&#38634;&#26102;&#20272;&#31639;&#38634;&#38754;&#39030;&#23618;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#20004;&#20010;&#24212;&#21464;&#35745;&#21644;&#19968;&#20010;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#21487;&#20197;&#22312;&#27599;&#20010;10&#31186;&#30340;&#36712;&#36857;&#27573;&#20013;&#27491;&#30830;&#22320;&#20998;&#37197;&#19977;&#20010;&#23450;&#24615;&#26631;&#31614;&#65288;&#31881;&#38634;&#65292;&#27877;&#27870;&#30340;&#65292;&#25110;&#20912;&#38634;/&#20462;&#21098;&#36807;&#30340;&#38634;&#65289;&#65292;&#32780;&#19988;&#29420;&#31435;&#20110;&#28369;&#38634;&#39118;&#26684;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;97%&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#32447;&#24615;&#27169;&#22411;&#26469;&#25551;&#36848;&#28369;&#38634;&#26495;&#21644;&#38634;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the structure, quantity, and type of snow in mountain landscapes is crucial for assessing avalanche safety, interpreting satellite imagery, building accurate hydrology models, and choosing the right pair of skis for your weekend trip. Currently, such characteristics of snowpack are measured using a combination of remote satellite imagery, weather stations, and laborious point measurements and descriptions provided by local forecasters, guides, and backcountry users. Here, we explore how characteristics of the top layer of snowpack could be estimated while skiing using strain sensors mounted to the top surface of an alpine ski. We show that with two strain gauges and an inertial measurement unit it is feasible to correctly assign one of three qualitative labels (powder, slushy, or icy/groomed snow) to each 10 second segment of a trajectory with 97% accuracy, independent of skiing style. Our algorithm uses a combination of a data-driven linear model of the ski-snow interact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33889;&#33796;&#31958; - &#33008;&#23707;&#32032;&#25968;&#25454;&#21644;&#39184;&#39278;&#21327;&#21464;&#37327;&#20013;&#23398;&#20064;&#23439;&#37327;&#33829;&#20859;&#25104;&#20998;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20010;&#20154;&#30340;&#33889;&#33796;&#31958;&#21560;&#25910;&#29575;&#65292;&#24182;&#22312;&#33889;&#33796;&#31958;&#21160;&#21147;&#23398;&#30340;&#24494;&#20998;&#26041;&#31243;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14300</link><description>&lt;p&gt;
&#20174;&#39184;&#39135;&#21327;&#21464;&#37327;&#20013;&#23398;&#20064;&#33889;&#33796;&#31958; - &#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#20013;&#30340;&#21560;&#25910;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning Absorption Rates in Glucose-Insulin Dynamics from Meal Covariates. (arXiv:2304.14300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33889;&#33796;&#31958; - &#33008;&#23707;&#32032;&#25968;&#25454;&#21644;&#39184;&#39278;&#21327;&#21464;&#37327;&#20013;&#23398;&#20064;&#23439;&#37327;&#33829;&#20859;&#25104;&#20998;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20010;&#20154;&#30340;&#33889;&#33796;&#31958;&#21560;&#25910;&#29575;&#65292;&#24182;&#22312;&#33889;&#33796;&#31958;&#21160;&#21147;&#23398;&#30340;&#24494;&#20998;&#26041;&#31243;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33889;&#33796;&#31958; - &#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#27169;&#22411;&#20381;&#36182;&#20110;&#36873;&#25321;&#29992;&#20110;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#25311;&#21512;&#35266;&#23519;&#32467;&#26524;&#30340;&#21551;&#21457;&#24335;&#21442;&#25968;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25551;&#36848;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#33889;&#33796;&#31958;&#21160;&#21147;&#23398;&#12290;&#22833;&#36133;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;&#39184;&#21518;&#33889;&#33796;&#31958;&#21560;&#25910;&#36895;&#29575;&#30340;&#25551;&#36848;&#20013;&#12290;&#39184;&#30340;&#23439;&#37327;&#33829;&#20859;&#25104;&#20998;&#23545;&#21560;&#25910;&#21078;&#38754;&#26377;&#24494;&#22937;&#30340;&#24433;&#21709;&#65292;&#36825;&#24456;&#38590;&#36890;&#36807;&#26426;&#26800;&#24314;&#27169;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33889;&#33796;&#31958; - &#33008;&#23707;&#32032;&#25968;&#25454;&#21644;&#39184;&#39278;&#21327;&#21464;&#37327;&#20013;&#23398;&#20064;&#23439;&#37327;&#33829;&#20859;&#25104;&#20998;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23439;&#37327;&#33829;&#20859;&#20449;&#24687;&#21644;&#39184;&#39135;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#20010;&#20154;&#30340;&#33889;&#33796;&#31958;&#21560;&#25910;&#29575;&#12290;&#25105;&#20204;&#22312;&#33889;&#33796;&#31958;&#21160;&#21147;&#23398;&#30340;&#24494;&#20998;&#26041;&#31243;&#20013;&#20351;&#29992;&#36825;&#20010;&#31070;&#32463;&#36895;&#29575;&#20989;&#25968;&#20316;&#20026;&#25511;&#21046;&#20989;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32039;&#23494;&#22320;&#36924;&#36817;&#30495;&#23454;&#30340;&#21560;&#25910;&#36895;&#29575;&#65292;&#27604;&#21551;&#21457;&#24335;&#21442;&#25968;&#21270;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#23613;&#31649;&#21482;&#35266;&#23519;&#21040;&#33889;&#33796;&#31958;&#12289;&#33008;&#23707;&#32032;&#20004;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional models of glucose-insulin dynamics rely on heuristic parameterizations chosen to fit observations within a laboratory setting. However, these models cannot describe glucose dynamics in daily life. One source of failure is in their descriptions of glucose absorption rates after meal events. A meal's macronutritional content has nuanced effects on the absorption profile, which is difficult to model mechanistically. In this paper, we propose to learn the effects of macronutrition content from glucose-insulin data and meal covariates. Given macronutrition information and meal times, we use a neural network to predict an individual's glucose absorption rate. We use this neural rate function as the control function in a differential equation of glucose dynamics, enabling end-to-end training. On simulated data, our approach is able to closely approximate true absorption rates, resulting in better forecast than heuristic parameterizations, despite only observing glucose, insulin, a
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14283</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals. (arXiv:2304.14283v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#21306;&#20998;&#34892;&#26143;&#20940;&#21644;&#35823;&#21028;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20687;TESS&#36825;&#26679;&#30340;&#31354;&#38388;&#20219;&#21153;&#25552;&#20379;&#20102;&#22823;&#37327;&#24517;&#39035;&#39640;&#25928;&#12289;&#31995;&#32479;&#22320;&#20998;&#26512;&#30340;&#20809;&#21464;&#26354;&#32447;&#25968;&#25454;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#24050;&#34987;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#20505;&#36873;&#22806;&#34892;&#26143;&#30340;&#20940;&#21464;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;CNN&#20855;&#26377;&#19968;&#20123;&#32570;&#38519;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#38656;&#35201;&#35768;&#22810;&#23618;&#26469;&#25429;&#33719;&#24207;&#21015;&#25968;&#25454;&#65288;&#20363;&#22914;&#20809;&#21464;&#26354;&#32447;&#65289;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#32593;&#32476;&#21464;&#24471;&#36807;&#20110;&#24222;&#22823;&#65292;&#26368;&#32456;&#21464;&#24471;&#19981;&#23454;&#29992;&#12290;&#33258;&#27880;&#24847;&#26426;&#21046;&#26159;&#19968;&#31181;DL&#25216;&#26415;&#65292;&#35797;&#22270;&#27169;&#20223;&#26377;&#36873;&#25321;&#22320;&#32858;&#28966;&#20110;&#19968;&#20123;&#30456;&#20851;&#20107;&#29289;&#32780;&#24573;&#30053;&#20854;&#20182;&#20107;&#29289;&#30340;&#34892;&#20026;&#12290;&#26368;&#36817;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;Transformer&#26550;&#26500;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#25104;&#21151;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#20940;&#21464;&#20449;&#21495;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#26088;&#22312;&#39640;&#25928;&#20934;&#30830;&#22320;&#25429;&#25417;&#20940;&#21464;&#20449;&#21495;&#24182;&#23558;&#20854;&#19982;&#35823;&#21028;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#26469;&#20445;&#30041;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;CNN&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current space-based missions, such as the Transiting Exoplanet Survey Satellite (TESS), provide a large database of light curves that must be analysed efficiently and systematically. In recent years, deep learning (DL) methods, particularly convolutional neural networks (CNN), have been used to classify transit signals of candidate exoplanets automatically. However, CNNs have some drawbacks; for example, they require many layers to capture dependencies on sequential data, such as light curves, making the network so large that it eventually becomes impractical. The self-attention mechanism is a DL technique that attempts to mimic the action of selectively focusing on some relevant things while ignoring others. Models, such as the Transformer architecture, were recently proposed for sequential data with successful results. Based on these successful models, we present a new architecture for the automatic classification of transit signals. Our proposed architecture is designed to capture t
&lt;/p&gt;</description></item><item><title>ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#36136;&#37327;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#26356;&#39640;&#65292;&#20889;&#20316;&#39118;&#26684;&#26356;&#27969;&#30021;&#65292;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2304.14276</link><description>&lt;p&gt;
AI&#65292;&#20026;&#25105;&#20889;&#19968;&#31687;&#25991;&#31456;&#65306;&#20154;&#31867;&#20889;&#20316;&#19982;ChatGPT&#29983;&#25104;&#25991;&#31456;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays. (arXiv:2304.14276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14276
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#36136;&#37327;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#26356;&#39640;&#65292;&#20889;&#20316;&#39118;&#26684;&#26356;&#27969;&#30021;&#65292;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26368;&#36817;&#65292;ChatGPT&#21450;&#31867;&#20284;&#30340;AI&#29983;&#25104;&#27169;&#22411;&#21560;&#24341;&#20102;&#25968;&#20159;&#29992;&#25143;&#65292;&#25104;&#20026;&#20844;&#20247;&#35805;&#39064;&#30340;&#19968;&#37096;&#20998;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#20250;&#25171;&#20081;&#31038;&#20250;&#65292;&#23548;&#33268;&#26410;&#26469;&#25945;&#32946;&#31995;&#32479;&#21644;&#20449;&#24687;&#29983;&#25104;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#31181;&#20449;&#24565;&#22522;&#20110;&#30340;&#26159;&#21475;&#22836;&#35777;&#25454;&#25110;&#27169;&#22411;&#25317;&#26377;&#32773;&#30340;&#22522;&#20934;&#8212;&#8212;&#36825;&#20004;&#32773;&#32570;&#20047;&#31185;&#23398;&#20005;&#35880;&#24615;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#27604;&#36739;&#20154;&#31867;&#20889;&#20316;&#21644;ChatGPT&#29983;&#25104;&#35770;&#35777;&#24615;&#23398;&#29983;&#25991;&#31456;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;&#65306;&#19968;&#22823;&#25209;&#25991;&#31456;&#35821;&#26009;&#24211;&#34987;&#35768;&#22810;&#20154;&#31867;&#19987;&#23478;&#65288;&#25945;&#24072;&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#20934;&#35780;&#20998;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#20013;&#21152;&#20837;&#20102;&#23545;&#29983;&#25104;&#25991;&#31456;&#30340;&#35821;&#35328;&#29305;&#24449;&#30340;&#32771;&#34385;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#29983;&#25104;&#30340;&#25991;&#31456;&#27604;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#31456;&#36136;&#37327;&#26356;&#39640;&#12290;AI&#30340;&#20889;&#20316;&#39118;&#26684;&#26356;&#21152;&#27969;&#30021;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#20889;&#20316;&#20986;&#29616;&#26356;&#23569;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Recently, ChatGPT and similar generative AI models have attracted hundreds of millions of users and become part of the public discourse. Many believe that such models will disrupt society and will result in a significant change in the education system and information generation in the future. So far, this belief is based on either colloquial evidence or benchmarks from the owners of the models -- both lack scientific rigour.  Objective: Through a large-scale study comparing human-written versus ChatGPT-generated argumentative student essays, we systematically assess the quality of the AI-generated content.  Methods: A large corpus of essays was rated using standard criteria by a large number of human experts (teachers). We augment the analysis with a consideration of the linguistic characteristics of the generated essays.  Results: Our results demonstrate that ChatGPT generates essays that are rated higher for quality than human-written essays. The writing style of the AI m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#36719;&#20214;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#37096;&#20214;&#20851;&#32852;&#30340;&#23453;&#36149;&#26469;&#28304;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#30340;&#26377;&#29992;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#25968;&#25454;&#38598; CAD-120&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010;&#35013;&#37197;&#65292;&#24182;&#25552;&#20379;&#20102;&#35821;&#20041;&#20851;&#31995;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.14275</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#24847;&#20041;&#65306;&#36890;&#36807; CAD &#25991;&#20214;&#20013;&#29992;&#25143;&#25552;&#20379;&#30340;&#21517;&#31216;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35013;&#37197; - &#38646;&#20214;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files. (arXiv:2304.14275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#36719;&#20214;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#37096;&#20214;&#20851;&#32852;&#30340;&#23453;&#36149;&#26469;&#28304;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#30340;&#26377;&#29992;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#25968;&#25454;&#38598; CAD-120&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010;&#35013;&#37197;&#65292;&#24182;&#25552;&#20379;&#20102;&#35821;&#20041;&#20851;&#31995;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#37197;&#20013;&#38646;&#20214;&#20043;&#38388;&#21644;&#38646;&#20214;&#19982;&#25972;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#30693;&#35782;&#23545;&#20110;&#20174;&#25628;&#32034;&#35774;&#35745;&#23384;&#20648;&#24211;&#21040;&#26500;&#24314;&#24037;&#31243;&#30693;&#35782;&#24211;&#31561;&#21508;&#31181;&#20219;&#21153;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35774;&#35745;&#24072;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745; (CAD) &#36719;&#20214;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#26159;&#36825;&#31181;&#30693;&#35782;&#23453;&#36149;&#30340;&#26469;&#28304;&#65292;&#24182;&#19988;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21253;&#21547;&#20102;&#29992;&#20110;&#22788;&#29702;&#27492;&#25968;&#25454;&#20197;&#21450;&#20854;&#20182; CAD &#21644;&#24037;&#31243;&#30456;&#20851;&#20219;&#21153;&#30340;&#26377;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#21462;&#24182;&#28165;&#29702;&#20102;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#38646;&#20214;&#12289;&#29305;&#24449;&#21644;&#25991;&#26723;&#21517;&#31216;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#23450;&#37327;&#35777;&#26126;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19977;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#20248;&#20110;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#32780;&#19988;&#20174;&#26410;&#35265;&#36807;&#36825;&#20123;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25991;&#26412;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#36804;&#20170;&#20026;&#27490;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#26356;&#22810;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; CAD-120 &#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 120 &#20010; CAD &#35013;&#37197;&#20214;&#65292;&#20855;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic knowledge of part-part and part-whole relationships in assemblies is useful for a variety of tasks from searching design repositories to the construction of engineering knowledge bases. In this work we propose that the natural language names designers use in Computer Aided Design (CAD) software are a valuable source of such knowledge, and that Large Language Models (LLMs) contain useful domain-specific information for working with this data as well as other CAD and engineering-related tasks.  In particular we extract and clean a large corpus of natural language part, feature and document names and use this to quantitatively demonstrate that a pre-trained language model can outperform numerous benchmarks on three self-supervised tasks, without ever having seen this data before. Moreover, we show that fine-tuning on the text data corpus further boosts the performance on all tasks, thus demonstrating the value of the text data which until now has been largely ignored. We also ide
&lt;/p&gt;</description></item><item><title>&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14274</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#23545;&#33410;&#28857;&#20998;&#31867;&#26377;&#24110;&#21161;&#65306;&#30740;&#31350;&#21516;&#28304;&#24615;&#21407;&#21017;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14274
&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#25351;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#65288;NC&#65289;&#20219;&#21153;&#19978;&#24615;&#33021;&#20248;&#36234;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#29702;&#35770;&#32467;&#26524;&#35748;&#20026;&#65292;&#21363;&#20351;&#21516;&#28304;&#24615;&#21407;&#21017;&#34987;&#25171;&#30772;&#65292;&#21482;&#35201;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#20998;&#20139;&#30456;&#20284;&#30340;&#37051;&#23621;&#27169;&#24335;&#65292;GNN&#30340;&#20248;&#21183;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#23545;&#21516;&#28304;&#24615;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35770;&#28857;&#20165;&#32771;&#34385;&#20102;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24573;&#30053;&#20102;&#36328;&#31867;&#21035;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#36825;&#26159;&#30740;&#31350;&#21516;&#28304;&#24615;&#25928;&#24212;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20363;&#23376;&#35777;&#26126;&#20102;&#19978;&#36848;&#19981;&#36275;&#65292;&#24182;&#35748;&#20026;&#21487;&#21306;&#20998;&#24615;&#30340;&#29702;&#24819;&#24773;&#20917;&#26159;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#23567;&#20110;&#36328;&#31867;&#21035;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#20010;&#24819;&#27861;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21516;&#28304;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Contextual Stochastic Block Model for Homophily (CSBM-H)&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#20013;&#30340;&#20256;&#24863;&#21644;&#25968;&#25454;&#22788;&#29702;&#29616;&#29366;&#65292;&#21450;&#20854;&#23545;&#33021;&#28304;&#21644;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36827;&#19968;&#27493;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#29616;&#26377;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14271</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#33021;&#28304;&#39640;&#25928;&#36817;&#20284;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services. (arXiv:2304.14271v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14271
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#20013;&#30340;&#20256;&#24863;&#21644;&#25968;&#25454;&#22788;&#29702;&#29616;&#29366;&#65292;&#21450;&#20854;&#23545;&#33021;&#28304;&#21644;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36827;&#19968;&#27493;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#29616;&#26377;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#65292;&#22914;&#25668;&#20687;&#26426;&#12289;&#28608;&#20809;&#38647;&#36798;&#12289;&#38647;&#36798;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#22788;&#29702;&#20256;&#24863;&#25968;&#25454;&#30340;&#36890;&#24120;&#20570;&#27861;&#26159;&#22312;&#36710;&#36742;&#20869;&#37096;&#25918;&#32622;&#39640;&#24615;&#33021;&#35745;&#31639;&#21333;&#20803;&#65292;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20316;&#20026;&#36710;&#36742;&#30340;&#22823;&#33041;&#25110;&#31649;&#29702;&#21592;&#12290;&#20174;&#24179;&#22343;&#34892;&#39542;&#26102;&#38388;&#29983;&#25104;&#30340;&#36710;&#36742;&#25968;&#25454;&#21487;&#20197;&#39640;&#36798;20TB&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36895;&#29575;&#21644;&#35268;&#26684;&#12290;&#37492;&#20110;&#33258;&#20027;&#39550;&#39542;&#26381;&#21153;&#30340;&#35268;&#27169;&#21644;&#24555;&#36895;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#22312;&#21521;&#36710;&#36742;&#30005;&#27668;&#21270;&#65288;&#20363;&#22914;&#65292;&#30005;&#27744;&#39537;&#21160;&#65289;&#30340;&#36235;&#21183;&#20013;&#65292;&#25552;&#39640;&#24635;&#20307;&#33021;&#28304;&#21644;&#29615;&#22659;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290; &#34429;&#28982;&#36825;&#20123;&#39046;&#22495;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#12289;&#26080;&#32447;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;AI / ML&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22914;&#20309;&#24212;&#29992;&#21644;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#21019;&#26032;&#20197;&#23454;&#29616;&#33021;&#28304;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#36830;&#25509;&#30340;&#36710;&#36742;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving services rely heavily on sensors such as cameras, LiDAR, radar, and communication modules. A common practice of processing the sensed data is using a high-performance computing unit placed inside the vehicle, which deploys AI models and algorithms to act as the brain or administrator of the vehicle. The vehicular data generated from average hours of driving can be up to 20 Terabytes depending on the data rate and specification of the sensors. Given the scale and fast growth of services for autonomous driving, it is essential to improve the overall energy and environmental efficiency, especially in the trend towards vehicular electrification (e.g., battery-powered). Although the areas have seen significant advancements in sensor technologies, wireless communications, computing and AI/ML algorithms, the challenge still exists in how to apply and integrate those technology innovations to achieve energy efficiency. This survey reviews and compares the connected vehicular
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14251</link><description>&lt;p&gt;
&#31616;&#21270;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#30340;&#25512;&#23548;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#20854;&#25512;&#23548;&#36807;&#31243;&#21487;&#33021;&#24456;&#32321;&#29712;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#23547;&#25214;&#20851;&#20110;&#24050;&#30693;&#20998;&#24067;&#26399;&#26395;&#30340;&#32447;&#24615;&#24615;&#65292;&#26469;&#30830;&#23450;&#21518;&#39564;&#20998;&#24067;&#24418;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#8220;&#35835;&#21462;&#8221;&#36825;&#20123;&#26399;&#26395;&#21069;&#30340;&#39033;&#65292;&#20889;&#20986;&#26356;&#26032;&#12290;&#36825;&#20010;&#26041;&#27861;&#20351;&#24471;&#25512;&#23548;&#26356;&#21152;&#31616;&#21333;&#65292;&#24555;&#36895;&#65292;&#31616;&#30701;&#21644;&#36890;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;&#38416;&#37322;&#20102;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#24046;&#24322;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22914;&#20309;&#23548;&#33268;&#23545;&#19968;&#20010;&#20462;&#27491;&#36807;&#30340;&#38382;&#39064;&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#36825;&#20123;&#38382;&#39064;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.14248</link><description>&lt;p&gt;
&#20851;&#20110;&#27931;&#20811;&#26031;&#27934;&#31348;&#30340;&#27969;&#24418;&#23398;&#20064;&#65306;&#20851;&#20110;&#27969;&#24418;&#23398;&#20064;&#21644;&#29289;&#29702;&#29616;&#35937;&#30340;&#35780;&#35770;&#65288;arXiv:2304.14248v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena. (arXiv:2304.14248v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;&#38416;&#37322;&#20102;&#20998;&#26512;&#25968;&#25454;&#26102;&#65292;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#24046;&#24322;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#31181;&#24046;&#24322;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22914;&#20309;&#23548;&#33268;&#23545;&#19968;&#20010;&#20462;&#27491;&#36807;&#30340;&#38382;&#39064;&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#36825;&#20123;&#38382;&#39064;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23581;&#35797;&#36890;&#36807;&#27979;&#37327;&#19981;&#38656;&#35201;&#23545;&#29289;&#29702;&#29616;&#35937;&#25110;&#27979;&#37327;&#35774;&#22791;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#30340;&#20302;&#32500;&#27969;&#24418;&#32467;&#26500;&#26469;&#25512;&#26029;&#28508;&#22312;&#29289;&#29702;&#29616;&#35937;&#30340;&#20302;&#32500;&#27969;&#24418;&#32467;&#26500;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#27979;&#37327;&#20960;&#20309;&#21644;&#24213;&#23618;&#29616;&#35937;&#20960;&#20309;&#20043;&#38388;&#24046;&#24322;&#30340;&#35686;&#31034;&#25925;&#20107;&#12290;&#22312;&#26222;&#36890;&#24773;&#20917;&#19979;&#65292;&#36825;&#31687;&#35770;&#25991;&#25152;&#23637;&#31034;&#30340;&#24230;&#37327;&#24418;&#21464;&#22312;&#25968;&#23398;&#19978;&#26159;&#30452;&#25509;&#32780;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#23427;&#21482;&#26159;&#25968;&#20010;&#31867;&#20284;&#25928;&#24212;&#20013;&#30340;&#19968;&#20010;&#12290;&#34429;&#28982;&#36825;&#24182;&#19981;&#24635;&#26159;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#19988;&#26080;&#23475;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#30340;&#20363;&#23376;&#65292;&#20854;&#20013;&#36825;&#31181;&#24433;&#21709;&#23548;&#33268;&#23545;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#38382;&#39064;&#32473;&#20986;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#23613;&#31649;&#25105;&#20204;&#20851;&#27880;&#27969;&#24418;&#23398;&#20064;&#65292;&#20294;&#36825;&#20123;&#38382;&#39064;&#24191;&#27867;&#36866;&#29992;&#20110;&#38477;&#32500;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many techniques in machine learning attempt explicitly or implicitly to infer a low-dimensional manifold structure of an underlying physical phenomenon from measurements without an explicit model of the phenomenon or the measurement apparatus. This paper presents a cautionary tale regarding the discrepancy between the geometry of measurements and the geometry of the underlying phenomenon in a benign setting. The deformation in the metric illustrated in this paper is mathematically straightforward and unavoidable in the general case, and it is only one of several similar effects. While this is not always problematic, we provide an example of an arguably standard and harmless data processing procedure where this effect leads to an incorrect answer to a seemingly simple question. Although we focus on manifold learning, these issues apply broadly to dimensionality reduction and unsupervised learning.
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#36890;&#36947;&#33258;&#24459;&#8221;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#33258;&#25105;&#33976;&#39311;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14224</link><description>&lt;p&gt;
&#22810;&#36890;&#36947;&#33258;&#24459;
&lt;/p&gt;
&lt;p&gt;
Self-discipline on multiple channels. (arXiv:2304.14224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#36890;&#36947;&#33258;&#24459;&#8221;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#33258;&#25105;&#33976;&#39311;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#33976;&#39311;&#20381;&#38752;&#33258;&#36523;&#20449;&#24687;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#24191;&#38420;&#30340;&#21457;&#23637;&#21069;&#26223;&#12290;&#29616;&#26377;&#30340;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#65292;&#35201;&#20040;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#65292;&#35201;&#20040;&#38656;&#35201;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#21152;&#20102;&#20351;&#29992;&#38590;&#24230;&#12289;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#22810;&#36890;&#36947;&#33258;&#24459;&#8221;&#65288;SMC&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#19982;&#33258;&#25105;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#30340;&#27010;&#24565;&#12290;SMC&#20174;&#27010;&#24565;&#19978;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#27599;&#20010;&#36890;&#36947;&#25968;&#25454;&#21516;&#26102;&#36890;&#36807;&#27169;&#22411;&#65292;&#33719;&#24471;&#20854;&#30456;&#24212;&#30340;&#36719;&#26631;&#31614;&#65307;2&#65289;&#23384;&#20648;&#22312;&#19978;&#19968;&#27493;&#30340;&#36719;&#26631;&#31614;&#19982;&#20174;&#24403;&#21069;&#36890;&#36947;&#25968;&#25454;&#36890;&#36807;&#27169;&#22411;&#33719;&#24471;&#30340;&#36719;&#26631;&#31614;&#19968;&#36215;&#35835;&#21462;&#65292;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#12290;SMC&#20351;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#33258;&#25105;&#33976;&#39311;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-distillation relies on its own information to improve the generalization ability of the model and has a bright future. Existing self-distillation methods either require additional models, model modification, or batch size expansion for training, which increases the difficulty of use, memory consumption, and computational cost. This paper developed Self-discipline on multiple channels(SMC), which combines consistency regularization with self-distillation using the concept of multiple channels. Conceptually, SMC consists of two steps: 1) each channel data is simultaneously passed through the model to obtain its corresponding soft label, and 2) the soft label saved in the previous step is read together with the soft label obtained from the current channel data through the model to calculate the loss function. SMC uses consistent regularization and self-distillation to improve the generalization ability of the model and the robustness of the model to noisy labels. We named the SMC con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#19981;&#23545;&#25968;&#25454;&#36827;&#34892;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#21644;&#37096;&#20998;&#24050;&#30693;&#29289;&#29702;&#23398;&#65292;&#33021;&#22815;&#20174;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#37319;&#26679;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.14214</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#26576;&#20123;&#21464;&#37327;&#65292;&#26576;&#20123;&#21442;&#25968;&#65292;&#26576;&#20123;&#26102;&#38388;&#65292;&#26576;&#20123;&#24050;&#30693;&#29289;&#29702;&#23398;&#65306;&#24102;&#26377;&#37096;&#20998;&#20449;&#24687;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information. (arXiv:2304.14214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#19981;&#23545;&#25968;&#25454;&#36827;&#34892;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#31215;&#20998;&#21644;&#37096;&#20998;&#24050;&#30693;&#29289;&#29702;&#23398;&#65292;&#33021;&#22815;&#20174;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#37319;&#26679;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#25968;&#25454;&#36890;&#24120;&#30001;&#29420;&#31435;&#27979;&#37327;&#30340;&#21464;&#37327;&#32452;&#25104;&#65292;&#22312;&#19981;&#21516;&#30340;&#37319;&#26679;&#29575;(&#36830;&#32493;&#27979;&#37327;&#20043;&#38388;&#30340;&#38750;&#22343;&#21248;${\Delta}$t)&#19979;&#65292;&#20165;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#25165;&#23545;&#25152;&#26377;&#21464;&#37327;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25554;&#20540;&#12289;&#25554;&#20540;&#25110;&#23376;&#37319;&#26679;&#26469;&#37325;&#26032;&#32452;&#32455;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;$ \textit {prior}$ &#23398;&#20064;&#12290;&#37096;&#20998;&#29289;&#29702;&#30693;&#35782;&#20063;&#21487;&#33021;&#22312;$\textit {a priori}$&#65288;&#31934;&#30830;&#25110;&#36817;&#20284;&#65289;&#20013;&#21487;&#29992;&#65292;&#24182;&#19988;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#21487;&#20197;&#34917;&#20805;&#27492;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;&#21644;$\textit {a priori}$&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35782;&#21035;&#22522;&#26412;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#21491;&#25163;&#36793;&#12290;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36845;&#20195;&#20801;&#35768;&#20174;&#22312;&#20219;&#24847;&#26102;&#38388;&#28857;&#37319;&#26679;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;$\textit {without}$&#25968;&#25454;&#20462;&#25913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#32593;&#32476;&#19982;&#21487;&#29992;&#30340;&#37096;&#20998;&#29289;&#29702;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Experimental data is often comprised of variables measured independently, at different sampling rates (non-uniform ${\Delta}$t between successive measurements); and at a specific time point only a subset of all variables may be sampled. Approaches to identifying dynamical systems from such data typically use interpolation, imputation or subsampling to reorganize or modify the training data $\textit{prior}$ to learning. Partial physical knowledge may also be available $\textit{a priori}$ (accurately or approximately), and data-driven techniques can complement this knowledge. Here we exploit neural network architectures based on numerical integration methods and $\textit{a priori}$ physical knowledge to identify the right-hand side of the underlying governing differential equations. Iterates of such neural-network models allow for learning from data sampled at arbitrary time points $\textit{without}$ data modification. Importantly, we integrate the network with available partial physical
&lt;/p&gt;</description></item><item><title>LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.14211</link><description>&lt;p&gt;
LLT&#65306;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14211
&lt;/p&gt;
&lt;p&gt;
LLT&#26159;&#19968;&#20010;R&#21253;&#65292;&#29992;&#20110;&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#21464;&#25442;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#23450;&#24459;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;(LLT )&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#12290;LLT R&#21253;&#20197;&#28789;&#27963;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35813;&#31639;&#27861;&#12290;&#35813;&#21253;&#23558;&#23454;&#20363;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#65292;&#24182;&#21033;&#29992;&#26102;&#24310;&#23884;&#20837;&#21644;&#35889;&#20998;&#35299;&#25216;&#26415;&#65292;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#27599;&#20010;&#36755;&#20837;&#24207;&#21015;(&#21021;&#22987;&#29305;&#24449;)&#30340;&#25511;&#21046;&#27169;&#24335;(&#31216;&#20026;&#32447;&#24615;&#23450;&#24459;)&#12290;&#26368;&#21518;&#65292;&#23427;&#24212;&#29992;&#35757;&#32451;&#38598;&#30340;&#32447;&#24615;&#23450;&#24459;&#26469;&#36716;&#25442;&#27979;&#35797;&#38598;&#30340;&#21021;&#22987;&#29305;&#24449;&#12290;trainTest&#12289;trainLaw&#21644;testTrans&#19977;&#20010;&#21333;&#29420;&#30340;&#20989;&#25968;&#26469;&#25191;&#34892;&#36825;&#20123;&#27493;&#39588;&#65292;&#23427;&#20204;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#32467;&#26500;;&#28982;&#32780;&#65292;&#20026;&#20102;&#24555;&#36895;&#35745;&#31639;&#65292;&#23427;&#20204;&#21482;&#20351;&#29992;&#20869;&#32622;&#20989;&#25968;&#12290;LLT R&#21253;&#21644;&#36866;&#24403;&#25968;&#25454;&#32467;&#26500;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#22312;GitHub&#19978;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#27169;&#22411;&#23545;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#19988;&#38656;&#35201;&#36739;&#23569;&#23646;&#24615;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.14209</link><description>&lt;p&gt;
&#19968;&#31181;&#36879;&#26126;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transparent approach to data representation. (arXiv:2304.14209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14209
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#27169;&#22411;&#23545;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#19988;&#38656;&#35201;&#36739;&#23569;&#23646;&#24615;&#21363;&#21487;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20108;&#20803;&#23646;&#24615;&#34920;&#31034;&#65288;BAR&#65289;&#27169;&#22411;&#26469;&#25551;&#36848;Netflix&#35266;&#20247;&#23545;&#30005;&#24433;&#30340;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#30340;&#20108;&#36827;&#21046;&#20301;&#32780;&#19981;&#26159;&#36830;&#32493;&#30340;&#21442;&#25968;&#23545;&#35266;&#20247;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20351;&#24471;&#34920;&#31034;&#32039;&#20945;&#32780;&#36879;&#26126;&#12290;&#36825;&#20123;&#23646;&#24615;&#26131;&#20110;&#35299;&#37322;&#65292;&#25105;&#20204;&#38656;&#35201;&#27604;&#31867;&#20284;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#23646;&#24615;&#25165;&#33021;&#36798;&#21040;&#30456;&#21516;&#27700;&#24179;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30005;&#24433;&#35780;&#20998;&#30340;&#38750;&#22343;&#21248;&#20998;&#24067;&#65292;&#22312;&#19981;&#24433;&#21709;&#20854;&#20313;&#30005;&#24433;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#23569;&#37327;&#30005;&#24433;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a binary attribute representation (BAR) model to describe a data set of Netflix viewers' ratings of movies. We classify the viewers with discrete bits rather than continuous parameters, which makes the representation compact and transparent. The attributes are easy to interpret, and we need far fewer attributes than similar methods do to achieve the same level of error. We also take advantage of the nonuniform distribution of ratings among the movies in the data set to train on a small selection of movies without compromising performance on the rest of the movies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;&#35745;&#31639;&#20986;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#26159;&#30446;&#21069;&#31532;&#19968;&#20010;&#23454;&#29616;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#37327;&#23376;&#32447;&#24615;&#32534;&#31243;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14197</link><description>&lt;p&gt;
&#38646;&#21644;&#28216;&#25103;&#30340;&#23545;&#25968;&#36951;&#25022;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games. (arXiv:2304.14197v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;&#35745;&#31639;&#20986;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#26159;&#30446;&#21069;&#31532;&#19968;&#20010;&#23454;&#29616;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#37327;&#23376;&#32447;&#24615;&#32534;&#31243;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#22312;&#28216;&#25103;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102; $\tilde O(1)$ &#30340;&#36951;&#25022;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#37327;&#23376;&#31639;&#27861;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;$\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$&#20869;&#35745;&#31639;$m\times n$&#30697;&#38453;&#38646;&#21644;&#28216;&#25103;&#30340;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#22312;$m, n$&#26041;&#38754;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#26631;&#20934;&#30340;&#37327;&#23376;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#31616;&#26126;&#30340;&#25551;&#36848;&#24615;&#30340;&#32463;&#20856;&#36755;&#20986;&#65292;&#26041;&#20415;&#36827;&#34892;&#31471;&#21040;&#31471;&#24212;&#29992;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;&#37327;&#23376;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#30340;&#22312;&#32447;&#37327;&#23376;&#31639;&#27861;&#22522;&#20110;&#20048;&#35266;&#30340;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#8220;&#37327;&#23376;&#21270;&#8221;&#20102;&#32463;&#20856;&#31639;&#27861;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#24555;&#36895;&#37327;&#23376;&#22810;&#37325;&#37319;&#26679;&#36807;&#31243;&#65292;&#29992;&#20110;Gibbs&#37319;&#26679;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first online quantum algorithm for zero-sum games with $\tilde O(1)$ regret under the game setting. Moreover, our quantum algorithm computes an $\varepsilon$-approximate Nash equilibrium of an $m \times n$ matrix zero-sum game in quantum time $\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$, yielding a quadratic improvement over classical algorithms in terms of $m, n$. Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. As an application, we obtain a fast quantum linear programming solver. Technically, our online quantum algorithm "quantizes" classical algorithms based on the optimistic multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14185</link><description>&lt;p&gt;
ClusterNet&#65306;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ClusterNet&#65292;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#28857;&#22270;&#20013;&#30340;&#32858;&#31867;&#20998;&#31163;&#26159;&#19968;&#20010;&#36890;&#24120;&#30001;&#24191;&#27867;&#20351;&#29992;&#30340;&#32858;&#31867;&#25216;&#26415;&#65288;&#20363;&#22914;k-means&#25110;DBSCAN&#65289;&#26469;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#38750;&#24863;&#30693;&#24230;&#37327;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#32463;&#24120;&#19981;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#12290;&#20026;&#20102;&#24357;&#21512;&#20154;&#31867;&#32858;&#31867;&#24863;&#30693;&#21644;&#26426;&#22120;&#35745;&#31639;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#23398;&#20064;&#24863;&#30693;&#32858;&#31867;&#20998;&#31163;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20247;&#21253;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#65292;&#20854;&#20013;&#21253;&#25324;384&#20010;&#20154;&#32676;&#24037;&#20316;&#32773;&#23545;&#21452;&#21464;&#37327;&#25968;&#25454;&#30340;7,320&#20010;&#28857;&#32858;&#31867;&#20174;&#23646;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;ClusterNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#34987;&#35757;&#32451;&#25104;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#30340;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#22312;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;ClusterNet&#65292;&#25105;&#20204;&#30465;&#30053;&#20102;&#22312;2D&#30011;&#24067;&#19978;&#28210;&#26579;&#25955;&#28857;&#22270;&#65292;&#32780;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;PointNet++&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#30452;&#25509;&#25512;&#29702;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#32858;&#31867;&#27169;&#22411;&#65292;ClusterNet&#12290;
&lt;/p&gt;
&lt;p&gt;
Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;</title><link>http://arxiv.org/abs/2304.14176</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#21619;&#36947;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#21619;&#36947;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#20855;&#20307;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22522;&#26412;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340; $U(1)$ &#33655;&#36827;&#34892;&#23398;&#20064;&#65292;&#20195;&#29702;&#26041;&#26696;&#25214;&#21040;&#20102;21&#20010;&#19982;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#23454;&#39564;&#27979;&#37327;&#36136;&#37327;&#21644;&#28151;&#21512;&#35282;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#27491;&#24207;&#30340;&#22266;&#26377;&#20540;&#24448;&#24448;&#22823;&#20110;&#21453;&#24207;&#65292;&#27491;&#24207;&#19982;&#30446;&#21069;&#30340;&#23454;&#39564;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#31526;&#21512;&#12290;&#20195;&#29702;&#30340;&#33258;&#20027;&#34892;&#20026;&#26681;&#25454;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21494;&#23376;&#22330;&#30340;&#35282;&#25104;&#20998;&#24341;&#36215;&#30340;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#29992;&#33539;&#30068;&#35770;&#26500;&#36896;&#26469;&#23454;&#29616;&#30340;Brauer&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#20056;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Kronecker&#31215;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2304.14165</link><description>&lt;p&gt;
&#19968;&#31181;&#35745;&#31639;Brauer&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Algorithm for Computing with Brauer's Group Equivariant Neural Network Layers. (arXiv:2304.14165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#29992;&#33539;&#30068;&#35770;&#26500;&#36896;&#26469;&#23454;&#29616;&#30340;Brauer&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#20056;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Kronecker&#31215;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;arXiv&#65306;2212.08630&#20013;&#65292;&#23545;&#20171;&#20110;$\mathbb{R}^{n}$&#30340;&#24352;&#37327;&#24130;&#31354;&#38388;&#20043;&#38388;&#30340;&#21487;&#31561;&#21464;&#20110;&#27491;&#20132;&#32676;&#65292;$O(n)$&#65292;&#29305;&#27530;&#27491;&#20132;&#32676;&#65292;$SO(n)$&#65292;&#21644;&#36763;&#32676;&#65292;$Sp(n)$&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#29992;&#33539;&#30068;&#35770;&#26500;&#36896;&#26469;&#23454;&#29616;&#36807;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;Kronecker&#31215;&#30697;&#38453;&#26469;&#25191;&#34892;&#20056;&#27861;&#65292;&#19982;&#31616;&#21333;&#30340;&#23454;&#29616;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#23545;&#31216;&#32452;&#65292;$S_n$&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#24674;&#22797;&#20102;arXiv&#65306;2303.06208&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The learnable, linear neural network layers between tensor power spaces of $\mathbb{R}^{n}$ that are equivariant to the orthogonal group, $O(n)$, the special orthogonal group, $SO(n)$, and the symplectic group, $Sp(n)$, were characterised in arXiv:2212.08630. We present an algorithm for multiplying a vector by any weight matrix for each of these groups, using category theoretic constructions to implement the procedure. We achieve a significant reduction in computational cost compared with a naive implementation by making use of Kronecker product matrices to perform the multiplication. We show that our approach extends to the symmetric group, $S_n$, recovering the algorithm of arXiv:2303.06208 in the process.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#21453;&#39304;&#22343;&#34913;&#22120;&#65292;&#29992;&#20110;IM/DD&#31995;&#32479;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#22343;&#34913;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#22343;&#34913;&#22120;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14152</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#21453;&#39304;&#22343;&#34913;&#22120;&#29992;&#20110;IM/DD&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Network Decision Feedback Equalization for IM/DD Systems. (arXiv:2304.14152v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#21453;&#39304;&#22343;&#34913;&#22120;&#65292;&#29992;&#20110;IM/DD&#31995;&#32479;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#22343;&#34913;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#22343;&#34913;&#22120;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#21453;&#39304;&#22343;&#34913;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;IM/DD&#38142;&#36335;&#30340;&#21508;&#31181;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32447;&#24615;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#22343;&#34913;&#22120;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A spiking neural network (SNN) equalizer with a decision feedback structure is applied to an IM/DD link with various parameters. The SNN outperforms linear and artificial neural network (ANN) based equalizers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#24555;&#36895;&#35745;&#31639;&#20102;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#23618;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#20854;&#20182;&#39046;&#22495;&#20570;&#20986;&#20102;&#26377;&#30410;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.14144</link><description>&lt;p&gt;
&#20998;&#31867;&#21270;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Categorification of Group Equivariant Neural Networks. (arXiv:2304.14144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#24555;&#36895;&#35745;&#31639;&#20102;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#23618;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#20854;&#20182;&#39046;&#22495;&#20570;&#20986;&#20102;&#26377;&#30410;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#33539;&#30068;&#35770;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33539;&#30068;&#35770;&#26469;&#29702;&#35299;&#21644;&#22788;&#29702;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#23618;&#20989;&#25968;&#65292;&#20854;&#20013;&#23618;&#26159;$\mathbb{R}^{n}$&#30340;&#26576;&#20123;&#24352;&#37327;&#24130;&#31354;&#38388;&#65292;&#23545;&#24212;&#20110;&#32676;$S_n$&#12289;$O(n)$&#12289;$Sp(n)$&#21644;$SO(n)$&#12290;&#36890;&#36807;&#20351;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#27604;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#22987;&#20844;&#24335;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#36890;&#36807;&#27599;&#20010;&#38382;&#39064;&#20013;&#30340;&#32676;&#31561;&#21464;&#32447;&#24615;&#23618;&#20256;&#36882;&#30340;&#21521;&#37327;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#33539;&#30068;&#35770;&#21487;&#20197;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#20854;&#20182;&#39046;&#22495;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel application of category theory for deep learning. We show how category theory can be used to understand and work with the linear layer functions of group equivariant neural networks whose layers are some tensor power space of $\mathbb{R}^{n}$ for the groups $S_n$, $O(n)$, $Sp(n)$, and $SO(n)$. By using category theoretic constructions, we build a richer structure that is not seen in the original formulation of these neural networks, leading to new insights. In particular, we outline the development of an algorithm for quickly computing the result of a vector that is passed through an equivariant, linear layer for each group in question. The success of our approach suggests that category theory could be beneficial for other areas of deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;TempEE&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.14131</link><description>&lt;p&gt;
TempEE&#65306;&#22522;&#20110;&#26102;&#31354;&#24179;&#34892;Transformer&#23454;&#29616;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;TempEE&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#35937;&#38647;&#36798;&#21453;&#23556;&#29575;&#25968;&#25454;&#65288;&#20063;&#31216;&#20026;&#22238;&#27874;&#65289;&#22312;&#39044;&#27979;&#38477;&#27700;&#21644;&#36827;&#34892;&#30701;&#26399;&#24378;&#38477;&#38632;&#30340;&#31934;&#30830;&#24555;&#36895;&#39044;&#27979;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#21487;&#38752;&#19988;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#19977;&#20010;&#20027;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#32047;&#31215;&#35823;&#24046;&#25193;&#25955;&#12289;&#31232;&#30095;&#22238;&#27874;&#20998;&#24067;&#30340;&#19981;&#31934;&#30830;&#34920;&#31034;&#20197;&#21450;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#30340;&#19981;&#20934;&#30830;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#30340;&#26032;&#22411;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;Temporal-Spatial Parallel Transformer&#65288;TempEE&#65289;&#65292;&#30830;&#20445;&#20102;&#22312;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;NWP&#27169;&#22411;&#21644;&#33258;&#22238;&#24402;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#22312;&#22238;&#27874;&#25345;&#32493;&#26102;&#38388;&#20043;&#22806;&#36827;&#34892;&#22806;&#25512;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The meteorological radar reflectivity data, also known as echo, plays a crucial role in predicting precipitation and enabling accurate and fast forecasting of short-term heavy rainfall without the need for complex Numerical Weather Prediction (NWP) model. Compared to conventional model, Deep Learning (DL)-based radar echo extrapolation algorithms are more effective and efficient. However, the development of highly reliable and generalized algorithms is hindered by three main bottlenecks: cumulative error spreading, imprecise representation of sparse echo distribution, and inaccurate description of non-stationary motion process. To address these issues, this paper presents a novel radar echo extrapolation algorithm that utilizes temporal-spatial correlation features and the Transformer technology. The algorithm extracts features from multi-frame echo images that accurately represent non-stationary motion processes for precipitation prediction. The proposed algorithm uses a novel paralle
&lt;/p&gt;</description></item><item><title>SCARY&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#38468;&#21152;&#29238;&#22240;&#26524;&#20851;&#31995;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;40&#20010;&#22330;&#26223;&#12289;&#19977;&#20010;&#19981;&#21516;&#30340;&#31181;&#23376;&#12289;&#32447;&#24615;&#21644;&#28151;&#21512;&#22240;&#26524;&#26426;&#21046;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#22240;&#26524;&#20851;&#31995;&#25506;&#32034;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.14109</link><description>&lt;p&gt;
&#32467;&#26500;&#22797;&#26434;&#12289;&#20855;&#26377;&#38468;&#21152;&#29238;&#22240;&#26524;&#20851;&#31995;&#30340;SCARY&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Structurally Complex with Additive Parent Causality (SCARY) Dataset. (arXiv:2304.14109v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14109
&lt;/p&gt;
&lt;p&gt;
SCARY&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#38468;&#21152;&#29238;&#22240;&#26524;&#20851;&#31995;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;40&#20010;&#22330;&#26223;&#12289;&#19977;&#20010;&#19981;&#21516;&#30340;&#31181;&#23376;&#12289;&#32447;&#24615;&#21644;&#28151;&#21512;&#22240;&#26524;&#26426;&#21046;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#22240;&#26524;&#20851;&#31995;&#25506;&#32034;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#25968;&#25454;&#38598;&#22312;&#25512;&#21160;&#22240;&#26524;&#23398;&#39046;&#22495;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#36873;&#25321;&#20559;&#24046;&#12289;&#19981;&#24544;&#23454;&#25968;&#25454;&#21644;&#28151;&#28102;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#22240;&#26524;&#25968;&#25454;&#38598;&#65292;&#21363;&#32467;&#26500;&#22797;&#26434;&#12289;&#20855;&#26377;&#38468;&#21152;&#29238;&#22240;&#26524;&#20851;&#31995;&#30340;SCARY&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#20197;&#19979;&#29305;&#24449;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;40&#20010;&#22330;&#26223;&#65292;&#27599;&#20010;&#22330;&#26223;&#29983;&#25104;3&#20010;&#19981;&#21516;&#30340;&#31181;&#23376;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#21033;&#29992;&#30456;&#20851;&#25968;&#25454;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#29983;&#25104;&#29238;&#33410;&#28857;&#21644;&#23376;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#28151;&#21512;&#22240;&#26524;&#26426;&#21046;&#20197;&#21450;&#22810;&#20010;&#23376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21463;&#21040;&#22240;&#26524;&#21457;&#29616;&#24037;&#20855;&#31665;&#30340;&#21551;&#21457;&#65292;&#20165;&#29983;&#25104;&#21152;&#24615;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30340;Varsortability&#20026;0.5&#12290;&#25105;&#20204;&#30340;SCARY&#25968;&#25454;&#38598;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#19979;&#25506;&#32034;&#22240;&#26524;&#21457;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal datasets play a critical role in advancing the field of causality. However, existing datasets often lack the complexity of real-world issues such as selection bias, unfaithful data, and confounding. To address this gap, we propose a new synthetic causal dataset, the Structurally Complex with Additive paRent causalitY (SCARY) dataset, which includes the following features. The dataset comprises 40 scenarios, each generated with three different seeds, allowing researchers to leverage relevant subsets of the dataset. Additionally, we use two different data generation mechanisms for generating the causal relationship between parents and child nodes, including linear and mixed causal mechanisms with multiple sub-types. Our dataset generator is inspired by the Causal Discovery Toolbox and generates only additive models. The dataset has a Varsortability of 0.5. Our SCARY dataset provides a valuable resource for researchers to explore causal discovery under more realistic scenarios. The
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.14102</link><description>&lt;p&gt;
SocNavGym&#65306;&#19968;&#20010;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20223;&#30495;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#19979;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#23548;&#33322;&#26102;&#38656;&#35201;&#36981;&#23432;&#31038;&#20132;&#35268;&#33539;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#36817;&#22312;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110;&#29983;&#25104;&#30340;&#31574;&#30053;&#19981;&#21463;&#20195;&#30721;&#22797;&#26434;&#24615;&#25110;&#22788;&#29702;&#30340;&#21464;&#37327;&#25968;&#37327;&#31561;&#20154;&#31867;&#38480;&#21046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DRL&#31639;&#27861;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#20223;&#30495;&#29615;&#22659;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#20808;&#36827;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;SocNavGym&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#21487;&#36731;&#26494;&#37197;&#32622;&#20197;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#37197;&#32622;&#20026;&#20351;&#29992;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#25903;&#25345;&#26080;&#38556;&#30861;&#29615;&#22659;&#30340;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.14082</link><description>&lt;p&gt;
JaxPruner&#65306;&#19968;&#20010;&#29992;&#20110;&#31232;&#30095;&#24615;&#30740;&#31350;&#30340;&#31616;&#26126;&#24211;
&lt;/p&gt;
&lt;p&gt;
JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#24320;&#28304;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#21152;&#36895;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#12290;JaxPruner&#23454;&#29616;&#30340;&#31639;&#27861;&#20351;&#29992;&#36890;&#29992;API&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#24211;Optax&#26080;&#32541;&#21327;&#20316;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#31034;&#20363;&#24182;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#22522;&#20934;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#38598;&#25104;&#30340;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;ClusterFlow&#65292;&#21487;&#20197;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#23494;&#38598;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#26469;&#26500;&#24314;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14081</link><description>&lt;p&gt;
&#32858;&#31867;&#27969;&#65288;Cluster Flow&#65289;&#65306;&#22914;&#20309;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20855;&#40065;&#26834;&#24615;&#12289;&#26356;&#31526;&#21512;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#12289;&#26356;&#26131;&#20110;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning. (arXiv:2304.14081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;ClusterFlow&#65292;&#21487;&#20197;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#23494;&#38598;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#26469;&#26500;&#24314;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23454;&#29616;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#31361;&#30772;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65306;&#23427;&#20204;&#21487;&#20197;&#34987;&#26234;&#33021;&#25915;&#20987;&#65292;&#32780;&#20154;&#20204;&#26080;&#27861;&#34987;&#27450;&#39575;&#65292;&#20063;&#32570;&#20047;&#24120;&#35782;&#12290;&#24050;&#32463;&#26377;&#20154;&#35748;&#20026;&#65292;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#30784;&#26159;&#20154;&#31867;&#33021;&#22815;&#36827;&#34892;&#20851;&#31995;&#25512;&#29702;&#30340;&#33021;&#21147;&#65306;&#27604;&#36739;&#19981;&#21516;&#23545;&#35937;&#65292;&#27979;&#37327;&#30456;&#20284;&#24615;&#65292;&#25484;&#25569;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32858;&#31867;&#27969;&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#30340;&#23618;&#27425;&#32858;&#31867;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#39044; SoftMax &#23618;&#20013;&#21457;&#29616;&#30340;&#20016;&#23500;&#30340;&#22810;&#32500;&#31867;&#21035;&#21644;&#29305;&#24449;&#25968;&#25454;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340; NNs &#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#26500;&#24314;&#31867;/&#29305;&#24449;&#30340;&#36229;&#31354;&#38388;&#22320;&#22270;&#65292;&#24182;&#23558;&#36825;&#20123;&#21151;&#33021;&#28155;&#21152;&#21040;&#29616;&#20195;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26356;&#20855;&#20154;&#31867;&#24605;&#32771;&#26041;&#24335;&#30340;&#21151;&#33021;&#12290;&#20316;&#32773;&#36890;&#36807;3&#20010;&#20219;&#21153;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical l
&lt;/p&gt;</description></item><item><title>ganX&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#20174;RGB&#22270;&#20687;&#20013;&#29983;&#25104;X&#23556;&#32447;&#33639;&#20809;&#23439;&#35266;&#22270;&#35889;(MA-XRF)&#65292;&#20351;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#39068;&#26009;&#29305;&#24449;XRF&#20449;&#21495;&#30340;&#25968;&#25454;&#24211;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14078</link><description>&lt;p&gt;
ganX -- &#19968;&#20010;&#29983;&#25104;MA-XRF&#21407;&#22987;&#25968;&#25454;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#20174;RGB&#22270;&#20687;&#20013;&#29983;&#25104;X&#23556;&#32447;&#33639;&#20809;&#23439;&#35266;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
ganX -- generate artificially new XRF a python library to generate MA-XRF raw data out of RGB images. (arXiv:2304.14078v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14078
&lt;/p&gt;
&lt;p&gt;
ganX&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#20174;RGB&#22270;&#20687;&#20013;&#29983;&#25104;X&#23556;&#32447;&#33639;&#20809;&#23439;&#35266;&#22270;&#35889;(MA-XRF)&#65292;&#20351;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#39068;&#26009;&#29305;&#24449;XRF&#20449;&#21495;&#30340;&#25968;&#25454;&#24211;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ganX -- generate artificially new XRF&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#20174;&#24425;&#33394;RGB&#22270;&#20687;&#20013;&#29983;&#25104;X&#23556;&#32447;&#33639;&#20809;&#23439;&#35266;&#22270;&#35889;(MA-XRF)&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#65292;&#20351;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;MA-XRF&#20687;&#32032;&#20449;&#21495;&#37117;&#26159;&#20174;XRF&#20449;&#21495;&#27010;&#29575;&#20989;&#25968;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#12290;&#36825;&#20010;&#27010;&#29575;&#20989;&#25968;&#26159;&#36890;&#36807;&#20351;&#29992;(&#39068;&#26009;&#29305;&#24449;XRF&#20449;&#21495;&#65292;RGB)&#30340;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#35745;&#31639;&#39068;&#26009;XRF&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#65292;&#20197;&#21450;&#22270;&#20687;RGB&#19982;&#39068;&#26009;&#29305;&#24449;RGB&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#35745;&#31639;&#30340;&#12290;&#35813;&#24211;&#24050;&#22312;PyPi&#19978;&#21457;&#24067;&#65292;&#24182;&#22312;GitHub&#19978;&#20197;&#24320;&#28304;&#30340;&#26041;&#24335;&#25552;&#20379;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the first version of ganX -- generate artificially new XRF, a Python library to generate X-ray fluorescence Macro maps (MA-XRF) from a coloured RGB image. To do that, a Monte Carlo method is used, where each MA-XRF pixel signal is sampled out of an XRF signal probability function. Such probability function is computed using a database of couples (pigment characteristic XRF signal, RGB), by a weighted sum of such pigment XRF signal by proximity of the image RGB to the pigment characteristic RGB. The library is released to PyPi and the code is available open source on GitHub.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.14070</link><description>&lt;p&gt;
&#32452;&#21512;&#24335;3D&#20154;&#29289;-&#29289;&#20307;&#31070;&#32463;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Compositional 3D Human-Object Neural Animation. (arXiv:2304.14070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#23545;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;&#22330;&#26223;&#29702;&#35299;&#24212;&#29992;&#65292;&#22914;&#20154;&#31867;&#20013;&#24515;&#30340;&#35270;&#35273;&#29983;&#25104;&#12289;AR/VR&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#24335;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#21160;&#30011;&#30340;&#25361;&#25112;&#65292;&#21363;&#36890;&#36807;&#26032;&#30340;&#23039;&#21183;&#24207;&#21015;&#65292;&#21160;&#30011;&#21270;&#26032;&#30340;&#20154;&#29289;&#21644;/&#25110;&#26032;&#30340;&#29289;&#20307;&#30340;&#26032;&#30340;&#20132;&#20114;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#20154;&#29289;-&#29289;&#20307;&#21464;&#24418;&#26469;&#24314;&#27169;&#21644;&#28210;&#26579;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;HOI&#21160;&#24577;&#12290;&#20026;&#20102;&#20351;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#23039;&#21183;&#22312;&#19981;&#21516;&#30340;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#36716;&#31227;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#24335;&#26465;&#20214;&#31070;&#32463;&#36752;&#23556;&#22330;(CC-NeRF)&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28508;&#22312;&#30721;&#23558;&#20154;&#29289;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#35299;&#32806;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;HOI&#30340;&#32452;&#21512;&#24335;&#21160;&#30011;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#21508;&#31181;&#26032;&#30340;HOI&#21160;&#30011;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-object interactions (HOIs) are crucial for human-centric scene understanding applications such as human-centric visual generation, AR/VR, and robotics. Since existing methods mainly explore capturing HOIs, rendering HOI remains less investigated. In this paper, we address this challenge in HOI animation from a compositional perspective, i.e., animating novel HOIs including novel interaction, novel human and/or novel object driven by a novel pose sequence. Specifically, we adopt neural human-object deformation to model and render HOI dynamics based on implicit neural representations. To enable the interaction pose transferring among different persons and objects, we then devise a new compositional conditional neural radiance field (or CC-NeRF), which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Experiments show that the proposed method can generalize well to various novel HOI animation setting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.14068</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27010;&#24565;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;DCR&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;&#65292;&#24182;&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;&#65292;&#36866;&#24212;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#38459;&#27490;&#20102;&#23427;&#20204;&#33719;&#24471;&#23436;&#20840;&#30340;&#20154;&#31867;&#20449;&#20219;&#12290;&#27010;&#24565;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32500;&#27010;&#24565;&#23884;&#20837;&#34920;&#31034;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#27492;&#36136;&#30097;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Concept Reasoner(DCR)&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#27010;&#24565;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#12290;&#22312;DCR&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#19981;&#30452;&#25509;&#36827;&#34892;&#20219;&#21153;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#27010;&#24565;&#23884;&#20837;&#24314;&#31435;&#35821;&#27861;&#35268;&#21017;&#32467;&#26500;&#12290;&#28982;&#21518;DCR&#22312;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#30495;&#20540;&#24230;&#19978;&#25191;&#34892;&#36825;&#20123;&#35268;&#21017;&#65292;&#20197;&#19981;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#25552;&#20379;&#26368;&#32456;&#30340;&#21487;&#35299;&#37322;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DCR&#65306;(i)&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#25552;&#39640;&#20102;&#39640;&#36798;+25&#65285;;(ii)&#20135;&#29983;&#33021;&#22815;&#35299;&#37322;&#20854;&#39044;&#27979;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#21644;&#30495;&#20540;&#24230;;(iii)&#24456;&#23481;&#26131;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#23558;&#27495;&#20041;&#38598;&#21512;&#20256;&#36882;&#21040;&#38750;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#25511;&#21046;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14057</link><description>&lt;p&gt;
&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#20013;&#20256;&#25773;&#20869;&#26680;&#27495;&#20041;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Propagating Kernel Ambiguity Sets in Nonlinear Data-driven Dynamics Models. (arXiv:2304.14057v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#23558;&#27495;&#20041;&#38598;&#21512;&#20256;&#36882;&#21040;&#38750;&#32447;&#24615;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#40065;&#26834;&#25511;&#21046;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#22914;&#20869;&#26680;&#26465;&#20214;&#22343;&#20540;&#23884;&#20837;&#21644;Koopman&#31639;&#23376;&#65292;&#22914;&#20309;&#22312;&#22810;&#20010;&#27493;&#39588;&#20013;&#21521;&#21069;&#20256;&#25773;&#27495;&#20041;&#38598;&#21512;&#65311;&#36825;&#20010;&#38382;&#39064;&#26159;&#35299;&#20915;&#20998;&#24067;&#24335;&#40065;&#26834;&#25511;&#21046;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#20851;&#38190;&#12290;&#19982;&#20808;&#21069;&#20351;&#29992;&#38745;&#24577;&#27495;&#20041;&#38598;&#21512;&#65288;&#20363;&#22914;&#22266;&#23450;&#30340;Wasserstein&#29699;&#65289;&#25110;&#22312;&#24050;&#30693;&#20998;&#27573;&#32447;&#24615;&#65288;&#25110;&#20223;&#23556;&#65289;&#21160;&#21147;&#23398;&#19979;&#20351;&#29992;&#21160;&#24577;&#27495;&#20041;&#38598;&#21512;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;Koopman&#31639;&#23376;&#21644;CME&#65292;&#36890;&#36807;&#20869;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#20960;&#20309;&#20256;&#36882;&#27495;&#20041;&#38598;&#21512;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#25968;&#23383;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20869;&#26680;&#27495;&#20041;&#38598;&#21512;&#26159;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#30340;&#33258;&#28982;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides answers to an open problem: given a nonlinear data-driven dynamical system model, e.g., kernel conditional mean embedding (CME) and Koopman operator, how can one propagate the ambiguity sets forward for multiple steps? This problem is the key to solving distributionally robust control and learning-based control of such learned system models under a data-distribution shift. Different from previous works that use either static ambiguity sets, e.g., fixed Wasserstein balls, or dynamic ambiguity sets under known piece-wise linear (or affine) dynamics, we propose an algorithm that exactly propagates ambiguity sets through nonlinear data-driven models using the Koopman operator and CME, via the kernel maximum mean discrepancy geometry. Through both theoretical and numerical analysis, we show that our kernel ambiguity sets are the natural geometric structure for the learned data-driven dynamical system models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#23569;&#26679;&#26412;&#20998;&#21106;&#26694;&#26550;&#65292;&#20934;&#30830;&#26080;&#35823;&#22320;&#29983;&#25104;&#32929;&#39592;&#32908;&#32905;&#25513;&#34109;&#65292;&#21076;&#38500;IMF&#65292;&#26377;&#21161;&#20110;&#32908;&#32905;&#23481;&#31215;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#20047;&#22823;&#37327;&#31934;&#30830;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.14053</link><description>&lt;p&gt;
&#31934;&#30830;&#30340;T1&#21152;&#26435;MRI&#23569;&#25968;&#26679;&#26412;&#26080;&#33026;&#32938;&#32908;&#32905;&#20999;&#21106;
&lt;/p&gt;
&lt;p&gt;
Precise Few-shot Fat-free Thigh Muscle Segmentation in T1-weighted MRI. (arXiv:2304.14053v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#23569;&#26679;&#26412;&#20998;&#21106;&#26694;&#26550;&#65292;&#20934;&#30830;&#26080;&#35823;&#22320;&#29983;&#25104;&#32929;&#39592;&#32908;&#32905;&#25513;&#34109;&#65292;&#21076;&#38500;IMF&#65292;&#26377;&#21161;&#20110;&#32908;&#32905;&#23481;&#31215;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#20047;&#22823;&#37327;&#31934;&#30830;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#32929;&#39592;&#32908;&#20307;&#31215;&#23545;&#20110;&#30417;&#27979;&#24739;&#26377;&#21508;&#31181;&#31243;&#24230;&#32929;&#39592;&#32908;&#25439;&#22833;&#30142;&#30149;&#24739;&#32773;&#30340;&#36816;&#21160;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#22312;&#32908;&#32905;&#21644;&#33026;&#32938;&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#27604;&#65292;T1&#21152;&#26435;MRI&#26159;&#33719;&#24471;&#32929;&#39592;&#32908;&#32905;&#25513;&#34109;&#30340;&#40664;&#35748;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#20998;&#21106;&#33719;&#24471;&#36825;&#20123;&#25513;&#34109;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31934;&#30830;&#23450;&#20041;&#30340;&#27880;&#37322;&#25968;&#37327;&#19981;&#36275;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#32929;&#39592;&#32908;&#32905;&#25513;&#34109;&#24448;&#24448;&#20250;&#23558;&#32908;&#20869;&#33026;&#32938;(IMF)&#35823;&#20998;&#31867;&#20026;&#32908;&#32905;&#65292;&#24433;&#21709;&#32908;&#32905;&#23481;&#31215;&#20998;&#26512;&#12290;&#30001;&#20110;IMF&#34987;&#28024;&#28070;&#22312;&#32908;&#32905;&#20869;&#65292;&#20154;&#31867;&#27880;&#37322;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21076;&#38500;IMF&#30340;&#31934;&#30830;&#32908;&#32905;&#25513;&#34109;&#26159;&#26377;&#38480;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#20998;&#21106;&#26694;&#26550;&#20197;&#29983;&#25104;&#21076;&#38500;IMF&#30340;&#32929;&#39592;&#32908;&#32905;&#25513;&#34109;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#20266;&#26631;&#31614;&#20462;&#27491;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#25239;&#22122;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise thigh muscle volumes are crucial to monitor the motor functionality of patients with diseases that may result in various degrees of thigh muscle loss. T1-weighted MRI is the default surrogate to obtain thigh muscle masks due to its contrast between muscle and fat signals. Deep learning approaches have recently been widely used to obtain these masks through segmentation. However, due to the insufficient amount of precise annotations, thigh muscle masks generated by deep learning approaches tend to misclassify intra-muscular fat (IMF) as muscle impacting the analysis of muscle volumetrics. As IMF is infiltrated inside the muscle, human annotations require expertise and time. Thus, precise muscle masks where IMF is excluded are limited in practice. To alleviate this, we propose a few-shot segmentation framework to generate thigh muscle masks excluding IMF. In our framework, we design a novel pseudo-label correction and evaluation scheme, together with a new noise robust loss for e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#21333;&#35270;&#35282;&#22270;&#20687;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#24573;&#30053;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14045</link><description>&lt;p&gt;
&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Interweaved Graph and Attention Network for 3D Human Pose Estimation. (arXiv:2304.14045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#21333;&#35270;&#35282;&#22270;&#20687;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#24573;&#30053;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#22312;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21333;&#35270;&#35282;&#22270;&#20687;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20197;&#24448;&#30340;&#24037;&#20316;&#24456;&#23569;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#30456;&#20114;&#20851;&#31995;&#65292;&#23548;&#33268;&#23545;&#20154;&#20307;&#39592;&#39612;&#34920;&#31034;&#30340;&#23398;&#20064;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#38169;&#22270;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;IGANet&#65289;&#65292;&#23427;&#20801;&#35768;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGA&#27169;&#22359;&#65292;&#20854;&#20013;GCNs&#21521;&#27880;&#24847;&#21147;&#25552;&#20379;&#23616;&#37096;&#20449;&#24687;&#65292;&#27880;&#24847;&#21147;&#21521;GCNs&#27880;&#20837;&#20840;&#23616;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;uMLP&#65289;&#65292;&#21487;&#20197;&#25429;&#25417;&#36523;&#20307;&#20851;&#33410;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35797;&#39564;&#65288;&#21363;Human3. 6M&#21644;MPI-INF-3DHP&#65289;&#65292;&#20197;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;IGANet&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xiu-cs/IGANet&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from GCNs and GCNs are injected with global information from attentions. Additionally, we design a simple yet effective U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity information for body joints. Extensive experiments on two popular benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our proposed method.The results show that IGANet achieves state-of-the-art performance on both datasets. Code is available at https://github.com/xiu-cs/IGANet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#27491;&#20132;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21363;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#26469;&#32531;&#35299;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14034</link><description>&lt;p&gt;
&#27491;&#20132;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#29699;&#24418;&#24863;&#24212;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes. (arXiv:2304.14034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#27491;&#20132;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21363;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#26469;&#32531;&#35299;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23398;&#20064;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#32463;&#24120;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#35825;&#23548;&#21464;&#37327;&#19982;&#21069;&#39304;NN&#30340;&#38544;&#34255;&#21333;&#20803;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#36328;&#22495;&#21464;&#20998;GPs&#26469;&#24357;&#21512; GPs&#21644;&#28145;&#24230;NN&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#30740;&#31350;&#27492;&#26041;&#27861;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21033;&#29992;GPs&#30340;&#27491;&#20132;&#20998;&#35299;&#26469;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#65292;&#29992;&#20110;GP&#36924;&#36817;&#30340;&#20027;&#35201;&#21644;&#27491;&#20132;&#20998;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#27492;&#26694;&#26550;&#19979;&#21152;&#20837;NN&#28608;&#27963;&#29305;&#24449;&#65292;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#25935;&#24863;&#24615;&#26354;&#32447;&#26368;&#22823;&#21270;&#65288;SCM&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#27880;&#20837;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#25200;&#21160;&#26469;&#30772;&#22351;&#29616;&#26377;&#30340;&#40065;&#26834;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14024</link><description>&lt;p&gt;
&#36890;&#36807;&#25935;&#24863;&#24615;&#26354;&#32447;&#26368;&#22823;&#21270;&#23545;&#40065;&#26834;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacks on Robust Distributed Learning Schemes via Sensitivity Curve Maximization. (arXiv:2304.14024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#25935;&#24863;&#24615;&#26354;&#32447;&#26368;&#22823;&#21270;&#65288;SCM&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#27880;&#20837;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#25200;&#21160;&#26469;&#30772;&#22351;&#29616;&#26377;&#30340;&#40065;&#26834;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#35832;&#22914;&#32852;&#37030;&#23398;&#20064;&#25110;&#20998;&#25955;&#24335;&#23398;&#20064;&#65292;&#20801;&#35768;&#38598;&#21512;&#20195;&#29702;&#36890;&#36807;&#26377;&#38480;&#30340;&#23616;&#37096;&#20132;&#20114;&#35299;&#20915;&#20840;&#23616;&#23398;&#20064;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#31574;&#30053;&#20381;&#36182;&#20110;&#26412;&#22320;&#36866;&#24212;&#21644;&#32858;&#21512;&#27493;&#39588;&#30340;&#28151;&#21512;&#65292;&#26080;&#35770;&#26159;&#22312;&#23545;&#31561;&#26041;&#20043;&#38388;&#36824;&#26159;&#22312;&#20013;&#22830;&#34701;&#21512;&#20013;&#24515;&#12290;&#20256;&#32479;&#19978;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#32858;&#21512;&#26159;&#22522;&#20110;&#24179;&#22343;&#20540;&#30340;&#65292;&#36825;&#26159;&#32479;&#35745;&#19978;&#26377;&#25928;&#30340;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#23569;&#25968;&#24694;&#24847;&#20195;&#29702;&#30340;&#25915;&#20987;&#12290;&#36825;&#19968;&#35266;&#23519;&#20419;&#20351;&#20102;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#37319;&#29992;&#22343;&#20540;&#30340;&#40065;&#26834;&#21464;&#20307;&#26469;&#24320;&#21457;&#20581;&#22766;&#30340;&#32858;&#21512;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24615;&#26354;&#32447;&#26368;&#22823;&#21270;&#65288;SCM&#65289;&#30340;&#26032;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#36890;&#36807;&#27880;&#20837;&#24494;&#23567;&#20294;&#26377;&#25928;&#30340;&#25200;&#21160;&#26469;&#30772;&#22351;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning paradigms, such as federated or decentralized learning, allow a collection of agents to solve global learning and optimization problems through limited local interactions. Most such strategies rely on a mixture of local adaptation and aggregation steps, either among peers or at a central fusion center. Classically, aggregation in distributed learning is based on averaging, which is statistically efficient, but susceptible to attacks by even a small number of malicious agents. This observation has motivated a number of recent works, which develop robust aggregation schemes by employing robust variations of the mean. We present a new attack based on sensitivity curve maximization (SCM), and demonstrate that it is able to disrupt existing robust aggregation schemes by injecting small, but effective perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#36755;&#20837;&#34920;&#31034;&#27861;&#30340;&#27169;&#22411;&#65292;&#21363;&#21407;&#22987;&#27874;&#24418;&#20449;&#21495;&#21644;&#26102;&#38388;-&#39057;&#29575;&#35889;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25581;&#31034;&#20102;&#19981;&#21516;&#34920;&#31034;&#27861;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20026;&#27491;&#30830;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.14019</link><description>&lt;p&gt;
&#22522;&#20110;XAI&#30340;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#36755;&#20837;&#34920;&#31034;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
XAI-based Comparison of Input Representations for Audio Event Classification. (arXiv:2304.14019v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#36755;&#20837;&#34920;&#31034;&#27861;&#30340;&#27169;&#22411;&#65292;&#21363;&#21407;&#22987;&#27874;&#24418;&#20449;&#21495;&#21644;&#26102;&#38388;-&#39057;&#29575;&#35889;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25581;&#31034;&#20102;&#19981;&#21516;&#34920;&#31034;&#27861;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20026;&#27491;&#30830;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#30456;&#23545;&#20110;&#33258;&#28982;&#22270;&#20687;&#31561;&#20854;&#20182;&#25968;&#25454;&#65292;&#38899;&#39057;&#25968;&#25454;&#26377;&#35768;&#22810;&#26126;&#26234;&#21644;&#38750;&#26174;&#32780;&#26131;&#35265;&#30340;&#34920;&#31034;&#27861;&#65292;&#36825;&#20123;&#34920;&#31034;&#27861;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26469;&#20102;&#35299;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#22522;&#26412;&#20998;&#31867;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20004;&#31181;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20102;&#35299;&#19982;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#8220;Siren&#8221;&#65288;&#23618;&#38754;&#30456;&#20851;&#20256;&#36882;&#65289;&#33719;&#24471;&#30340;&#30456;&#20851;&#28909;&#22270;&#22914;&#20309;&#25581;&#31034;&#34920;&#31034;&#27861;&#30456;&#20851;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#20123;&#28145;&#20837;&#30340;&#27934;&#23519;&#65292;&#25105;&#20204;&#33021;&#22815;&#20316;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are a promising tool for Audio Event Classification. In contrast to other data like natural images, there are many sensible and non-obvious representations for audio data, which could serve as input to these models. Due to their black-box nature, the effect of different input representations has so far mostly been investigated by measuring classification performance. In this work, we leverage eXplainable AI (XAI), to understand the underlying classification strategies of models trained on different input representations. Specifically, we compare two model architectures with regard to relevant input features used for Audio Event Detection: one directly processes the signal as the raw waveform, and the other takes in its time-frequency spectrogram representation. We show how relevance heatmaps obtained via "Siren"{Layer-wise Relevance Propagation} uncover representation-dependent decision strategies. With these insights, we can make a well-informed decision about the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ViC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;ViT&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#23618;&#26469;&#35299;&#20915;ViT&#32570;&#20047;&#22270;&#20687;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13991</link><description>&lt;p&gt;
Vision Conformer: &#23558;&#21367;&#31215;&#34701;&#20837;&#35270;&#35273;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
Vision Conformer: Incorporating Convolutions into Vision Transformer Layers. (arXiv:2304.13991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ViC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;ViT&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#23618;&#26469;&#35299;&#20915;ViT&#32570;&#20047;&#22270;&#20687;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#23618;&#21644;&#23884;&#20837;&#24335;&#20196;&#29260;&#30340;&#20840;&#36830;&#25509;&#33410;&#28857;&#23618;&#12290;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#23558;Transformer&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22270;&#20687;&#34987;&#20998;&#25104;&#34917;&#19969;&#24182;&#29992;&#20316;&#20196;&#29260;&#12290;ViT&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#32570;&#20047;&#23545;&#22270;&#20687;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22240;&#20026;ViT&#26159;&#20174;&#35821;&#35328;&#24314;&#27169;&#30340;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#25913;&#32534;&#30340;&#65292;&#25152;&#20197;&#32593;&#32476;&#19981;&#33021;&#30452;&#25509;&#22788;&#29702;&#23616;&#37096;&#24179;&#31227;&#12289;&#20687;&#32032;&#20449;&#24687;&#20197;&#21450;&#22810;&#20010;&#34917;&#19969;&#20849;&#20139;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#30456;&#21453;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21017;&#21253;&#21547;&#20102;&#36825;&#20123;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;ViT&#20013;&#20351;&#29992;&#21367;&#31215;&#23618;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35270;&#35273;Conformer&#65288;ViC&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#23558;ViT&#23618;&#20013;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#26367;&#25442;&#20026;CNN&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#29992;CNN&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#33258;&#25105;&#27880;&#24847;&#21147;&#20043;&#21518;&#21453;&#21521;&#23884;&#20837;&#37325;&#24314;&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are popular neural network models that use layers of self-attention and fully-connected nodes with embedded tokens. Vision Transformers (ViT) adapt transformers for image recognition tasks. In order to do this, the images are split into patches and used as tokens. One issue with ViT is the lack of inductive bias toward image structures. Because ViT was adapted for image data from language modeling, the network does not explicitly handle issues such as local translations, pixel information, and information loss in the structures and features shared by multiple patches. Conversely, Convolutional Neural Networks (CNN) incorporate this information. Thus, in this paper, we propose the use of convolutional layers within ViT. Specifically, we propose a model called a Vision Conformer (ViC) which replaces the Multi-Layer Perceptron (MLP) in a ViT layer with a CNN. In addition, to use the CNN, we proposed to reconstruct the image data after the self-attention in a reverse embedding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36718;&#24275;&#23436;&#25972;&#24230;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23383;&#20307;&#36718;&#24275;&#12290;</title><link>http://arxiv.org/abs/2304.13988</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#23436;&#25972;&#24230;&#21450;&#24212;&#29992;&#20110;&#30690;&#37327;&#23383;&#20307;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contour Completion by Transformers and Its Application to Vector Font Data. (arXiv:2304.13988v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36718;&#24275;&#23436;&#25972;&#24230;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23383;&#20307;&#36718;&#24275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#21644;&#22270;&#24418;&#20013;&#65292;&#36718;&#24275;&#26159;&#25551;&#36848;&#29305;&#23450;&#24418;&#29366;&#30340;&#27969;&#34892;&#26684;&#24335;&#12290;&#20363;&#22914;&#65292;&#22312;True Type Font&#65288;TTF&#65289;&#25991;&#20214;&#26684;&#24335;&#20013;&#65292;&#36718;&#24275;&#25551;&#36848;&#23383;&#24418;&#30340;&#30690;&#37327;&#36718;&#24275;&#12290;&#27599;&#20010;&#36718;&#24275;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#28857;&#24207;&#21015;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36718;&#24275;&#23436;&#25972;&#24230;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#36755;&#20837;&#26159;&#19968;&#20010;&#21253;&#21547;&#32570;&#22833;&#28857;&#30340;&#36718;&#24275;&#24207;&#21015;&#65292;&#36755;&#20986;&#26159;&#19968;&#20010;&#29983;&#25104;&#30340;&#23436;&#25972;&#36718;&#24275;&#12290;&#36825;&#20010;&#20219;&#21153;&#27604;&#22270;&#20687;&#23436;&#25104;&#20219;&#21153;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#23545;&#20110;&#22270;&#20687;&#65292;&#32570;&#22833;&#30340;&#20687;&#32032;&#26159;&#21487;&#20197;&#25351;&#31034;&#30340;&#65292;&#32780;&#22312;&#36718;&#24275;&#23436;&#25972;&#24230;&#20219;&#21153;&#20013;&#27809;&#26377;&#36825;&#26679;&#30340;&#25351;&#31034;&#65292;&#25105;&#20204;&#24517;&#39035;&#21516;&#26102;&#35299;&#20915;&#32570;&#22833;&#37096;&#20998;&#26816;&#27979;&#21644;&#23436;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23383;&#20307;&#36718;&#24275;&#23436;&#25972;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In documents and graphics, contours are a popular format to describe specific shapes. For example, in the True Type Font (TTF) file format, contours describe vector outlines of typeface shapes. Each contour is often defined as a sequence of points. In this paper, we tackle the contour completion task. In this task, the input is a contour sequence with missing points, and the output is a generated completed contour. This task is more difficult than image completion because, for images, the missing pixels are indicated. Since there is no such indication in the contour completion task, we must solve the problem of missing part detection and completion simultaneously. We propose a Transformer-based method to solve this problem and show the results of the typeface contour completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#23376;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13976</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Moderately Distributional Exploration for Domain Generalization. (arXiv:2304.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;&#23376;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#35299;&#20915;&#35757;&#32451;&#39046;&#22495;&#19982;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#29983;&#25104;&#26032;&#30340;&#39046;&#22495;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28982;&#32780;&#20854;&#24615;&#33021;&#22686;&#30410;&#21462;&#20915;&#20110;&#29983;&#25104;&#30340;&#39046;&#22495;&#19982;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26377;&#26395;&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#20013;&#25506;&#32034;&#39046;&#22495;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#38598;&#21487;&#33021;&#38750;&#24120;&#24222;&#22823;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#20250;&#23548;&#33268;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#22240;&#20026;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21487;&#33021;&#20250;&#24341;&#20837;&#21253;&#21547;&#19982;&#35757;&#32451;&#39046;&#22495;&#35821;&#20041;&#19981;&#21516;&#30340;&#22240;&#32032;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#20013;&#24230;&#20998;&#24067;&#25506;&#32034;&#65288;MODE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MODE&#22312;&#19968;&#20010;&#19982;&#35757;&#32451;&#39046;&#22495;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#22240;&#32032;&#30340;&#19981;&#30830;&#23450;&#24615;$\textit{&#23376;&#38598;}$&#20013;&#36827;&#34892;&#20998;&#24067;&#25506;&#32034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;MODE&#21487;&#20197;&#20026;&#27169;&#22411;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#39046;&#22495;&#27867;&#21270;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\textbf{mo}$derately $\textbf{d}$istributional $\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.13972</link><description>&lt;p&gt;
&#26494;&#24347;&#20551;&#35774;&#19979;Adam&#25910;&#25947;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#24191;&#27867;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#23545;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;Adam&#65289;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#12290;&#34429;&#28982;Adam&#31639;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27969;&#34892;&#24230;&#21644;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#38656;&#35201;&#36807;&#20110;&#24378;&#30340;&#20551;&#35774;&#65292;&#22914;&#20840;&#23616;&#26799;&#24230;&#26377;&#30028;&#65292;&#20197;&#35777;&#26126;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26356;&#20026;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#20197;$\mathcal{O}(\epsilon^{-4})$&#26799;&#24230;&#22797;&#26434;&#24230;&#25910;&#25947;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#26159;&#26681;&#25454;&#19968;&#31181;&#24191;&#20041;&#20809;&#28369;&#24615;&#20551;&#35774;&#32473;&#20986;&#30340;&#65292;&#27839;&#30528;&#20248;&#21270;&#36712;&#36857;&#30340;&#26799;&#24230;&#26377;&#30028;&#30340;&#26032;&#35777;&#26126;&#12290;&#26681;&#25454;&#35813;&#20551;&#35774;&#65292;&#23616;&#37096;&#20809;&#28369;&#24615;(&#21363;&#23384;&#22312;&#26102;&#30340;Hessian norm)&#21463;&#26799;&#24230;&#33539;&#25968;&#30340;&#27425;&#24179;&#26041;&#20989;&#25968;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#32422;&#20943;&#29256;&#26412;&#30340;Adam&#19982;&#21152;&#36895;Gradient&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#20844;&#24179;&#24863;&#30693;&#27169;&#22411;&#25552;&#20379;&#20102;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#26469;&#35780;&#20272;&#20854;&#27979;&#35797;&#19981;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13950</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;: &#24744;&#26377;&#22810;&#22823;&#25226;&#25569;&#27169;&#22411;&#26159;&#20844;&#24179;&#30340;?
&lt;/p&gt;
&lt;p&gt;
Fairness Uncertainty Quantification: How certain are you that the model is fair?. (arXiv:2304.13950v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#20844;&#24179;&#24863;&#30693;&#27169;&#22411;&#25552;&#20379;&#20102;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#26469;&#35780;&#20272;&#20854;&#27979;&#35797;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#22312;&#21496;&#27861;&#31995;&#32479;&#31561;&#25935;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25552;&#20986;&#20102;&#21508;&#31181;&#21551;&#21457;&#24335;&#21644;&#20248;&#21270;&#26694;&#26550;&#26469;&#24378;&#21046;&#23454;&#29616;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20854;&#20013;&#21518;&#19968;&#31181;&#26041;&#27861;&#35201;&#20040;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#65292;&#35201;&#20040;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#31934;&#30830;&#26368;&#23567;&#21270;&#22120;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20960;&#20046;&#24635;&#26159;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31867;&#22411;&#30340;&#31639;&#27861;&#20316;&#20026;&#35757;&#32451;&#31639;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#23398;&#20064;&#30340;&#27169;&#22411;&#20197;&#21450;&#20854;&#20844;&#24179;&#24615;&#23646;&#24615;&#26159;&#38543;&#26426;&#30340;&#12290;&#22240;&#27492;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#24517;&#39035;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#20197;&#35780;&#20272;&#25152;&#23398;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#27979;&#35797;&#19981;&#20844;&#24179;&#24615;&#25552;&#20379;&#20102;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#32771;&#34385;&#21040;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#21363;&#24046;&#24322;&#24433;&#21709;&#65288;DI&#65289;&#21644;&#19981;&#20844;&#24179;&#24433;&#21709;&#65288;DM&#65289;&#24863;&#30693;&#30340;&#32447;&#24615;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware machine learning has garnered significant attention in recent years because of extensive use of machine learning in sensitive applications like judiciary systems. Various heuristics, and optimization frameworks have been proposed to enforce fairness in classification \cite{del2020review} where the later approaches either provides empirical results or provides fairness guarantee for the exact minimizer of the objective function \cite{celis2019classification}. In modern machine learning, Stochastic Gradient Descent (SGD) type algorithms are almost always used as training algorithms implying that the learned model, and consequently, its fairness properties are random. Hence, especially for crucial applications, it is imperative to construct Confidence Interval (CI) for the fairness of the learned model. In this work we provide CI for test unfairness when a group-fairness-aware, specifically, Disparate Impact (DI), and Disparate Mistreatment (DM) aware linear binary classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13940</link><description>&lt;p&gt;
1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#30340;&#20027;&#23548;-&#26368;&#23567;&#21270;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Majorization-Minimization Gauss-Newton Method for 1-Bit Matrix Completion. (arXiv:2304.13940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#20013;&#65292;&#26088;&#22312;&#20174;&#37096;&#20998;&#20108;&#36827;&#21046;&#35266;&#27979;&#20540;&#20013;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MMGN&#30340;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#65288;MM&#65289;&#21407;&#21017;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#20135;&#29983;&#19968;&#31995;&#21015;&#26631;&#20934;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#24378;&#21046;&#20551;&#23450;&#30340;&#20302;&#31209;&#32467;&#26500;&#30340;&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#21644;&#23545;&#23454;&#38469;&#25968;&#25454;&#30340;&#24212;&#29992;&#34920;&#26126;&#65292;MMGN&#36755;&#20986;&#30340;&#20272;&#35745;&#32467;&#26524;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#36739;&#20855;&#26377;&#21487;&#27604;&#24615;&#19988;&#26356;&#20934;&#30830;&#12289;&#36895;&#24230;&#36890;&#24120;&#26356;&#24555;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#30697;&#38453;&#30340;&#23574;&#38160;&#24230;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called MMGN. Our method is based on the majorization-minimization (MM) principle, which yields a sequence of standard low-rank matrix completion problems in our setting. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Our numerical studies and application to a real-data example illustrate that MMGN outputs comparable if not more accurate estimates, is often significantly faster, and is less sensitive to the spikiness of the underlying matrix than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#37197;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#37327;&#21270;&#26089;&#26399;RA&#20013;&#30340;JSN&#36827;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#20316;&#20026;&#30417;&#27979;&#20851;&#33410;&#31354;&#38388;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2304.13938</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31867;&#39118;&#28287;&#20851;&#33410;&#28814;&#20013;&#20851;&#33410;&#31354;&#38553;&#21464;&#31364;&#36827;&#23637;&#20934;&#30830;&#37327;&#21270;&#30340;&#28145;&#24230;&#37197;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Registration Method for Accurate Quantification of Joint Space Narrowing Progression in Rheumatoid Arthritis. (arXiv:2304.13938v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#37197;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#37327;&#21270;&#26089;&#26399;RA&#20013;&#30340;JSN&#36827;&#23637;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#20316;&#20026;&#30417;&#27979;&#20851;&#33410;&#31354;&#38388;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#39118;&#28287;&#24615;&#20851;&#33410;&#28814;&#65288;RA&#65289;&#26159;&#19968;&#31181;&#24930;&#24615;&#33258;&#36523;&#20813;&#30123;&#24615;&#28814;&#30151;&#24615;&#30142;&#30149;&#65292;&#23548;&#33268;&#20851;&#33410;&#36880;&#28176;&#30772;&#22351;&#21644;&#20005;&#37325;&#27531;&#30142;&#12290;&#20851;&#33410;&#31354;&#38553;&#21464;&#31364;&#65288;JSN&#65289;&#36827;&#23637;&#34987;&#35748;&#20026;&#26159;RA&#36827;&#23637;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#24182;&#21463;&#21040;&#25345;&#32493;&#20851;&#27880;&#12290;&#22312;RA&#30340;&#35786;&#26029;&#21644;&#30417;&#27979;&#20013;&#65292;&#25918;&#23556;&#23398;&#22312;&#30417;&#27979;&#20851;&#33410;&#31354;&#38388;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#23556;&#24615;&#24433;&#20687;&#37197;&#20934;&#26469;&#37327;&#21270;JSN&#36827;&#23637;&#65292;&#20174;&#32780;&#30417;&#27979;&#20851;&#33410;&#31354;&#38388;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#20248;&#28857;&#65292;&#20294;&#20943;&#23569;&#19981;&#21305;&#37197;&#21644;&#25552;&#39640;&#21487;&#38752;&#24615;&#31561;&#25361;&#25112;&#20173;&#38656;&#38754;&#23545;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21333;&#20307;&#20869;&#21018;&#24615;&#37197;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#33258;&#21160;&#37327;&#21270;&#26089;&#26399;RA&#20013;&#30340;JSN&#36827;&#23637;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#36816;&#21160;&#22270;&#20687;&#19982;&#22266;&#23450;&#22270;&#20687;&#20043;&#38388;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#22343;&#26041;&#35823;&#24046;&#20026;0.0031&#65292;&#26631;&#20934;&#20559;&#24046;&#20026;0.0661 mm&#65292;&#19981;&#21305;&#37197;&#29575;&#20026;0.48&#65285;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#23376;
&lt;/p&gt;
&lt;p&gt;
Rheumatoid arthritis (RA) is a chronic autoimmune inflammatory disease that results in progressive articular destruction and severe disability. Joint space narrowing (JSN) progression has been regarded as an important indicator for RA progression and has received sustained attention. In the diagnosis and monitoring of RA, radiology plays a crucial role to monitor joint space. A new framework for monitoring joint space by quantifying JSN progression through image registration in radiographic images has been developed. This framework offers the advantage of high accuracy, however, challenges do exist in reducing mismatches and improving reliability. In this work, a deep intra-subject rigid registration network is proposed to automatically quantify JSN progression in the early stage of RA. In our experiments, the mean-square error of Euclidean distance between moving and fixed image is 0.0031, standard deviation is 0.0661 mm, and the mismatching rate is 0.48\%. The proposed method has sub
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#35028;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13933</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#32676;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#38477;&#20302;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#23545;&#34920;&#29616;&#26356;&#22909;&#30340;&#23569;&#25968;&#26063;&#35028;&#36827;&#34892;&#36807;&#37319;&#26679;&#20250;&#31245;&#24494;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#36827;&#34892;&#21592;&#24037;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#23545;&#23569;&#25968;&#26063;&#35028;&#65288;&#40657;&#20154;&#21644;&#35199;&#29677;&#29273;&#35028;&#65289;&#36827;&#34892;&#20102;&#27424;&#37319;&#26679;&#21644;&#36807;&#37319;&#26679;&#65292;&#20197;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#24433;&#21709;&#27604;&#29575;&#65292;&#24182;&#35843;&#26597;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#33391;&#24433;&#21709;&#27604;&#29575;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#30003;&#35831;&#20154;&#30340;&#33258;&#25105;&#25253;&#21578;&#21644;&#38754;&#35797;&#35760;&#24405;&#65288;N = 2,501&#65289;&#35757;&#32451;&#20102;9,702&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#31579;&#36873;&#20915;&#31574;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#33391;&#24433;&#21709;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#21576;&#32447;&#24615;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#28040;&#38500;&#19981;&#33391;&#24433;&#21709;&#20165;&#31245;&#24494;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#32780;&#19988;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VisionGuard&#21644;&#22810;&#25968;&#25237;&#31080;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#33258;&#20027;&#31995;&#32479;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13919</link><description>&lt;p&gt;
&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Detection of Adversarial Physical Attacks in Time-Series Image Data. (arXiv:2304.13919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VisionGuard&#21644;&#22810;&#25968;&#25237;&#31080;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#33258;&#20027;&#31995;&#32479;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#33258;&#20027;&#31995;&#32479;&#20013;&#24120;&#35265;&#30340;&#24863;&#30693;&#27169;&#24577;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#23545;&#29615;&#22659;&#36827;&#34892;&#35821;&#20041;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;DNN&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25968;&#23383;&#21644;&#29289;&#29702;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#26816;&#27979;&#26694;&#26550;&#26469;&#26816;&#27979;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#26159;&#21542;&#21463;&#21040;&#23545;&#25239;&#24615;&#25968;&#23383;&#22122;&#22768;&#30340;&#25805;&#32437;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;VisionGuard&#65288;VG&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#30340;DNN&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#25915;&#20987;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VisionGuard *&#65288;VG&#65289;&#65292;&#23427;&#23558;VG&#19982;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#25915;&#20987;&#65292;&#20363;&#22914;&#35270;&#39057;&#12290;&#36825;&#21463;&#21040;&#33258;&#20027;&#31995;&#32479;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20351;&#29992;&#26426;&#36733;&#20256;&#24863;&#22120;&#38543;&#26102;&#38388;&#25910;&#38598;&#22270;&#20687;&#20197;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#22312;&#33258;&#20027;&#31995;&#32479;&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have become a common sensing modality in autonomous systems as they allow for semantically perceiving the ambient environment given input images. Nevertheless, DNN models have proven to be vulnerable to adversarial digital and physical attacks. To mitigate this issue, several detection frameworks have been proposed to detect whether a single input image has been manipulated by adversarial digital noise or not. In our prior work, we proposed a real-time detector, called VisionGuard (VG), for adversarial physical attacks against single input images to DNN models. Building upon that work, we propose VisionGuard* (VG), which couples VG with majority-vote methods, to detect adversarial physical attacks in time-series image data, e.g., videos. This is motivated by autonomous systems applications where images are collected over time using onboard sensors for decision-making purposes. We emphasize that majority-vote mechanisms are quite common in autonomous system ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.13917</link><description>&lt;p&gt;
&#27604;&#20363;&#20195;&#34920;&#24615;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20844;&#24179;&#27010;&#24565;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#32858;&#31867;&#65292;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#30784;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#25105;&#20204;&#35748;&#20026;&#35813;&#27010;&#24565;&#20197;&#19968;&#31181;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;&#26041;&#24335;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#20960;&#20010;&#29616;&#23384;&#27010;&#24565;&#30340;&#29702;&#30001;&#12290;&#20294;&#29616;&#26377;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#19981;&#33021;&#28385;&#36275;&#25105;&#20204;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#28385;&#36275;&#26080;&#32422;&#26463;&#32858;&#31867;&#21644;&#31163;&#25955;&#32858;&#31867;&#38382;&#39064;&#30340;PRF&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#36827;&#34892;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#38450;&#23433;&#20840;&#23041;&#32961;&#21644;&#26816;&#27979;&#28431;&#27934;&#20026;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.13905</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LSTM based IoT Device Identification. (arXiv:2304.13905v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LSTM&#36827;&#34892;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#38450;&#23433;&#20840;&#23041;&#32961;&#21644;&#26816;&#27979;&#28431;&#27934;&#20026;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#22823;&#37327;&#35774;&#22791;&#36896;&#25104;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#19979;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#25104;&#20026;&#37325;&#35201;&#30340;&#39044;&#38450;&#24615;&#23433;&#20840;&#25514;&#26045;&#65292;&#21487;&#20197;&#35782;&#21035;&#36825;&#20123;&#35774;&#22791;&#24182;&#26816;&#27979;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;Aalto&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using Long short-term memory (LSTM)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.13892</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23637;&#29616;&#20102;&#20174;&#39640;&#32500;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#26159;&#25163;&#24037;&#36741;&#21161;&#20219;&#21153;&#21644;&#20266;&#22870;&#21169;&#12290;&#33258;&#21160;&#21270;&#22320;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#65292;&#20197;&#26399;&#23454;&#29616;&#25511;&#21046;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35797;&#22270;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#34987;&#21457;&#29616;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#23545;&#20219;&#21153;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#25928;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20026;&#22797;&#26434;&#38598;&#32676;&#20998;&#24067;&#36866;&#37197;&#39640;&#26031;&#23376;&#38598;&#32676;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#26377;&#30340;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#25928;&#26524;&#20063;&#26356;&#22909;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.13886</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#22788;&#29702;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving the Utility of Differentially Private Clustering through Dynamical Processing. (arXiv:2304.13886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#25928;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20026;&#22797;&#26434;&#38598;&#32676;&#20998;&#24067;&#36866;&#37197;&#39640;&#26031;&#23376;&#38598;&#32676;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#26377;&#30340;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#25928;&#26524;&#20063;&#26356;&#22909;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#20219;&#21153;&#20013;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#38598;&#32676;&#26041;&#27861;&#65292;&#23545;&#20110;&#38750;&#20984;&#38598;&#32676;&#30340;&#32858;&#31867;&#25928;&#26524;&#36739;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#65292;&#25105;&#20204;&#23558;&#39640;&#26031;&#23376;&#38598;&#32676;&#25353;&#23618;&#27425;&#36830;&#25509;&#20197;&#36866;&#24212;&#26356;&#22797;&#26434;&#30340;&#38598;&#32676;&#20998;&#24067;&#12290;&#30001;&#20110;&#24046;&#20998;&#31169;&#26377;&#23376;&#32676;&#26159;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#33719;&#24471;&#30340;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32972;&#26223;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#24402;&#32435;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#25968;&#37327;&#30340;&#32676;&#38598;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to alleviate the trade-off between utility and privacy in the task of differentially private clustering. Existing works focus on simple clustering methods, which show poor clustering performance for non-convex clusters. By utilizing Morse theory, we hierarchically connect the Gaussian sub-clusters to fit complex cluster distributions. Because differentially private sub-clusters are obtained through the existing methods, the proposed method causes little or no additional privacy loss. We provide a theoretical background that implies that the proposed method is inductive and can achieve any desired number of clusters. Experiments on various datasets show that our framework achieves better clustering performance at the same privacy level, compared to the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;&#65292;&#24182;&#22312;SemEval-2023&#20219;&#21153;8&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13875</link><description>&lt;p&gt;
SemEval-2023&#31532;8&#39033;&#20219;&#21153;&#20013;&#30340;MasonNLP+&#65306;&#20351;&#29992;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models. (arXiv:2304.13875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21307;&#30103;&#38382;&#39064;&#12289;&#32463;&#39564;&#21644;&#22768;&#26126;&#65292;&#24182;&#22312;SemEval-2023&#20219;&#21153;8&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#35770;&#22363;&#20013;&#65292;&#29992;&#25143;&#20998;&#20139;&#20182;&#20204;&#30340;&#21307;&#30103;&#29366;&#20917;&#21644;&#27835;&#30103;&#32463;&#39564;&#65292;&#21253;&#25324;&#25552;&#20986;&#22768;&#26126;&#12289;&#25552;&#38382;&#20197;&#21450;&#35752;&#35770;&#27835;&#30103;&#23545;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#30340;&#24433;&#21709;&#12290;&#26500;&#24314;&#21487;&#29702;&#35299;&#36825;&#20123;&#20449;&#24687;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#30417;&#27979;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#24182;&#39564;&#35777;&#29992;&#25143;&#30340;&#22768;&#26126;&#12290; SemEval-2023&#30340;&#31532;8&#39033;&#20219;&#21153;&#19987;&#27880;&#20110;&#21307;&#23398;&#24212;&#29992;&#65292;&#20855;&#20307;&#21253;&#25324;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#24086;&#23376;&#20013;&#25552;&#21462;&#19982;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290; Reddit&#20581;&#24247;&#22312;&#32447;&#20132;&#27969;&#65288;RedHot&#65289;&#35821;&#26009;&#24211;&#21253;&#21547;&#26469;&#33258;&#19982;&#21307;&#30103;&#29366;&#20917;&#30456;&#20851;&#30340;subreddit&#30340;&#24086;&#23376;&#65292;&#24102;&#26377;&#34920;&#24449;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#30340;&#27880;&#37322;&#12290;&#22312;&#23376;&#20219;&#21153;1&#20013;&#65292;&#24739;&#32773;&#32463;&#39564;&#36890;&#36807;&#20010;&#20154;&#32463;&#39564;&#12289;&#38382;&#39064;&#21644;&#22768;&#26126;&#26469;&#25551;&#36848;&#12290;&#22312;&#23376;&#20219;&#21153;2&#20013;&#65292;&#21307;&#30103;&#29366;&#20917;&#36890;&#36807;&#20154;&#32676;&#12289;&#24178;&#39044;&#21644;&#32467;&#26524;&#26469;&#25551;&#36848;&#12290;&#20026;&#33258;&#21160;&#25552;&#21462;&#24739;&#32773;&#32463;&#39564;&#21644;&#21307;&#30103;&#29366;&#20917;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;MasonNLP+&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;SemEval-2023&#31532;8&#39033;&#20219;&#21153;&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online forums like Reddit, users share their experiences with medical conditions and treatments, including making claims, asking questions, and discussing the effects of treatments on their health. Building systems to understand this information can effectively monitor the spread of misinformation and verify user claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused on medical applications, specifically extracting patient experience- and medical condition-related entities from user posts on social media. The Reddit Health Online Talk (RedHot) corpus contains posts from medical condition-related subreddits with annotations characterizing the patient experience and medical conditions. In Subtask-1, patient experience is characterized by personal experience, questions, and claims. In Subtask-2, medical conditions are characterized by population, intervention, and outcome. For the automatic extraction of patient experiences and medical condition informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#23384;&#22312;&#30528;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#65292;&#36825;&#23545;&#20110;&#20108;&#20803;&#24773;&#20917;&#20013;&#30340;&#31639;&#27861;&#34892;&#20026;&#26377;&#30528;&#24456;&#24378;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13871</link><description>&lt;p&gt;
&#36830;&#32493;&#21644;&#31163;&#25955;&#26435;&#37325;&#19979;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20856;&#22411;&#19982;&#38750;&#20856;&#22411;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
Typical and atypical solutions in non-convex neural networks with discrete and continuous weights. (arXiv:2304.13871v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#23384;&#22312;&#30528;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#65292;&#36825;&#23545;&#20110;&#20108;&#20803;&#24773;&#20917;&#20013;&#30340;&#31639;&#27861;&#34892;&#20026;&#26377;&#30528;&#24456;&#24378;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#36127;&#36793;&#36317;&#24863;&#30693;&#22120;&#20316;&#20026;&#31616;&#21333;&#30340;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23398;&#20064;&#38543;&#26426;&#35268;&#21017;&#21644;&#20851;&#32852;&#26102;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#20960;&#20309;&#24418;&#24577;&#65292;&#24182;&#25214;&#21040;&#20102;&#37325;&#35201;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#26497;&#20026;&#24179;&#22374;&#21644;&#23485;&#24191;&#30340;&#20122;&#20248;&#35299;&#12290;&#36825;&#20123;&#20122;&#20248;&#35299;&#19982;&#20108;&#20803;&#24773;&#20917;&#19979;&#26080;&#27861;&#36882;&#24402;&#31639;&#27861;&#35775;&#38382;&#30340;&#22823;&#37327;&#34067;&#24310;&#23567;&#38598;&#32676;&#65288;&#20923;&#32467;1-RSB&#30456;&#65289;&#32452;&#25104;&#30340;&#20027;&#35201;&#35299;&#38598;&#20849;&#23384;&#65292;&#25110;&#32773;&#29699;&#38754;&#24773;&#20917;&#19979;&#19981;&#21516;&#22823;&#23567;&#32858;&#31867;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#23436;&#20840;RSB&#30456;&#65289;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#24403;&#20132;&#21449;&#19968;&#23450;&#23494;&#24230;&#32422;&#26463;&#30340;&#38408;&#20540;&#26102;&#65292;&#23485;&#24191;&#24179;&#22374;&#26497;&#23567;&#20540;&#30340;&#23616;&#37096;&#29109;&#21464;&#24471;&#38750;&#21333;&#35843;&#65292;&#34920;&#26126;&#40065;&#26834;&#35299;&#30340;&#31354;&#38388;&#34987;&#20998;&#25104;&#20102;&#19981;&#36830;&#36890;&#30340;&#32452;&#20214;&#12290;&#36825;&#23545;&#20108;&#20803;&#27169;&#22411;&#20013;&#31639;&#27861;&#30340;&#34892;&#20026;&#20135;&#29983;&#20102;&#24456;&#24378;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#35775;&#38382;&#21097;&#20313;&#30340;&#23396;&#31435;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the binary and continuous negative-margin perceptrons as simple non-convex neural network models learning random rules and associations. We analyze the geometry of the landscape of solutions in both models and find important similarities and differences. Both models exhibit subdominant minimizers which are extremely flat and wide. These minimizers coexist with a background of dominant solutions which are composed by an exponential number of algorithmically inaccessible small clusters for the binary case (the frozen 1-RSB phase) or a hierarchical structure of clusters of different sizes for the spherical case (the full RSB phase). In both cases, when a certain threshold in constraint density is crossed, the local entropy of the wide flat minima becomes non-monotonic, indicating a break-up of the space of robust solutions into disconnected components. This has a strong impact on the behavior of algorithms in binary models, which cannot access the remaining isolated clusters. For
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13865</link><description>&lt;p&gt;
highway2vec&#8212;&#8212;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#23558;OpenStreetMap&#24494;&#21306;&#22495;&#34920;&#31034;&#20986;&#26469;
&lt;/p&gt;
&lt;p&gt;
highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics. (arXiv:2304.13865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#23436;&#25104;&#19968;&#20123;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#38656;&#35201;&#32771;&#34385;&#31354;&#38388;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22320;&#22270;&#21306;&#22495;&#34920;&#31034;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#29305;&#24449;&#34920;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#30340;&#22320;&#22270;&#21306;&#22495;&#34920;&#31034;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenStreetMap&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#24494;&#21306;&#22495;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#21033;&#29992;H3&#31354;&#38388;&#32034;&#24341;&#23454;&#29616;&#21487;&#37325;&#22797;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#23398;&#20064;&#24182;&#33719;&#24471;&#30690;&#37327;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#22320;&#22270;&#20845;&#36793;&#24418;&#22312;&#21253;&#21547;&#30340;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years brought advancements in using neural networks for representation learning of various language or visual phenomena. New methods freed data scientists from hand-crafting features for common tasks. Similarly, problems that require considering the spatial variable can benefit from pretrained map region representations instead of manually creating feature tables that one needs to prepare to solve a task. However, very few methods for map area representation exist, especially with respect to road network characteristics. In this paper, we propose a method for generating microregions' embeddings with respect to their road infrastructure characteristics. We base our representations on OpenStreetMap road networks in a selection of cities and use the H3 spatial index to allow reproducible and scalable representation learning. We obtained vector representations that detect how similar map hexagons are in the road networks they contain. Additionally, we observe that embeddings yield a
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20276;&#38543;&#26041;&#27861;&#22312;AEM&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; NeuLag&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13860</link><description>&lt;p&gt;
&#21033;&#29992;&#20934;&#30830;&#30340;&#27169;&#25311;&#22120;&#21644;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#26696;&#22686;&#24378;&#21453;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates. (arXiv:2304.13860v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13860
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20276;&#38543;&#26041;&#27861;&#22312;AEM&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; NeuLag&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21453;&#38382;&#39064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#20854;&#20013;&#31070;&#32463;&#20276;&#38543;&#65288;NA&#65289;&#26041;&#27861;&#37319;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#65292;&#22312;&#20154;&#24037;&#30005;&#30913;&#26448;&#26009;&#65288;AEM&#65289;&#35774;&#35745;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#20934;&#30830;&#24615;&#23545;NA&#26041;&#27861;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#25311;&#22120;&#24222;&#22823;&#19988;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#26102;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#23454;&#29616;&#36275;&#22815;&#30340;&#20248;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20174;&#24037;&#31243;&#35282;&#24230;&#26469;&#30475;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#34892;&#20026;&#23578;&#26410;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#25311;&#22120;&#20934;&#30830;&#24615;&#23545;&#35299;&#20915;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#35299;&#20915;&#26041;&#26696;&#36234;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;NA&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21517;&#20026;&#31070;&#32463;&#25289;&#26684;&#26391;&#26085;&#65288;NeuLag&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning inverse techniques have attracted significant attention in recent years. Among them, the neural adjoint (NA) method, which employs a neural network surrogate simulator, has demonstrated impressive performance in the design tasks of artificial electromagnetic materials (AEM). However, the impact of the surrogate simulators' accuracy on the solutions in the NA method remains uncertain. Furthermore, achieving sufficient optimization becomes challenging in this method when the surrogate simulator is large, and computational resources are limited. Additionally, the behavior under constraints has not been studied, despite its importance from the engineering perspective. In this study, we investigated the impact of surrogate simulators' accuracy on the solutions and discovered that the more accurate the surrogate simulator is, the better the solutions become. We then developed an extension of the NA method, named Neural Lagrangian (NeuLag) method, capable of efficiently optimizi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;(MCAS)&#65292;&#29992;&#20110;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#19982;&#21333;&#27169;&#24577;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#27979;&#37327;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.13855</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;&#65306;&#34913;&#37327;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models. (arXiv:2304.13855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;(MCAS)&#65292;&#29992;&#20110;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#19982;&#21333;&#27169;&#24577;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#27979;&#37327;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#20687;DALL-E&#21644;Stable Diffusion&#36825;&#26679;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25104;&#21151;&#22320;&#20174;&#25991;&#26412;&#20013;&#21019;&#24314;&#22270;&#20687;&#65292;&#32463;&#24120;&#32467;&#21512;&#25277;&#35937;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#21453;&#26144;&#20102;&#20174;&#20114;&#32852;&#32593;&#20013;&#29228;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#25163;&#21160;&#23457;&#26680;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#19988;&#30001;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#25509;&#21463;&#30340;&#36755;&#20837;&#30340;&#26080;&#38480;&#21046;&#21644;&#19981;&#21463;&#32422;&#26463;&#30340;&#24615;&#36136;&#20351;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#12290; &#23545;&#20110;&#20559;&#35265;&#30340;&#24230;&#37327;&#19982;&#37327;&#21270;&#30340;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#21333;&#27169;&#24577;&#30340;&#23567;&#22411;&#21333;&#38454;&#27573;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22810;&#38454;&#27573;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20986;&#29616;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22797;&#21512;&#20851;&#32852;&#35780;&#20998;&#65288;MCAS&#65289;&#20316;&#20026;&#34913;&#37327;&#22810;&#27169;&#24577;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#24046;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;MCAS&#35780;&#20272;DALL-E 2&#21644;&#31283;&#23450;&#25193;&#25955;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#27979;&#24182;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#20013;&#24615;&#21035;&#20559;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DALL-E 2&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#26368;&#23567;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32780;&#31283;&#23450;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;MCAS&#25351;&#26631;&#20801;&#35768;&#23545;&#22823;&#22411;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative multimodal models based on diffusion models have seen tremendous growth and advances in recent years. Models such as DALL-E and Stable Diffusion have become increasingly popular and successful at creating images from texts, often combining abstract ideas. However, like other deep learning models, they also reflect social biases they inherit from their training data, which is often crawled from the internet. Manually auditing models for biases can be very time and resource consuming and is further complicated by the unbounded and unconstrained nature of inputs these models can take. Research into bias measurement and quantification has generally focused on small single-stage models working on a single modality. Thus the emergence of multistage multimodal models requires a different approach. In this paper, we propose Multimodal Composite Association Score (MCAS) as a new method of measuring gender bias in multimodal generative models. Evaluating both DALL-E 2 and Stable Diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#23558;&#19981;&#21516;&#27169;&#22411;&#32452;&#21512;&#26469;&#33258;&#21160;&#20998;&#31867;&#21830;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#19981;&#38169;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#24179;&#22343;F1&#24471;&#20998;&#20026;0.82&#12290;</title><link>http://arxiv.org/abs/2304.13852</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24066;&#22330;&#19978;&#23545;&#21830;&#21697;&#36827;&#34892;&#20998;&#31867;&#65306;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Categorising Products in an Online Marketplace: An Ensemble Approach. (arXiv:2304.13852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#23558;&#19981;&#21516;&#27169;&#22411;&#32452;&#21512;&#26469;&#33258;&#21160;&#20998;&#31867;&#21830;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#19981;&#38169;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#24179;&#22343;F1&#24471;&#20998;&#20026;0.82&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21830;&#21697;&#20998;&#31867;&#19968;&#30452;&#26159;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#38754;&#20020;&#30340;&#20849;&#21516;&#38382;&#39064;&#65292;&#20182;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#23454;&#29616;&#33258;&#21160;&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#32452;&#21512;&#26469;&#21333;&#29420;&#39044;&#27979;&#27599;&#20010;&#20135;&#21697;&#30340;&#31867;&#21035;&#12289;&#23376;&#31867;&#21035;&#21644;&#39068;&#33394;&#65292;&#26368;&#21518;&#23558;&#27599;&#20010;&#20135;&#21697;&#30340;&#32467;&#26524;&#39044;&#27979;&#27719;&#24635;&#12290;&#36890;&#36807;&#19978;&#36848;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;XGBoost&#21644;k&#36817;&#37051;&#32467;&#21512;&#26469;&#39044;&#27979;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#24179;&#22343;F1&#24471;&#20998;&#20026;0.82&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, product categorisation has been a common issue for E-commerce companies who have utilised machine learning to categorise their products automatically. In this study, we propose an ensemble approach, using a combination of different models to separately predict each product's category, subcategory, and colour before ultimately combining the resultant predictions for each product. With the aforementioned approach, we show that an average F1-score of 0.82 can be achieved using a combination of XGBoost and k-nearest neighbours to predict said features.
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13850</link><description>&lt;p&gt;
SSL&#27169;&#22411;&#26159;&#21542;&#24863;&#21040;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#65311;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13850
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#23558;&#33258;&#28982;&#22270;&#20687;&#30340;&#19981;&#21516;&#37096;&#20998;&#30456;&#20114;&#20851;&#32852;&#26469;&#20135;&#29983;&#26377;&#29992;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#25512;&#21521;&#26497;&#31471;&#26102;&#65292;SSL&#27169;&#22411;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20851;&#32852;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;SSL&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#29616;&#35937;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#27169;&#22411;&#21644;&#19968;&#20010;&#20165;&#21253;&#21547;&#32972;&#26223;&#65288;&#22914;&#27700;&#12289;&#22825;&#31354;&#12289;&#33609;&#22320;&#65289;&#30340;&#35757;&#32451;&#22270;&#20687;&#35009;&#21098;&#21518;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#25110;&#29978;&#33267;&#35270;&#35273;&#37325;&#26500;&#22320;&#25512;&#26029;&#20986;&#21069;&#26223;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#26159;&#19981;&#21516;SSL&#31639;&#27861;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#24182;&#19988;&#20250;&#22240;&#26576;&#20123;&#35774;&#35745;&#36873;&#25321;&#32780;&#24694;&#21270;&#65292;&#32780;&#19988;&#19981;&#33021;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#34920;&#31034;&#36136;&#37327;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#12290;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#19979;&#19968;&#20010;token&#65292;&#20026;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13840</link><description>&lt;p&gt;
&#38754;&#21521;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#30340;Verilog&#33258;&#21160;&#23436;&#25104;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation. (arXiv:2304.13840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#19979;&#19968;&#20010;token&#65292;&#20026;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#21160;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#28385;&#36275;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#30005;&#23376;&#35774;&#22791;&#30340;&#35774;&#35745;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290; Verilog&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25968;&#23383;&#30005;&#36335;&#35774;&#35745;&#21644;&#39564;&#35777;&#30340;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;EDA&#24037;&#20855;&#36827;&#34892;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#32534;&#20889;&#20195;&#30721;&#26159;&#19968;&#39033;&#37325;&#22797;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;Verilog&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#65292;&#20854;&#27425;&#25552;&#20379;&#20102;&#20174;&#24320;&#28304;&#23384;&#20648;&#24211;&#33719;&#21462;&#30340;Verilog&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#19978;&#65292;&#28982;&#21518;&#22312;&#30456;&#20284;&#20110;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;&#27604;&#36739;&#22312;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#35757;&#32451;&#19981;&#21516;&#23376;&#38598;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#25552;&#35758;&#30340;Verilog&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;BLEU&#65292;ROUGE-L&#21644;chrF&#24471;&#20998;&#65292;&#24182;&#19988;&#26377;&#25928;&#39044;&#27979;&#20102;&#37096;&#20998;&#32534;&#20889;&#30340;Verilog&#35821;&#21477;&#30340;&#19979;&#19968;&#20010;token&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative Electronic Design Automation (EDA) solutions are important to meet the design requirements for increasingly complex electronic devices. Verilog, a hardware description language, is widely used for the design and verification of digital circuits and is synthesized using specific EDA tools. However, writing code is a repetitive and time-intensive task. This paper proposes, primarily, a novel deep learning framework for training a Verilog autocompletion model and, secondarily, a Verilog dataset of files and snippets obtained from open-source repositories. The framework involves integrating models pretrained on general programming language data and finetuning them on a dataset curated to be similar to a target downstream task. This is validated by comparing different pretrained models trained on different subsets of the proposed Verilog dataset using multiple evaluation metrics. These experiments demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF sco
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#32500;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13833</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussian process experts based on kernel stick-breaking processes. (arXiv:2304.13833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13833
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#32500;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31867;&#33021;&#21516;&#26102;&#35299;&#20915;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#21487;&#25193;&#23637;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#20316;&#20026;&#38376;&#20989;&#25968;&#30340;&#27169;&#22411;&#33021;&#22815;&#30452;&#35266;&#22320;&#35299;&#37322;&#21644;&#33258;&#21160;&#36873;&#25321;&#28151;&#21512;&#29289;&#20013;&#19987;&#23478;&#30340;&#25968;&#37327;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#22312;&#24863;&#30693;&#38750;&#24179;&#31283;&#24615;&#12289;&#22810;&#27169;&#24615;&#21644;&#24322;&#26041;&#24046;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#38376;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#22312;&#24212;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#30456;&#20851;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#25991;&#29486;&#20013;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#26032;&#22411;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20351;&#20854;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21518;&#39564;&#35745;&#31639;&#30340;&#20999;&#29255;&#25277;&#26679;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussian process experts is a class of models that can simultaneously address two of the key limitations inherent in standard Gaussian processes: scalability and predictive performance. In particular, models that use Dirichlet processes as gating functions permit straightforward interpretation and automatic selection of the number of experts in a mixture. While the existing models are intuitive and capable of capturing non-stationarity, multi-modality and heteroskedasticity, the simplicity of their gating functions may limit the predictive performance when applied to complex data-generating processes. Capitalising on the recent advancement in the dependent Dirichlet processes literature, we propose a new mixture model of Gaussian process experts based on kernel stick-breaking processes. Our model maintains the intuitive appeal yet improve the performance of the existing models. To make it practical, we design a sampler for posterior computation based on the slice sampling. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#21270;&#36172;&#21338;&#38382;&#39064;&#20013;&#23545;&#26680;&#20989;&#25968;&#35268;&#21017;&#24615;&#38169;&#35823;&#30340;&#33258;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#35268;&#21017;&#24615;&#30340;&#19968;&#23545;RKHS&#20013;&#21516;&#26102;&#23454;&#29616;&#26368;&#20339;&#32047;&#35745;&#36951;&#25022;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#24182;&#36890;&#36807;&#29616;&#26377;&#31639;&#27861;&#32467;&#21512;&#26497;&#23567;&#21270;&#38750;&#33258;&#36866;&#24212;&#30340;&#26680;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#19979;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13830</link><description>&lt;p&gt;
&#26680;&#21270;&#36172;&#21338;&#26426;&#31639;&#27861;&#20013;&#23545;&#26680;&#20989;&#25968;&#35268;&#24459;&#24615;&#38169;&#35823;&#30340;&#33258;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptation to Misspecified Kernel Regularity in Kernelised Bandits. (arXiv:2304.13830v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#21270;&#36172;&#21338;&#38382;&#39064;&#20013;&#23545;&#26680;&#20989;&#25968;&#35268;&#21017;&#24615;&#38169;&#35823;&#30340;&#33258;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#35268;&#21017;&#24615;&#30340;&#19968;&#23545;RKHS&#20013;&#21516;&#26102;&#23454;&#29616;&#26368;&#20339;&#32047;&#35745;&#36951;&#25022;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#24182;&#36890;&#36807;&#29616;&#26377;&#31639;&#27861;&#32467;&#21512;&#26497;&#23567;&#21270;&#38750;&#33258;&#36866;&#24212;&#30340;&#26680;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#19979;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#27494;&#35013;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#26524;&#24213;&#23618;&#20989;&#25968;&#20301;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#65292;&#21363;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#26159;&#65292;&#22914;&#26524;&#30456;&#20851;&#30340;&#26680;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#26159;&#26410;&#30693;&#30340;&#65292;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22810;&#20040;&#22909;&#22320;&#36866;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24179;&#31227;&#19981;&#21464;&#26680;&#35268;&#21017;&#24615;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#22312;&#36172;&#21338;&#35774;&#32622;&#20013;&#65292;&#35813;&#35268;&#21017;&#24615;&#30001;&#26680;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#34928;&#20943;&#29575;&#25152;&#25551;&#36848;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#24615;&#30340;&#19979;&#38480;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#35268;&#21017;&#24615;&#30340;&#19968;&#23545;RKHS&#20013;&#21516;&#26102;&#23454;&#29616;&#26368;&#20339;&#32047;&#35745;&#36951;&#25022;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#19979;&#38480;&#30340;&#32039;&#23494;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#36172;&#21338;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#19982;&#26497;&#23567;&#21270;&#38750;&#33258;&#36866;&#24212;&#30340;&#26680;&#36172;&#21338;&#26426;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#22312;&#24635;&#27493;&#25968;T&#30340;&#20381;&#36182;&#19979;&#21305;&#37197;&#20102;&#19979;&#38480;&#65292;&#38500;&#20102;&#23545;&#25968;&#22240;&#23376;&#12290;&#36890;&#36807;&#22635;&#20889;RKHS&#20043;&#38388;&#36866;&#24212;&#24615;&#30340;&#36951;&#25022;&#30028;&#65292;&#25105;&#20204;&#36830;&#25509;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continuum-armed bandit problems where the underlying function resides in a reproducing kernel Hilbert space (RKHS), namely, the kernelised bandit problems, an important open problem remains of how well learning algorithms can adapt if the regularity of the associated kernel function is unknown. In this work, we study adaptivity to the regularity of translation-invariant kernels, which is characterized by the decay rate of the Fourier transformation of the kernel, in the bandit setting. We derive an adaptivity lower bound, proving that it is impossible to simultaneously achieve optimal cumulative regret in a pair of RKHSs with different regularities. To verify the tightness of this lower bound, we show that an existing bandit model selection algorithm applied with minimax non-adaptive kernelised bandit algorithms matches the lower bound in dependence of $T$, the total number of steps, except for log factors. By filling in the regret bounds for adaptivity between RKHSs, we connect the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#31435;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20248;&#21270;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#26469;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13812</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#30340;&#37327;&#21270;&#35823;&#24046;&#35745;&#31639;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Quantization Error Computation for Neural Network Model Compression. (arXiv:2304.13812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#31435;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20248;&#21270;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#26469;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#24037;&#19994;&#31995;&#32479;&#20013;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#38382;&#39064;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24102;&#26377;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20445;&#35777;&#36755;&#20986;&#35823;&#24046;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#37327;&#21270;&#29256;&#26412;&#21512;&#24182;&#65292;&#20197;&#20135;&#29983;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#31934;&#30830;&#36755;&#20986;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21644;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#21040;&#21512;&#24182;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35745;&#31639;&#20445;&#35777;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#20363;&#23376;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#39640;&#25928;&#28151;&#21512;&#33258;&#21160;&#26426;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#25429;&#25417;&#26410;&#30693;&#30340;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#38598;&#21512;&#20540;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#21487;&#36798;&#38598;&#21512;&#35745;&#31639;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#24314;&#27169;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13811</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28151;&#21512;&#33258;&#21160;&#26426;&#26694;&#26550;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Hybrid Automaton Framework to Modeling Complex Dynamical Systems. (arXiv:2304.13811v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#39640;&#25928;&#28151;&#21512;&#33258;&#21160;&#26426;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#25429;&#25417;&#26410;&#30693;&#30340;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#38598;&#21512;&#20540;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#21487;&#36798;&#38598;&#21512;&#35745;&#31639;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#24314;&#27169;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#28151;&#21512;&#33258;&#21160;&#26426;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26410;&#30693;&#30340;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#34892;&#20026;&#12290;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#34987;&#26377;&#25928;&#21010;&#20998;&#20026;&#19982;&#20854;&#25299;&#25169;&#23545;&#24212;&#30340;&#32452;&#65292;&#24182;&#22522;&#20110;&#27492;&#23450;&#20041;&#36716;&#25442;&#38376;&#12290;&#28982;&#21518;&#65292;&#19968;&#32452;&#35745;&#31639;&#39640;&#25928;&#30340;&#23567;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#20026;&#20854;&#30456;&#24212;&#25299;&#25169;&#30340;&#23616;&#37096;&#21160;&#24577;&#25551;&#36848;&#12290;&#22312;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#33258;&#21160;&#26426;&#23545;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#20043;&#21518;&#65292;&#22522;&#20110;&#21306;&#38388;&#20998;&#26512;&#21644;&#20998;&#35010;&#21512;&#24182;&#36807;&#31243;&#25552;&#20379;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#38598;&#21512;&#20540;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#26497;&#38480;&#29615;&#30340;&#25968;&#20540;&#31034;&#20363;&#35828;&#26126;&#20102;&#24320;&#21457;&#30340;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#21487;&#36798;&#38598;&#21512;&#35745;&#31639;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20219;&#20309;&#24314;&#27169;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a computationally efficient data-driven hybrid automaton model is proposed to capture unknown complex dynamical system behaviors using multiple neural networks. The sampled data of the system is divided by valid partitions into groups corresponding to their topologies and based on which, transition guards are defined. Then, a collection of small-scale neural networks that are computationally efficient are trained as the local dynamical description for their corresponding topologies. After modeling the system with a neural-network-based hybrid automaton, the set-valued reachability analysis with low computation cost is provided based on interval analysis and a split and combined process. At last, a numerical example of the limit cycle is presented to illustrate that the developed models can significantly reduce the computational cost in reachable set computation without sacrificing any modeling precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#20445;&#35777;&#20102;&#36739;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13800</link><description>&lt;p&gt;
&#28508;&#25351;&#32441;&#35782;&#21035;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#23884;&#20837;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Latent Fingerprint Recognition: Fusion of Local and Global Embeddings. (arXiv:2304.13800v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#20445;&#35777;&#20102;&#36739;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25351;&#32441;&#35782;&#21035;&#20013;&#65292;&#19968;&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#30830;&#23450;&#19982;&#29359;&#32618;&#29616;&#22330;&#30041;&#19979;&#30340;&#37096;&#20998;&#21644;&#27169;&#31946;&#30340;&#25351;&#32441;&#65288;&#21363;&#28508;&#25351;&#32441;&#25110;&#25351;&#32441;&#30165;&#36857;&#65289;&#30456;&#20851;&#32852;&#30340;&#23244;&#30097;&#20154;&#36523;&#20221;&#12290;&#26412;&#25991;&#23558;&#20840;&#23616;&#23884;&#20837;&#19982;&#23616;&#37096;&#23884;&#20837;&#30456;&#32467;&#21512;&#65292;&#20026;&#28508;&#22312;&#21367;&#31215;&#21644;&#25293;&#25171;&#25351;&#32441;&#21305;&#37197;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#36825;&#31181;&#23616;&#37096;&#21644;&#20840;&#23616;&#34920;&#31034;&#30340;&#32452;&#21512;&#20351;&#35782;&#21035;&#20934;&#30830;&#29575;&#22312;NIST SD 27&#12289;NIST SD 302&#12289;MSP&#12289;MOLF DB1/DB4&#21644;MOLF DB2/DB4&#28508;&#25351;&#32441;&#25968;&#25454;&#38598;&#19978;&#37117;&#24471;&#21040;&#20102;&#26174;&#30528;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging problems in fingerprint recognition continues to be establishing the identity of a suspect associated with partial and smudgy fingerprints left at a crime scene (i.e., latent prints or fingermarks). Despite the success of fixed-length embeddings for rolled and slap fingerprint recognition, the features learned for latent fingerprint matching have mostly been limited to local minutiae-based embeddings and have not directly leveraged global representations for matching. In this paper, we combine global embeddings with local embeddings for state-of-the-art latent to rolled matching accuracy with high throughput. The combination of both local and global representations leads to improved recognition accuracy across NIST SD 27, NIST SD 302, MSP, MOLF DB1/DB4, and MOLF DB2/DB4 latent fingerprint datasets for both closed-set (84.11%, 54.36%, 84.35%, 70.43%, 62.86% rank-1 retrieval rate, respectively) and open-set (0.50, 0.74, 0.44, 0.60, 0.68 FNIR at FPIR=0.02, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#39044;&#27979;&#26612;&#27833;&#26426;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20026;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13799</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#26612;&#27833;&#26426;&#27668;&#20307;&#27969;&#21160;&#21160;&#21147;&#23398;&#21644;&#26410;&#30693;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks for predicting gas flow dynamics and unknown parameters in diesel engines. (arXiv:2304.13799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#39044;&#27979;&#26612;&#27833;&#26426;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20026;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#26612;&#27833;&#26426;&#20581;&#24247;&#30417;&#27979;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#21457;&#21160;&#26426;&#21160;&#21147;&#23398;&#65292;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#24182;&#39044;&#27979;&#32500;&#25252;&#38656;&#27714;&#12290;PINN&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#20960;&#20309;&#28065;&#36718;&#22686;&#21387;&#22120;&#21644;&#24223;&#27668;&#20877;&#24490;&#29615;&#30340;&#26612;&#27833;&#26426;&#65292;&#20351;&#29992;&#36873;&#23450;&#29366;&#24577;&#21464;&#37327;&#30340;&#27979;&#37327;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#19979;&#65292;PINN&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#22320;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#33258;&#36866;&#24212;&#26435;&#37325;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21487;&#21152;&#36895;&#25910;&#25947;&#12290;&#36825;&#20123;&#27169;&#25311;&#30340;&#36755;&#20837;&#25968;&#25454;&#26469;&#33258;&#23454;&#38469;&#21457;&#21160;&#26426;&#36816;&#34892;&#26465;&#20214;&#65292;&#32780;&#36755;&#20986;&#25968;&#25454;&#26159;&#27169;&#25311;&#25968;&#25454;&#65292;&#20351;&#36825;&#25104;&#20026;PINN&#39044;&#27979;&#30495;&#23454;&#21160;&#24577;&#31995;&#32479;&#33021;&#21147;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#26612;&#27833;&#26426;&#30340;&#24179;&#22343;&#20540;&#27169;&#22411;&#21253;&#25324;&#32463;&#39564;&#20844;&#24335;&#26469;&#34920;&#31034;&#26576;&#20123;&#29366;&#24577;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;PINN&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#21160;&#24577;&#24182;&#35782;&#21035;&#26410;&#30693;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a physics-informed neural network (PINN) approach for monitoring the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown parameters in a "mean value" model, and anticipate maintenance requirements. The PINN model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas recirculation, using measurement data of selected state variables. The results demonstrate the ability of the PINN model to predict simultaneously both unknown parameters and dynamics accurately with both clean and noisy data, and the importance of the self-adaptive weight in the loss function for faster convergence. The input data for these simulations are derived from actual engine running conditions, while the outputs are simulated data, making this a practical case study of PINN's ability to predict real-world dynamical systems. The mean value model of the diesel engine incorporates empirical formulae to represent certain states, but the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#26102;&#38388;&#31354;&#38388;&#25968;&#25454;&#20013;&#20381;&#36182;&#20851;&#31995;&#30340;&#24191;&#20041;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GGLM&#65289;&#21442;&#25968;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20351;&#29992;&#21333;&#35843;&#36816;&#31639;&#31526;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#26041;&#27861;&#20811;&#26381;&#20102;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#38750;&#20984;&#24615;&#24182;&#20026;&#21442;&#25968;&#24674;&#22797;&#25552;&#20379;&#20445;&#35777;</title><link>http://arxiv.org/abs/2304.13793</link><description>&lt;p&gt;
&#24191;&#20041;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65306;&#20984;&#20272;&#35745;&#21644;&#22312;&#32447;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalized generalized linear models: Convex estimation and online bounds. (arXiv:2304.13793v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13793
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#26102;&#38388;&#31354;&#38388;&#25968;&#25454;&#20013;&#20381;&#36182;&#20851;&#31995;&#30340;&#24191;&#20041;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GGLM&#65289;&#21442;&#25968;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20351;&#29992;&#21333;&#35843;&#36816;&#31639;&#31526;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#26041;&#27861;&#20811;&#26381;&#20102;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#38750;&#20984;&#24615;&#24182;&#20026;&#21442;&#25968;&#24674;&#22797;&#25552;&#20379;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#24191;&#20041;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GGLM&#65289;&#20013;&#30340;&#21442;&#25968;&#12290;&#36825;&#26159;&#19968;&#31867;&#23558;&#27969;&#34892;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#25193;&#23637;&#21040;&#32771;&#34385;&#26102;&#31354;&#25968;&#25454;&#20013;&#35266;&#27979;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#21333;&#35843;&#36816;&#31639;&#31526;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#26041;&#27861;&#26469;&#20811;&#26381;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#38750;&#20984;&#24615;&#24182;&#20026;&#21442;&#25968;&#24674;&#22797;&#25552;&#20379;&#20445;&#35777;&#12290;&#32467;&#26524;&#21487;&#20197;&#24212;&#29992;&#20110;GLM&#21644;GGLM&#65292;&#37325;&#28857;&#20851;&#27880;&#26102;&#31354;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#38789;&#38598;&#20013;&#19981;&#31561;&#24335;&#25552;&#20379;&#20102;&#22312;&#32447;&#23454;&#20363;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#27169;&#25311;&#21644;&#37326;&#28779;&#20107;&#20214;&#30340;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#26469;&#23637;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new computational framework for estimating parameters in generalized generalized linear models (GGLM), a class of models that extends the popular generalized linear models (GLM) to account for dependencies among observations in spatio-temporal data. The proposed approach uses a monotone operator-based variational inequality method to overcome non-convexity in parameter estimation and provide guarantees for parameter recovery. The results can be applied to GLM and GGLM, focusing on spatio-temporal models. We also present online instance-based bounds using martingale concentrations inequalities. Finally, we demonstrate the performance of the algorithm using numerical simulations and a real data example for wildfire incidents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.13787</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#29615;&#22659;&#21644;&#29992;&#25143;&#19979;&#35780;&#20272;&#21644;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#32570;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#20219;&#21153;&#30340;&#31995;&#32479;&#22833;&#25928;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#31574;&#30053;&#21644;&#20154;&#31867;&#34892;&#20026;&#26469;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#30340;&#22330;&#26223;&#12290;&#36825;&#20123;&#35780;&#20272;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#26469;&#22686;&#24378;&#22330;&#26223;&#29983;&#25104;&#31995;&#32479;&#30340;&#24314;&#35758;&#12290;&#22312;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#22495;&#21644;&#26356;&#22797;&#26434;&#30340;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26367;&#20195;&#27169;&#22411;&#36741;&#21161;&#30340;&#22330;&#26223;&#29983;&#25104;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25925;&#38556;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#20114;&#20013;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36317;&#31163;&#21152;&#26435;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;&#20132;&#20114;&#25968;&#25454;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#36739;&#20043;&#20197;&#24448;&#30340;&#30417;&#30563;&#26041;&#27861;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13774</link><description>&lt;p&gt;
&#31163;&#32447;&#20132;&#20114;&#25968;&#25454;&#30340;&#36317;&#31163;&#21152;&#26435;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distance Weighted Supervised Learning for Offline Interaction Data. (arXiv:2304.13774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36317;&#31163;&#21152;&#26435;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;&#20132;&#20114;&#25968;&#25454;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#36739;&#20043;&#20197;&#24448;&#30340;&#30417;&#30563;&#26041;&#27861;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20915;&#31574;&#31639;&#27861;&#36890;&#24120;&#24456;&#38590;&#21033;&#29992;&#19981;&#21516;&#26469;&#28304;&#30340;&#38750;&#32467;&#26500;&#21270;&#31163;&#32447;&#20132;&#20114;&#25968;&#25454;&#12290;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#26159;&#31283;&#20581;&#30340;&#65292;&#20294;&#38656;&#35201;&#25910;&#38598;&#26368;&#20339;&#28436;&#31034;&#65292;&#32780;&#36825;&#24456;&#38590;&#12290;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25215;&#35834;&#20174;&#27425;&#20248;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#38754;&#23545;&#39640;&#32500;&#25968;&#25454;&#26102;&#38754;&#20020;&#20248;&#21270;&#25361;&#25112;&#12290;&#20026;&#24357;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36317;&#31163;&#21152;&#26435;&#30417;&#30563;&#23398;&#20064;&#25110;DWSL&#65292;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;DWSL&#21482;&#29992;&#30417;&#30563;&#23398;&#20064;&#27169;&#25311;&#31163;&#32447;&#25968;&#25454;&#29366;&#24577;&#20043;&#38388;&#30340;&#25152;&#26377;&#26102;&#38388;&#27493;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#20998;&#24067;&#26469;&#36817;&#20284;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#12290;&#20026;&#20102;&#25552;&#21462;&#31574;&#30053;&#65292;&#25105;&#20204;&#36890;&#36807;&#23427;&#20204;&#22312;&#36317;&#31163;&#20272;&#35745;&#20013;&#30340;&#20943;&#23569;&#31243;&#24230;&#26469;&#21152;&#26435;&#34892;&#21160;&#12290;&#29702;&#35770;&#19978;&#65292;DWSL&#25910;&#25947;&#20110;&#21463;&#25968;&#25454;&#20998;&#24067;&#32422;&#26463;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#23545;&#20110;&#31163;&#32447;&#23398;&#20064;&#30340;&#20195;&#29702;&#20154;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#23646;&#24615;&#12290;&#23454;&#36341;&#19978;&#65292;DWSL&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#26631;&#20934;&#22522;&#20934;&#19978;&#19982;&#31163;&#32447;RL&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised learning are robust, but require optimal demonstrations, which are hard to collect. Offline goal-conditioned reinforcement learning (RL) algorithms promise to learn from sub-optimal data, but face optimization challenges especially with high-dimensional data. To bridge the gap between IL and RL, we introduce Distance Weighted Supervised Learning or DWSL, a supervised method for learning goal-conditioned policies from offline data. DWSL models the entire distribution of time-steps between states in offline data with only supervised learning, and uses this distribution to approximate shortest path distances. To extract a policy, we weight actions by their reduction in distance estimates. Theoretically, DWSL converges to an optimal policy constrained to the data distribution, an attractive property for offline lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#20197;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#21644;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.13764</link><description>&lt;p&gt;
&#25581;&#31034;&#24040;&#22124;&#32454;&#32990;&#21534;&#22124;&#20316;&#29992;&#65306;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20998;&#26512;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis. (arXiv:2304.13764v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#20197;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#21644;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21160;&#24577;&#26080;&#26579;&#33394;&#32454;&#32990;&#30340;&#21534;&#22124;&#20316;&#29992;&#23545;&#20110;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#30456;&#34924;&#26174;&#24494;&#38236;&#35270;&#39057;&#26102;&#65292;&#27979;&#37327;&#24555;&#36895;&#32454;&#32990;&#30456;&#20114;&#20316;&#29992;&#21644;&#21306;&#20998;&#32454;&#32990;&#19982;&#32972;&#26223;&#20351;&#24471;&#36825;&#39033;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#22810;&#21151;&#33021;&#30340;&#23454;&#26102;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#27169;&#22359;&#20197;&#25269;&#28040;&#21487;&#33021;&#30340;&#26174;&#24494;&#38236;&#36816;&#21160;&#21644;&#24103;&#27169;&#31946;&#31561;&#25200;&#21160;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#19982;&#40657;&#21283;&#23376;&#31639;&#27861;&#30456;&#27604;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#21253;&#25324;&#20004;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#65306;&#35270;&#35273;&#35828;&#26126;&#21644;&#27169;&#22411;&#31616;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#19981;&#26159;&#39640;&#24615;&#33021;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#25552;&#20379;&#24517;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the phagocytosis of dynamic, unstained cells is essential for evaluating neurodegenerative diseases. However, measuring rapid cell interactions and distinguishing cells from backgrounds make this task challenging when processing time-lapse phase-contrast video microscopy. In this study, we introduce a fully automated, scalable, and versatile realtime framework for quantifying and analyzing phagocytic activity. Our proposed pipeline can process large data-sets and includes a data quality verification module to counteract potential perturbations such as microscope movements and frame blurring. We also propose an explainable cell segmentation module to improve the interpretability of deep learning methods compared to black-box algorithms. This includes two interpretable deep learning capabilities: visual explanation and model simplification. We demonstrate that interpretability in deep learning is not the opposite of high performance, but rather provides essential deep learnin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13761</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13761
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#21644;&#27491;&#21017;&#21270;&#25552;&#39640;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#23545;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#36827;&#34892;&#25311;&#21512;&#21487;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#20013;&#30340;&#23567;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#29420;&#28909;&#32534;&#30721;&#23558;GBDT&#27169;&#22411;&#36716;&#25442;&#20026;&#32447;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26641;&#21494;&#32534;&#30721;&#20026;&#19968;&#20010;&#34394;&#25311;&#21464;&#37327;&#12290;&#36825;&#20801;&#35768;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#20998;&#35299;&#26041;&#27861;&#26469;&#35780;&#20272;GBDT&#27169;&#22411;&#23545;&#21327;&#21464;&#37327;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#37325;&#26032;&#25311;&#21512;&#20854;&#24102;&#26377;$L_1$&#25110;$L_2$&#27491;&#21017;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#24418;&#24335;&#65292;&#25552;&#39640;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#20102;&#27491;&#21017;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29420;&#28909;&#32534;&#30721;GBDT&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#24230;&#20219;&#24847;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#24182;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.13742</link><description>&lt;p&gt;
TR0N&#65306;0-Shot&#21363;&#25554;&#21363;&#29992;&#26465;&#20214;&#29983;&#25104;&#30340;&#32763;&#35793;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation. (arXiv:2304.13742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36716;&#21270;&#20026;&#39640;&#24230;&#20219;&#24847;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#24182;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TR0N&#65292;&#19968;&#20010;&#39640;&#24230;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GAN&#21644;VAE&#65292;&#36716;&#25442;&#20026;&#26465;&#20214;&#27169;&#22411;&#12290;&#26465;&#20214;&#21487;&#20197;&#26159;&#39640;&#24230;&#20219;&#24847;&#30340;&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20998;&#31867;&#22120;&#23558;&#26080;&#26465;&#20214;&#27169;&#22411;&#36716;&#21270;&#20026;&#31867;&#21035;&#26465;&#20214;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;CLIP&#23558;&#20854;&#36716;&#21270;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;TR0N&#23398;&#20064;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#38543;&#26426;&#26144;&#23556;&#65292;&#35813;&#26144;&#23556;&#22312;&#26465;&#20214;&#31354;&#38388;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#8220;&#32763;&#35793;&#8221;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#24212;&#20110;&#28385;&#36275;&#25152;&#38656;&#26465;&#20214;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Langevin&#21160;&#24577;&#36827;&#19968;&#27493;&#25913;&#36827;&#32763;&#35793;&#21518;&#30340;&#28508;&#22312;&#26679;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;TR0N&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#24494;&#35843;&#65292;&#20294;&#21487;&#20197;&#22312;MS-COCO&#19978;&#23454;&#29616;&#38646;-shot FID 10.9&#65292;&#19981;&#20165;&#22312;&#36825;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#31454;&#21697;&#65292;&#32780;&#19988;&#22312;&#37319;&#26679;&#36895;&#24230;&#19978;&#20063;&#19982;&#20854;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TR0N, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models. The conditioning can be highly arbitrary, and requires only a pre-trained auxiliary model. For example, we show how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP. TR0N learns a lightweight stochastic mapping which "translates" between the space of conditions and the latent space of the generative model, in such a way that the generated latent corresponds to a data sample satisfying the desired condition. The translated latent samples are then further improved upon through Langevin dynamics, enabling us to obtain higher-quality data samples. TR0N requires no training data nor fine-tuning, yet can achieve a zero-shot FID of 10.9 on MS-COCO, outperforming competing alternatives not only on this metric, but also in sampling speed -- all while retaining 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#27010;&#36848;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#12289;&#25506;&#35752;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#35752;&#35770;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13738</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65306;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#27010;&#36848;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#12289;&#25506;&#35752;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#35752;&#35770;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#20113;&#35745;&#31639;&#30456;&#32467;&#21512;&#24050;&#25104;&#20026;&#35299;&#20915;AI&#24212;&#29992;&#31243;&#24207;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#21033;&#29992;&#20113;&#35745;&#31639;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;AI&#26694;&#26550;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#24120;&#29992;&#30340;AI&#26694;&#26550;&#21644;&#20113;&#26381;&#21153;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#20113;&#30340;AI&#31995;&#32479;&#20013;&#25968;&#25454;&#23384;&#20648;&#21644;&#31649;&#29702;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#30340;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#20998;&#21306;&#12289;&#36890;&#20449;&#31574;&#30053;&#21644;&#22522;&#20110;&#20113;&#30340;&#35757;&#32451;&#26550;&#26500;&#12290;&#22312;&#38543;&#21518;&#30340;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20113;&#20013;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#33258;&#21160;&#32553;&#25918;&#21644;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc
&lt;/p&gt;</description></item><item><title>AIRIVA&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;TCR&#24211;&#23384;&#30340;&#20302;&#32500;&#12289;&#21487;&#35299;&#37322;&#21644;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#20197;&#35299;&#24320;&#31995;&#32479;&#25928;&#24212;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#19981;&#21516;&#20256;&#26579;&#30149;&#21644;&#30284;&#30151;&#30456;&#20851;&#30340;TCR&#12290;</title><link>http://arxiv.org/abs/2304.13737</link><description>&lt;p&gt;
AIRIVA&#65306;&#33258;&#36866;&#24212;&#20813;&#30123;&#24211;&#23384;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AIRIVA: A Deep Generative Model of Adaptive Immune Repertoires. (arXiv:2304.13737v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13737
&lt;/p&gt;
&lt;p&gt;
AIRIVA&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;TCR&#24211;&#23384;&#30340;&#20302;&#32500;&#12289;&#21487;&#35299;&#37322;&#21644;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#20197;&#35299;&#24320;&#31995;&#32479;&#25928;&#24212;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#19981;&#21516;&#20256;&#26579;&#30149;&#21644;&#30284;&#30151;&#30456;&#20851;&#30340;TCR&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20813;&#30123;&#22522;&#22240;&#32452;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;T&#32454;&#32990;&#21463;&#20307;&#65288;TCR&#65289;&#26631;&#35760;&#21487;&#20197;&#21033;&#29992;TCR&#19982;&#30142;&#30149;&#25239;&#21407;&#32467;&#21512;&#30340;&#39640;&#29305;&#24322;&#24615;&#20934;&#30830;&#39044;&#27979;&#27963;&#21160;&#25110;&#26368;&#36817;&#30340;&#24863;&#26579;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#24615;&#20813;&#30123;&#24211;&#23384;&#30340;&#26497;&#31471;&#22810;&#26679;&#24615;&#22312;&#21487;&#38752;&#22320;&#35782;&#21035;&#30142;&#30149;&#29305;&#24322;&#24615;TCR&#26041;&#38754;&#23384;&#22312;&#30528;&#25361;&#25112;&#12290;&#31181;&#32676;&#36951;&#20256;&#23398;&#21644;&#27979;&#24207;&#28145;&#24230;&#36824;&#21487;&#20197;&#23545;&#24211;&#23384;&#20135;&#29983;&#24378;&#28872;&#30340;&#31995;&#32479;&#24433;&#21709;&#65292;&#22312;&#24320;&#21457;&#35786;&#26029;&#27169;&#22411;&#26102;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20813;&#30123;&#24211;&#26174;&#19981;&#21464;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;AIRIVA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;TCR&#24211;&#23384;&#30340;&#20302;&#32500;&#65292;&#21487;&#35299;&#37322;&#21644;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#20197;&#35299;&#24320;&#36825;&#31181;&#24211;&#23384;&#20013;&#30340;&#31995;&#32479;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;AIRIVA&#24212;&#29992;&#20110;&#20004;&#20010;&#20256;&#26579;&#30149;&#26696;&#20363;&#30740;&#31350;&#65306;COVID-19&#65288;&#33258;&#28982;&#24863;&#26579;&#21644;&#25509;&#31181;&#65289;&#21644;&#21333;&#32431;&#30129;&#30137;&#30149;&#27602;&#65288;HSV-1&#21644;HSV-2&#65289;&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#35299;&#24320;&#20010;&#20307;&#30142;&#30149;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#40657;&#33394;&#32032;&#30244;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20351;&#29992;AIRIVA&#26469;&#35782;&#21035;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;TCR&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AIRIVA&#26159;&#29702;&#35299;&#20256;&#26579;&#30149;&#20813;&#30123;&#24212;&#31572;&#20197;&#21450;&#24320;&#21457;&#30284;&#30151;&#20813;&#30123;&#30103;&#27861;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in immunomics have shown that T-cell receptor (TCR) signatures can accurately predict active or recent infection by leveraging the high specificity of TCR binding to disease antigens. However, the extreme diversity of the adaptive immune repertoire presents challenges in reliably identifying disease-specific TCRs. Population genetics and sequencing depth can also have strong systematic effects on repertoires, which requires careful consideration when developing diagnostic models. We present an Adaptive Immune Repertoire-Invariant Variational Autoencoder (AIRIVA), a generative model that learns a low-dimensional, interpretable, and compositional representation of TCR repertoires to disentangle such systematic effects in repertoires. We apply AIRIVA to two infectious disease case-studies: COVID-19 (natural infection and vaccination) and the Herpes Simplex Virus (HSV-1 and HSV-2), and empirically show that we can disentangle the individual disease signals. We further demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;TCN-LSTM&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13732</link><description>&lt;p&gt;
&#36890;&#36807;TCN-LSTM&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models. (arXiv:2304.13732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;TCN-LSTM&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#21464;&#25442;&#65288;LC&#65289;&#26159;&#19968;&#20010;&#36830;&#32493;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#36807;&#31243;&#12290;&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#39044;&#27979;LC&#36807;&#31243;&#21487;&#20197;&#24110;&#21161;&#20132;&#36890;&#21442;&#19982;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#21608;&#22260;&#29615;&#22659;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;LC&#23433;&#20840;&#38544;&#24739;&#65292;&#24182;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#21333;&#20803;&#65288;TCN-LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20102;&#19977;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#65288;MTL-LSTM&#12289;MTL-TCN&#12289;MTL-TCN-LSTM&#65289;&#26469;&#25429;&#25417;&#36755;&#20986;&#25351;&#26631;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;LC&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#65288;LC-IR-SP&#65289;&#30340;&#32479;&#19968;&#24314;&#27169;&#26694;&#26550;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;CitySim&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20102;1023&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;&#20351;&#29992;Pearson&#31995;&#25968;&#26469;&#35780;&#20215;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lane change (LC) is a continuous and complex operation process. Accurately detecting and predicting LC processes can help traffic participants better understand their surrounding environment, recognize potential LC safety hazards, and improve traffic safety. This present paper focuses on LC processes, developing an LC intention recognition (LC-IR) model and an LC status prediction (LC-SP) model. A novel ensemble temporal convolutional network with Long Short-Term Memory units (TCN-LSTM) is first proposed to capture long-range dependencies in sequential data. Then, three multi-task models (MTL-LSTM, MTL-TCN, MTL-TCN -LSTM) are developed to capture the intrinsic relationship among output indicators. Furthermore, a unified modeling framework for LC intention recognition and driving status prediction (LC-IR-SP) is developed. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. The Pearson coefficient is emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#21335;&#25746;&#21704;&#25289;&#38750;&#27954;&#22269;&#23478;&#20013;&#23558;&#32844;&#19994;&#21644;&#25216;&#26415;&#22521;&#35757;&#32622;&#20110;&#23398;&#26415;&#25945;&#32946;&#20043;&#19978;&#30340;&#25945;&#32946;&#25919;&#31574;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21152;&#24378;&#21435;&#27542;&#27665;&#21270;&#65292;&#23454;&#29616;&#31038;&#20250;&#32463;&#27982;&#27969;&#21160;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13728</link><description>&lt;p&gt;
ChatGPT&#26159;&#38750;&#27954;&#25746;&#21704;&#25289;&#20197;&#21335;&#32844;&#19994;&#25945;&#32946;&#21435;&#27542;&#27665;&#21270;&#30340;&#24517;&#38656;&#21697;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is all you need to decolonize sub-Saharan Vocational Education. (arXiv:2304.13728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#21335;&#25746;&#21704;&#25289;&#38750;&#27954;&#22269;&#23478;&#20013;&#23558;&#32844;&#19994;&#21644;&#25216;&#26415;&#22521;&#35757;&#32622;&#20110;&#23398;&#26415;&#25945;&#32946;&#20043;&#19978;&#30340;&#25945;&#32946;&#25919;&#31574;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21152;&#24378;&#21435;&#27542;&#27665;&#21270;&#65292;&#23454;&#29616;&#31038;&#20250;&#32463;&#27982;&#27969;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#20855;&#26377;&#20132;&#20114;&#33021;&#21147;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#36827;&#27493;&#20026;&#31038;&#20250;&#32463;&#27982;&#27969;&#21160;&#24615;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26426;&#20250;&#12290;&#23427;&#20204;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#36127;&#25285;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#20415;&#21033;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#20026;&#36139;&#22256;&#22269;&#23478;&#36866;&#24212;&#21644;&#29616;&#20195;&#21270;&#20854;&#25945;&#32946;&#31209;&#24207;&#25552;&#20379;&#20102;&#19968;&#27969;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25945;&#32946;&#25919;&#31574;&#26694;&#26550;&#30340;&#35770;&#28857;&#65292;&#36890;&#36807;&#23558;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22269;&#23478;&#30340;&#32844;&#19994;&#21644;&#25216;&#26415;&#22521;&#35757;&#32622;&#20110;&#23398;&#26415;&#25945;&#32946;&#20043;&#19978;&#65292;&#26469;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#38656;&#27714;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20197;&#21152;&#24378;&#20854;&#31995;&#32479;&#21435;&#27542;&#27665;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21382;&#21490;&#20363;&#23376;&#65292;&#35828;&#26126;&#19981;&#21516;&#22269;&#23478;&#22312;&#20854;&#31038;&#20250;&#32463;&#27982;&#36716;&#22411;&#30340;&#22522;&#26412;&#27493;&#39588;&#20013;&#25104;&#21151;&#23454;&#26045;&#20102;&#27492;&#31867;&#25919;&#31574;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#23545;&#21335;&#25746;&#21704;&#25289;&#38750;&#27954;&#22269;&#23478;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances of Generative AI models with interactive capabilities over the past few years offer unique opportunities for socioeconomic mobility. Their potential for scalability, accessibility, affordability, personalizing and convenience sets a first-class opportunity for poverty-stricken countries to adapt and modernize their educational order. As a result, this position paper makes the case for an educational policy framework that would succeed in this transformation by prioritizing vocational and technical training over academic education in sub-Saharan African countries. We highlight substantial applications of Large Language Models, tailor-made to their respective cultural background(s) and needs, that would reinforce their systemic decolonization. Lastly, we provide specific historical examples of diverse states successfully implementing such policies in the elementary steps of their socioeconomic transformation, in order to corroborate our proposal to sub-Saharan African countr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#38598;&#25104;CNN&#30340;&#26041;&#27861;&#23545;&#20083;&#33146;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;5%&#20197;&#19978;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13727</link><description>&lt;p&gt;
&#38598;&#25104;CNN&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Ensemble CNNs for Breast Tumor Classification. (arXiv:2304.13727v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#38598;&#25104;CNN&#30340;&#26041;&#27861;&#23545;&#20083;&#33146;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;5%&#20197;&#19978;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#26426;&#36741;&#21161;&#20083;&#33146;&#32959;&#22359;&#20998;&#31867;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26426;&#21046;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65292;&#28982;&#21518;&#20998;&#21035;&#35757;&#32451;&#19977;&#20010;&#27169;&#22411;&#65292;&#21363;XceptionNet&#12289;DenseNet&#21644;EfficientNet&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#32593;&#32476;&#36755;&#20986;&#30340;&#27010;&#29575;&#30456;&#21152;&#26469;&#38598;&#25104;&#26426;&#21046;&#65292;&#20174;&#32780;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102;5%&#12290;&#35813;&#26041;&#26696;&#24050;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#24182;&#23454;&#29616;&#20102;88%&#30340;&#20934;&#30830;&#29575;&#12289;85%&#30340;&#31934;&#24230;&#21644;76%&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the recognition ability of computer-aided breast mass classification among mammographic images, in this work we explore the state-of-the-art classification networks to develop an ensemble mechanism. First, the regions of interest (ROIs) are obtained from the original dataset, and then three models, i.e., XceptionNet, DenseNet, and EfficientNet, are trained individually. After training, we ensemble the mechanism by summing the probabilities outputted from each network which enhances the performance up to 5%. The scheme has been validated on a public dataset and we achieved accuracy, precision, and recall 88%, 85%, and 76% respectively.
&lt;/p&gt;</description></item><item><title>SamurAI&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20026;&#29289;&#32852;&#32593;&#24212;&#29992;&#25552;&#20379;&#20102;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13726</link><description>&lt;p&gt;
SamurAI: &#19968;&#31181;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration. (arXiv:2304.13726v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13726
&lt;/p&gt;
&lt;p&gt;
SamurAI&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#21796;&#37266;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20026;&#29289;&#32852;&#32593;&#24212;&#29992;&#25552;&#20379;&#20102;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#24212;&#29992;&#29616;&#22312;&#38656;&#35201;&#20855;&#22791;&#35782;&#21035;&#21644;&#33258;&#36866;&#24212;&#31561;&#21151;&#33021;&#12290;&#34429;&#28982;&#29289;&#32852;&#32593;&#33410;&#28857;&#30340;&#21151;&#32791;&#26159;&#36825;&#20123;&#24212;&#29992;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#20294;&#30001;&#20110;&#26080;&#32447;&#32593;&#32476;&#19978;&#36830;&#32493;&#20256;&#36755;&#20256;&#24863;&#22120;&#25110;&#22270;&#20687;&#25968;&#25454;&#65292;&#22522;&#20110;&#20113;&#30340;&#22788;&#29702;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#22240;&#27492;&#65292;&#24212;&#22312;&#29289;&#32852;&#32593;&#33410;&#28857;&#20013;&#38598;&#25104;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#21644;&#25968;&#25454;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#29289;&#32852;&#32593;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#38388;&#27463;&#24615;&#25968;&#25454;&#35760;&#24405;&#21644;&#33021;&#32791;&#39640;&#30340;&#25968;&#25454;&#22788;&#29702;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#12290;&#22240;&#27492;&#65292;&#33410;&#28857;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#35299;&#20915;&#22788;&#29702;&#21644;&#33021;&#28304;&#30340;&#24191;&#27867;&#38656;&#27714;&#26041;&#38754;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SamurAI&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#29289;&#32852;&#32593;&#33410;&#28857;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#33455;&#29255;&#20869;&#23376;&#31995;&#32479;&#26469;&#24357;&#21512;&#22788;&#29702;&#21644;&#33021;&#37327;&#26041;&#38754;&#30340;&#24046;&#36317;&#65306;&#19968;&#20010;&#20302;&#21151;&#29575;&#12289;&#26080;&#26102;&#38047;&#12289;&#20107;&#20214;&#39537;&#21160;&#30340;Always-Responsive (AR) &#37096;&#20998;&#21644;&#19968;&#20010;&#39640;&#25928;&#30340;On-Demand (OD) &#37096;&#20998;&#12290; AR&#21253;&#21547;&#19968;&#20010;1.7MOPS&#20107;&#20214;&#39537;&#21160;&#30340;&#24322;&#27493; Wake-up &#25511;&#21046;&#22120;&#65288;WuC&#65289;&#65292;207ns&#21796;&#37266;&#26102;&#38388;&#30340;&#20248;&#21270;&#36866;&#29992;&#20110;&#38388;&#27463;&#24615;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased capabilities such as recognition and self-adaptability are now required from IoT applications. While IoT node power consumption is a major concern for these applications, cloud-based processing is becoming unsustainable due to continuous sensor or image data transmission over the wireless network. Thus optimized ML capabilities and data transfers should be integrated in the IoT node. Moreover, IoT applications are torn between sporadic data-logging and energy-hungry data processing (e.g. image classification). Thus, the versatility of the node is key in addressing this wide diversity of energy and processing needs. This paper presents SamurAI, a versatile IoT node bridging this gap in processing and in energy by leveraging two on-chip sub-systems: a low power, clock-less, event-driven Always-Responsive (AR) part and an energy-efficient On-Demand (OD) part. AR contains a 1.7MOPS event-driven, asynchronous Wake-up Controller (WuC) with a 207ns wake-up time optimized for sporadi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#24182;&#39044;&#27979;&#22797;&#21457;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2304.13725</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning. (arXiv:2304.13725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#24182;&#39044;&#27979;&#22797;&#21457;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#23548;&#33268;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#39640;&#32423;&#21035;&#30340;&#33041;&#32959;&#30244;&#29978;&#33267;&#22312;&#26631;&#20934;&#27835;&#30103;&#21518;&#23481;&#26131;&#22797;&#21457;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#31181;&#39044;&#27979;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#26377;&#21487;&#33021;&#24310;&#38271;&#24739;&#32773;&#30340;&#29983;&#23384;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22797;&#21457;&#20301;&#32622;&#39044;&#27979;&#32593;&#32476;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;BraTS 2021&#19978;&#35757;&#32451;&#19968;&#20010;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36716;&#31227;&#21040;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#20197;&#25552;&#21462;&#20016;&#23500;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#22810;&#36890;&#36947;&#29305;&#24449;&#34701;&#21512;&#27169;&#22411;&#21644;&#19968;&#20010;&#38750;&#32447;&#24615;&#30456;&#20851;&#23398;&#20064;&#27169;&#22359;&#26469;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#22810;&#36890;&#36947;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30001;&#19968;&#20010;&#38750;&#32447;&#24615;&#20851;&#31995;&#27169;&#22411;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumor is one of the leading causes of cancer death. The high-grade brain tumors are easier to recurrent even after standard treatment. Therefore, developing a method to predict brain tumor recurrence location plays an important role in the treatment planning and it can potentially prolong patient's survival time. There is still little work to deal with this issue. In this paper, we present a deep learning-based brain tumor recurrence location prediction network. Since the dataset is usually small, we propose to use transfer learning to improve the prediction. We first train a multi-modal brain tumor segmentation network on the public dataset BraTS 2021. Then, the pre-trained encoder is transferred to our private dataset for extracting the rich semantic features. Following that, a multi-scale multi-channel feature fusion model and a nonlinear correlation learning module are developed to learn the effective features. The correlation between multi-channel features is modeled by a no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#30340;GPU&#21152;&#36895;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#21152;&#24555;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20998;&#35299;&#24182;&#36991;&#20813;&#20869;&#23384;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13724</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#30340;GPU&#21152;&#36895;&#30697;&#38453;&#20998;&#35299;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
GPU accelerated matrix factorization of large scale data using block based approach. (arXiv:2304.13724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#30340;GPU&#21152;&#36895;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#21152;&#24555;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#20998;&#35299;&#24182;&#36991;&#20813;&#20869;&#23384;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#22830;&#22788;&#29702;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#30697;&#38453;&#20998;&#35299;&#38656;&#35201;&#30456;&#24403;&#38271;&#30340;&#26102;&#38388;&#65292;&#32780;&#22270;&#24418;&#22788;&#29702;&#22120;&#21487;&#20197;&#21152;&#24555;&#30697;&#38453;&#20998;&#35299;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#20294;GPU&#19978;&#21487;&#29992;&#30340;&#20869;&#23384;&#26159;&#26377;&#38480;&#30340;&#12290;&#21033;&#29992;GPU&#38656;&#35201;&#26367;&#20195;&#25216;&#26415;&#65292;&#19981;&#20165;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#12290;&#22312;&#21033;&#29992;GPU&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#26102;&#65292;&#35745;&#31639;&#21333;&#20803;&#20043;&#38388;&#30340;&#21516;&#27493;&#12289;&#19982;&#35745;&#31639;&#21333;&#20803;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#38548;&#31163;&#12289;&#35745;&#31639;&#21333;&#20803;&#20043;&#38388;&#25968;&#25454;&#30340;&#20849;&#20139;&#20197;&#21450;&#35782;&#21035;&#35745;&#31639;&#21333;&#20803;&#20043;&#38388;&#30340;&#29420;&#31435;&#20219;&#21153;&#31561;&#26159;&#19968;&#20123;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22522;&#20110;&#22359;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;GPU&#19978;&#30340;&#30697;&#38453;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#65292;&#22312;&#26377;&#38480;&#30340;&#30828;&#20214;&#19978;&#22788;&#29702;&#26497;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#24517;&#22312;&#32467;&#26524;&#19978;&#22949;&#21327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29420;&#31435;&#22359;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#30697;&#38453;&#20998;&#35299;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#20248;&#21270;&#23427;&#20204;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix Factorization (MF) on large scale data takes substantial time on a Central Processing Unit (CPU). While Graphical Processing Unit (GPU)s could expedite the computation of MF, the available memory on a GPU is finite. Leveraging GPUs require alternative techniques that allow not only parallelism but also address memory limitations. Synchronization between computation units, isolation of data related to a computational unit, sharing of data between computational units and identification of independent tasks among computational units are some of the challenges while leveraging GPUs for MF. We propose a block based approach to matrix factorization using Stochastic Gradient Descent (SGD) that is aimed at accelerating MF on GPUs. The primary motivation for the approach is to make it viable to factorize extremely large data sets on limited hardware without having to compromise on results. The approach addresses factorization of large scale data by identifying independent blocks, each of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13534</link><description>&lt;p&gt;
&#29992;&#22343;&#22330;&#21338;&#24328;&#20026;&#29983;&#25104;&#27169;&#22411;&#25645;&#24314;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22343;&#22330;&#21338;&#24328; (MFGs) &#20316;&#20026;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#12289;&#22686;&#24378;&#21644;&#35774;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102; MFGs &#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#20195;&#20215;&#20989;&#25968;&#25512;&#23548;&#20102;&#36825;&#19977;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#8212;&#8212;&#19968;&#32452;&#32806;&#21512;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#29305;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#21512;&#25104;&#26679;&#26412;&#65292;&#21478;&#19968;&#20010;&#20195;&#29702;&#23545;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#19988;&#36924;&#30495;&#65292;&#21516;&#26102;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#31361;&#26174;&#20102; MFGs &#20316;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#39564;&#23460;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13455</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#20013;&#36856;&#21457;&#20986;&#31209;&#24207;&#65306;&#20026;&#29289;&#20307;&#26816;&#27979;&#25490;&#24207;&#20107;&#20214;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Gromov-Wasserstein Discrepancy&#36873;&#25321;&#26368;&#20339;&#20107;&#20214;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#65292;&#26412;&#25991;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#27861;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22788;&#29702;&#20107;&#20214;&#30340;&#39030;&#23574;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#29616;&#25104;&#32593;&#32476;&#20043;&#21069;&#65292;&#39318;&#20808;&#23558;&#20854;&#36716;&#25442;&#20026;&#31264;&#23494;&#30340;&#32593;&#26684;&#29366;&#36755;&#20837;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#20026;&#20219;&#21153;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#31034;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#34920;&#31034;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#20998;&#25968;&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#65292;&#36825;&#38750;&#24120;&#32791;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20107;&#20214;&#21450;&#20854;&#34920;&#31034;&#20043;&#38388;&#30340;Gromov-Wasserstein Discrepancy (GWD)&#36873;&#25321;&#26368;&#20339;&#34920;&#31034;&#26469;&#28040;&#38500;&#36825;&#20010;&#29942;&#39048;&#12290;&#23427;&#30340;&#35745;&#31639;&#36895;&#24230;&#22823;&#32422;&#27604;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;200&#20493;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#34920;&#31034;&#12289;&#32593;&#32476;&#39592;&#24178;&#21644;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20107;&#20214;&#34920;&#31034;&#27861;&#20219;&#21153;&#24615;&#33021;&#25490;&#21517;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#25214;&#21040;&#20855;&#26377;&#39640;&#20219;&#21153;&#20998;&#25968;&#30340;&#34920;&#31034;&#30456;&#24403;&#20110;&#25214;&#21040;&#20855;&#26377;&#20302;GWD&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#39318;&#27425;&#23545;&#22823;&#22411;&#20107;&#20214;&#34920;&#31034;&#27861;&#23478;&#26063;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29289;&#20307;&#26816;&#27979;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Moving MNIST&#21644;N-Caltech101&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#21518;&#32773;&#36798;&#21040;&#20102;83.0%&#30340;1%&#35823;&#25253;&#29575;&#19979;&#30340;mAP&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;Shot&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;Shot&#25968;&#37327;&#30340;&#20248;&#21270;&#19982;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12950</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;Shot&#20248;&#21270;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training. (arXiv:2304.12950v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;Shot&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;Shot&#25968;&#37327;&#30340;&#20248;&#21270;&#19982;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Shot&#20248;&#21270;&#30340;QML&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;QML&#27169;&#22411;&#23545;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#30701;&#29256;&#26412;&#21644;&#23436;&#25972;&#29256;&#26412;&#30340;Shot&#25968;&#30446;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#23436;&#25972;&#29256;&#26412;&#27604;&#35757;&#32451;&#30701;&#29256;&#26412;&#25552;&#20379;&#20102;5-6%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#35757;&#32451;&#20013;&#30340;Shot&#25968;&#37327;&#21487;&#39640;&#36798;10&#20493;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#20943;&#23567;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21152;&#36895;&#35757;&#32451;&#26102;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#30701;&#29256;&#26412;&#25968;&#25454;&#38598;&#65292;&#20197;&#20248;&#21270;&#35757;&#32451;&#21608;&#26399;&#20869;&#30340;Shot&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#65288;a&#65289;&#32447;&#24615;&#20989;&#25968;&#65292;&#20854;&#20013;Shot&#25968;&#37327;&#38543;&#30528;&#35757;&#32451;&#21608;&#26399;&#32447;&#24615;&#20943;&#23569;&#65292;&#21644;&#65288;b&#65289;&#27493;&#20989;&#25968;&#65292;&#20854;&#20013;Shot&#25968;&#37327;&#38543;&#30528;&#35757;&#32451;&#21608;&#26399;&#27493;&#36827;&#24335;&#20943;&#23569;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#20943;&#23569;Shot&#25968;&#37327;&#20250;&#23548;&#33268;0.01&#30340;&#25439;&#22833;&#22686;&#21152;&#21644;&#32422;4%&#65288;1%&#65289;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose shot optimization method for QML models at the expense of minimal impact on model performance. We use classification task as a test case for MNIST and FMNIST datasets using a hybrid quantum-classical QML model. First, we sweep the number of shots for short and full versions of the dataset. We observe that training the full version provides 5-6% higher testing accuracy than short version of dataset with up to 10X higher number of shots for training. Therefore, one can reduce the dataset size to accelerate the training time. Next, we propose adaptive shot allocation on short version dataset to optimize the number of shots over training epochs and evaluate the impact on classification accuracy. We use a (a) linear function where the number of shots reduce linearly with epochs, and (b) step function where the number of shots reduce in step with epochs. We note around 0.01 increase in loss and around 4% (1%) reduction in testing accuracy for reduction in shots by u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;IR&#32534;&#31243;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.12568</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22411;&#24314;&#27169;&#21644;&#24322;&#26500;GNN&#30340;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Performance Optimization using Multimodal Modeling and Heterogeneous GNN. (arXiv:2304.12568v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;IR&#32534;&#31243;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#20013;&#30340;&#24322;&#26500;&#24615;&#21644;&#21487;&#37197;&#32622;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#20351;&#24471;&#22312;&#36825;&#20123;&#31995;&#32479;&#19978;&#36827;&#34892;&#33258;&#21160;&#35843;&#20248;&#21644;&#36816;&#34892;&#26102;&#21442;&#25968;&#37197;&#32622;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#20026;&#20102;&#32553;&#30701;&#36798;&#21040;&#26368;&#20339;&#37197;&#32622;&#30340;&#26102;&#38388;&#65292;&#38500;&#20102;&#37319;&#29992;&#38754;&#21521;&#29305;&#23450;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#22806;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#36890;&#29992;&#25628;&#32034;&#31574;&#30053;&#65292;&#20294;&#24448;&#24448;&#19981;&#33021;&#25214;&#21040;&#26368;&#20339;&#30340;&#37197;&#32622;&#25110;&#20854;&#25910;&#25947;&#25152;&#38656;&#26102;&#38388;&#22826;&#38271;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#35843;&#20248;&#26041;&#27861;&#65292;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#36866;&#24212;&#22810;&#31181;&#35843;&#20248;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#65292;&#20854;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#36866;&#24212;&#20110;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#22522;&#20110;IR&#30340;&#32534;&#31243;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;MGA&#65289;&#35843;&#35856;&#22120;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoizing Autoencode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;(TAFL)&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#32858;&#28966;&#25439;&#22833;&#19982;&#22522;&#20110;&#22320;&#38754;&#30495;&#20540;&#21644;&#39044;&#27979;&#20998;&#21106;&#25513;&#30721;&#30340;&#25345;&#20037;&#22270;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#25299;&#25169;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#19977;&#32500;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25299;&#25169;&#38169;&#35823;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12223</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#22270;&#20687;&#20998;&#21106;&#30340;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Focal Loss for 3D Image Segmentation. (arXiv:2304.12223v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12223
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;(TAFL)&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#32858;&#28966;&#25439;&#22833;&#19982;&#22522;&#20110;&#22320;&#38754;&#30495;&#20540;&#21644;&#39044;&#27979;&#20998;&#21106;&#25513;&#30721;&#30340;&#25345;&#20037;&#22270;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#25299;&#25169;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35299;&#20915;&#19977;&#32500;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25299;&#25169;&#38169;&#35823;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#32463;&#24120;&#21463;&#21040;&#25299;&#25169;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#22914;&#37325;&#21472;&#21306;&#22495;&#65292;&#20013;&#26029;&#36830;&#25509;&#21644;&#31354;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#32858;&#28966;&#25439;&#22833;(TAFL)&#65292;
&lt;/p&gt;
&lt;p&gt;
The efficacy of segmentation algorithms is frequently compromised by topological errors like overlapping regions, disrupted connections, and voids. To tackle this problem, we introduce a novel loss function, namely Topology-Aware Focal Loss (TAFL), that incorporates the conventional Focal Loss with a topological constraint term based on the Wasserstein distance between the ground truth and predicted segmentation masks' persistence diagrams. By enforcing identical topology as the ground truth, the topological constraint can effectively resolve topological errors, while Focal Loss tackles class imbalance. We begin by constructing persistence diagrams from filtered cubical complexes of the ground truth and predicted segmentation masks. We subsequently utilize the Sinkhorn-Knopp algorithm to determine the optimal transport plan between the two persistence diagrams. The resultant transport plan minimizes the cost of transporting mass from one distribution to the other and provides a mapping
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;</title><link>http://arxiv.org/abs/2304.11692</link><description>&lt;p&gt;
BN&#19982;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#24341;&#36215;&#26799;&#24230;&#29190;&#28856;&#65292;&#20294;&#34987;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25152;&#25269;&#28040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25209;&#26631;&#20934;&#21270;&#21644;ReLU&#31561;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21021;&#26399;&#30001;&#20110;&#26102;&#38388;&#26799;&#24230;&#29190;&#28856;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;ReLU&#22914;&#20309;&#27604;&#39044;&#26399;&#26356;&#22810;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#21450;&#25209;&#26631;&#20934;&#21270;&#22914;&#20309;&#22312;&#24674;&#22797;&#26399;&#38388;&#25918;&#22823;&#26799;&#24230;&#65292;&#23548;&#33268;&#21069;&#21521;&#20256;&#25773;&#20445;&#25345;&#31283;&#23450;&#32780;&#26799;&#24230;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#20197;&#21450;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#32553;&#25918;&#26041;&#27861;&#65292;&#24182;&#21487;&#26367;&#25442;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;WarmUp&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#20998;&#31867;&#30340;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#21487;&#22609;&#24615;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11106</link><description>&lt;p&gt;
&#19968;&#31181;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#22312;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Convolutional Spiking Network for Gesture Recognition in Brain-Computer Interfaces. (arXiv:2304.11106v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#20998;&#31867;&#30340;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#21487;&#22609;&#24615;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#27491;&#22312;&#34987;&#24191;&#27867;&#22320;&#25506;&#32034;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#27979;&#37327;&#21644;&#20998;&#26512;&#36830;&#32493;&#26102;&#38388;&#30340;&#33041;&#30005;&#27963;&#21160;&#65292;&#22914;&#30005;&#30382;&#23618;&#22270;&#25110;&#33041;&#30005;&#22270;&#26469;&#39537;&#21160;&#22806;&#37096;&#35774;&#22791;&#24182;&#29992;&#20110;&#27835;&#30103;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27979;&#37327;&#20013;&#30340;&#22122;&#38899;&#21644;&#21464;&#24322;&#24615;&#65292;&#23545;&#36825;&#20123;&#20449;&#21495;&#30340;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#38656;&#35201;&#31163;&#32447;&#22788;&#29702;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#33041;&#20449;&#21495;&#30340;&#25163;&#21183;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#20107;&#20214;&#39537;&#21160;&#31361;&#35302;&#21487;&#22609;&#24615;&#35268;&#21017;&#65292;&#29992;&#20110;&#33033;&#20914;&#22495;&#32534;&#30721;&#30340;&#27169;&#25311;&#20449;&#21495;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#30340;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#21463;&#35797;&#32773;&#30340;&#33041;&#30005;&#21644;&#30005;&#30382;&#23618;&#22270;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-computer interfaces are being explored for a wide variety of therapeutic applications. Typically, this involves measuring and analyzing continuous-time electrical brain activity via techniques such as electrocorticogram (ECoG) or electroencephalography (EEG) to drive external devices. However, due to the inherent noise and variability in the measurements, the analysis of these signals is challenging and requires offline processing with significant computational resources. In this paper, we propose a simple yet efficient machine learning-based approach for the exemplary problem of hand gesture classification based on brain signals. We use a hybrid machine learning approach that uses a convolutional spiking neural network employing a bio-inspired event-driven synaptic plasticity rule for unsupervised feature learning of the measured analog signals encoded in the spike domain. We demonstrate that this approach generalizes to different subjects with both EEG and ECoG data and achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#24471;&#20197;&#35299;&#32544;&#65292;&#36866;&#24403;&#20849;&#20139;&#20449;&#24687;&#65292;&#36798;&#21040;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07939</link><description>&lt;p&gt;
&#21033;&#29992;&#31232;&#30095;&#21644;&#20849;&#20139;&#29305;&#24449;&#28608;&#27963;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging sparse and shared feature activations for disentangled representation learning. (arXiv:2304.07939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#24471;&#20197;&#35299;&#32544;&#65292;&#36866;&#24403;&#20849;&#20139;&#20449;&#24687;&#65292;&#36798;&#21040;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#24674;&#22797;&#39640;&#32500;&#25968;&#25454;&#30340;&#28508;&#22312;&#21464;&#21270;&#22240;&#32032;&#19968;&#30452;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#12290;&#22312;&#22823;&#22810;&#25968;&#22522;&#20110;&#26080;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#30446;&#26631;&#30340;&#20808;&#21069;&#30740;&#31350;&#20013;&#65292;&#20154;&#20204;&#24573;&#30053;&#20102;&#36825;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#30417;&#30563;&#20219;&#21153;&#20165;&#20381;&#36182;&#20110;&#26410;&#30693;&#22240;&#32032;&#30340;&#23376;&#38598;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#32544;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#24182;&#36866;&#24403;&#22320;&#20849;&#20139;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20174;&#26410;&#30452;&#25509;&#35266;&#23519;&#21040;&#21464;&#24322;&#22240;&#32032;&#65292;&#20294;&#22312;&#20805;&#20998;&#24615;&#21644;&#26368;&#23567;&#24615;&#20551;&#35774;&#19979;&#65292;&#35775;&#38382;&#22810;&#20010;&#20219;&#21153;&#36275;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#36716;&#31227;&#22522;&#20934;&#20197;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#65288;&#22270;&#20687;&#65292;&#25991;&#26412;&#65289;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.07788</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#27010;&#29575;&#20915;&#31574;&#26641;&#30340;&#20020;&#24202;&#23454;&#36341;&#36741;&#21161;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07788
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24847;&#35782;&#21040;&#38656;&#35201;&#23436;&#20840;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#26102;&#65292;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36741;&#21161;&#25935;&#24863;&#39046;&#22495;&#20915;&#31574;&#30340;&#36235;&#21183;&#23558;&#20250;&#22686;&#38271;&#65292;&#24182;&#19988;&#21363;&#23558;&#20986;&#21488;&#30340;&#27861;&#35268;&#23558;&#20250;&#21152;&#24378;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20542;&#26012;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20999;&#20837;&#28857;&#20043;&#19968;&#26159;&#21307;&#23398;&#23454;&#36341;&#65292;&#23427;&#21487;&#20197;&#21463;&#30410;&#20110;&#31934;&#30830;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20250;&#20135;&#29983;&#20449;&#20219;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;MedFP&#65292;&#23427;&#32467;&#21512;&#20102;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#26469;&#36741;&#21161;&#20020;&#24202;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65307;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#22330;&#26223;&#20013;&#65306;&#32959;&#30244;&#20998;&#31867;&#21644;&#31958;&#23615;&#30149;&#31867;&#22411;2&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05350</link><description>&lt;p&gt;
Astroformer&#65306;&#20998;&#31867;&#24182;&#19981;&#24635;&#26159;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#25110;&#37096;&#32626;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26143;&#31995;&#24418;&#24577;&#23545;&#20110;&#29702;&#35299;&#26143;&#31995;&#30340;&#24418;&#25104;&#21644;&#28436;&#21270;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#38656;&#35201;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#31867;&#26143;&#31995;&#24418;&#24577;&#65292;&#24182;&#20174;&#29616;&#20195;&#22825;&#25991;&#23398;&#35843;&#26597;&#20013;&#25552;&#21462;&#29289;&#29702;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#65292;&#20174;CoAtNet&#21644;MaxViT&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26032;&#22534;&#26632;&#35774;&#35745;&#21644;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#30340;Transformer - &#21367;&#31215;&#28151;&#21512;&#12290;&#24182;&#23558;&#20854;&#19982;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20180;&#32454;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OSTTP&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#25152;&#24341;&#20837;&#30340;&#38480;&#21046;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05124</link><description>&lt;p&gt;
&#24102;&#30446;&#26631;&#25237;&#24433;&#30340;&#22312;&#32447;&#26102;&#31354;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Spatio-Temporal Learning with Target Projection. (arXiv:2304.05124v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OSTTP&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#25152;&#24341;&#20837;&#30340;&#38480;&#21046;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#26102;&#38388;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;BPTT&#24341;&#20837;&#20102;&#20005;&#37325;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#38656;&#35201;&#21521;&#21518;&#36890;&#36807;&#26102;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#26435;&#37325;&#23545;&#31216;&#35201;&#27714;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#26356;&#26032;&#38145;&#23450;&#12290;&#36825;&#20123;&#38382;&#39064;&#25104;&#20026;&#22312;&#32447;&#35757;&#32451;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#30340;AI&#31995;&#32479;&#30340;&#38556;&#30861;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#30446;&#26631;&#25237;&#24433;&#30340;&#22312;&#32447;&#26102;&#31354;&#23398;&#20064;&#65288;OSTTP&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;BPTT&#30340;&#25152;&#26377;&#21069;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;OSTTP&#20351;&#32593;&#32476;&#21516;&#26102;&#22788;&#29702;&#21644;&#23398;&#20064;&#26032;&#30340;&#20256;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#32531;&#35299;&#20102;&#26435;&#37325;&#23545;&#31216;&#21644;&#26356;&#26032;&#38145;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26102;&#38388;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;OSTTP&#65292;&#23637;&#31034;&#20102;&#19982;BPTT&#30456;&#27604;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks trained with the backpropagation through time (BPTT) algorithm have led to astounding successes in various temporal tasks. However, BPTT introduces severe limitations, such as the requirement to propagate information backwards through time, the weight symmetry requirement, as well as update-locking in space and time. These problems become roadblocks for AI systems where online training capabilities are vital. Recently, researchers have developed biologically-inspired training algorithms, addressing a subset of those problems. In this work, we propose a novel learning algorithm called online spatio-temporal learning with target projection (OSTTP) that resolves all aforementioned issues of BPTT. In particular, OSTTP equips a network with the capability to simultaneously process and learn from new incoming data, alleviating the weight symmetry and update-locking problems. We evaluate OSTTP on two temporal tasks, showcasing competitive performance compared to BPTT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17823</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#22238;&#24212;&#26377;&#24207;&#22238;&#24402;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;&#65288;N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#20854;&#20013;&#21453;&#24212;&#21464;&#37327;&#19981;&#20165;&#21487;&#20197;&#21462;&#31163;&#25955;&#20540;&#65292;&#20063;&#21487;&#20197;&#21462;&#36830;&#32493;&#20540;&#65292;&#32780;&#22238;&#24402;&#31995;&#25968;&#26681;&#25454;&#39044;&#27979;&#39034;&#24207;&#21453;&#24212;&#20063;&#19981;&#21516;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#20174;&#31163;&#25955;&#21453;&#24212;&#20272;&#35745;&#32447;&#24615;&#31995;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20197;&#21453;&#24212;&#20026;&#36755;&#20837;&#20135;&#29983;&#32447;&#24615;&#31995;&#25968;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;N$^3$POM&#21487;&#20197;&#22312;&#20445;&#30041;&#20256;&#32479;&#26377;&#24207;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#22312;&#25351;&#23450;&#30340;&#29992;&#25143;&#21306;&#22495;&#20869;&#65292;&#39044;&#27979;&#30340;&#26465;&#20214;&#32047;&#31215;&#27010;&#29575;&#65288;CCP&#65289;&#28385;&#36275;&#23616;&#37096;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#21333;&#35843;&#24615;&#30340;&#38543;&#26426;&#65288;MPS&#65289;&#31639;&#27861;&#26469;&#20805;&#20998;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.14483</link><description>&lt;p&gt;
&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#38754;&#21521;&#22478;&#24066;&#35745;&#31639;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#29616;&#29366;&#65292;&#21253;&#25324;&#20854;&#26694;&#26550;&#12289;&#23454;&#29616;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#20256;&#24863;&#22120;&#21644;&#22823;&#22411;&#25968;&#25454;&#24211;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22478;&#24066;&#31995;&#32479;&#26102;&#31354;&#25968;&#25454;&#34987;&#35760;&#24405;&#21644;&#23384;&#20648;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#28436;&#21270;&#27169;&#24335;&#30340;&#39044;&#27979;&#23398;&#20064;&#26159;&#22478;&#24066;&#35745;&#31639;&#20013;&#22522;&#26412;&#20294;&#37325;&#35201;&#30340;&#24490;&#29615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#22478;&#24066;&#26234;&#33021;&#31649;&#29702;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#36890;&#12289;&#29615;&#22659;&#12289;&#23433;&#20840;&#12289;&#20844;&#20849;&#21355;&#29983;&#31561;&#39046;&#22495;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#22797;&#26434;&#30456;&#20851;&#24615;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#30340;&#26694;&#26550;&#12290;STGNN&#36890;&#36807;&#38598;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21508;&#31181;&#26102;&#38388;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#35774;&#35745;&#31354;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#26102;&#38388;&#20381;&#36182;&#23398;&#20064;&#27169;&#22359;&#12289;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of sophisticated sensors and large database technologies, more and more spatio-temporal data in urban systems are recorded and stored. Predictive learning for the evolution patterns of these spatio-temporal data is a basic but important loop in urban computing, which can better support urban intelligent management decisions, especially in the fields of transportation, environment, security, public health, etc. Since traditional statistical learning and deep learning methods can hardly capture the complex correlations in the urban spatio-temporal data, the framework of spatio-temporal graph neural network (STGNN) has been proposed in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. However, for different predictive learning tasks, it is a challenging problem to effectively design the spatial dependencies learning modules, temporal dependencies learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#39044;&#27979;&#30340;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.10794</link><description>&lt;p&gt;
PheME&#65306;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#65292;&#21487;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#25552;&#39640;&#34920;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data. (arXiv:2303.10794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#39044;&#27979;&#30340;&#28145;&#24230;&#38598;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35814;&#32454;&#30340;&#34920;&#22411;&#20449;&#24687;&#23545;&#20110;&#30142;&#30149;&#30340;&#20934;&#30830;&#35786;&#26029;&#21644;&#39118;&#38505;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#34920;&#22411;&#20449;&#24687;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#25215;&#35834;&#36171;&#20104;&#35786;&#26029;&#21464;&#24322;&#35299;&#37322;&#30340;&#26435;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20174;&#24322;&#26500;&#30340;EHR&#25968;&#25454;&#20013;&#20934;&#30830;&#39640;&#25928;&#22320;&#25552;&#21462;&#34920;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PheME&#65292;&#19968;&#31181;Ensemble&#26694;&#26550;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;EHR&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#30340;&#34920;&#22411;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#31232;&#30095;&#30340;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#21644;&#20887;&#20313;&#30340;&#20020;&#24202;&#31508;&#35760;&#20013;&#23398;&#20064;&#21487;&#38752;&#30340;&#34920;&#31034;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#21040;&#21516;&#19968;&#28508;&#22312;&#31354;&#38388;&#20197;&#39044;&#27979;&#34920;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23558;&#21333;&#27169;&#22411;&#21644;&#22810;&#27169;&#22411;&#30340;&#36755;&#20986;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#34920;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19971;&#31181;&#30142;&#30149;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#34920;&#22411;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detailed phenotype information is fundamental to accurate diagnosis and risk estimation of diseases. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from the heterogeneous EHR data remains a challenge. In this work, we present PheME, an Ensemble framework using Multi-modality data of structured EHRs and unstructured clinical notes for accurate Phenotype prediction. Firstly, we employ multiple deep neural networks to learn reliable representations from the sparse structured EHR data and redundant clinical notes. A multi-modal model then aligns multi-modal features onto the same latent space to predict phenotypes. Secondly, we leverage ensemble learning to combine outputs from single-modal models and multi-modal models to improve phenotype predictions. We choose seven diseases to evaluate the phenotyping performance of the proposed framework. Exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.08011</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22320;&#39044;&#27979;&#21508;&#31181;&#28151;&#27788;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Large statistical learning models effectively forecast diverse chaotic systems. (arXiv:2303.08011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#28151;&#27788;&#21644;&#19981;&#21487;&#39044;&#27979;&#26159;&#21516;&#20041;&#35789;&#65292;&#20294;&#26368;&#36817;&#32479;&#35745;&#39044;&#27979;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#35266;&#27979;&#20013;&#33719;&#24471;&#24847;&#24819;&#19981;&#21040;&#30340;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19978;&#30340;&#28151;&#27788;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545; 135 &#31181;&#19981;&#21516;&#20302;&#32500;&#28151;&#27788;&#31995;&#32479;&#30340;&#20247;&#21253;&#25968;&#25454;&#24211;&#36827;&#34892; 24 &#31181;&#20195;&#34920;&#24615;&#26368;&#39640;&#30340;&#22810;&#20803;&#39044;&#27979;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22987;&#32456;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20135;&#29983;&#25345;&#32493;&#25968;&#21313;&#20010;&#26446;&#38597;&#26222;&#35834;&#22827;&#26102;&#38388;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#26368;&#20339;&#30340;&#28151;&#27788;&#39044;&#27979;&#32467;&#26524;&#30001;&#26368;&#36817;&#24341;&#20837;&#30340;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#23454;&#29616;&#65292;&#20294;&#21363;&#20351;&#26159;&#36890;&#29992;&#30340;&#21464;&#21387;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#21551;&#21457;&#24335;&#28151;&#21512;&#26041;&#27861;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20648;&#23618;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;HealthSyn&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#21457;&#23637;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2303.01954</link><description>&lt;p&gt;
&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generator for Adaptive Interventions in Global Health. (arXiv:2303.01954v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01954
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;HealthSyn&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#22312;&#20840;&#29699;&#21355;&#29983;&#39046;&#22495;&#20013;&#21457;&#23637;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#23383;&#20581;&#24247;&#26377;&#26395;&#25913;&#21464;&#20840;&#29699;&#21355;&#29983;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#31639;&#27861;&#27979;&#35797;&#21644;&#39564;&#35777;&#30340;&#20851;&#38190;&#26159;&#33021;&#22815;&#35775;&#38382;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HealthSyn&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#27979;&#35797;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#20013;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#65288;&#20363;&#22914;&#25552;&#37266;&#12289;&#25512;&#33616;&#21644;&#28608;&#21169;&#65289;&#12290;&#29983;&#25104;&#22120;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20855;&#26377;&#20010;&#20307;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#20010;&#24615;&#21270;&#24178;&#39044;&#32780;&#25913;&#21464;&#12290;&#36825;&#20123;&#34892;&#20026;&#36716;&#21270;&#20026;&#23454;&#38469;&#26085;&#24535;&#65292;&#20351;&#29992;ML&#19987;&#29992;&#30340;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#23450;&#20110;HealthKit&#19982;&#24320;&#28304;SDK&#20013;&#21253;&#21547;&#30340;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#21151;&#33021;&#12290;&#36825;&#20123;&#26085;&#24535;&#21487;&#20197;&#25552;&#20379;&#29992;&#25143;&#25351;&#26631;&#12290;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#34892;&#20026;&#21644;&#27169;&#25311;&#25216;&#26415;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#20197;&#25104;&#26412;&#25928;&#30410;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#21516;&#26102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and digital health have the potential to transform global health. However, having access to representative data to test and validate algorithms in realistic production environments is essential. We introduce HealthSyn, an open-source synthetic data generator of user behavior for testing reinforcement learning algorithms in the context of mobile health interventions. The generator utilizes Markov processes to generate diverse user actions, with individual user behavioral patterns that can change in reaction to personalized interventions (i.e., reminders, recommendations, and incentives). These actions are translated into actual logs using an ML-purposed data schema specific to the mobile health application functionality included with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain user metrics. The generated data, which is based on real-world behaviors and simulation techniques, can be used to develop, test, and evaluate, both ML algori
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#65292;&#22312;&#20445;&#30041;&#22270;&#24418;&#20851;&#38190;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#31561;&#12290;</title><link>http://arxiv.org/abs/2302.06114</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#24418;&#27010;&#25324;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Summarization with Graph Neural Networks. (arXiv:2302.06114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#65292;&#22312;&#20445;&#30041;&#22270;&#24418;&#20851;&#38190;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#25361;&#25112;&#26292;&#38706;&#20986;&#26469;&#65292;&#38656;&#35201;&#25552;&#21462;&#12289;&#22788;&#29702;&#21644;&#35299;&#37322;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#24191;&#38420;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20851;&#38190;&#29305;&#24449;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#36807;&#21435;&#65292;&#22823;&#22810;&#25968;&#22270;&#24418;&#27010;&#25324;&#25216;&#26415;&#26088;&#22312;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25429;&#25417;&#22270;&#24418;&#30340;&#26368;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#65292;&#29616;&#20195;&#22270;&#24418;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26356;&#21152;&#27969;&#34892;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#28145;&#24230;&#23398;&#20064;&#27010;&#25324;&#25216;&#26415;&#36827;&#23637;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#22238;&#39038;&#65292;&#21253;&#25324;&#24490;&#29615;GNNs&#12289;&#21367;&#31215;GNNs&#12289;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#19968;&#26465;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#20351;&#29992;&#22270;&#24418;&#24378;&#21270;&#23398;&#20064;&#26469;&#35780;&#20272;&#21644;&#25913;&#36827;&#22270;&#24418;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large-scale graphs become more widespread, more and more computational challenges with extracting, processing, and interpreting large graph data are being exposed. It is therefore natural to search for ways to summarize these expansive graphs while preserving their key characteristics. In the past, most graph summarization techniques sought to capture the most important part of a graph statistically. However, today, the high dimensionality and complexity of modern graph data are making deep learning techniques more popular. Hence, this paper presents a comprehensive survey of progress in deep learning summarization techniques that rely on graph neural networks (GNNs). Our investigation includes a review of the current state-of-the-art approaches, including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph attention networks. A new burgeoning line of research is also discussed where graph reinforcement learning is being used to evaluate and improve the quality of grap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HydroGraphs &#30340;&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#24320;&#28304;&#25968;&#25454;&#26500;&#24314;&#65292;&#20855;&#26377;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;&#33021;&#22815;&#24110;&#21161;&#20934;&#30830;&#23450;&#20301;&#27745;&#26579;&#28304;&#21644;&#33030;&#24369;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.04991</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#36861;&#36394;&#34920;&#38754;&#27700;&#20307;&#20013;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters. (arXiv:2302.04991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HydroGraphs &#30340;&#22522;&#20110;&#22270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#27700;&#25991;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#24320;&#28304;&#25968;&#25454;&#26500;&#24314;&#65292;&#20855;&#26377;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;&#33021;&#22815;&#24110;&#21161;&#20934;&#30830;&#23450;&#20301;&#27745;&#26579;&#28304;&#21644;&#33030;&#24369;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27745;&#26579;&#23545;&#20840;&#29699;&#21508;&#22320;&#30340;&#31038;&#21306;&#21644;&#29983;&#24577;&#31995;&#32479;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#27169;&#24037;&#20855;&#22312;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#20851;&#38190;&#28304;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#27700;&#25991;&#31995;&#32479;&#20013;&#36861;&#36394;&#20256;&#36755;&#21644;&#37327;&#21270;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;${\tt HydroGraphs}$&#8221;&#30340;&#22270;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#27700;&#20307;&#12289;&#27827;&#27969;&#21644;&#27969;&#22495;&#20013;&#27745;&#26579;&#29289;&#30340;&#20256;&#36755;&#21644;&#21629;&#36816;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31616;&#21270;&#30340;&#27700;&#25991;&#31995;&#32479;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#24320;&#25918;&#28304;&#25968;&#25454;&#65288;&#22269;&#23478;&#27700;&#25991;&#25968;&#25454;&#38598;&#21644;&#27969;&#22495;&#36793;&#30028;&#25968;&#25454;&#38598;&#65289;&#26500;&#24314;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#22270;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#25216;&#26415;&#36827;&#34892;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#23545;&#19978;&#23494;&#35199;&#35199;&#27604;&#27827;&#27969;&#22495;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#21457;&#29616;&#27745;&#26579;&#28304;&#12289;&#29702;&#35299;&#36816;&#36755;&#36884;&#24452;&#21644;&#20934;&#30830;&#23450;&#20301;&#33030;&#24369;&#21306;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anthropogenic pollution of hydrological systems affects diverse communities and ecosystems around the world. Data analytics and modeling tools play a key role in fighting this challenge, as they can help identify key sources as well as trace transport and quantify impact within complex hydrological systems. Several tools exist for simulating and tracing pollutant transport throughout surface waters using detailed physical models; these tools are powerful, but can be computationally intensive, require significant amounts of data to be developed, and require expert knowledge for their use (ultimately limiting application scope). In this work, we present a graph modeling framework -which we call ${\tt HydroGraphs}$ -- for understanding pollutant transport and fate across waterbodies, rivers, and watersheds. This framework uses a simplified representation of hydrological systems that can be constructed based purely on open-source data (National Hydrography Dataset and Watershed Boundary 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#32500;&#24230;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#26469;&#28040;&#38500;&#32852;&#37030;&#23398;&#20064;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04464</link><description>&lt;p&gt;
&#22810;&#32500;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#22312;&#23454;&#29616;&#26356;&#20844;&#24179;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models. (arXiv:2302.04464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#32500;&#24230;&#20010;&#24615;&#21270;&#36793;&#32536;&#27169;&#22411;&#26469;&#28040;&#38500;&#32852;&#37030;&#23398;&#20064;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#25252;&#38544;&#31169;&#35757;&#32451;&#22823;&#35268;&#27169;&#22320;&#29702;&#20998;&#24067;&#24335;&#36793;&#32536;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#30340;&#24322;&#36136;&#24615;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#22312;&#20844;&#24179;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#36890;&#24120;&#20250;&#23548;&#33268;&#26368;&#36817;&#19968;&#27969;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65288;CFL&#65289;&#26469;&#28040;&#38500;&#22810;&#32500;&#24230;&#32852;&#37030;&#23398;&#20064;&#30340;&#24322;&#36136;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;CFL &#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#37327;&#36523;&#23450;&#21046;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#22312;&#32447;&#35757;&#32451;&#30340;&#27169;&#22411;&#25628;&#32034;&#21161;&#25163;&#21644;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#20849;&#21516;&#24341;&#23548;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CFL &#22312;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20840;&#26632;&#20248;&#21183;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#65288;&#22312;&#38750;&#24322;&#36136;&#29615;&#22659;&#19979;&#39640;&#36798; 7.2%&#65292;&#22312;&#24322;&#36136;&#29615;&#22659;&#19979;&#39640;&#36798; 21.8%&#65289;&#12289;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging technique that trains massive and geographically distributed edge data while maintaining privacy. However, FL has inherent challenges in terms of fairness and computational efficiency due to the rising heterogeneity of edges, and thus usually results in sub-optimal performance in recent state-of-the-art (SOTA) solutions. In this paper, we propose a Customized Federated Learning (CFL) system to eliminate FL heterogeneity from multiple dimensions. Specifically, CFL tailors personalized models from the specially designed global model for each client jointly guided by an online trained model-search helper and a novel aggregation algorithm. Extensive experiments demonstrate that CFL has full-stack advantages for both FL training and edge reasoning and significantly improves the SOTA performance w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up to 21.8% in the heterogeneous environment), efficiency, and FL fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#22823;&#26377;&#29992;&#22788;&#12290;</title><link>http://arxiv.org/abs/2302.03281</link><description>&lt;p&gt;
&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65306;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#22823;&#26377;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#38754;&#23545;&#38750;&#31283;&#24577;&#38382;&#39064;&#26102;&#24448;&#24448;&#38590;&#20197;&#24555;&#36895;&#36866;&#24212;&#65292;&#22240;&#20026;&#23427;&#20204;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#34928;&#20943;&#30340;&#21487;&#22609;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#23398;&#20064;&#32773;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#22240;&#20026;&#20182;&#20204;&#21487;&#33021;&#20250;&#36951;&#24536;&#26377;&#29992;&#30340;&#29305;&#24449;&#25110;&#38590;&#20197;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21464;&#24471;&#26080;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36825;&#31181;&#31639;&#27861;&#38750;&#24120;&#36866;&#21512;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#12290;UPGD&#20445;&#25252;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#19981;&#34987;&#36951;&#24536;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#30340;&#25928;&#29992;&#25200;&#21160;&#19981;&#22826;&#26377;&#29992;&#30340;&#26435;&#37325;&#25110;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;UPGD&#26377;&#21161;&#20110;&#20943;&#23569;&#36951;&#24536;&#21644;&#20445;&#25345;&#21487;&#22609;&#24615;&#65292;&#20351;&#29616;&#20195;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#26377;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#26041;&#24046;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#21516;&#26102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#30028;&#38480;&#26159;&#31532;&#19968;&#27425;&#34987;&#35777;&#26126;&#20986;&#26469;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.13446</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23574;&#38160;&#26041;&#24046;&#30456;&#20851;&#30028;&#38480;&#65306;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#26368;&#20339;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments. (arXiv:2301.13446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#26041;&#24046;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#29615;&#22659;&#21516;&#26102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#30028;&#38480;&#26159;&#31532;&#19968;&#27425;&#34987;&#35777;&#26126;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#12290;&#20855;&#26377;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#30340;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#21033;&#29992;&#20855;&#26377;&#20302;&#26041;&#24046;&#65288;&#20363;&#22914;&#65292;&#22312;&#30830;&#23450;&#24615;MDP&#19978;&#20139;&#26377;&#24120;&#37327;&#36951;&#25022;&#65289;&#30340;&#29615;&#22659;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#29420;&#31435;&#20110;&#26041;&#24046;&#35201;&#20040;&#27425;&#20248;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20004;&#20010;&#26032;&#30340;&#29615;&#22659;&#35268;&#33539;&#26469;&#34920;&#24449;&#29615;&#22659;&#30340;&#32454;&#31890;&#24230;&#26041;&#24046;&#23646;&#24615;&#12290;&#23545;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;MVP&#31639;&#27861;(Zhang&#31561;&#65292;2021a)&#30340;&#21464;&#31181;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#20998;&#26512;&#25216;&#26415;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#35268;&#33539;&#20139;&#26377;&#26041;&#24046;&#30456;&#20851;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#19968;&#30028;&#38480;&#23545;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;MDP&#21516;&#26102;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65292;&#36825;&#26159;&#20854;&#31181;&#31867;&#20013;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21442;&#32771;&#20989;&#25968;&#30340;&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#24102;&#26377;&#19978;&#38480;&#21152;&#20493;&#21442;&#32771;&#26356;&#26032;&#36827;&#24230;&#34920;&#30340;&#31574;&#30053;&#21551;&#21160;&#20102;&#20851;&#20110;&#20855;&#26377;&#26041;&#24046;&#30456;&#20851;&#36951;&#25022;&#30028;&#38480;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new analysis techniques show to this algorithm enjoys variance-dependent bounds with respect to our proposed norms. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind. We further initiate the study on model-free algorithms with variance-dependent regret bounds by designing a reference-function-based algorithm with a novel capped-doubling reference update schedule. Lastly, we also provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861; $\textit{MPC-RRL}$&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#24471;&#21040;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.13313</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#32435;&#20837;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#65292;&#23454;&#29616;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving. (arXiv:2301.13313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861; $\textit{MPC-RRL}$&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#24471;&#21040;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290; MPC&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#24378;&#28872;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#20869;&#37096;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290; &#28982;&#32780;&#65292;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#30340;&#38745;&#24577;&#21442;&#25968;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24448;&#24448;&#26080;&#27861;&#36866;&#24212;&#20869;&#37096;&#21644;&#22806;&#37096;&#24178;&#25200;&#12290; &#26412;&#25991;&#39318;&#20808;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#21560;&#25910;&#21040;&#35266;&#23519;&#20013;&#24182;&#23558;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#32500;&#25252;&#21040;&#38544;&#34255;&#29366;&#24577;&#20013;; &#20854;&#27425;&#65292;&#36890;&#36807;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65288;RRL&#65289;&#23398;&#20064;&#36882;&#24402;&#31574;&#30053;&#65292;&#25345;&#32493;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#21442;&#25968;&#20197;&#23454;&#29616;&#26368;&#20248;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;; &#26368;&#21518;&#65292;&#22312;CARLA&#27169;&#25311;&#22120;&#20013;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65288;&#31216;&#20026; $\textit{MPC-RRL}$&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25200;&#21160;&#33539;&#22260;&#20869;&#23454;&#29616;&#40065;&#26834;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) is attracting tremendous attention in the autonomous driving task as a powerful control technique. The success of an MPC controller strongly depends on an accurate internal dynamics model. However, the static parameters, usually learned by system identification, often fail to adapt to both internal and external perturbations in real-world scenarios. In this paper, we firstly (1) reformulate the problem as a Partially Observed Markov Decision Process (POMDP) that absorbs the uncertainties into observations and maintains Markov property into hidden states; and (2) learn a recurrent policy continually adapting the parameters of the dynamics model via Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and (3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in CARLA simulator and leading to robust behaviours under a wide range of perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38598;&#21512;&#31232;&#30095;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#20013;&#30340;&#33258;&#21161;&#27861;&#24207;&#21015;&#38408;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#35823;&#24046;&#29575;&#21644;&#21487;&#35777;&#26126;&#27491;&#30830;&#30340;&#21464;&#37327;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12649</link><description>&lt;p&gt;
Ensemble&#21644;Bayesian&#31232;&#30095;&#27169;&#22411;&#21457;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of uncertainty estimates in Ensemble and Bayesian sparse model discovery. (arXiv:2301.12649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38598;&#21512;&#31232;&#30095;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#20013;&#30340;&#33258;&#21161;&#27861;&#24207;&#21015;&#38408;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#35823;&#24046;&#29575;&#21644;&#21487;&#35777;&#26126;&#27491;&#30830;&#30340;&#21464;&#37327;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#35782;&#21035;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#25968;&#25454;&#21644;&#39640;&#22122;&#22768;&#30340;&#38480;&#21046;&#19979;&#65292;&#25511;&#21046;&#31232;&#30095;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#30740;&#31350;&#38598;&#21512;&#31232;&#30095;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#33258;&#21161;&#27861;&#30340;&#39034;&#24207;&#38408;&#20540;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#33258;&#21161;&#27861;&#30340;&#38598;&#25104;&#25216;&#26415;&#21487;&#20197;&#25191;&#34892;&#21487;&#35777;&#26126;&#27491;&#30830;&#30340;&#21464;&#37327;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#35823;&#24046;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#21512;&#31232;&#30095;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#27604;&#20351;&#29992;&#26114;&#36149;&#30340;&#22522;&#20110;MCMC&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25910;&#25947;&#24615;&#21644;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse model identification enables nonlinear dynamical system discovery from data. However, the control of false discoveries for sparse model identification is challenging, especially in the low-data and high-noise limit. In this paper, we perform a theoretical study on ensemble sparse model discovery, which shows empirical success in terms of accuracy and robustness to noise. In particular, we analyse the bootstrapping-based sequential thresholding least-squares estimator. We show that this bootstrapping-based ensembling technique can perform a provably correct variable selection procedure with an exponential convergence rate of the error rate. In addition, we show that the ensemble sparse model discovery method can perform computationally efficient uncertainty estimation, compared to expensive Bayesian uncertainty quantification methods via MCMC. We demonstrate the convergence properties and connection to uncertainty quantification in various numerical studies on synthetic sparse li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36235;&#21183;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#24314;&#31435;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#30830;&#23450;&#20102;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20248;&#21270;&#21644;&#27169;&#22411;&#20989;&#25968;&#22797;&#26434;&#24230;&#38480;&#21046;&#26159;&#20851;&#38190;&#22240;&#32032;&#65292;&#35813;&#30740;&#31350;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12309</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#21644;&#21452;&#37325;&#19979;&#38477;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36235;&#21183;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#24314;&#31435;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#30830;&#23450;&#20102;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20248;&#21270;&#21644;&#27169;&#22411;&#20989;&#25968;&#22797;&#26434;&#24230;&#38480;&#21046;&#26159;&#20851;&#38190;&#22240;&#32032;&#65292;&#35813;&#30740;&#31350;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;&#30028;&#38480;&#37117;&#26159;&#22522;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#24179;&#28369;&#25110;&#26377;&#30028;&#20381;&#36182;&#24615;&#65292;&#27809;&#26377;&#30740;&#31350;&#25506;&#31350;&#23454;&#36341;&#20013;&#25511;&#21046;&#36825;&#20123;&#22240;&#32032;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#23545;&#32463;&#21382;&#21452;&#37325;&#34928;&#20943;&#30340;&#28145;&#24230;&#32593;&#32476;&#30340;&#23454;&#39564;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#38750;&#21333;&#35843;&#30340;&#36235;&#21183;&#65292;&#19982;&#27979;&#35797;&#35823;&#24046;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#24314;&#31435;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21442;&#25968;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#26799;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#20998;&#31163;&#20986;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#21644;&#36317;&#31163;&#21021;&#22987;&#21270;&#21442;&#25968;&#30340;&#36317;&#31163;&#65292;&#20998;&#21035;&#25511;&#21046;&#20851;&#38190;&#28857;&#21608;&#22260;&#30340;&#20248;&#21270;&#21160;&#24577;&#65292;&#24182;&#38480;&#21046;&#27169;&#22411;&#20989;&#25968;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#21270;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#23454;&#36341;&#20013;&#32593;&#32476;&#30340;&#26377;&#25928;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#20551;&#35774;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#24182;&#36890;&#36807;&#20195;&#29702;&#30340;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#21487;&#20197;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12050</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#20855;&#36523;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#20551;&#35774;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#24182;&#36890;&#36807;&#20195;&#29702;&#30340;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#21487;&#20197;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#22312;&#27809;&#26377;&#20808;&#21069;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21021;&#22987;&#21270;&#39640;&#23618;&#23376;&#30446;&#26631;&#21644;&#23376;&#30446;&#26631;&#20043;&#38388;&#30340;&#36716;&#25442;&#30693;&#35782;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#27492;&#25277;&#35937;&#19990;&#30028;&#27169;&#22411;&#65288;AWM&#65289;&#36827;&#34892;&#35268;&#21010;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20551;&#35774;AWM&#65292;&#36890;&#36807;&#19990;&#30028;&#32463;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;DECKARD&#20195;&#29702;&#23558;LLM&#24341;&#23548;&#30340;&#25506;&#32034;&#24212;&#29992;&#20110;Minecraft&#20013;&#30340;&#29289;&#21697;&#21046;&#20316;&#65292;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#26790;&#24819;&#38454;&#27573;&#65292;&#20195;&#29702;&#20351;&#29992;LLM&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#21363;&#20551;&#35774;&#30340;AWM&#65307;&#65288;2&#65289;&#21796;&#37266;&#38454;&#27573;&#65292;&#20195;&#29702;&#20026;&#27599;&#20010;&#23376;&#30446;&#26631;&#23398;&#20064;&#27169;&#22359;&#21270;&#31574;&#30053;&#24182;&#39564;&#35777;&#25110;&#32416;&#27491;&#20551;&#35774;&#30340;AWM&#12290;&#25105;&#20204;&#36890;&#36807;LLMs&#20551;&#35774;AWM&#65292;&#28982;&#21518;&#26681;&#25454;&#20195;&#29702;&#32463;&#39564;&#39564;&#35777;AWM&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#23558;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#30340;&#20248;&#32570;&#28857;&#65292;&#23427;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#27604;&#26222;&#36890;&#22343;&#34913;&#36924;&#36817;&#22120;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#26524;&#65292;&#21516;&#26102;&#23427;&#20063;&#23384;&#22312;&#22343;&#34913;&#36873;&#25321;&#21644;&#31038;&#20250;&#31119;&#21033;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2301.11481</link><description>&lt;p&gt;
&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#26377;&#30410;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Equivariant Equilibrium Approximators Beneficial?. (arXiv:2301.11481v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#30340;&#20248;&#32570;&#28857;&#65292;&#23427;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#27604;&#26222;&#36890;&#22343;&#34913;&#36924;&#36817;&#22120;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#26524;&#65292;&#21516;&#26102;&#23427;&#20063;&#23384;&#22312;&#22343;&#34913;&#36873;&#25321;&#21644;&#31038;&#20250;&#31119;&#21033;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20989;&#25968;&#36924;&#36817;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12289;&#30456;&#20851;&#22343;&#34913;&#65288;CE&#65289;&#21644;&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#22312;&#27491;&#24577;&#21338;&#24328;&#20013;&#35774;&#35745;&#36825;&#31181;&#22343;&#34913;&#36924;&#36817;&#22120;&#36890;&#24120;&#37319;&#29992;&#31561;&#21464;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#27604;&#26222;&#36890;&#22343;&#34913;&#36924;&#36817;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#24403;&#25910;&#30410;&#20998;&#24067;&#26159;&#32622;&#25442;&#19981;&#21464;&#30340;&#26102;&#65292;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#26524;&#65307;&#28982;&#32780;&#65292;&#31561;&#21464;&#22343;&#34913;&#36924;&#36817;&#22120;&#22312;&#22343;&#34913;&#36873;&#25321;&#21644;&#31038;&#20250;&#31119;&#21033;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;&#31561;&#21464;&#24615;&#22312;&#22343;&#34913;&#36924;&#36817;&#22120;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made by approximating Nash equilibrium (NE), correlated equilibrium (CE), and coarse correlated equilibrium (CCE) through function approximation that trains a neural network to predict equilibria from game representations. Furthermore, equivariant architectures are widely adopted in designing such equilibrium approximators in normal-form games. In this paper, we theoretically characterize benefits and limitations of equivariant equilibrium approximators. For the benefits, we show that they enjoy better generalizability than general ones and can achieve better approximations when the payoff distribution is permutation-invariant. For the limitations, we discuss their drawbacks in terms of equilibrium selection and social welfare. Together, our results help to understand the role of equivariance in equilibrium approximators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#24674;&#22797;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20851;&#33410;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#24182;&#35757;&#32451;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#32570;&#23569;&#26412;&#20307;&#24863;&#30693;&#26102;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.02051</link><description>&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#24674;&#22797;&#26426;&#22120;&#20154;&#20851;&#33410;&#35282;&#24230;&#30340;&#36317;&#31163;&#20960;&#20309;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image. (arXiv:2301.02051v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#24674;&#22797;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20851;&#33410;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#24182;&#35757;&#32451;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#22312;&#32570;&#23569;&#26412;&#20307;&#24863;&#30693;&#26102;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#24178;&#39044;&#24456;&#38590;&#25110;&#19981;&#21487;&#33021;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#27700;&#19979;&#65292;&#22826;&#31354;&#25110;&#21361;&#38505;&#29615;&#22659;&#65289;&#25805;&#20316;&#30340;&#33258;&#20027;&#25805;&#32437;&#31995;&#32479;&#38656;&#35201;&#39640;&#24230;&#24378;&#20581;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#25925;&#38556;&#12290;&#20851;&#38190;&#26159;&#65292;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31639;&#27861;&#38656;&#35201;&#25552;&#20379;&#20851;&#33410;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#20934;&#30830;&#20851;&#33410;&#35282;&#24230;&#25968;&#25454;&#27969;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#23548;&#33268;&#21151;&#33021;&#20007;&#22833;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#20154;&#24403;&#21069;&#37197;&#32622;&#30340;&#21333;&#20010;RGB&#22270;&#20687;&#23601;&#21487;&#20197;&#26816;&#32034;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#20851;&#33410;&#35282;&#24230;&#65292;&#20026;&#24674;&#22797;&#31995;&#32479;&#21151;&#33021;&#24320;&#36767;&#20102;&#19968;&#26465;&#36884;&#24452;&#65292;&#36825;&#26102;&#24120;&#29992;&#30340;&#26412;&#20307;&#24863;&#30693;&#26080;&#27861;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#37197;&#32622;&#31354;&#38388;&#30340;&#36317;&#31163;&#20960;&#20309;&#34920;&#31034;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#25191;&#34892;&#19982;&#26816;&#27979;&#21040;&#30340;&#32467;&#26500;&#20851;&#38190;&#28857;&#30456;&#20851;&#30340;&#36317;&#31163;&#30340;2D&#21040;3D&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous manipulation systems operating in domains where human intervention is difficult or impossible (e.g., underwater, extraterrestrial or hazardous environments) require a high degree of robustness to sensing and communication failures. Crucially, motion planning and control algorithms require a stream of accurate joint angle data provided by joint encoders, the failure of which may result in an unrecoverable loss of functionality. In this paper, we present a novel method for retrieving the joint angles of a robot manipulator using only a single RGB image of its current configuration, opening up an avenue for recovering system functionality when conventional proprioceptive sensing is unavailable. Our approach, based on a distance-geometric representation of the configuration space, exploits the knowledge of a robot's kinematic model with the goal of training a shallow neural network that performs a 2D-to-3D regression of distances associated with detected structural keypoints. It
&lt;/p&gt;</description></item><item><title>&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2301.01188</link><description>&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming. (arXiv:2301.01188v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;&#26159;&#19968;&#38376;&#20840;&#38754;&#30340;&#35838;&#31243;&#65292;&#37325;&#28857;&#20171;&#32461;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#27969;&#34892;&#30340;&#35821;&#35328;&#20043;&#19968;&#8212;&#8212;R&#35821;&#35328;&#65288;&#32479;&#35745;&#35745;&#31639;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#28165;&#27927;&#21644;&#20998;&#26512;&#65289;&#12290;&#23427;&#28145;&#20837;&#20171;&#32461;&#20102;R&#35821;&#35328;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#26088;&#22312;&#22521;&#20859;&#26377;&#25265;&#36127;&#30340;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#65292;&#35753;&#20182;&#20204;&#25104;&#20026;&#29420;&#31435;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#29615;&#22659;&#30340;&#29992;&#25143;&#12290;&#36825;&#20010;&#25945;&#26448;&#26159;&#19968;&#20010;&#38750;&#30408;&#21033;&#39033;&#30446;&#65292;&#23427;&#30340;&#22312;&#32447;&#21644;PDF&#29256;&#26412;&#21487;&#20197;&#22312; &lt;https://deepr.gagolewski.com/&gt; &#20813;&#36153;&#33719;&#21462;&#12290;&#24076;&#26395;&#36825;&#20010;&#26089;&#26399;&#33609;&#26696;&#21457;&#25918;&#20986;&#26469;&#21518;&#33021;&#23545;&#35835;&#32773;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming is a comprehensive course on one of the most popular languages in data science (statistical computing, graphics, machine learning, data wrangling and analytics). It introduces the base language in-depth and is aimed at ambitious students, practitioners, and researchers who would like to become independent users of this powerful environment. This textbook is a non-profit project. Its online and PDF versions are freely available at &lt;https://deepr.gagolewski.com/&gt;. This early draft is distributed in the hope that it will be useful.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20018;&#32852;&#36339;&#36807;&#36830;&#25509;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#29992;&#20110;&#38109;&#30005;&#35299;&#27133;&#35782;&#21035;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#24320;&#29615;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#25552;&#20379;&#31283;&#23450;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.00582</link><description>&lt;p&gt;
&#20855;&#26377;&#36339;&#36807;&#36830;&#25509;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38109;&#30005;&#35299;&#27133;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Sparse neural networks with skip-connections for identification of aluminum electrolysis cell. (arXiv:2301.00582v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20018;&#32852;&#36339;&#36807;&#36830;&#25509;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#29992;&#20110;&#38109;&#30005;&#35299;&#27133;&#35782;&#21035;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#30340;&#24320;&#29615;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#25552;&#20379;&#31283;&#23450;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#33021;&#22815;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#25429;&#33719;&#22797;&#26434;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20851;&#31995;&#30340;&#27169;&#22411;&#33021;&#21147;&#32780;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#20013;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#20173;&#28982;&#23384;&#22312;&#26377;&#20851;&#36825;&#20123;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#28508;&#22312;&#26114;&#36149;&#30340;&#25968;&#25454;&#12290;&#38109;&#30005;&#35299;&#26159;&#19968;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#29983;&#20135;&#36807;&#31243;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#24517;&#39035;&#25163;&#21160;&#37319;&#26679;&#65292;&#20351;&#37319;&#26679;&#36807;&#31243;&#26114;&#36149;&#19988;&#39057;&#29575;&#20302;&#12290;&#22312;&#29366;&#24577;&#21464;&#37327;&#23569;&#37327;&#27979;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#38271;&#26399;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24320;&#29615;&#31283;&#23450;&#24615;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#24456;&#38590;&#25552;&#20379;&#31283;&#23450;&#30340;&#38271;&#26399;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#20018;&#32852;&#36339;&#36807;&#36830;&#25509;&#21644;&#20419;&#36827;&#31232;&#30095;&#30340;$\ell _{1}$&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#23545;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#30340;&#24320;&#29615;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are rapidly gaining interest in nonlinear system identification due to the model's ability to capture complex input-output relations directly from data. However, despite the flexibility of the approach, there are still concerns about the safety of these models in this context, as well as the need for large amounts of potentially expensive data. Aluminum electrolysis is a highly nonlinear production process, and most of the data must be sampled manually, making the sampling process expensive and infrequent. In the case of infrequent measurements of state variables, the accuracy and open-loop stability of the long-term predictions become highly important. Standard neural networks struggle to provide stable long-term predictions with limited training data. In this work, we investigate the effect of combining concatenated skip-connections and the sparsity-promoting $\ell_1$ regularization on the open-loop stability and accuracy of forecasts with short, medium, and long pred
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#65292;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#20197;&#21069;&#30340;&#31574;&#30053;&#36136;&#37327;&#36739;&#39640;&#20294;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.11431</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Local Policy Improvement for Recommender Systems. (arXiv:2212.11431v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#65292;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#20197;&#21069;&#30340;&#31574;&#30053;&#36136;&#37327;&#36739;&#39640;&#20294;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22522;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#20114;&#21160;&#34892;&#20026;&#32780;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#20250;&#19982;&#21738;&#20123;&#39033;&#30446;&#20132;&#20114;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#26368;&#36817;&#30340;&#36827;&#23637;&#36716;&#21521;&#20102;&#22522;&#20110;&#22870;&#21169;&#65288;&#20363;&#22914;&#29992;&#25143;&#21442;&#19982;&#24230;&#65289;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#21518;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#31574;&#30053;&#19981;&#21305;&#37197;&#65306;&#25105;&#20204;&#21482;&#33021;&#22522;&#20110;&#20197;&#21069;&#37096;&#32626;&#31574;&#30053;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26032;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23454;&#38469;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#21644;&#20248;&#21270;&#30446;&#26631;&#31574;&#30053;&#39044;&#26399;&#22870;&#21169;&#30340;&#19979;&#38480;&#65292;&#36825;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#24182;&#19988;&#19981;&#28041;&#21450;&#23494;&#24230;&#27604;&#65288;&#20363;&#22914;&#22312;&#37325;&#35201;&#24615;&#37319;&#26679;&#26657;&#27491;&#20013;&#20986;&#29616;&#30340;&#27604;&#29575;&#65289;&#12290;&#36825;&#31181;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#33539;&#20363;&#38750;&#24120;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;&#31574;&#30053;&#36890;&#24120;&#36136;&#37327;&#36739;&#39640;&#65292;&#31574;&#30053;&#30340;&#25968;&#37327;&#20063;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems predict what items a user will interact with next, based on their past interactions. The problem is often approached through supervised learning, but recent advancements have shifted towards policy optimization of rewards (e.g., user engagement). One challenge with the latter is policy mismatch: we are only able to train a new policy given data collected from a previously-deployed policy. The conventional way to address this problem is through importance sampling correction, but this comes with practical limitations. We suggest an alternative approach of local policy improvement without off-policy correction. Our method computes and optimizes a lower bound of expected reward of the target policy, which is easy to estimate from data and does not involve density ratios (such as those appearing in importance sampling correction). This local policy improvement paradigm is ideal for recommender systems, as previous policies are typically of decent quality and policies ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.07430</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interactive Concept Bottleneck Models. (arXiv:2212.07430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#26159;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#39318;&#20808;&#38024;&#23545;&#19982;&#39044;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#39044;&#27979;&#26631;&#31614;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#39044;&#27979;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;CBMs&#25193;&#23637;&#21040;&#20132;&#20114;&#24335;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21521;&#20154;&#31867;&#21327;&#20316;&#32773;&#26597;&#35810;&#26576;&#20123;&#27010;&#24565;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20132;&#20114;&#31574;&#30053;&#65292;&#22312;&#39044;&#27979;&#26102;&#36873;&#25321;&#35201;&#35831;&#27714;&#26631;&#31614;&#30340;&#27010;&#24565;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#26368;&#32456;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#27010;&#24565;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27010;&#24565;&#23545;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#22312;Caltech-UCSD Birds&#12289;CheXpert&#21644;OAI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#38745;&#24577;&#26041;&#27861;&#21644;&#20027;&#21160;&#29305;&#24449;&#37319;&#38598;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#20114;&#24335;CBM&#21482;&#38656;&#36827;&#34892;5&#27425;&#20132;&#20114;&#65292;&#23601;&#33021;&#22312;&#31454;&#20105;&#22522;&#20934;&#19978;&#23454;&#29616;5-10%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#30456;&#27604;&#20110;&#35768;&#22810;&#20854;&#20182; fine-tuning &#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.03220</link><description>&lt;p&gt;
&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65306;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning. (arXiv:2212.03220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#30456;&#27604;&#20110;&#35768;&#22810;&#20854;&#20182; fine-tuning &#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#35757;&#32451;&#27169;&#22411;&#30340;&#20013;&#38388;&#29305;&#24449;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#38750;&#24120;&#26377;&#29992;&#65292;&#21363;&#20351;&#27169;&#22411;&#39592;&#24178;&#20445;&#25345;&#20923;&#32467;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20013;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;-&#35270;&#35273;&#26597;&#35810;&#35843;&#25972;&#65288;VQT&#65289;&#65292;&#29992;&#20110;&#32858;&#21512;Vision Transformers&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#23569;&#37327;&#21487;&#23398;&#20064;&#30340;&#8220;&#26597;&#35810;&#8221;&#20196;&#29260;&#65292;VQT&#21033;&#29992;Transformer&#30340;&#20869;&#37096;&#36816;&#34892;&#26426;&#21046;&#26469;&#8220;&#24635;&#32467;&#8221;&#27599;&#20010;&#23618;&#30340;&#20016;&#23500;&#20013;&#38388;&#29305;&#24449;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#22836;&#12290;&#30001;&#20110;VQT&#20445;&#25345;&#20102;&#20013;&#38388;&#29305;&#24449;&#30340;&#23436;&#25972;&#24615;&#24182;&#20165;&#23398;&#20064;&#20102;&#22914;&#20309;&#32452;&#21512;&#23427;&#20204;&#65292;&#22240;&#27492;&#19982;&#35768;&#22810;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#65292;VQT&#22312;&#35757;&#32451;&#20013;&#20855;&#26377;&#20869;&#23384;&#25928;&#29575;&#65292;&#21518;&#32773;&#38656;&#35201;&#23398;&#20064;&#22914;&#20309;&#36866;&#24212;&#29305;&#24449;&#24182;&#38656;&#35201;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#36825;&#20063;&#34920;&#26126;&#20102;VQT&#19982;&#36825;&#20123;&#26041;&#27861;&#22312;&#32763;&#35793;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;VQT&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#23545;&#35937;&#26816;&#27979;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is kept frozen. The key challenge is how to utilize these intermediate features given their gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable ``query'' tokens to each layer, VQT leverages the inner workings of Transformers to ``summarize'' rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efficiency in training, compared to many other parameter-efficient fine-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in tran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35299;&#20915;&#37326;&#28779;&#21069;&#27839;&#24314;&#27169;&#38382;&#39064;&#65292;&#22312;&#20248;&#21270;&#27169;&#22411;&#20013;&#25913;&#36827;&#20102;&#25104;&#26412;&#20989;&#25968;&#20197;&#25552;&#39640;&#22312;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#20540;&#19982;&#21069;&#27839;&#35266;&#27979;&#20540;&#30456;&#32467;&#21512;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;PINN&#65288;B-PINN&#65289;&#20197;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.00970</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#37326;&#28779;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Physics Informed Neural Networks for Data Assimilation and Spatio-Temporal Modelling of Wildfires. (arXiv:2212.00970v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35299;&#20915;&#37326;&#28779;&#21069;&#27839;&#24314;&#27169;&#38382;&#39064;&#65292;&#22312;&#20248;&#21270;&#27169;&#22411;&#20013;&#25913;&#36827;&#20102;&#25104;&#26412;&#20989;&#25968;&#20197;&#25552;&#39640;&#22312;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#20540;&#19982;&#21069;&#27839;&#35266;&#27979;&#20540;&#30456;&#32467;&#21512;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;PINN&#65288;B-PINN&#65289;&#20197;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#37326;&#28779;&#21069;&#27839;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#35813;&#32593;&#32476;&#35299;&#20915;&#20102;&#27700;&#24179;&#38598;&#26041;&#31243;&#65292;&#24314;&#31435;&#20102;&#27169;&#25311;&#37326;&#28779;&#21069;&#27839;&#38543;&#26102;&#38388;&#25512;&#31227;&#28436;&#21270;&#30340;PINN&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#20248;&#21270;&#25104;&#26412;&#20989;&#25968;&#20197;&#25552;&#39640;&#22312;&#26497;&#31471;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#20540;&#19982;&#21069;&#27839;&#35266;&#27979;&#20540;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;PINN&#65288;B-PINN&#65289;&#20197;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply the Physics Informed Neural Network (PINN) to the problem of wildfire fire-front modelling. We use the PINN to solve the level-set equation, which is a partial differential equation that models a fire-front through the zero-level-set of a level-set function. The result is a PINN that simulates a fire-front as it propagates through the spatio-temporal domain. We show that popular optimisation cost functions used in the literature can result in PINNs that fail to maintain temporal continuity in modelled fire-fronts when there are extreme changes in exogenous forcing variables such as wind direction. We thus propose novel additions to the optimisation cost function that improves temporal continuity under these extreme changes. Furthermore, we develop an approach to perform data assimilation within the PINN such that the PINN predictions are drawn towards observations of the fire-front. Finally, we incorporate our novel approaches into a Bayesian PINN (B-PINN) to provide uncertain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#32422;&#26463;&#20989;&#25968;&#20316;&#20026;&#24809;&#32602;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#20197;&#23548;&#33268;&#36739;&#24369;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#24809;&#32602;&#39033;&#36827;&#34892;&#31616;&#21333;&#20462;&#27491;&#65292;&#33021;&#22815;&#24378;&#21046;&#32422;&#26463;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.16069</link><description>&lt;p&gt;
&#23545;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting Primal-Dual Algorithms for Constrained Multiagent Reinforcement Learning. (arXiv:2211.16069v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#32422;&#26463;&#20989;&#25968;&#20316;&#20026;&#24809;&#32602;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#20197;&#23548;&#33268;&#36739;&#24369;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#24809;&#32602;&#39033;&#36827;&#34892;&#31616;&#21333;&#20462;&#27491;&#65292;&#33021;&#22815;&#24378;&#21046;&#32422;&#26463;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(C-MARL)&#22312;&#23454;&#38469;&#31995;&#32479;(&#20174;&#33021;&#28304;&#31995;&#32479;&#21040;&#26080;&#20154;&#26426;&#32676;)&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;C-MARL&#31639;&#27861;&#36890;&#36807;&#23558;&#24809;&#32602;&#20989;&#25968;&#28155;&#21152;&#21040;&#22870;&#21169;&#20013;&#65292;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#26469;&#23454;&#29616;&#32422;&#26463;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#31181;&#24809;&#32602;&#39033;&#23545;MARL&#38382;&#39064;&#30340;&#32467;&#26500;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#32422;&#26463;&#20989;&#25968;&#20316;&#20026;&#24809;&#32602;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#20197;&#23548;&#33268;&#19968;&#31181;&#36739;&#24369;&#30340;&#23433;&#20840;&#24615;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23545;&#24809;&#32602;&#39033;&#36827;&#34892;&#31616;&#21333;&#20462;&#27491;&#65292;&#25105;&#20204;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#26377;&#24847;&#20041;&#30340;&#27010;&#29575;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#24809;&#32602;&#39033;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;&#30340;&#20215;&#20540;&#20272;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;&#35780;&#35770;&#23478;(C-MAA2C)&#31639;&#27861;&#12290;&#22312;&#31616;&#21333;&#30340;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#23545;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#30340;&#37325;&#26032;&#35299;&#37322;&#21487;&#20197;&#22312;MARL&#38382;&#39064;&#20013;&#23548;&#33268;&#26356;&#22909;&#30340;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained multiagent reinforcement learning (C-MARL) is gaining importance as MARL algorithms find new applications in real-world systems ranging from energy systems to drone swarms. Most C-MARL algorithms use a primal-dual approach to enforce constraints through a penalty function added to the reward. In this paper, we study the structural effects of this penalty term on the MARL problem. First, we show that the standard practice of using the constraint function as the penalty leads to a weak notion of safety. However, by making simple modifications to the penalty term, we can enforce meaningful probabilistic (chance and conditional value at risk) constraints. Second, we quantify the effect of the penalty term on the value function, uncovering an improved value estimation procedure. We use these insights to propose a constrained multiagent advantage actor critic (C-MAA2C) algorithm. Simulations in a simple constrained multiagent environment affirm that our reinterpretation of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crown-CAM&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#12289;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#21644;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#31561;&#27493;&#39588;&#65292;&#26082;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#21448;&#21487;&#20197;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13126</link><description>&lt;p&gt;
Crown-CAM&#65306;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images. (arXiv:2211.13126v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crown-CAM&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#12289;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#21644;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#31561;&#27493;&#39588;&#65292;&#26082;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#21448;&#21487;&#20197;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#30340;&#21487;&#35270;&#35299;&#37322;&#20801;&#35768;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#20197;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26641;&#20896;&#26816;&#27979;&#31867;&#28608;&#27963;&#26144;&#23556;&#26041;&#27861;&#65288;Crown-CAM&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#30340;&#23450;&#20301;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21516;&#26102;&#20026;&#39640;&#31354;&#22270;&#20687;&#20013;&#26641;&#20896;&#26816;&#27979;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21160;&#24577;&#30340;&#38382;&#39064;&#25552;&#20379;&#21487;&#38752;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#12290;&#23427;&#21253;&#25324;&#26080;&#30417;&#30563;&#36873;&#25321;&#28608;&#27963;&#26144;&#23556;&#65292;&#35745;&#31639;&#23616;&#37096;&#24471;&#20998;&#26144;&#23556;&#20197;&#21450;&#38750;&#19978;&#19979;&#25991;&#32972;&#26223;&#25233;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#26641;&#20896;&#30340;&#31934;&#32454;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#23494;&#26519;&#25110;&#32773;&#27809;&#26377;&#26641;&#20896;&#30340;&#22330;&#26223;&#12290;&#21478;&#22806;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20110;IoU&#30340;&#25351;&#26631;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37327;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#22270;&#20687;&#20013;&#26641;&#20896;&#25110;&#26080;&#26641;&#20896;&#21306;&#22495;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual explanation of ``black-box'' models allows researchers in explainable artificial intelligence (XAI) to interpret the model's decisions in a human-understandable manner. In this paper, we propose interpretable class activation mapping for tree crown detection (Crown-CAM) that overcomes inaccurate localization &amp; computational complexity of previous methods while generating reliable visual explanations for the challenging and dynamic problem of tree crown detection in aerial images. It consists of an unsupervised selection of activation maps, computation of local score maps, and non-contextual background suppression to efficiently provide fine-grain localization of tree crowns in scenarios with dense forest trees or scenes without tree crowns. Additionally, two Intersection over Union (IoU)-based metrics are introduced to effectively quantify both the accuracy and inaccuracy of generated explanations with respect to regions with or even without tree crowns in the image. Empirical e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; GCPNet&#65292;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20960;&#20309;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20854;&#20013;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#24471;&#21040;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.02504</link><description>&lt;p&gt;
&#29992;&#20110;&#19977;&#32500;&#20998;&#23376;&#22270;&#30340;&#20960;&#20309;&#23436;&#22791;&#24863;&#30693;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Complete Perceptron Networks for 3D Molecular Graphs. (arXiv:2211.02504v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; GCPNet&#65292;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20960;&#20309;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20854;&#20013;&#26368;&#20339;&#34920;&#29616;&#26159;&#22312;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#24471;&#21040;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#21019;&#26032;&#21644;&#24378;&#22823;&#30340;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21457;&#23637;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#31561;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#23398;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#20174;&#32780;&#22312;&#31185;&#23398;&#39046;&#22495;&#22914;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#21644;&#35774;&#35745;&#20013;&#23454;&#29616;&#20102;&#31361;&#30772;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GCPNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#23436;&#22791;&#12289;SE(3)-&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;3D&#20998;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22235;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20219;&#21153;&#30340;&#20005;&#23494;&#23454;&#39564;&#35777;&#26126;&#20102;GCPNet&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#30456;&#20851;&#31995;&#25968;&#20026;0.608&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#39640;&#20986;5%&#20197;&#19978;&#65307;&#65288;2&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#25490;&#21517;&#22312;&#30446;&#26631;&#26412;&#22320;&#21644;&#25968;&#25454;&#38598;&#20840;&#23616;&#20043;&#38388;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#65292;&#20998;&#21035;&#20026;0.616&#21644;0.871&#65307;&#65288;3&#65289;Newtownian&#22810;&#20307;&#31995;&#32479;&#30340;&#24314;&#27169;&#24179;&#22343;&#25104;&#32489;&#36798;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
The field of geometric deep learning has had a profound impact on the development of innovative and powerful graph neural network architectures. Disciplines such as computer vision and computational biology have benefited significantly from such methodological advances, which has led to breakthroughs in scientific domains such as protein structure prediction and design. In this work, we introduce GCPNet, a new geometry-complete, SE(3)-equivariant graph neural network designed for 3D molecular graph representation learning. Rigorous experiments across four distinct geometric tasks demonstrate that GCPNet's predictions (1) for protein-ligand binding affinity achieve a statistically significant correlation of 0.608, more than 5% greater than current state-of-the-art methods; (2) for protein structure ranking achieve statistically significant target-local and dataset-global correlations of 0.616 and 0.871, respectively; (3) for Newtownian many-body systems modeling achieve a task-averaged 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#39046;&#22495;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#21644;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.16484</link><description>&lt;p&gt;
&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Chemical Pre-trained Models. (arXiv:2210.16484v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#39046;&#22495;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#22238;&#39038;&#65292;&#25552;&#20986;&#20102;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#23637;&#21644;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#23545;&#20110;&#21508;&#31181;&#29983;&#21270;&#24212;&#29992;&#65292;&#20174;&#23646;&#24615;&#39044;&#27979;&#21040;&#33647;&#29289;&#35774;&#35745;&#65292;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20998;&#23376;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33719;&#21462;&#36825;&#20123;&#26631;&#35760;&#36890;&#24120;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#20184;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#20998;&#23376;&#25968;&#25454;&#24211;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#38024;&#23545;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#23613;&#31649;&#29289;&#26377;&#25152;&#20540;&#65292;&#20294;&#36825;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#39046;&#22495;&#32570;&#20047;&#31995;&#32479;&#30340;&#22238;&#39038;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24635;&#32467;CPMs&#36827;&#23637;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20174;&#22836;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#25512;&#21160;CPM&#30340;&#30740;&#31350;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#35282;&#24230;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20998;&#23376;&#25551;&#36848;&#31526;&#65292;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Molecular Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder arc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;&#65292;&#24182;&#34920;&#26126;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#20998;&#32452;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2210.16315</link><description>&lt;p&gt;
&#36229;&#36234;&#26657;&#20934;&#65306;&#20272;&#35745;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#32452;&#25439;&#22833;&#65292;&#24182;&#34920;&#26126;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#20998;&#32452;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20998;&#31867;&#22120;&#32473;&#20986;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#30830;&#20445;&#30693;&#24773;&#20915;&#31574;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35823;&#26657;&#20934;&#19978;&#65292;&#21363;&#27169;&#22411;&#20998;&#25968;&#30340;&#36807;&#24230;&#25110;&#19981;&#36275;&#32622;&#20449;&#12290;&#28982;&#32780;&#65292;&#26657;&#20934;&#36824;&#19981;&#22815;&#65306;&#21363;&#20351;&#20934;&#30830;&#29575;&#26368;&#39640;&#30340;&#23436;&#32654;&#26657;&#20934;&#20998;&#31867;&#22120;&#20063;&#21487;&#33021;&#20855;&#26377;&#19982;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#30456;&#21435;&#29978;&#36828;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#36825;&#26159;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#25152;&#36896;&#25104;&#30340;&#65292;&#21363;&#20197;&#30456;&#21516;&#32622;&#20449;&#24230;&#24471;&#20998;&#20294;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#26657;&#20934;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#24449;&#21333;&#20010;&#38169;&#35823;&#30340;&#32570;&#22833;&#37096;&#20998;&#26159;&#20998;&#32452;&#25439;&#22833;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#26657;&#20934;&#25439;&#22833;&#30340;&#20272;&#35745;&#22120;&#65292;&#20294;&#22312;&#26631;&#20934;&#35774;&#32622;&#20013;&#19981;&#23384;&#22312;&#20998;&#32452;&#25439;&#22833;&#30340;&#20272;&#35745;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#20998;&#32452;&#25439;&#22833;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#35270;&#35273;&#21644;NLP&#20013;&#34920;&#29616;&#20986;&#20998;&#32452;&#25439;&#22833;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#20559;&#31227;&#35774;&#32622;&#20013;&#65292;&#36825;&#31361;&#26174;&#20102;&#23427;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13179</link><description>&lt;p&gt;
Occam&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#30340;&#20998;&#24067;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#36825;&#31181;&#20307;&#31995;&#26550;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#35768;&#22810;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#36136;&#12290;&#20363;&#22914;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36873;&#25321;&#20026;&#31616;&#21333;&#19988;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#32780;&#19988;&#22312;&#28909;&#21147;&#23398;&#24847;&#20041;&#19979;&#65292;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#24403;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32570;&#20047;&#29305;&#24449;&#30340;&#29366;&#24577;&#23545;&#24212;&#20110;&#22312;&#29305;&#24449;&#26041;&#38754;&#26368;&#22823;&#31243;&#24230;&#30340;&#26080;&#30693;&#29366;&#24577;&#65292;&#24182;&#19988;&#65292;&#23398;&#20064;&#31532;&#19968;&#20010;&#29305;&#24449;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#38750;&#39640;&#26031;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26681;&#25454;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#36873;&#25321;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#21518;&#35265;&#24615;&#30340; LMDP &#24378;&#21270;&#23398;&#20064;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#22522;&#30784;&#31639;&#27861;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#35745;&#21010;&#35270;&#37326;&#23545;&#25968;&#30456;&#20851;&#30340; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#36951;&#25022;&#24230;&#19978;&#38480;&#65292;&#24182;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\Omega\left(\sqrt{M S A K}\right)$ &#30340;&#36951;&#25022;&#24230;&#19979;&#38480;&#65292;&#23427;&#22312; $\Gamma=2$ &#26102;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.11604</link><description>&lt;p&gt;
&#26080;&#35270;&#35268;&#21010;&#22320;&#25512;&#24191;&#27178;&#36328;&#21464;&#37327;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes. (arXiv:2210.11604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#21518;&#35265;&#24615;&#30340; LMDP &#24378;&#21270;&#23398;&#20064;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#22522;&#30784;&#31639;&#27861;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#19982;&#35745;&#21010;&#35270;&#37326;&#23545;&#25968;&#30456;&#20851;&#30340; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#36951;&#25022;&#24230;&#19978;&#38480;&#65292;&#24182;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\Omega\left(\sqrt{M S A K}\right)$ &#30340;&#36951;&#25022;&#24230;&#19979;&#38480;&#65292;&#23427;&#22312; $\Gamma=2$ &#26102;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21518;&#35265;&#24615;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (LMDPs) &#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#20048;&#35266;&#25110;&#20540;&#20048;&#35266;&#27714;&#35299;&#22120;&#23454;&#20363;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#36951;&#25022;&#24230;&#30340;&#36739;&#23567;&#37327;&#32423;&#20026; $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ &#30340;&#30028;&#38480;&#65292;&#20854;&#20013; $M$ &#26159;&#19978;&#19979;&#25991;&#25968;&#37327;&#65292;$S$ &#26159;&#29366;&#24577;&#25968;&#37327;&#65292;$A$ &#26159;&#21160;&#20316;&#25968;&#37327;&#65292;$K$ &#26159;&#22238;&#21512;&#25968;&#37327;&#65292;&#32780; $\Gamma \le S$ &#26159;&#20219;&#20309;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#26368;&#22823;&#36716;&#31227;&#27425;&#25968;&#12290;&#36951;&#25022;&#24230;&#21482;&#22312;&#35268;&#21010;&#35270;&#37326;&#20013;&#20197;&#23545;&#25968;&#24418;&#24335;&#32553;&#25918;&#65292;&#25152;&#20197; LMDP &#30340;&#35268;&#21010;&#35270;&#37326;&#30340;&#31532;&#19968;&#20010;(&#20960;&#20046;)&#26080;&#35270;&#30028;&#38480;&#23601;&#34987;&#20135;&#29983;&#20102;&#12290;&#25105;&#20204;&#30340;&#35770;&#35777;&#30340;&#20851;&#38190;&#26159;&#23545; alpha &#21521;&#37327;&#30340;&#24635;&#26041;&#24046;&#36827;&#34892;&#20998;&#26512;&#65292;&#35813;&#26041;&#24046;&#36890;&#36807;&#36882;&#24402;&#25216;&#26415;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340; $\Omega\left(\sqrt{M S A K}\right)$ &#36951;&#25022;&#24615;&#19979;&#38480;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#27491;&#34917;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403; $\Gamma=2$ &#26102;&#65292;&#25105;&#20204;&#30340;&#19978;&#30028;&#26159;&#26497;&#23567;&#21270;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ regret bound where $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, and $\Gamma \le S$ is the maximum transition degree of any state-action pair. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. Key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. We complement our positive result with a novel $\Omega\left(\sqrt{M S A K}\right)$ regret lower bound with $\Gamma = 2$, which shows our upper bound minimax optimal when $\Gamma$
&lt;/p&gt;</description></item><item><title>Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.09184</link><description>&lt;p&gt;
&#32039;&#20945;&#38598;&#25104;&#29992;&#20110;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles for Efficient Uncertainty Estimation. (arXiv:2210.09184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09184
&lt;/p&gt;
&lt;p&gt;
Packed-Ensembles&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#20869;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#25928;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#23454;&#29616;&#20851;&#38190;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#12289;&#26657;&#20934;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#65289;&#21331;&#36234;&#24615;&#33021;&#30340;&#31361;&#20986;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#31995;&#32479;&#30340;&#30828;&#20214;&#38480;&#21046;&#38480;&#21046;&#20102;&#26356;&#23567;&#30340;&#38598;&#21512;&#21644;&#36739;&#20302;&#23481;&#37327;&#30340;&#32593;&#32476;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Packed-Ensembles&#65288;PE&#65289;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31934;&#24515;&#35843;&#33410;&#20854;&#32534;&#30721;&#31354;&#38388;&#30340;&#32500;&#24230;&#26469;&#35774;&#35745;&#21644;&#35757;&#32451;&#36731;&#37327;&#32423;&#32467;&#26500;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#32452;&#21367;&#31215;&#23558;&#38598;&#21512;&#24182;&#34892;&#21270;&#20026;&#21333;&#20010;&#20849;&#20139;&#39592;&#24178;&#65292;&#24182;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;PE&#26088;&#22312;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38480;&#21046;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#26469;&#25171;&#30772;betaTCVAE&#20013;&#30340;&#20840;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;VAE&#33021;&#22815;&#26356;&#21152;&#28789;&#27963;&#22320;&#21010;&#20998;&#25968;&#25454;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#23481;&#37327;&#21644;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.08794</link><description>&lt;p&gt;
&#25171;&#30772;betaTCVAE&#20013;&#20840;&#30456;&#20851;&#24615;&#30340;&#39764;&#21650;
&lt;/p&gt;
&lt;p&gt;
Break The Spell Of Total Correlation In betaTCVAE. (arXiv:2210.08794v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#26469;&#25171;&#30772;betaTCVAE&#20013;&#30340;&#20840;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;VAE&#33021;&#22815;&#26356;&#21152;&#28789;&#27963;&#22320;&#21010;&#20998;&#25968;&#25454;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#23481;&#37327;&#21644;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#20154;&#24037;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#20013;&#30340;&#29420;&#31435;&#29305;&#24449;&#21644;&#20381;&#36182;&#29305;&#24449;&#26159;&#28151;&#20081;&#30340;&#12290;&#22914;&#20309;&#26500;&#24314;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20197;&#28789;&#27963;&#21010;&#20998;&#24182;&#26377;&#25928;&#22320;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#29305;&#24449;&#65292;&#26159;&#26080;&#30417;&#30563;&#30340;&#20998;&#31163;&#34920;&#24449;&#23398;&#20064;&#30340;&#20027;&#35201;&#28966;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24635;&#30456;&#20851;&#24615;&#30340;&#36845;&#20195;&#20998;&#35299;&#36335;&#24452;&#65292;&#24182;&#20174;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;VAE&#30340;&#20998;&#31163;&#34920;&#24449;&#33021;&#21147;&#12290;&#26032;&#24320;&#21457;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#28508;&#21464;&#37327;&#32500;&#24230;&#32452;&#21512;&#25104;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#20943;&#36731;&#32452;&#21512;&#20013;&#36793;&#32536;&#20998;&#24067;&#30340;&#29420;&#31435;&#24615;&#32422;&#26463;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#26356;&#26131;&#25805;&#20316;&#20808;&#39564;&#20998;&#24067;&#30340;&#28508;&#21464;&#37327;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20351;&#24471;VAE&#33021;&#22815;&#35843;&#25972;&#21442;&#25968;&#23481;&#37327;&#65292;&#20197;&#28789;&#27963;&#21010;&#20998;&#30456;&#20851;&#21644;&#29420;&#31435;&#30340;&#25968;&#25454;&#29305;&#24449;&#12290;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#27169;&#22411;&#23481;&#37327;&#19982;&#28508;&#21464;&#37327;&#20998;&#32452;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the absence of artificial labels, the independent and dependent features in the data are cluttered. How to construct the inductive biases of the model to flexibly divide and effectively contain features with different complexity is the main focal point of unsupervised disentangled representation learning. This paper proposes a new iterative decomposition path of total correlation and explains the disentangled representation ability of VAE from the perspective of model capacity allocation. The newly developed objective function combines latent variable dimensions into joint distribution while relieving the independence constraints of marginal distributions in combination, leading to latent variables with a more manipulable prior distribution. The novel model enables VAE to adjust the parameter capacity to divide dependent and independent data features flexibly. Experimental results on various datasets show an interesting relevance between model capacity and the latent variable groupi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-net&#21644;CBAM&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38051;&#37329;&#24037;&#31243;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#35270;&#35273;&#20449;&#24687;&#33258;&#21160;&#20998;&#21106;&#29305;&#23450;&#22270;&#24418;&#21333;&#20803;&#65292;&#22312;&#33258;&#21160;&#20999;&#21106;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14102</link><description>&lt;p&gt;
&#22522;&#20110;CBAM&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;U-net&#38051;&#37329;&#24037;&#31243;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Segmentation method of U-net sheet metal engineering drawing based on CBAM attention mechanism. (arXiv:2209.14102v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-net&#21644;CBAM&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38051;&#37329;&#24037;&#31243;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#35270;&#35273;&#20449;&#24687;&#33258;&#21160;&#20998;&#21106;&#29305;&#23450;&#22270;&#24418;&#21333;&#20803;&#65292;&#22312;&#33258;&#21160;&#20999;&#21106;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#37325;&#22411;&#24037;&#19994;&#35774;&#22791;&#30340;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#36827;&#34892;&#28938;&#25509;&#22270;&#20013;&#30340;&#29305;&#23450;&#22270;&#24418;&#21333;&#20803;&#37325;&#26032;&#32472;&#21046;&#65292;&#28982;&#21518;&#25165;&#33021;&#20999;&#21106;&#30456;&#24212;&#30340;&#38051;&#37329;&#38646;&#37096;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-net&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28938;&#25509;&#24037;&#31243;&#22270;&#30340;&#29305;&#23450;&#22270;&#24418;&#21333;&#20803;&#30340;&#20998;&#21106;&#21644;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#35270;&#35273;&#20449;&#24687;&#33258;&#21160;&#20998;&#21106;&#29305;&#23450;&#22270;&#24418;&#21333;&#20803;&#65292;&#24182;&#26681;&#25454;&#20998;&#21106;&#32467;&#26524;&#33258;&#21160;&#20999;&#21106;&#20986;&#30456;&#24212;&#24418;&#29366;&#30340;&#38051;&#37329;&#38646;&#20214;&#12290;&#36825;&#20010;&#36807;&#31243;&#27604;&#20256;&#32479;&#30340;&#20154;&#24037;&#36741;&#21161;&#20999;&#21106;&#26356;&#26377;&#25928;&#29575;&#12290;&#22312;U-net&#32593;&#32476;&#20013;&#23384;&#22312;&#20004;&#20010;&#24369;&#28857;&#65292;&#20250;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#38477;&#20302;&#65306;&#39318;&#20808;&#65292;&#23545;&#20840;&#23616;&#35821;&#20041;&#29305;&#24449;&#20449;&#24687;&#30340;&#20851;&#27880;&#19981;&#22815;&#65292;&#20854;&#27425;&#65292;&#27973;&#23618;&#32534;&#30721;&#22120;&#29305;&#24449;&#21644;&#28145;&#23618;&#35299;&#30721;&#22120;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#22823;&#30340;&#23610;&#24230;&#24046;&#24322;&#12290;&#22522;&#20110;CBAM&#65288;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#65289;&#27880;&#24847;&#26426;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38051;&#37329;&#24037;&#31243;&#22270;&#20687;&#20998;&#21106;&#30340;U-net&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#24369;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;98.97&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the manufacturing process of heavy industrial equipment, the specific unit in the welding diagram is first manually redrawn and then the corresponding sheet metal parts are cut, which is inefficient. To this end, this paper proposes a U-net-based method for the segmentation and extraction of specific units in welding engineering drawings. This method enables the cutting device to automatically segment specific graphic units according to visual information and automatically cut out sheet metal parts of corresponding shapes according to the segmentation results. This process is more efficient than traditional human-assisted cutting. Two weaknesses in the U-net network will lead to a decrease in segmentation performance: first, the focus on global semantic feature information is weak, and second, there is a large dimensional difference between shallow encoder features and deep decoder features. Based on the CBAM (Convolutional Block Attention Module) attention mechanism, this paper pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28041;&#21450;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#12289;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#12289;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#26426;&#36935;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26377;&#21161;&#20110;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.11234</link><description>&lt;p&gt;
&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering. (arXiv:2209.11234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28041;&#21450;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#12289;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#12289;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#26426;&#36935;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26377;&#21161;&#20110;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#39640;&#24615;&#33021;&#35745;&#31639;&#26426;&#30340;&#21457;&#23637;&#20351;&#24471;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#26469;&#20811;&#26381;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65289;&#22312;&#24615;&#33021;&#39044;&#27979;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#22522;&#20110;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#26041;&#27861;&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26377;&#21161;&#20110;&#22312;&#19981;&#20351;&#29992;&#26230;&#20307;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#12290;&#36825;&#20123;&#21457;&#23637;&#23545;&#26448;&#26009;&#24037;&#31243;&#21644;&#30740;&#31350;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#27010;&#25324;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#20197;&#21450;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#31561;&#20851;&#38190;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#34987;&#29992;&#26469;&#20248;&#21270;&#21644;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of artificial intelligence (AI) in material science and engineering (MSE) is becoming increasingly important as AI technology advances. The development of high-performance computing has made it possible to test deep learning (DL) models with significant parameters, providing an opportunity to overcome the limitation of traditional computational methods, such as density functional theory (DFT), in property prediction. Machine learning (ML)-based methods are faster and more accurate than DFT-based methods. Furthermore, the generative adversarial networks (GANs) have facilitated the generation of chemical compositions of inorganic materials without using crystal structure information. These developments have significantly impacted material engineering (ME) and research. Some of the latest developments in AI in ME herein are reviewed. First, the development of AI in the critical areas of ME, such as in material processing, the study of structure and material property, and measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#25511;&#21046;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#36825;&#20123;&#36827;&#23637;&#20027;&#35201;&#22260;&#32469;&#22312;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#21644;&#23398;&#20064;&#26041;&#38754;&#65292;&#22522;&#20110;&#29616;&#20195;&#39640;&#32500;&#32479;&#35745;&#23398;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#34701;&#20837;&#25511;&#21046;&#39046;&#22495;&#30340;&#20154;&#25552;&#20379;&#20102;&#33258;&#21253;&#21547;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2209.05423</link><description>&lt;p&gt;
&#25511;&#21046;&#30340;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#65306;&#26377;&#38480;&#26679;&#26412;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Statistical Learning Theory for Control: A Finite Sample Perspective. (arXiv:2209.05423v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#25511;&#21046;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#36825;&#20123;&#36827;&#23637;&#20027;&#35201;&#22260;&#32469;&#22312;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#21644;&#23398;&#20064;&#26041;&#38754;&#65292;&#22522;&#20110;&#29616;&#20195;&#39640;&#32500;&#32479;&#35745;&#23398;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#34701;&#20837;&#25511;&#21046;&#39046;&#22495;&#30340;&#20154;&#25552;&#20379;&#20102;&#33258;&#21253;&#21547;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#19982;&#25511;&#21046;&#21644;&#31995;&#32479;&#36776;&#35782;&#30456;&#20851;&#30340;&#38750;&#28176;&#36817;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#25152;&#26377;&#25511;&#21046;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20294;&#22312;&#32447;&#24615;&#31995;&#32479;&#36776;&#35782;&#21644;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#23398;&#20064;&#26041;&#38754;&#65292;&#35813;&#29702;&#35770;&#26368;&#20026;&#25104;&#29087;&#65292;&#32780;&#36825;&#20063;&#26159;&#26412;&#25991;&#30340;&#37325;&#28857;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#35762;&#65292;&#36825;&#20123;&#36827;&#23637;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#22312;&#20110;&#20511;&#37492;&#29616;&#20195;&#39640;&#32500;&#32479;&#35745;&#23398;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#24037;&#20855;&#12290;&#34429;&#28982;&#23545;&#20110;&#37027;&#20123;&#26377;&#20852;&#36259;&#23558;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#34701;&#20837;&#25511;&#21046;&#39046;&#22495;&#30340;&#25511;&#21046;&#29702;&#35770;&#23478;&#26469;&#35828;&#38750;&#24120;&#30456;&#20851;&#65292;&#20294;&#36825;&#20123;&#22522;&#30784;&#26448;&#26009;&#24182;&#19981;&#24635;&#26159;&#26131;&#20110;&#33719;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#32032;&#26448;&#30340;&#33258;&#21253;&#21547;&#28436;&#31034;&#65292;&#27010;&#36848;&#20102;&#25152;&#26377;&#20851;&#38190;&#24605;&#24819;&#21644;&#25216;&#26415;&#26426;&#26800;&#65292;&#20026;&#26368;&#36817;&#30340;&#32467;&#26524;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial survey provides an overview of recent non-asymptotic advances in statistical learning theory as relevant to control and system identification. While there has been substantial progress across all areas of control, the theory is most well-developed when it comes to linear system identification and learning for the linear quadratic regulator, which are the focus of this manuscript. From a theoretical perspective, much of the labor underlying these advances has been in adapting tools from modern high-dimensional statistics and learning theory. While highly relevant to control theorists interested in integrating tools from machine learning, the foundational material has not always been easily accessible. To remedy this, we provide a self-contained presentation of the relevant material, outlining all the key ideas and the technical machinery that underpin recent results. We also present a number of open problems and future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32454;&#32990;Latent Go-Explore&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2208.14928</link><description>&lt;p&gt;
&#26080;&#32454;&#32990;Latent Go-Explore
&lt;/p&gt;
&lt;p&gt;
Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32454;&#32990;Latent Go-Explore&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Latent Go-Explore (LGE)&#30340;&#31616;&#21333;&#36890;&#29992;&#26041;&#27861;&#65292;&#22522;&#20110;Go-Explore&#33539;&#24335;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;Go-Explore&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;&#27809;&#26377;&#39046;&#22495;&#30693;&#35782;&#21644;&#21333;&#20803;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#20219;&#20309;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LGE&#21487;&#20197;&#28789;&#27963;&#22320;&#19982;&#20219;&#20309;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#30340;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LGE&#27604;Go-Explore&#26356;&#21152;&#40065;&#26834;&#65292;&#22312;&#22810;&#20010;&#38590;&#20197;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#65288;&#21253;&#25324;Montezuma&#30340;&#22797;&#20167;&#65289;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25506;&#32034;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;Deep Learning Approximate Matching&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;AI&#19982;AM&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#21407;&#22987;&#24037;&#20214;&#30340;&#30862;&#29255;&#21487;&#29992;&#26102;&#65292;&#21487;&#38752;&#39640;&#25928;&#22320;&#26816;&#27979;&#40657;&#21517;&#21333;&#20013;&#19982;&#26696;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2208.11367</link><description>&lt;p&gt;
AI&#19982;AM&#30340;&#32467;&#21512;&#8212;&#8212;&#36890;&#36807;Transformer&#32593;&#32476;&#25552;&#39640;&#36817;&#20284;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Combining AI and AM - Improving Approximate Matching through Transformer Networks. (arXiv:2208.11367v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;Deep Learning Approximate Matching&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;AI&#19982;AM&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#21407;&#22987;&#24037;&#20214;&#30340;&#30862;&#29255;&#21487;&#29992;&#26102;&#65292;&#21487;&#38752;&#39640;&#25928;&#22320;&#26816;&#27979;&#40657;&#21517;&#21333;&#20013;&#19982;&#26696;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#21305;&#37197;&#65288;AM&#65289;&#26159;&#25968;&#23383;&#21462;&#35777;&#20013;&#29992;&#20110;&#30830;&#23450;&#25968;&#23383;&#24037;&#20214;&#30456;&#20284;&#24230;&#30340;&#27010;&#24565;&#12290; AM&#30340;&#37325;&#35201;&#29992;&#20363;&#26159;&#22312;&#21482;&#26377;&#21407;&#22987;&#24037;&#20214;&#30340;&#30862;&#29255;&#21487;&#29992;&#26102;&#65292;&#21487;&#38752;&#39640;&#25928;&#22320;&#26816;&#27979;&#40657;&#21517;&#21333;&#20013;&#19982;&#26696;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;Transformer&#27169;&#22411;&#30340;&#25913;&#36827;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;Deep Learning Approximate Matching&#65288;DLAM&#65289;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate matching (AM) is a concept in digital forensics to determine the similarity between digital artifacts. An important use case of AM is the reliable and efficient detection of case-relevant data structures on a blacklist, if only fragments of the original are available. For instance, if only a cluster of indexed malware is still present during the digital forensic investigation, the AM algorithm shall be able to assign the fragment to the blacklisted malware. However, traditional AM functions like TLSH and ssdeep fail to detect files based on their fragments if the presented piece is relatively small compared to the overall file size. A second well-known issue with traditional AM algorithms is the lack of scaling due to the ever-increasing lookup databases. We propose an improved matching algorithm based on transformer models from the field of natural language processing. We call our approach Deep Learning Approximate Matching (DLAM). As a concept from artificial intelligence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#23567;&#22411;CNN&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;T-RECX&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20013;&#38388;&#20998;&#31867;&#22120;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#33410;&#30465;&#27169;&#22411;&#23481;&#37327;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#26089;&#26399;&#36864;&#20986;&#21151;&#33021;&#30340;tiny-RECX&#27169;&#22411;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2207.06613</link><description>&lt;p&gt;
T-RECX&#65306;&#20855;&#26377;&#26089;&#26399;&#36864;&#20986;&#30340;&#23567;&#22411;&#36164;&#28304;&#26377;&#25928;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit. (arXiv:2207.06613v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#23567;&#22411;CNN&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;T-RECX&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20013;&#38388;&#20998;&#31867;&#22120;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#33410;&#30465;&#27169;&#22411;&#23481;&#37327;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#26089;&#26399;&#36864;&#20986;&#21151;&#33021;&#30340;tiny-RECX&#27169;&#22411;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#31361;&#30772;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#37096;&#32626;&#22312;&#27627;&#29926;&#32423;&#36793;&#32536;&#35774;&#22791;&#65288;tinyML&#65289;&#19978;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#22823;&#22810;&#25968;tinyML&#30740;&#31350;&#32858;&#28966;&#20110;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#20102;&#36866;&#24212;KB&#32423;tiny-edge&#35774;&#22791;&#32780;&#20197;&#31934;&#24230;&#65288;&#21644;&#27169;&#22411;&#23481;&#37327;&#65289;&#20026;&#20195;&#20215;&#33719;&#24471;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20013;&#38388;&#20998;&#31867;&#22120;&#26469;&#22686;&#24378;&#36825;&#26679;&#30340;&#27169;&#22411;&#12290;&#22914;&#26524;&#20013;&#38388;&#20998;&#31867;&#22120;&#23545;&#20854;&#39044;&#27979;&#20855;&#26377;&#36275;&#22815;&#30340;&#20449;&#24515;&#65292;&#37027;&#20040;&#32593;&#32476;&#23558;&#25552;&#21069;&#36864;&#20986;&#65292;&#20174;&#32780;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#12290;&#34429;&#28982;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#22823;&#22411;&#32593;&#32476;&#65292;&#20351;&#23427;&#20204;&#30340;&#25216;&#26415;&#22312;tinyML&#24212;&#29992;&#20013;&#19981;&#22826;&#20248;&#21270;/&#23454;&#29992;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#19987;&#38376;&#38024;&#23545;&#23567;&#22411;CNN&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#20943;&#36731;&#32593;&#32476;&#36807;&#24230;&#24605;&#32771;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20855;&#26377;&#26089;&#26399;&#36864;&#20986;&#21151;&#33021;&#30340;tiny-RECX&#27169;&#22411;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2206.04530</link><description>&lt;p&gt;
DORA&#65306;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20540;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#24182;&#21487;&#20197;&#35782;&#21035;&#19981;&#31526;&#21512;&#20154;&#31867;&#30452;&#35266;&#35748;&#30693;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#23398;&#20064;&#22797;&#26434;&#25277;&#35937;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#24847;&#22806;&#22320;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#26816;&#26597;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24847;&#22806;&#30340;&#27010;&#24565;&#24448;&#24448;&#34920;&#29616;&#20026;&#19982;&#25152;&#38656;&#30340;&#20219;&#21153;&#19981;&#31526;&#30340;&#24322;&#24120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DORA&#65288;Data-agnOstic Representation Analysis&#65289;&#65306;&#29992;&#20110;&#20998;&#26512;DNN&#34920;&#31034;&#31354;&#38388;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#19981;&#21487;&#30693;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#26497;&#31471;&#28608;&#27963;&#65288;EA&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#22312;&#19981;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#32593;&#32476;&#20869;&#33258;&#35828;&#26126;&#33021;&#21147;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#24230;&#37327;&#30340;&#27491;&#30830;&#24615;&#21644;&#19982;&#20154;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#36317;&#31163;&#30340;&#19968;&#33268;&#24615;&#12290;EA&#36317;&#31163;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#34920;&#24449;&#65292;&#20854;&#22522;&#26412;&#27010;&#24565;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;RL&#20013;&#38598;&#25104;&#24494;&#20998;&#22806;&#22312;&#21487;&#22609;&#24615;&#65288;DEP&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#21040;&#36798;&#21644;&#36816;&#21160;&#65292;&#24182;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#20219;&#21153;&#20013;&#65292;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.00484</link><description>&lt;p&gt;
DEP-RL&#65306;&#36807;&#24230;&#20316;&#21160;&#21644;&#32908;&#39592;&#31995;&#32479;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems. (arXiv:2206.00484v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00484
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;RL&#20013;&#38598;&#25104;&#24494;&#20998;&#22806;&#22312;&#21487;&#22609;&#24615;&#65288;DEP&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#21040;&#36798;&#21644;&#36816;&#21160;&#65292;&#24182;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#20219;&#21153;&#20013;&#65292;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#32905;&#20316;&#21160;&#30340;&#29983;&#29289;&#33021;&#22815;&#22312;&#25317;&#26377;&#22823;&#37327;&#32908;&#32905;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26080;&#19982;&#20262;&#27604;&#30340;&#24039;&#22937;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#32908;&#39592;&#31995;&#32479;&#27169;&#22411;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23578;&#26410;&#33021;&#23637;&#31034;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#38024;&#23545;&#22823;&#22411;&#36807;&#24230;&#20316;&#21160;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26080;&#25928;&#25506;&#32034;&#25552;&#20986;&#20102;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#24120;&#35265;&#30340;&#25506;&#32034;&#22122;&#22768;&#31574;&#30053;&#22312;&#36807;&#24230;&#20316;&#21160;&#31995;&#32479;&#30340;&#21512;&#25104;&#31034;&#20363;&#20013;&#19981;&#22815;&#20805;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#24494;&#20998;&#22806;&#22312;&#21487;&#22609;&#24615;&#65288;DEP&#65289;&#65292;&#19968;&#31181;&#33258;&#32452;&#32455;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20114;&#21160;&#20960;&#31186;&#38047;&#20869;&#35825;&#23548;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#25506;&#32034;&#12290;&#36890;&#36807;&#23558;DEP&#19982;RL&#38598;&#25104;&#65292;&#25105;&#20204;&#22312;&#32908;&#39592;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#21040;&#36798;&#21644;&#36816;&#21160;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#20219;&#21153;&#20013;&#65292;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20803;&#23398;&#20064;&#22120;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#65292;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2205.14714</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#20316;&#29992;&#20272;&#35745;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects. (arXiv:2205.14714v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20803;&#23398;&#20064;&#22120;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#65292;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#65292;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20272;&#35745;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#38500;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#22806;&#65292;&#36824;&#24320;&#21457;&#20986;&#20102;&#31216;&#20026;&#20803;&#23398;&#20064;&#22120;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#20197;&#20272;&#35745;CATE&#65292;&#20854;&#20027;&#35201;&#20248;&#28857;&#26159;&#19981;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#19981;&#26159;&#20108;&#36827;&#21046;&#30340;&#26102;&#65292;&#19968;&#20123;&#26420;&#32032;&#25193;&#23637;&#30340;&#38480;&#21046;&#20250;&#20986;&#29616;&#65292;&#36825;&#26679;&#30340;&#20219;&#21153;&#23601;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#20272;&#35745;&#22810;&#20540;&#22788;&#29702;&#24322;&#36136;&#25928;&#24212;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#29702;&#35770;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35823;&#24046;&#19978;&#30028;&#20316;&#20026;&#37325;&#35201;&#21442;&#25968;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22788;&#29702;&#27700;&#24179;&#30340;&#25968;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#26420;&#32032;&#25193;&#23637;&#24182;&#19981;&#24635;&#26159;&#25552;&#20379;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#21644;&#35752;&#35770;&#20102;&#19968;&#20123;&#20803;&#23398;&#20064;&#22120;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#25968;&#37327;&#22686;&#22810;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#19968;&#39033;&#20057;&#32925;&#27835;&#30103;&#30740;&#31350;&#30340;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#20803;&#23398;&#20064;&#22120;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multi-valued treatments. We consider different meta-learners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2205.14410</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20943;&#23569;&#26234;&#33021;&#20307;&#22312;&#25484;&#25569;&#32473;&#23450;&#20219;&#21153;&#26102;&#38656;&#35201;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#27425;&#25968;&#12290;&#36801;&#31227;&#23398;&#20064;&#25552;&#20986;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21738;&#20010;&#28304;&#20219;&#21153;&#26377;&#36164;&#26684;&#29992;&#20110;&#30693;&#35782;&#25552;&#21462;&#65292;&#20197;&#21450;&#36873;&#25321;&#21738;&#20123;&#31639;&#27861;&#32452;&#20214;&#36827;&#34892;&#36801;&#31227;&#65292;&#26159;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#22810;&#28304;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#32780;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#22870;&#21169;&#20989;&#25968;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#22495;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#26410;&#25552;&#20379;&#27604;&#32852;&#37030;&#23398;&#20064;&#26356;&#22909;&#30340;&#23433;&#20840;&#20248;&#21183;&#65292;&#21453;&#32780;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#32780;&#19988;&#65292;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#23454;&#38469;&#20248;&#21183;&#65292;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2205.08443</link><description>&lt;p&gt;
&#20851;&#20110;&#28857;&#23545;&#28857;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#65288;&#19981;&#65289;&#23433;&#20840;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the (In)security of Peer-to-Peer Decentralized Machine Learning. (arXiv:2205.08443v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#26410;&#25552;&#20379;&#27604;&#32852;&#37030;&#23398;&#20064;&#26356;&#22909;&#30340;&#23433;&#20840;&#20248;&#21183;&#65292;&#21453;&#32780;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#32780;&#19988;&#65292;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#23454;&#38469;&#20248;&#21183;&#65292;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#38544;&#31169;&#20998;&#26512;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21253;&#25324;&#34987;&#21160;&#21644;&#20027;&#21160;&#30340;&#21435;&#20013;&#24515;&#21270;&#25932;&#25163;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#25552;&#20986;&#32773;&#25152;&#22768;&#31216;&#30340;&#30456;&#21453;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#24182;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#23427;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#65292;&#20351;&#24471;&#31995;&#32479;&#20013;&#30340;&#20219;&#20309;&#29992;&#25143;&#37117;&#33021;&#22815;&#36827;&#34892;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#26799;&#24230;&#36870;&#25512;&#65292;&#29978;&#33267;&#33719;&#24471;&#23545;&#35802;&#23454;&#29992;&#25143;&#30340;&#26412;&#22320;&#27169;&#22411;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#37492;&#20110;&#24403;&#21069;&#38450;&#25252;&#25216;&#26415;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#37197;&#32622;&#38656;&#35201;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#22833;&#21435;&#20102;&#19982;&#32852;&#37030;&#35774;&#32622;&#30340;&#20219;&#20309;&#23454;&#38469;&#20248;&#21183;&#65292;&#22240;&#27492;&#23436;&#20840;&#25171;&#36133;&#20102;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.
&lt;/p&gt;</description></item><item><title>SIBILA&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.06234</link><description>&lt;p&gt;
SIBILA: &#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#22312;&#21307;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts. (arXiv:2205.06234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06234
&lt;/p&gt;
&lt;p&gt;
SIBILA&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#27835;&#30103;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#30103;&#20173;&#28982;&#26159;&#31185;&#23398;&#23478;&#20204;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#39044;&#27979;&#20010;&#20307;&#24739;&#32773;&#26368;&#21512;&#36866;&#27835;&#30103;&#26041;&#27861;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#24320;&#21457;&#23450;&#21046;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#32570;&#20047;&#23545;&#32467;&#26524;&#30340;&#35299;&#37322;&#20197;&#21450;&#39640;&#35745;&#31639;&#35201;&#27714;&#20351;&#35768;&#22810;&#20154;&#19981;&#24895;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#33410;&#30465;&#26102;&#38388;&#24182;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;SIBILA&#24212;&#36816;&#32780;&#29983;&#12290;SIBILA&#26159;&#19968;&#32452;&#24212;&#29992;&#20102;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#35782;&#21035;&#26368;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#30001;&#20110;&#35299;&#37322;&#24615;&#31639;&#27861;&#21487;&#33021;&#19981;&#24444;&#27492;&#19968;&#33268;&#65292;&#22240;&#27492;&#23454;&#26045;&#20102;&#20849;&#35782;&#38454;&#27573;&#26469;&#20272;&#35745;&#27599;&#20010;&#21464;&#37327;&#23545;&#39044;&#27979;&#30340;&#20840;&#23616;&#36129;&#29486;&#12290;SIBILA&#34987;&#35013;&#31665;&#20197;&#22312;&#20219;&#20309;&#39640;&#24615;&#33021;&#35745;&#31639;&#24179;&#21488;&#19978;&#36816;&#34892;&#12290;&#23613;&#31649;&#26368;&#21021;&#30446;&#30340;&#26159;&#20316;&#20026;&#21629;&#20196;&#34892;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#21487;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;Web&#30028;&#38754;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized medicine remains a major challenge for scientists. The rapid growth of Machine learning and Deep learning has made them a feasible alternative for predicting the most appropriate therapy for individual patients. However, the need to develop a custom model for every dataset, the lack of interpretation of their results and high computational requirements make many reluctant to use these methods. Aiming to save time and bring light to the way models work internally, SIBILA has been developed. SIBILA is an ensemble of machine learning and deep learning models that applies a range of interpretability algorithms to identify the most relevant input features. Since the interpretability algo- rithms may not be in line with each other, a consensus stage has been imple- mented to estimate the global attribution of each variable to the predictions. SIBILA is containerized to be run on any high-performance computing plat- form. Although conceived as a command-line tool, it is also av
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#36139;&#22256;&#35843;&#26597;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#20854;&#21487;&#20197;&#25581;&#31034;&#27700;&#12289;&#30005;&#21147;&#21644;&#36164;&#20135;&#25152;&#26377;&#26435;&#31561;MPI&#25351;&#26631;&#23545;&#36139;&#22256;&#27700;&#24179;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2205.06131</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#36827;&#21046;&#25968;&#25454;&#25512;&#26029;&#32463;&#39564;&#22240;&#26524;&#22270;&#20197;&#25903;&#25345;&#22810;&#32500;&#36139;&#22256;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framework for inferring empirical causal graphs from binary data to support multidimensional poverty analysis. (arXiv:2205.06131v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#36139;&#22256;&#35843;&#26597;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#20854;&#21487;&#20197;&#25581;&#31034;&#27700;&#12289;&#30005;&#21147;&#21644;&#36164;&#20135;&#25152;&#26377;&#26435;&#31561;MPI&#25351;&#26631;&#23545;&#36139;&#22256;&#27700;&#24179;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36139;&#22256;&#26159;&#20154;&#31867;&#38754;&#20020;&#30340;&#22522;&#26412;&#38382;&#39064;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36139;&#22256;&#38382;&#39064;&#65292;&#38656;&#35201;&#30693;&#36947;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#22810;&#32500;&#36139;&#22256;&#25351;&#25968;(MPI)&#26159;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#32473;&#23450;&#22320;&#21306;&#36139;&#22256;&#38382;&#39064;&#30340;&#31243;&#24230;&#12290;&#35745;&#31639;MPI&#38656;&#35201;MPI&#25351;&#26631;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#25351;&#26631;&#26159;&#36890;&#36807;&#35843;&#26597;&#25910;&#38598;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#20195;&#34920;&#36139;&#22256;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#25945;&#32946;&#12289;&#20581;&#24247;&#12289;&#29983;&#27963;&#26465;&#20214;&#31561;&#12290;&#20351;&#29992;&#20256;&#32479;&#22238;&#24402;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;MPI&#25351;&#26631;&#23545;MPI&#25351;&#25968;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#19968;&#20010;MPI&#25351;&#26631;&#26159;&#21542;&#21487;&#33021;&#20250;&#35299;&#20915;&#25110;&#23548;&#33268;&#20854;&#20182;MPI&#25351;&#26631;&#26356;&#22810;&#30340;&#38382;&#39064;&#24182;&#19981;&#26126;&#26174;&#65292;&#24182;&#19988;&#27809;&#26377;&#19987;&#38376;&#29992;&#20110;&#25512;&#26029;MPI&#25351;&#26631;&#20043;&#38388;&#32463;&#39564;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#25512;&#26029;&#36139;&#22256;&#35843;&#26597;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25105;&#20204;&#30693;&#36947;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22467;&#22622;&#20420;&#27604;&#20122;&#23478;&#24237;&#28040;&#36153;&#25903;&#20986;&#35843;&#26597;&#65292;&#21457;&#29616;&#27700;&#12289;&#30005;&#21147;&#21644;&#36164;&#20135;&#25152;&#26377;&#26435;&#31561;MPI&#25351;&#26631;&#23545;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#36139;&#22256;&#27700;&#24179;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poverty is one of the fundamental issues that mankind faces. To solve poverty issues, one needs to know how severe the issue is. The Multidimensional Poverty Index (MPI) is a well-known approach that is used to measure a degree of poverty issues in a given area. To compute MPI, it requires information of MPI indicators, which are \textbf{binary variables} collecting by surveys, that represent different aspects of poverty such as lacking of education, health, living conditions, etc. Inferring impacts of MPI indicators on MPI index can be solved by using traditional regression methods. However, it is not obvious that whether solving one MPI indicator might resolve or cause more issues in other MPI indicators and there is no framework dedicating to infer empirical causal relations among MPI indicators.  In this work, we propose a framework to infer causal relations on binary variables in poverty surveys. Our approach performed better than baseline methods in simulated datasets that we kno
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SRCA&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#30340;&#20302;&#26679;&#26412;&#22823;&#23567;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#20307;&#25110;&#26925;&#29699;&#20307;&#65292;&#20445;&#30041;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#39640;&#36817;&#20284;&#20302;&#32500;&#27969;&#24418;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10975</link><description>&lt;p&gt;
&#24102;&#26377;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#30340;&#29699;&#38754;&#26059;&#36716;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Spherical Rotation Dimension Reduction with Geometric Loss Functions. (arXiv:2204.10975v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SRCA&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#30340;&#20302;&#26679;&#26412;&#22823;&#23567;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#20307;&#25110;&#26925;&#29699;&#20307;&#65292;&#20445;&#30041;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#39640;&#36817;&#20284;&#20302;&#32500;&#27969;&#24418;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#39640;&#32500;&#24615;&#65292;&#20294;&#25968;&#25454;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#20013;&#65292;&#21487;&#20197;&#25581;&#31034;&#23545;&#25968;&#25454;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#12290;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#20856;&#22411;&#20363;&#23376;&#26159;&#32454;&#32990;&#21608;&#26399;&#27979;&#37327;&#20540;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#36807;&#31243;&#30340;&#22266;&#26377;&#24490;&#29615;&#24615;&#21487;&#20197;&#34920;&#31034;&#20026;&#22278;&#25110;&#29699;&#12290;&#21463;&#20998;&#26512;&#36825;&#20123;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#31216;&#20026;&#29699;&#38754;&#26059;&#36716;&#25104;&#20998;&#20998;&#26512;&#65288;SRCA&#65289;&#65292;&#23427;&#23558;&#20960;&#20309;&#20449;&#24687;&#32435;&#20837;&#21040;&#38477;&#32500;&#36807;&#31243;&#20013;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#20302;&#32500;&#27969;&#24418;&#12290;SRCA&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#39640;&#32500;&#21644;&#23567;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#29699;&#20307;&#25110;&#26925;&#29699;&#20307;&#65292;SRCA&#25552;&#20379;&#20102;&#25968;&#25454;&#30340;&#20302;&#31209;&#29699;&#38754;&#34920;&#31034;&#65292;&#24182;&#22312;&#38477;&#32500;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#20197;&#21450;&#23545;&#20154;&#31867;&#32454;&#32990;&#21608;&#26399;&#25968;&#25454;&#30340;&#25104;&#21151;&#24212;&#29992;&#35777;&#26126;&#20102;SRCA&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern datasets often exhibit high dimensionality, yet the data reside in low-dimensional manifolds that can reveal underlying geometric structures critical for data analysis. A prime example of such a dataset is a collection of cell cycle measurements, where the inherently cyclical nature of the process can be represented as a circle or sphere. Motivated by the need to analyze these types of datasets, we propose a nonlinear dimension reduction method, Spherical Rotation Component Analysis (SRCA), that incorporates geometric information to better approximate low-dimensional manifolds. SRCA is a versatile method designed to work in both high-dimensional and small sample size settings. By employing spheres or ellipsoids, SRCA provides a low-rank spherical representation of the data with general theoretic guarantees, effectively retaining the geometric structure of the dataset during dimensionality reduction. A comprehensive simulation study, along with a successful application to human c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#23618;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;CNN&#65292;&#24182;&#25506;&#31350;&#20102;&#36825;&#23545;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#19982;&#30828;&#32422;&#26463;&#20004;&#31181;&#26041;&#27861;&#26469;&#31283;&#23450;MoE&#30340;&#35757;&#32451;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20351;&#19987;&#23478;&#21487;&#20197;&#26377;&#25928;&#20851;&#27880;&#36755;&#20837;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.10598</link><description>&lt;p&gt;
&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#23618;&#29992;&#20110;CNN&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability. (arXiv:2204.10598v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#23618;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;CNN&#65292;&#24182;&#25506;&#31350;&#20102;&#36825;&#23545;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#19982;&#30828;&#32422;&#26463;&#20004;&#31181;&#26041;&#27861;&#26469;&#31283;&#23450;MoE&#30340;&#35757;&#32451;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20351;&#19987;&#23478;&#21487;&#20197;&#26377;&#25928;&#20851;&#27880;&#36755;&#20837;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;(MoE)&#23618;&#26368;&#36817;&#25104;&#21151;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#21464;&#25442;&#22120;&#20013;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#12290; &#31232;&#30095;MoE&#23618;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#21103;&#20316;&#29992;&#26159;&#65292;&#36890;&#36807;&#33258;&#28982;&#30340;&#19987;&#23478;&#29305;&#21270;&#65292;&#23427;&#20204;&#20026;&#27169;&#22411;&#25552;&#20379;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;MoE&#23618;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;CNN&#65292;&#24182;&#20998;&#26512;&#36825;&#23545;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#31283;&#23450;MoE&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#21644;&#30828;&#32422;&#26463;&#20004;&#31181;&#26041;&#27861;&#12290;&#22312;&#30828;&#32422;&#26463;&#20013;&#65292;&#26576;&#20123;&#19987;&#23478;&#30340;&#26435;&#37325;&#34987;&#20801;&#35768;&#21464;&#20026;&#38646;&#65292;&#32780;&#36719;&#32422;&#26463;&#21017;&#36890;&#36807;&#39069;&#22806;&#30340;&#36741;&#21161;&#25439;&#22833;&#24179;&#34913;&#19987;&#23478;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#36719;&#32422;&#26463;&#26356;&#22909;&#22320;&#22788;&#29702;&#20102;&#19987;&#23478;&#21033;&#29992;&#65292;&#24182;&#25903;&#25345;&#19987;&#23478;&#19987;&#19994;&#21270;&#36827;&#31243;&#65292;&#32780;&#30828;&#32422;&#26463;&#20445;&#25345;&#20102;&#26356;&#24191;&#20041;&#30340;&#19987;&#23478;&#24182;&#22686;&#21152;&#20102;&#25972;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19987;&#23478;&#21487;&#20197;&#38544;&#24335;&#22320;&#20851;&#27880;&#36755;&#20837;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully applied for scaling large transformers, especially for language modeling tasks. An intriguing side effect of sparse MoE layers is that they convey inherent interpretability to a model via natural expert specialization. In this work, we apply sparse MoE layers to CNNs for computer vision tasks and analyze the resulting effect on model interpretability. To stabilize MoE training, we present both soft and hard constraint-based approaches. With hard constraints, the weights of certain experts are allowed to become zero, while soft constraints balance the contribution of experts with an additional auxiliary loss. As a result, soft constraints handle expert utilization better and support the expert specialization process, while hard constraints maintain more generalized experts and increase overall model performance. Our findings demonstrate that experts can implicitly focus on individual sub-domains of the input s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#21270;&#23398;&#22522;&#20803;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#26465;&#30446;&#12289;&#33258;&#21160;&#26500;&#24314;&#31895;&#31890;&#21270;&#21147;&#22330;&#20197;&#21450;&#35782;&#21035;&#21453;&#24212;&#22352;&#26631;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.16205</link><description>&lt;p&gt;
&#21270;&#23398;&#22522;&#20803;&#30340;&#33258;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Automatic Identification of Chemical Moieties. (arXiv:2203.16205v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#21270;&#23398;&#22522;&#20803;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#26465;&#30446;&#12289;&#33258;&#21160;&#26500;&#24314;&#31895;&#31890;&#21270;&#21147;&#22330;&#20197;&#21450;&#35782;&#21035;&#21453;&#24212;&#22352;&#26631;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#37327;&#23376;&#21147;&#23398;&#21487;&#35266;&#27979;&#37327;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#36890;&#36807;&#26500;&#24314;&#21407;&#23376;&#34920;&#31034;&#26469;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#20174;&#20013;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#21270;&#23398;&#22522;&#20803;&#65288;&#20998;&#23376;&#24314;&#31569;&#22359;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#31181;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#25152;&#38656;&#30340;&#34920;&#31034;&#21487;&#20197;&#26159;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;MPNN&#25552;&#20379;&#65292;&#20063;&#21487;&#20197;&#20165;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#20174;&#22836;&#23398;&#20064;&#12290;&#38500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#23376;&#25351;&#32441;&#35774;&#35745;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20063;&#36890;&#36807;&#20351;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26465;&#30446;&#30340;&#36873;&#25321;&#12289;&#33258;&#21160;&#26500;&#24314;&#31895;&#31890;&#21270;&#21147;&#22330;&#20197;&#21450;&#35782;&#21035;&#21453;&#24212;&#22352;&#26631;&#31561;&#26041;&#38754;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the prediction of quantum mechanical observables with machine learning methods has become increasingly popular. Message-passing neural networks (MPNNs) solve this task by constructing atomic representations, from which the properties of interest are predicted. Here, we introduce a method to automatically identify chemical moieties (molecular building blocks) from such representations, enabling a variety of applications beyond property prediction, which otherwise rely on expert knowledge. The required representation can either be provided by a pretrained MPNN, or learned from scratch using only structural information. Beyond the data-driven design of molecular fingerprints, the versatility of our approach is demonstrated by enabling the selection of representative entries in chemical databases, the automatic construction of coarse-grained force fields, as well as the identification of reaction coordinates.
&lt;/p&gt;</description></item><item><title>GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.07281</link><description>&lt;p&gt;
GrIPS&#65306;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25351;&#20196;&#25628;&#32034;&#65292;&#29992;&#20110;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07281
&lt;/p&gt;
&lt;p&gt;
GrIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#25552;&#31034;&#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#29992;&#26032;&#33539;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#36890;&#36807;&#25163;&#21160;&#37325;&#20889;&#25110;&#26799;&#24230;&#35843;&#25972;&#26469;&#25552;&#39640;&#36825;&#20123;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#32791;&#26102;&#19988;&#38656;&#35201;&#20027;&#35266;&#35299;&#37322;&#65292;&#32780;&#22522;&#20110;&#26799;&#24230;&#30340;&#35843;&#25972;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#65292;&#23545;&#20110;&#22522;&#20110;API&#30340;&#27169;&#22411;&#26469;&#35828;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gradient-free Instructional Prompt Search (GrIPS)&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#30340;&#26080;&#26799;&#24230;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#25351;&#20196;&#12290;GrIPS&#25509;&#21463;&#38754;&#21521;&#20154;&#31867;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#24182;&#33258;&#21160;&#36820;&#22238;&#23436;&#21892;&#30340;&#32534;&#36753;&#25552;&#31034;&#65292;&#21516;&#26102;&#20801;&#35768;&#22522;&#20110;API&#30340;&#35843;&#25972;&#12290;&#20351;&#29992;InstructGPT&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;GrIPS&#23558;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;4.30&#20010;&#30334;&#20998;&#28857;&#65288;OPT&#65292;BLOOM&#31561;&#20219;&#21153;&#20063;&#26377;&#31867;&#20284;&#30340;&#25913;&#36827;&#65289;
&lt;/p&gt;
&lt;p&gt;
Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;PU&#23398;&#20064;&#36827;&#34892;&#34920;&#22411;&#30740;&#31350;&#30340;&#20840;&#22522;&#22240;&#32452;&#32852;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#23558;&#38170;&#23450;&#23398;&#20064;&#21644;&#21464;&#25442;&#22120;&#32467;&#26500;&#32452;&#21512;&#26469;&#26816;&#27979;&#22522;&#22240;&#32452;&#20851;&#32852;&#65292;&#21363;&#20351;&#22312;&#20943;&#23569;&#23545;&#29031;&#32452;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#26356;&#22810;&#26174;&#33879;&#22522;&#22240;&#32452;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2202.07451</link><description>&lt;p&gt;
&#21033;&#29992;PU&#23398;&#20064;&#36827;&#34892;&#34920;&#22411;&#30740;&#31350;&#30340;&#20840;&#22522;&#22240;&#32452;&#32852;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Phenotyping with Positive Unlabelled Learning for Genome-Wide Association Studies. (arXiv:2202.07451v1 [stat.AP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;PU&#23398;&#20064;&#36827;&#34892;&#34920;&#22411;&#30740;&#31350;&#30340;&#20840;&#22522;&#22240;&#32452;&#32852;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#23558;&#38170;&#23450;&#23398;&#20064;&#21644;&#21464;&#25442;&#22120;&#32467;&#26500;&#32452;&#21512;&#26469;&#26816;&#27979;&#22522;&#22240;&#32452;&#20851;&#32852;&#65292;&#21363;&#20351;&#22312;&#20943;&#23569;&#23545;&#29031;&#32452;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#26356;&#22810;&#26174;&#33879;&#22522;&#22240;&#32452;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#30103;&#20445;&#20581;&#21644;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#35782;&#21035;&#34920;&#22411;&#22312;&#36827;&#19968;&#27493;&#29702;&#35299;&#30142;&#30149;&#29983;&#29289;&#23398;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#22122;&#22768;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23601;&#20419;&#20351;&#20102;&#22312;&#34920;&#22411;&#21457;&#29616;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#19981;&#26159;&#25214;&#21040;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#39044;&#27979;&#20122;&#22411;&#65292;&#32780;&#26159;&#20851;&#27880;&#23548;&#33268;&#34920;&#22411;&#35823;&#20998;&#31867;&#30340;&#22122;&#22768;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#34920;&#22411;&#22312;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65288;GWAS&#65289;&#20013;&#26816;&#27979;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#38170;&#23450;&#23398;&#20064;&#21644;&#21464;&#25442;&#22120;&#32467;&#26500;&#32452;&#21512;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;AnchorBERT&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;&#20165;&#22312;&#22823;&#22411;&#32852;&#21512;&#30740;&#31350;&#20013;&#25165;&#21457;&#29616;&#30340;&#22522;&#22240;&#32452;&#20851;&#32852;&#12290;&#24403;&#20943;&#23569;50\%&#30340;&#21487;&#29992;&#23545;&#29031;&#32452;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20445;&#25345;GWAS&#20013;40\%&#26356;&#22810;&#30340;&#26174;&#33879;&#22522;&#22240;&#32452;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying phenotypes plays an important role in furthering our understanding of disease biology through practical applications within healthcare and the life sciences. The challenge of dealing with the complexities and noise within electronic health records (EHRs) has motivated applications of machine learning in phenotypic discovery. While recent research has focused on finding predictive subtypes for clinical decision support, here we instead focus on the noise that results in phenotypic misclassification, which can reduce a phenotypes ability to detect associations in genome-wide association studies (GWAS). We show that by combining anchor learning and transformer architectures into our proposed model, AnchorBERT, we are able to detect genomic associations only previously found in large consortium studies with 5$\times$ more cases. When reducing the number of controls available by 50\%, we find our model is able to maintain 40\% more significant genomic associations from the GWAS 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Quantus&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35814;&#23613;&#36805;&#36895;&#22320;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#35299;&#37322;&#34920;&#29616;&#65292;&#24182;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.06861</link><description>&lt;p&gt;
Quantus: &#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#36127;&#36131;&#20219;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#21644;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond. (arXiv:2202.06861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Quantus&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35814;&#23613;&#36805;&#36895;&#22320;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#35299;&#37322;&#34920;&#29616;&#65292;&#24182;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26041;&#27861;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#35299;&#37322;&#24615;&#34987;&#35748;&#20026;&#33021;&#22686;&#24378;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;&#65292;&#26377;&#24517;&#35201;&#31995;&#32479;&#22320;&#23457;&#26680;&#21644;&#27604;&#36739;&#35299;&#37322;&#26041;&#27861;&#20197;&#30830;&#35748;&#20854;&#27491;&#30830;&#24615;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19987;&#27880;&#20110;XAI&#35780;&#20272;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#35814;&#23613;&#36805;&#36895;&#22320;&#35753;&#30740;&#31350;&#32773;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#22686;&#21152;&#39046;&#22495;&#20869;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Quantus&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#12289;&#33391;&#22909;&#32452;&#32455;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#37322;&#26041;&#27861;&#35780;&#20272;&#30340;&#25945;&#31243;&#38598;&#21512;&#12290;&#35813;&#24037;&#20855;&#21253;&#32463;&#36807;&#20102;&#24443;&#24213;&#30340;&#27979;&#35797;&#65292;&#21487;&#22312;PyPi&#19979;&#25110;https://github.com/understandable-machine-intelligence-lab/Quantus/&#19978;&#20197;&#24320;&#28304;&#35768;&#21487;&#35777;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PETRAW&#25361;&#25112;&#65292;&#25506;&#35752;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.05821</link><description>&lt;p&gt;
PEg TRAnsfer Workflow recognition challenge&#25253;&#21578;&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#26159;&#21542;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?. (arXiv:2202.05821v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PETRAW&#25361;&#25112;&#65292;&#25506;&#35752;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#25968;&#25454;&#36827;&#34892;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;PEg TRAnsfert Workflow recognition&#8221;&#65288;PETRAW&#65289;&#25361;&#25112;&#30340;&#35774;&#35745;&#19982;&#32467;&#26524;&#65292;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#21644;&#20998;&#21106;&#31561;&#21333;&#19968;&#25110;&#22810;&#31181;&#27169;&#24577;&#24320;&#21457;&#22806;&#31185;&#25163;&#26415;&#24037;&#20316;&#27969;&#31243;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;&#23427;&#20204;&#30340;&#38468;&#21152;&#20540;&#12290;PETRAW&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;&#34394;&#25311;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#30340;&#25554;&#31649;&#36716;&#31227;&#24207;&#21015;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35270;&#39057;&#12289;&#36816;&#21160;&#23398;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#24037;&#20316;&#27969;&#31243;&#27880;&#37322;&#32452;&#25104;&#65292;&#25152;&#36848;&#27880;&#37322;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#65288;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#27963;&#21160;&#65289;&#19978;&#25551;&#36848;&#20102;&#24207;&#21015;&#12290;&#21442;&#36187;&#32773;&#34987;&#25552;&#20986;&#20102;&#20116;&#39033;&#20219;&#21153;&#65306;&#20854;&#20013;&#19977;&#39033;&#20219;&#21153;&#19982;&#25152;&#26377;&#31890;&#24230;&#30340;&#35782;&#21035;&#19982;&#19968;&#31181;&#21487;&#29992;&#30340;&#27169;&#24577;&#30456;&#20851;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#36890;&#36807;&#22810;&#31181;&#27169;&#24577;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#35782;&#21035;&#38382;&#39064;&#12290;&#24179;&#22343;&#24212;&#29992;&#30456;&#20851;&#24179;&#34913;&#20934;&#30830;&#24615;&#65288;AD-Accuracy&#65289;&#34987;&#29992;&#20316;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#32771;&#34385;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#22240;&#20026;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20855;&#26377;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and results of the "PEg TRAnsfert Workflow recognition" (PETRAW) challenge whose objective was to develop surgical workflow recognition methods based on one or several modalities, among video, kinematic, and segmentation data, in order to study their added value. The PETRAW challenge provided a data set of 150 peg transfer sequences performed on a virtual simulator. This data set was composed of videos, kinematics, semantic segmentation, and workflow annotations which described the sequences at three different granularity levels: phase, step, and activity. Five tasks were proposed to the participants: three of them were related to the recognition of all granularities with one of the available modalities, while the others addressed the recognition with a combination of modalities. Average application-dependent balanced accuracy (AD-Accuracy) was used as evaluation metric to take unbalanced classes into account and because it is more clinically relevant tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;RL&#31639;&#27861;&#30340;&#24120;&#35268;&#36951;&#25022;&#30028;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.03463</link><description>&lt;p&gt;
&#23398;&#20064;Whittle&#32034;&#24341;&#31574;&#30053;&#20197;&#35299;&#20915;&#39640;&#25928;&#36951;&#25022;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On learning Whittle index policy for restless bandits with scalable regret. (arXiv:2202.03463v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;RL&#31639;&#27861;&#30340;&#24120;&#35268;&#36951;&#25022;&#30028;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31995;&#32479;&#27169;&#22411;&#26410;&#30693;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#26159;&#23398;&#20064;&#22522;&#20110;&#25968;&#25454;&#30340;&#33391;&#22909;&#36164;&#28304;&#20998;&#37197;&#21644;&#35843;&#24230;&#31574;&#30053;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32047;&#35745;&#36951;&#25022;&#20250;&#38543;$\tilde O(\mathsf{S} \sqrt{\mathsf{A} T})$&#35268;&#27169;&#22686;&#21152;&#65292;&#20854;&#20013;$\mathsf{S}$&#20026;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#65292;$\mathsf{A}$&#20026;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#65292;$T$&#20026;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;$\tilde{O}(\cdot)$&#31526;&#21495;&#34920;&#31034;&#38544;&#34255;&#30340;&#23545;&#25968;&#39033;&#12290;&#30001;&#20110;&#20855;&#26377;&#19982;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#32447;&#24615;&#27491;&#30456;&#20851;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#36825;&#20123;&#36951;&#25022;&#36793;&#30028;&#23545;&#20110;&#36164;&#28304;&#20998;&#37197;&#21644;&#35843;&#24230;&#38382;&#39064;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#27169;&#22411;&#22522;&#30784;&#32467;&#26500;&#35843;&#25972;&#30340;&#22522;&#20110;Thompson&#37319;&#26679;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20851;&#20110;Whittle&#32034;&#24341;&#31574;&#30053;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#36951;&#25022;&#29305;&#24615;&#35828;&#26126;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#19968;&#31181;Whittle&#32034;&#24341;&#31574;&#30053;&#65292;&#25152;&#25552;&#31639;&#27861;&#30340;&#36951;&#25022;&#28385;&#36275;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#23637;&#31034;&#65289;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an attractive approach to learn good resource allocation and scheduling policies based on data when the system model is unknown. However, the cumulative regret of most RL algorithms scales as $\tilde O(\mathsf{S} \sqrt{\mathsf{A} T})$, where $\mathsf{S}$ is the size of the state space, $\mathsf{A}$ is the size of the action space, $T$ is the horizon, and the $\tilde{O}(\cdot)$ notation hides logarithmic terms. Due to the linear dependence on the size of the state space, these regret bounds are prohibitively large for resource allocation and scheduling problems. In this paper, we present a model-based RL algorithm for such problems which has scalable regret. In particular, we consider a restless bandit model, and propose a Thompson-sampling based learning algorithm which is tuned to the underlying structure of the model. We present two characterizations of the regret of the proposed algorithm with respect to the Whittle index policy. First, we show that for a r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#21830;&#19994;&#26694;&#26550;&#20013;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21518;&#38376;&#23433;&#20840;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#25915;&#20987;&#23454;&#29616;&#21518;&#38376;&#35302;&#21457;&#24182;&#36867;&#36991;&#26816;&#27979;&#65292;&#20174;&#32780;&#21361;&#21450;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#23433;&#20840;&#65292;&#21487;&#33021;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2108.09187</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21830;&#19994;&#26694;&#26550;&#30340;&#37327;&#21270;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Quantization Backdoors to Deep Learning Commercial Frameworks. (arXiv:2108.09187v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#21830;&#19994;&#26694;&#26550;&#20013;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21518;&#38376;&#23433;&#20840;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#25915;&#20987;&#23454;&#29616;&#21518;&#38376;&#35302;&#21457;&#24182;&#36867;&#36991;&#26816;&#27979;&#65292;&#20174;&#32780;&#21361;&#21450;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#23433;&#20840;&#65292;&#21487;&#33021;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#30001;&#20110;&#20302;&#24310;&#36831;&#21644;&#39640;&#38544;&#31169;&#20445;&#25252;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#37096;&#32626;&#22312;&#26080;&#22788;&#19981;&#22312;&#30340;&#36793;&#32536;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, there is a burgeoning demand for deploying deep learning (DL) models on ubiquitous edge Internet of Things (IoT) devices attributed to their low latency and high privacy preservation. However, DL models are often large in size and require large-scale computation, which prevents them from being placed directly onto IoT devices, where resources are constrained and 32-bit floating-point (float-32) operations are unavailable. Commercial framework (i.e., a set of toolkits) empowered model quantization is a pragmatic solution that enables DL deployment on mobile devices and embedded systems by effortlessly post-quantizing a large high-precision model (e.g., float-32) into a small low-precision model (e.g., int-8) while retaining the model inference accuracy. However, their usability might be threatened by security vulnerabilities.  This work reveals that the standard quantization toolkits can be abused to activate a backdoor. We demonstrate that a full-precision backdoored model w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#20248;&#21270;&#20984;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#32463;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#37319;&#29992;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#21487;&#26174;&#33879;&#25552;&#21319;&#36319;&#36394;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2108.07356</link><description>&lt;p&gt;
&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#28418;&#31227;&#19979;&#20248;&#21270;&#20984;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#32463;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#37319;&#29992;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#21487;&#26174;&#33879;&#25552;&#21319;&#36319;&#36394;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26368;&#23567;&#21270;&#38543;&#26426;&#28436;&#21270;&#20984;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#36825;&#20010;&#28436;&#21270;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20381;&#36182;&#20110;&#26102;&#38388;&#21644;&#20915;&#31574;&#21464;&#37327;&#26412;&#36523;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12289;&#38543;&#26426;&#36319;&#36394;&#21644;&#25191;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#26399;&#26395;&#20540;&#21644;&#39640;&#27010;&#29575;&#19979;&#25104;&#31435;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#33719;&#24471;&#30340;&#25928;&#29575;&#20272;&#35745;&#26126;&#30830;&#22320;&#35299;&#32806;&#20102;&#20248;&#21270;&#35823;&#24046;&#12289;&#26799;&#24230;&#22122;&#22768;&#21644;&#26102;&#38388;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#20302;&#28418;&#31227;-&#22122;&#22768;&#27604;&#30340;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#37324;&#65292;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#36319;&#36394;&#25928;&#29575;&#22240;&#27493;&#38271;&#34928;&#20943;&#31574;&#30053;&#32780;&#21463;&#30410;&#26174;&#33879;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2108.00473</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#36825;&#31867;&#38382;&#39064;&#36817;&#24180;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#65288;ZO-AGP&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(d_{x}+d_{y})$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#26469;&#35299;&#20915;&#22359;&#29366;&#38750;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(K d_{x}+d_{y})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#20860;&#23481;&#24615;&#30340;&#32858;&#31867;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#30001;&#35757;&#32451;&#36807;&#31243;&#30340;&#30446;&#26631;&#25152;&#23450;&#20041;&#19988;&#20855;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2105.03692</link><description>&lt;p&gt;
&#8220;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#19981;&#20860;&#23481;&#32858;&#31867;&#26426;&#21046;&#8221;
&lt;/p&gt;
&lt;p&gt;
Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks. (arXiv:2105.03692v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#20860;&#23481;&#24615;&#30340;&#32858;&#31867;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#30001;&#35757;&#32451;&#36807;&#31243;&#30340;&#30446;&#26631;&#25152;&#23450;&#20041;&#19988;&#20855;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#32858;&#31867;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#25968;&#25454;&#23376;&#38598;&#19981;&#30456;&#23481;&#24615;&#23646;&#24615;&#12290;&#35813;&#26426;&#21046;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#21482;&#33021;&#27867;&#21270;&#21040;&#20854;&#33258;&#36523;&#30340;&#23376;&#38598;&#65292;&#21363;&#22312;&#19968;&#20010;&#23376;&#38598;&#19978;&#30340;&#35757;&#32451;&#19981;&#20250;&#25913;&#21892;&#20854;&#20182;&#23376;&#38598;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#25968;&#25454;&#38598;&#19982;&#35757;&#32451;&#36807;&#31243;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#32858;&#31867;&#26426;&#21046;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#30001;&#35757;&#32451;&#36807;&#31243;&#30340;&#30446;&#26631;&#25152;&#23450;&#20041;&#19988;&#20855;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32858;&#31867;&#26426;&#21046;&#24212;&#29992;&#20110;&#38450;&#24481;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#25915;&#20987;&#32773;&#23558;&#24694;&#24847;&#27602;&#23475;&#25968;&#25454;&#27880;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#24433;&#21709;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#20851;&#27880;&#21033;&#29992;GTSRB&#21644;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#36825;&#20123;&#25915;&#20987;&#20135;&#29983;&#30340;&#27602;&#23475;&#25968;&#25454;&#38598;&#26159;&#26377;&#27602;&#23475;&#25968;&#25454;&#21644;&#24178;&#20928;&#25968;&#25454;&#19981;&#30456;&#23481;&#30340;&#65307;2&#65289;&#25105;&#20204;&#30340;&#32858;&#31867;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by--and therefore meaningful to--the objective of the training process.  We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) o
&lt;/p&gt;</description></item></channel></rss>