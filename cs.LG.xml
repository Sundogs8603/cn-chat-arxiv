<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#23545;&#20998;&#21106;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#27492;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22810;&#31181;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2310.16999</link><description>&lt;p&gt;
&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#40065;&#26834;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#23545;&#20998;&#21106;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#27492;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22810;&#31181;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36755;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#38543;&#26426;&#21644;&#26368;&#22351;&#24773;&#20917;&#30340;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20316;&#32773;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#31216;&#20026;&#8220;&#20449;&#20219;&#65292;&#20294;&#35201;&#39564;&#35777;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#36741;&#21161;&#39564;&#35777;&#32593;&#32476;&#20351;&#29992;&#20998;&#21106;&#20316;&#20026;&#36755;&#20837;&#26469;&#23545;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#34987;&#36974;&#34109;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#35774;&#35745;&#33391;&#22909;&#30340;&#36741;&#21161;&#32593;&#32476;&#23558;&#22312;&#36755;&#20837;&#20998;&#21106;&#20934;&#30830;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39044;&#27979;&#65292;&#20294;&#22312;&#20998;&#21106;&#19981;&#27491;&#30830;&#26102;&#29983;&#25104;&#20302;&#36136;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#32593;&#32476;&#30340;&#39044;&#27979;&#19982;&#21407;&#22987;&#22270;&#20687;&#36827;&#34892;&#26816;&#26597;&#65292;&#25105;&#20204;&#21487;&#20197;&#26816;&#27979;&#20986;&#38169;&#35823;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#39564;&#35777;&#26041;&#27861;&#30495;&#27491;&#40065;&#26834;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#26816;&#26597;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#29992;&#20110;&#20998;&#21106;&#35780;&#20272;&#30340;&#26041;&#27861;&#26080;&#27861;&#24212;&#23545;&#40065;&#26834;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a method for verifying the output of a deep neural network for medical image segmentation that is robust to several classes of random as well as worst-case perturbations i.e. adversarial attacks. This method is based on a general approach recently developed by the authors called ``Trust, but Verify" wherein an auxiliary verification network produces predictions about certain masked features in the input image using the segmentation as an input. A well-designed auxiliary network will produce high-quality predictions when the input segmentations are accurate, but will produce low-quality predictions when the segmentations are incorrect. Checking the predictions of such a network with the original image allows us to detect bad segmentations. However, to ensure the verification method is truly robust, we need a method for checking the quality of the predictions that does not itself rely on a black-box neural network. Indeed, we show that previous methods for segmentation evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;</title><link>http://arxiv.org/abs/2310.07811</link><description>&lt;p&gt;
&#22312;&#32447;RL&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#21644;&#32447;&#24615;MDPs&#19968;&#26679;&#23481;&#26131;&#65292;&#21482;&#35201;&#20320;&#23398;&#20250;&#24573;&#30053;&#12290; (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#31163;&#25955;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#65292;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;&#20551;&#35774;&#19979;&#65292;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#21160;&#20316;&#20540;&#21487;&#20197;&#34920;&#31034;&#20026;&#29366;&#24577;-&#21160;&#20316;&#29305;&#24449;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20010;&#31867;&#21035;&#34987;&#35748;&#20026;&#27604;&#32447;&#24615;MDPs&#26356;&#19968;&#33324;&#21270;&#65292;&#20854;&#20013;&#36716;&#31227;&#20869;&#26680;&#21644;&#22870;&#21169;&#20989;&#25968;&#34987;&#20551;&#35774;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#23384;&#22312;&#19968;&#20123;&#29366;&#24577;&#65292;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#31574;&#30053;&#65292;&#25152;&#26377;&#30340;&#21160;&#20316;&#20540;&#37117;&#36817;&#20284;&#30456;&#31561;&#65292;&#36890;&#36807;&#36339;&#36807;&#36825;&#20123;&#29366;&#24577;&#24182;&#25353;&#29031;&#20219;&#24847;&#22266;&#23450;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#36827;&#34892;&#36716;&#25442;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#39062;&#65288;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#23398;&#20064;&#20102;&#24212;&#35813;&#36339;&#36807;&#30340;&#29366;&#24577;&#65292;&#24182;&#36816;&#34892;&#21478;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.06958</link><description>&lt;p&gt;
&#27604;&#36739;&#29616;&#20195;&#26080;&#21442;&#32771;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29616;&#20195;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20026;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26356;&#23433;&#20840;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#21464;&#24471;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#20998;&#25968;&#20294;&#19981;&#25913;&#21892;&#35270;&#35273;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36136;&#37327;&#24230;&#37327;&#22522;&#20934;&#23558;&#20854;&#24615;&#33021;&#19982;&#20027;&#35266;&#36136;&#37327;&#30456;&#20851;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20063;&#26159;&#19968;&#20010;&#20540;&#24471;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#20195;&#24230;&#37327;&#26041;&#27861;&#23545;&#19981;&#21516;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;15&#20010;&#26080;&#21442;&#32771;&#22270;&#20687;/&#35270;&#39057;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#19968;&#20123;&#24230;&#37327;&#26041;&#27861;&#23545;&#23545;&#25239;&#25915;&#20987;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#25269;&#25239;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20351;&#29992;&#27604;&#23481;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#26356;&#23433;&#20840;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25509;&#21463;&#30740;&#31350;&#20154;&#21592;&#25552;&#20132;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#20351;&#20182;&#20204;&#30340;&#26041;&#27861;&#23545;&#25915;&#20987;&#26356;&#21152;&#40065;&#26834;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#23547;&#25214;&#31526;&#21512;&#38656;&#27714;&#30340;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.01685</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26126;&#30830;&#20102;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#23450;&#20041;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#27169;&#31946;&#24863;&#12290;&#20026;&#20160;&#20040;&#38656;&#35201;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#37322;&#24615;&#65311;&#24403;&#38656;&#35201;&#35299;&#37322;&#24615;&#26102;&#65292;&#23454;&#38469;&#19978;&#36861;&#27714;&#30340;&#30446;&#26631;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#30446;&#26631;&#21644;&#35201;&#32032;&#38656;&#35201;&#24418;&#24335;&#21270;&#12290;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#20013;&#24120;&#35265;&#30340;&#23454;&#38469;&#20219;&#21153;&#21644;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#26680;&#24515;&#35201;&#32032;&#65306;&#23450;&#20301;&#12289;&#35270;&#35273;&#21487;&#35782;&#21035;&#24615;&#12289;&#29289;&#29702;&#24402;&#22240;&#21644;&#36879;&#26126;&#24230;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#22312;&#21307;&#23398;&#24433;&#20687;&#30340;&#32972;&#26223;&#19979;&#31995;&#32479;&#21270;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30340;&#23454;&#36341;&#35266;&#28857;&#28548;&#28165;&#20102;&#20855;&#20307;&#30340;&#21307;&#23398;&#24433;&#20687;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30446;&#26631;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#20197;&#25351;&#23548;&#26041;&#27861;&#35774;&#35745;&#24182;&#25913;&#36827;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#23454;&#29992;&#21644;&#25945;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model desig
&lt;/p&gt;</description></item><item><title>MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15312</link><description>&lt;p&gt;
MAPTree: &#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#20987;&#36133;&#8220;&#26368;&#20248;&#8221;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15312
&lt;/p&gt;
&lt;p&gt;
MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20173;&#28982;&#26159;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#24320;&#31665;&#21363;&#29992;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#24402;&#32435;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20915;&#31574;&#26641;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#19982;AND/OR&#25628;&#32034;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPTree&#30340;AND/OR&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;16&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#35201;&#20040;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#35201;&#20040;&#22312;&#24615;&#33021;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#26641;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;MAPTree&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#22320;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
&lt;/p&gt;</description></item><item><title>USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08023</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;
&lt;/p&gt;
&lt;p&gt;
USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08023
&lt;/p&gt;
&lt;p&gt;
USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#19968;&#20010;&#32463;&#36807;&#22823;&#37327;&#21463;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#32780;&#26469;&#30340;&#65292;&#23637;&#31034;&#20102;&#20174;&#22823;&#22411;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#26469;&#33258;96&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#26500;&#25104;&#30340;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;75%&#30340;&#24179;&#22343;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#12290;&#22312;&#32654;&#24335;&#33521;&#35821;&#19978;&#65292;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#20844;&#20849;&#21644;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;85.8%&#30340;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#21333;&#35821;&#35328;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;21%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21482;&#38656;&#35201;&#24494;&#35843;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#22235;&#20998;&#20043;&#19968;&#23601;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib
&lt;/p&gt;</description></item><item><title>Data-Juicer&#26159;&#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.02033</link><description>&lt;p&gt;
Data-Juicer: &#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Data-Juicer: A One-Stop Data Processing System for Large Language Models. (arXiv:2309.02033v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02033
&lt;/p&gt;
&lt;p&gt;
Data-Juicer&#26159;&#19968;&#20010;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31449;&#24335;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21457;&#23637;&#20984;&#26174;&#20102;&#22823;&#35268;&#27169;&#12289;&#24322;&#26500;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#25968;&#25454;&#37197;&#26041;&#26159;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#28151;&#21512;&#29289;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#24320;&#28304;&#24037;&#20855;&#20027;&#35201;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#37197;&#26041;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#20026;&#20102;&#19981;&#26029;&#25366;&#25496;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#26469;&#33258;&#26032;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Data-Juicer&#30340;&#26032;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#30340;&#25968;&#25454;&#37197;&#26041;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#25968;&#25454;&#28151;&#21512;&#26041;&#24335;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#19981;&#21516;&#65292;Data-Juicer&#38754;&#20020;&#30528;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#37197;&#26041;&#30340;&#21487;&#33021;&#25968;&#25454;&#28304;&#26159;&#30495;&#27491;&#30340;&#24322;&#26500;&#21644;&#22823;&#35268;&#27169;&#30340;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#31934;&#30830;&#35780;&#20272;&#25968;&#25454;&#37197;&#26041;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#38750;&#24120;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, heterogeneous, and high-quality data. A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance. Existing open-source tools for LLM data processing are mostly tailored for specific data recipes. To continuously uncover the potential of LLMs, incorporate data from new sources, and improve LLMs' performance, we build a new system named Data-Juicer, with which we can efficiently generate diverse data recipes, explore different possibilities in forming data mixtures, and evaluate their effects on model performance. Different from traditional data-analytics pipelines, Data-Juicer faces some unique challenges. Firstly, the possible data sources for forming data recipes are truly heterogeneous and massive with various qualities. Secondly, it is extremely expensive to precisely evaluate data recipes' impact on LLMs' performance. Third
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13380</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
In-context learning for model-free system identification. (arXiv:2308.13380v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32473;&#23450;&#30340;&#36755;&#20837;/&#36755;&#20986;&#24207;&#21015;&#21644;&#21487;&#29992;&#30340;&#29289;&#29702;&#30693;&#35782;&#26469;&#20272;&#35745;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26159;&#21542;&#36824;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#23427;&#20204;&#30340;&#36755;&#20837;/&#36755;&#20986;&#27169;&#24335;&#20013;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21602;&#65311;&#36825;&#20010;&#26680;&#24515;&#38382;&#39064;&#39537;&#21160;&#30528;&#26412;&#25991;&#30340;&#30740;&#31350;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#36776;&#35782;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65306;&#19968;&#27493;&#39044;&#27979;&#21644;&#22810;&#27493;&#27169;&#25311;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#30452;&#25509;&#23545;&#29305;&#23450;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#65292;&#32780;&#26159;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#20195;&#34920;&#21160;&#24577;&#31995;&#32479;&#31867;&#21035;&#30340;&#20803;&#27169;&#22411;&#12290;&#35813;&#20803;&#27169;&#22411;&#26159;&#36890;&#36807;&#20174;&#26576;&#20010;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#28508;&#22312;&#26080;&#38480;&#27969;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#20854;&#26680;&#24515;&#65292;&#20803;&#27169;&#22411;&#20316;&#20026;&#23545;&#20027;&#35201;&#29305;&#24449;&#30340;&#38544;&#24335;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
In traditional system identification, we estimate a model of an unknown dynamical system based on given input/output sequences and available physical knowledge. Yet, is it also possible to understand the intricacies of dynamical systems not solely from their input/output patterns, but by observing the behavior of other systems within the same class? This central question drives the study presented in this paper.  In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we pretrain a meta model that represents a class of dynamical systems. This meta model is trained from a potentially infinite stream of synthetic data, generated by systems randomly extracted from a certain distribution. At its core, the meta model serves as an implicit representation of the main characteristics of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2308.08511</link><description>&lt;p&gt;
&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;CT&#21644;MRI&#20013;&#36935;&#21040;&#30340;&#19981;&#21516;&#21453;&#38382;&#39064;&#65288;&#22914;&#31232;&#30095;&#35270;&#37326;CT&#21644;&#24555;&#36895;MRI&#37325;&#24314;&#65289;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#37325;&#24314;&#20108;&#32500;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#37325;&#24314;&#30340;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20013;&#23548;&#33268;&#30456;&#37051;&#20999;&#29255;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;TOSM&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#22312;&#19977;&#32500;&#20307;&#31215;&#19978;&#24037;&#20316;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37325;&#24314;&#38454;&#27573;&#65292;TOSM&#20250;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#65292;&#21033;&#29992;&#19977;&#20010;&#26041;&#21521;&#30340;&#20114;&#34917;&#24471;&#20998;&#65288;sag&#65289;
&lt;/p&gt;
&lt;p&gt;
Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sag
&lt;/p&gt;</description></item><item><title>DeSCo&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#35745;&#25968;&#20934;&#30830;&#24615;&#12289;&#22270;&#24418;&#21306;&#20998;&#21644;&#20986;&#29616;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08198</link><description>&lt;p&gt;
DeSCo:&#38754;&#21521;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting. (arXiv:2308.08198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08198
&lt;/p&gt;
&lt;p&gt;
DeSCo&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#35745;&#25968;&#20934;&#30830;&#24615;&#12289;&#22270;&#24418;&#21306;&#20998;&#21644;&#20986;&#29616;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#35745;&#25968;&#26159;&#22312;&#22823;&#22411;&#30446;&#26631;&#22270;&#20013;&#35745;&#31639;&#32473;&#23450;&#26597;&#35810;&#22270;&#30340;&#20986;&#29616;&#27425;&#25968;&#30340;&#38382;&#39064;&#12290;&#22823;&#35268;&#27169;&#30340;&#23376;&#22270;&#35745;&#25968;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#24456;&#26377;&#29992;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#22270;&#26696;&#35745;&#25968;&#21644;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#21453;&#27927;&#38065;&#26816;&#27979;&#20013;&#30340;&#24490;&#29615;&#35745;&#25968;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#23376;&#22270;&#35745;&#25968;&#30340;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#19977;&#20010;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#30456;&#21516;&#26597;&#35810;&#30340;&#35745;&#25968;&#22312;&#19981;&#21516;&#30446;&#26631;&#22270;&#19978;&#21487;&#20197;&#20174;&#38646;&#21040;&#25968;&#30334;&#19975;&#65292;&#27604;&#22823;&#22810;&#25968;&#22270;&#22238;&#24402;&#20219;&#21153;&#37117;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#20854;&#27425;&#65292;&#30446;&#21069;&#30340;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26080;&#27861;&#39640;&#25928;&#22320;&#21306;&#20998;&#35745;&#25968;&#39044;&#27979;&#20013;&#30340;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#26597;&#35810;&#22312;&#30446;&#26631;&#22270;&#20013;&#30340;&#20986;&#29616;&#20301;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeSCo&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#20934;&#30830;&#22320;&#39044;&#27979;&#23376;&#22270;&#30340;&#35745;&#25968;&#21644;&#20986;&#29616;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.  Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADR-GNN&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;ADR-GNN&#22312;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#26368;&#20808;&#36827;&#32593;&#32476;&#30456;&#27604;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.16092</link><description>&lt;p&gt;
ADR-GNN&#65306;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks. (arXiv:2307.16092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADR-GNN&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;ADR-GNN&#22312;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#26368;&#20808;&#36827;&#32593;&#32476;&#30456;&#27604;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GNN&#22312;&#24314;&#27169;&#28041;&#21450;&#24179;&#27969;&#30340;&#22797;&#26434;&#29616;&#35937;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#31216;&#20026;ADR-GNN&#12290;&#24179;&#27969;&#27169;&#24335;&#21270;&#20102;&#20449;&#24687;&#30340;&#26377;&#21521;&#20256;&#36755;&#65292;&#25193;&#25955;&#25429;&#25417;&#20102;&#20449;&#24687;&#30340;&#23616;&#37096;&#24179;&#28369;&#65292;&#21453;&#24212;&#20195;&#34920;&#20102;&#36890;&#36947;&#20013;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#23545;ADR-GNN&#30340;&#23450;&#24615;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#23558;&#24179;&#27969;&#12289;&#25193;&#25955;&#21644;&#21453;&#24212;&#32467;&#21512;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ADR-GNN&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;&#25913;&#36827;&#25110;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07063</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#32806;&#30340;&#35821;&#35328;&#39044;&#35757;&#32451;&#20026;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#24341;&#20837;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36164;&#28304;&#23494;&#38598;&#22411;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33539;&#24335;&#20351;&#29992;&#35270;&#35273;&#29305;&#24449;&#20316;&#20026;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#19982;&#30456;&#24212;&#25991;&#26412;&#26368;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#20855;&#20307;&#26159;&#30830;&#23450;&#19982;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt-Transformer&#65288;P-Former&#65289;&#65292;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#36825;&#20123;&#29702;&#24819;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#31471;&#21040;&#31471;&#30340;VL&#35757;&#32451;&#36807;&#31243;&#24039;&#22937;&#22320;&#20998;&#20026;&#20102;&#39069;&#22806;&#30340;&#29420;&#31435;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#20581;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#22522;&#32447;&#65288;BLIP-2&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#20351;&#29992;4M&#25110;129M&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14932</link><description>&lt;p&gt;
GloptiNets&#65306;&#20855;&#26377;&#35777;&#26126;&#30340;&#21487;&#25193;&#23637;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GloptiNets: Scalable Non-Convex Optimization with Certificates. (arXiv:2306.14932v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14932
&lt;/p&gt;
&lt;p&gt;
GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36229;&#31435;&#26041;&#20307;&#25110;&#29615;&#38754;&#19978;&#30340;&#20809;&#28369;&#20989;&#25968;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20381;&#36182;&#20195;&#25968;&#24615;&#36136;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#65292;&#35813;&#27491;&#21017;&#24615;&#22312;&#20854;&#20613;&#37324;&#21494;&#35889;&#30340;&#34928;&#20943;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#27169;&#22411;&#26063;&#65292;&#25105;&#20204;&#26082;&#33021;&#22815;&#33719;&#24471;&#31934;&#30830;&#30340;&#35777;&#26126;&#65292;&#21448;&#33021;&#22815;&#21033;&#29992;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#36827;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#36890;&#36807;&#19982;GPU&#30340;&#24182;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#25968;&#21315;&#20010;&#31995;&#25968;&#20294;&#32500;&#24230;&#36866;&#20013;&#30340;&#22810;&#39033;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#22522;&#20110;Lasserre&#23618;&#27425;&#30340;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31454;&#20105;&#32773;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow at the same time to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06041</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#37051;&#20808;&#39564;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Graph Prior for Relational Inference. (arXiv:2306.06041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06041
&lt;/p&gt;
&lt;p&gt;
DYGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#30340;&#21160;&#24577;&#22270;&#20808;&#39564;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25512;&#26029;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#35782;&#21035;&#37096;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22312;&#21487;&#23398;&#20064;&#30340;&#22270;&#19978;&#25311;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#23427;&#20204;&#20351;&#29992;&#19968;&#27493;&#28040;&#24687;&#20256;&#36882; GNN--&#30452;&#35266;&#19978;&#26469;&#35828;&#26159;&#27491;&#30830;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#22810;&#27493;&#25110;&#35889; GNN &#30340;&#38750;&#23616;&#37096;&#24615;&#21487;&#33021;&#20250;&#28151;&#28102;&#30452;&#25509;&#21644;&#38388;&#25509;&#30456;&#20114;&#20316;&#29992;&#12290;&#20294;&#26159;&#8220;&#26377;&#25928;&#8221;&#30340;&#20132;&#20114;&#22270;&#21462;&#20915;&#20110;&#37319;&#26679;&#36895;&#29575;&#65292;&#24456;&#23569;&#23616;&#38480;&#20110;&#30452;&#25509;&#37051;&#23621;&#65292;&#23548;&#33268;&#19968;&#20010;&#27493;&#39588;&#27169;&#22411;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21160;&#24577;&#22270;&#20808;&#39564;&#8221;(DYGR)&#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20808;&#39564;&#30340;&#21407;&#22240;&#26159;&#65292;&#19982;&#24050;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#39640;&#24230;&#38750;&#23616;&#37096;&#30340;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20013;&#26500;&#36896;&#24615;&#22320;&#21033;&#29992;&#35823;&#24046;&#25918;&#22823;&#26469;&#29983;&#25104;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#33391;&#22909;&#26799;&#24230;&#12290;&#20026;&#20102;&#22788;&#29702;&#38750;&#21807;&#19968;&#24615;&#65292;DYGR &#21516;&#26102;&#36866;&#29992;&#20110;&#20855;&#26377;&#20849;&#20139;&#22270;&#25299;&#25169;&#30340;&#8220;&#27973;&#23618;&#8221;&#19968;&#27493;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126; DYGR &#33021;&#22815;&#37325;&#26032;&#26500;&#24314;&#20132;&#20114;&#32467;&#26500;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational inference aims to identify interactions between parts of a dynamical system from the observed dynamics. Current state-of-the-art methods fit a graph neural network (GNN) on a learnable graph to the dynamics. They use one-step message-passing GNNs -- intuitively the right choice since non-locality of multi-step or spectral GNNs may confuse direct and indirect interactions. But the \textit{effective} interaction graph depends on the sampling rate and it is rarely localized to direct neighbors, leading to local minima for the one-step model. In this work, we propose a \textit{dynamical graph prior} (DYGR) for relational inference. The reason we call it a prior is that, contrary to established practice, it constructively uses error amplification in high-degree non-local polynomial filters to generate good gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously fits a ``shallow'' one-step model with shared graph topology. Experiments show that DYGR reconstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;MBP&#65289;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;ChEMBL-Dock&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#19981;&#21516;&#26631;&#31614;&#21644;&#23454;&#39564;&#26465;&#20214;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04886</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-task Bioassay Pre-training for Protein-ligand Binding Affinity Prediction. (arXiv:2306.04886v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;MBP&#65289;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;ChEMBL-Dock&#65292;&#35299;&#20915;&#20102;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#19981;&#21516;&#26631;&#31614;&#21644;&#23454;&#39564;&#26465;&#20214;&#23548;&#33268;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#65288;PLBA&#65289;&#39044;&#27979;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;&#19977;&#32500;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#24182;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#36827;&#23637;&#26469;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24403;&#21069;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#29983;&#29289;&#27979;&#23450;&#20351;&#29992;&#19981;&#21516;&#30340;&#20146;&#21644;&#21147;&#27979;&#37327;&#26631;&#31614;&#65288;&#21363;IC50&#65292;Ki&#65292;Kd&#65289;&#65292;&#19981;&#21516;&#30340;&#23454;&#39564;&#26465;&#20214;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#31995;&#32479;&#22122;&#22768;&#65292;&#36825;&#23545;&#26500;&#24314;&#39640;&#31934;&#24230;&#20146;&#21644;&#21147;&#39044;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#29983;&#29289;&#27979;&#23450;&#39044;&#35757;&#32451;&#65288;MBP&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;PLBA&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;;&#65288;2&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;300k&#20010;&#23454;&#39564;&#27979;&#23450;&#20146;&#21644;&#21147;&#26631;&#31614;&#21644;&#32422;2.8M&#20010;&#23545;&#25509;&#19977;&#32500;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChEMBL-Dock&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-ligand binding affinity (PLBA) prediction is the fundamental task in drug discovery. Recently, various deep learning-based models predict binding affinity by incorporating the three-dimensional structure of protein-ligand complexes as input and achieving astounding progress. However, due to the scarcity of high-quality training data, the generalization ability of current models is still limited. In addition, different bioassays use varying affinity measurement labels (i.e., IC50, Ki, Kd), and different experimental conditions inevitably introduce systematic noise, which poses a significant challenge to constructing high-precision affinity prediction models. To address these issues, we (1) propose Multi-task Bioassay Pre-training (MBP), a pre-training framework for structure-based PLBA prediction; (2) construct a pre-training dataset called ChEMBL-Dock with more than 300k experimentally measured affinity labels and about 2.8M docked three-dimensional structures. By introducing m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03625</link><description>&lt;p&gt;
&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#25919;&#31574;&#23398;&#20064;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning. (arXiv:2306.03625v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#19988;&#20581;&#22766;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20844;&#24179;&#32422;&#26463;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#65292;&#24182;&#21487;&#29992;&#20110;&#26435;&#34913;&#20844;&#24179;&#21644;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20844;&#24179;&#32422;&#26463;&#26465;&#20214;&#19979;&#38750;&#21442;&#25968;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#12290;&#22312;&#26631;&#20934;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#21452;&#37325;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#27492;&#26694;&#26550;&#26469;&#34920;&#24449;&#20844;&#24179;&#21644;&#26368;&#20339;&#25919;&#31574;&#21487;&#23454;&#29616;&#30340;&#26368;&#22823;&#31119;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.03410</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#25311;&#26641;&#26525;&#21160;&#21147;&#23398;&#20197;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#27169;&#25311;&#25805;&#32437;&#19979;&#30340;&#26641;&#26525;&#20851;&#33410;&#21160;&#21147;&#23398;&#12290;&#23398;&#20064;&#26525;&#24178;&#21160;&#24577;&#24182;&#33719;&#24471;&#25805;&#32437;&#21487;&#21464;&#24418;&#26893;&#34987;&#30340;&#33021;&#21147;&#21487;&#24110;&#21161;&#22788;&#29702;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#23494;&#38598;&#26641;&#21494;&#20013;&#37319;&#25688;&#27700;&#26524;&#12289;&#31227;&#21160;&#24748;&#22402;&#30340;&#34276;&#34067;&#21644;&#26641;&#26525;&#65292;&#20197;&#20415;&#22312;&#23494;&#38598;&#26893;&#34987;&#20013;&#23548;&#33322;&#12290;&#26893;&#29289;&#30340;&#21487;&#21464;&#24418;&#20960;&#20309;&#24418;&#29366;&#36890;&#36807;&#22312;&#24182;&#34892;&#12289;&#19981;&#21487;&#24494;&#27169;&#25311;&#22120;&#19978;&#25191;&#34892;&#30340;&#31895;&#30053;&#24377;&#31783;&#25277;&#35937;&#26469;&#23454;&#29616;&#12290;&#30001;&#27169;&#25311;&#22120;&#23450;&#20041;&#30340;&#38544;&#24335;&#32479;&#35745;&#27169;&#22411;&#12289;&#36890;&#36807;&#20027;&#21160;&#25506;&#27979;&#30340;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#33719;&#24471;&#30340;&#21442;&#32771;&#36712;&#36857;&#21644;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#20041;&#19968;&#36215;&#25351;&#23548;&#24377;&#31783;&#21442;&#25968;&#21518;&#39564;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26080;&#21442;&#25968;&#25512;&#29702;&#31639;&#27861;&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#24182;&#23558;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#23398;&#20064;&#32852;&#21512;&#20808;&#39564;&#21512;&#24182;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#26469;&#23545;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#22256;&#38590;&#21644;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to use a simulation driven inverse inference approach to model the joint dynamics of tree branches under manipulation. Learning branch dynamics and gaining the ability to manipulate deformable vegetation can help with occlusion-prone tasks, such as fruit picking in dense foliage, as well as moving overhanging vines and branches for navigation in dense vegetation. The underlying deformable tree geometry is encapsulated as coarse spring abstractions executed on parallel, non-differentiable simulators. The implicit statistical model defined by the simulator, reference trajectories obtained by actively probing the ground truth, and the Bayesian formalism, together guide the spring parameter posterior density estimation. Our non-parametric inference algorithm, based on Stein Variational Gradient Descent, incorporates biologically motivated assumptions into the inference process as neural network driven learnt joint priors; moreover, it leverages the finite difference scheme for g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#31561;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.02630</link><description>&lt;p&gt;
&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Covariance Adaptive Best Arm Identification. (arXiv:2306.02630v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#31561;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#27169;&#22411;&#19979;&#65292;&#22522;&#20110;&#22266;&#23450;&#32622;&#20449;&#24230;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;&#32622;&#20449;&#24230; $\delta$ &#30340;&#24773;&#20917;&#19979;&#65292;&#26088;&#22312;&#20197;&#33267;&#23569; 1 - $\delta$ &#30340;&#27010;&#29575;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;&#33218;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#33218;&#30340;&#25289;&#21160;&#27425;&#25968;&#12290;&#34429;&#28982;&#25991;&#29486;&#25552;&#20379;&#20102;&#38024;&#23545;&#29420;&#31435;&#33218;&#20998;&#24067;&#20551;&#35774;&#19979;&#35813;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#28789;&#27963;&#30340;&#24773;&#24418;&#65292;&#20854;&#20013;&#33218;&#21487;&#20197;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25910;&#30410;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#37319;&#26679;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#23398;&#20064;&#32773;&#20272;&#35745;&#33218;&#20043;&#38388;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#26368;&#20339;&#33218;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24212;&#33218;&#21327;&#26041;&#24046;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#35777;&#26126;&#20854;&#20855;&#26377;&#23454;&#36136;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of best arm identification in the multi-armed bandit model, under fixed confidence. Given a confidence input $\delta$, the goal is to identify the arm with the highest mean reward with a probability of at least 1 -- $\delta$, while minimizing the number of arm pulls. While the literature provides solutions to this problem under the assumption of independent arms distributions, we propose a more flexible scenario where arms can be dependent and rewards can be sampled simultaneously. This framework allows the learner to estimate the covariance among the arms distributions, enabling a more efficient identification of the best arm. The relaxed setting we propose is relevant in various applications, such as clinical trials, where similarities between patients or drugs suggest underlying correlations in the outcomes. We introduce new algorithms that adapt to the unknown covariance of the arms and demonstrate through theoretical guarantees that substantial improvement 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01266</link><description>&lt;p&gt;
&#33258;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Self Contrastive Learning for Session-based Recommendation. (arXiv:2306.01266v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#29616;&#26377;&#39033;&#30446;&#20132;&#20114;&#24207;&#21015;&#30340;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#24212;&#29992;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#25552;&#39640;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23545;&#27604;&#30446;&#26631;&#65306;&#65288;1&#65289;&#36215;&#21040;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#31867;&#20284;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#24573;&#30053;&#20102;&#39033;&#30446;&#34920;&#31034;&#31354;&#38388;&#20248;&#21270;&#65307;&#65288;2&#65289;&#36890;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#24314;&#27169;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#27491;/&#36127;&#26679;&#26412;&#26500;&#24314;&#21644;&#39069;&#22806;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#31616;&#21270;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#24182;&#22686;&#24378;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#25512;&#33616;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SCL&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65292;&#30452;&#25509;&#20419;&#36827;&#39033;&#30446;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#24182;&#26377;&#25928;&#22320;&#26367;&#25442;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#23545;&#27604;&#30446;&#26631;&#32452;&#20214;&#30340;&#29366;&#24577;-&#33402;&#26415;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SCL&#28040;&#38500;&#20102;&#20219;&#20309;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#21644;SCL&#28040;&#38500;&#20102;&#20219;&#20309;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04214</link><description>&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#65306;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04214
&lt;/p&gt;
&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PiML&#26159;&#19968;&#20010;&#32508;&#21512;&#19988;&#24320;&#25918;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#27169;&#22411;&#35786;&#26029;&#12290;&#23427;&#35774;&#35745;&#20102;&#20302;&#20195;&#30721;&#21644;&#39640;&#20195;&#30721;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#65292;&#21253;&#25324;&#25968;&#25454;&#31649;&#36947;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27169;&#22411;&#35299;&#37322;&#21644;&#35828;&#26126;&#20197;&#21450;&#27169;&#22411;&#35786;&#26029;&#21644;&#27604;&#36739;&#12290;&#35813;&#24037;&#20855;&#31665;&#25903;&#25345;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;&#20363;&#22914;GAM&#12289;GAMI-Net&#12289;XGB2&#65289;&#65292;&#20855;&#26377;&#26412;&#22320;&#21644;/&#25110;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;&#23427;&#36824;&#25903;&#25345;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;&#20363;&#22914;PFI&#12289;PDP&#12289;LIME&#12289;SHAP&#65289;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#26080;&#20851;&#35786;&#26029;&#22871;&#20214;&#65288;&#20363;&#22914;&#24369;&#28857;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#65289;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;&#39640;&#20195;&#30721; API&#65292;&#23558; PiML &#27169;&#22411;&#21644;&#27979;&#35797;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340; MLOps &#24179;&#21488;&#20197;&#23454;&#29616;&#36136;&#37327;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;PiML &#24037;&#20855;&#31665;&#36824;&#24102;&#26377;&#32508;&#21512;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#23454;&#36341;&#20363;&#23376;&#65292;&#21253;&#25324;&#38134;&#34892;&#19994;&#20013;&#30340;&#27169;&#22411;&#24320;&#21457;&#21644;&#39564;&#35777;&#24212;&#29992;&#12290;&#35813;&#39033;&#30446;&#21487;&#36890;&#36807;arXiv:2305.04214v1[cs.LG]&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
PiML (read $\pi$-ML, /`pai.`em.`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.04353</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#37327;&#23376;&#22810;&#20307;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Exponentially Improved Efficient Machine Learning for Quantum Many-body States with Provable Guarantees. (arXiv:2304.04353v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04353
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#65292;&#20854;&#31934;&#24230;&#20026; $\varepsilon$&#65292;&#24182;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#38556;&#65307;&#20294;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26679;&#26412;&#20010;&#25968; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#21482;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#32500;&#24230;&#36739;&#22823;&#65292;&#19988;&#31934;&#24230;&#19981;&#26159;&#32039;&#36843;&#22240;&#32032;&#65292;&#26080;&#27861;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32463;&#20856;&#31639;&#27861;&#32780;&#35328;&#65292;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#36890;&#24120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#23450;&#20041;&#22312;&#29289;&#29702;&#21442;&#25968; $m$ &#32500;&#31354;&#38388;&#19978;&#30340;&#21704;&#23494;&#39039;&#37327;&#26063;&#65292;&#21482;&#35201;&#21487;&#20197;&#39640;&#25928;&#22320;&#20934;&#22791;&#21644;&#27979;&#37327;&#19968;&#32452; $N$ &#20010;&#24577;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#39044;&#27979;&#20854;&#22522;&#24577;&#21450;&#20854;&#22312;&#20219;&#24847;&#21442;&#25968;&#37197;&#32622;&#19979;&#30340;&#24615;&#36136;&#65292;&#31934;&#24230;&#20026; $\varepsilon$&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350; [Huang &#31561;&#20154;&#65292;Science 377&#65292;eabk3333&#65288;2022&#65289;] &#23545;&#36825;&#31181;&#19968;&#33324;&#21270;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#20445;&#38556;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#26222;&#36941;&#30340;&#33021;&#38553;&#21704;&#23494;&#39039;&#37327;&#65292;&#26222;&#36866;&#30340;&#25351;&#25968;&#32553;&#25918;&#20026; $N = m^{{\cal{O}} \left(\frac{1}{\varepsilon}\right)}$&#65292;&#36825;&#20010;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#36739;&#22823;&#65292;&#32780;&#31934;&#24230;&#30340;&#32553;&#25918;&#21017;&#19981;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#22240;&#32032;&#65292;&#19981;&#33021;&#36827;&#20837;&#26356;&#31934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the ground state and the ground-state properties of quantum many-body systems is generically a hard task for classical algorithms. For a family of Hamiltonians defined on an $m$-dimensional space of physical parameters, the ground state and its properties at an arbitrary parameter configuration can be predicted via a machine learning protocol up to a prescribed prediction error $\varepsilon$, provided that a sample set (of size $N$) of the states can be efficiently prepared and measured. In a recent work [Huang et al., Science 377, eabk3333 (2022)], a rigorous guarantee for such an generalization was proved. Unfortunately, an exponential scaling, $N = m^{ {\cal{O}} \left(\frac{1}{\varepsilon} \right) }$, was found to be universal for generic gapped Hamiltonians. This result applies to the situation where the dimension of the parameter space is large while the scaling with the accuracy is not an urgent factor, not entering the realm of more precise learning and prediction. In th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03483</link><description>&lt;p&gt;
RED-PSM: &#24102;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25104;&#20687;&#26159;&#25351;&#21033;&#29992;&#34987;&#27424;&#37319;&#26679;&#30340;&#27979;&#37327;&#25968;&#25454;&#24674;&#22797;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#26102;&#21464;&#20108;&#32500;&#25110;&#19977;&#32500;&#29289;&#20307;&#12290;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#21482;&#26377;&#19968;&#20010;&#35270;&#35282;&#19979;&#30340;&#21333;&#20010;&#25237;&#24433;&#21487;&#29992;&#65292;&#20351;&#24471;&#38382;&#39064;&#20005;&#37325;&#31639;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20004;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25104;&#20687;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#25216;&#26415;&#26159;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#65292;&#24050;&#32463;&#29992;&#20110;&#39640;&#25928;&#22320;&#20026;&#26102;&#31354;&#30446;&#26631;&#24341;&#20837;&#20302;&#31209;&#20808;&#39564;&#12290;&#31532;&#20108;&#31181;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#21435;&#22122;&#27491;&#21017;&#21270;(RED)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#22788;&#29702;&#21508;&#31181;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#30446;&#26631;&#65292;&#36890;&#36807;&#21464;&#37327;&#20998;&#35010;&#21644;ADMM&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#25910;&#25947;&#20110;&#19968;&#20010;&#28385;&#36275;&#20248;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RED-PSM&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16521</link><description>&lt;p&gt;
&#22312;&#19981;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26159;&#25351;&#32852;&#21512;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#32858;&#31867;&#27169;&#22411;&#65292;&#20197;&#23558;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#25110;&#25209;&#22788;&#29702;&#20998;&#37197;&#21040;&#32858;&#31867;&#26631;&#31614;&#20013;&#12290;&#23613;&#31649;&#27604;&#31163;&#32447;&#26041;&#27861;&#26356;&#24555;&#36895;&#21644;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#32447;&#32858;&#31867;&#24456;&#23481;&#26131;&#36798;&#21040;&#23849;&#28291;&#35299;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#36755;&#20837;&#26144;&#23556;&#21040;&#21516;&#19968;&#28857;&#65292;&#24182;&#23558;&#25152;&#26377;&#36755;&#20837;&#25918;&#20837;&#21333;&#20010;&#32858;&#31867;&#20013;&#12290;&#29616;&#26377;&#25104;&#21151;&#27169;&#22411;&#37319;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#26088;&#22312;&#20351;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#32858;&#31867;&#30340;&#24179;&#22343;&#36719;&#20998;&#37197;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#23545;&#30828;&#20998;&#37197;&#36827;&#34892;&#20102;&#35268;&#21017;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23548;&#20986;&#19968;&#20010;&#30452;&#35266;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21487;&#20197;&#30452;&#25509;&#21253;&#21547;&#22312;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#31283;&#23450;&#22320;&#36991;&#20813;&#20102;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.15413</link><description>&lt;p&gt;
2D&#25193;&#25955;&#31639;&#27861;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. (arXiv:2303.15413v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#20559;&#32622;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#20986;&#29616;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#31216;&#20026;Janus&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#26469;&#33258;&#20110;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#20559;&#32622;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;3D&#23545;&#35937;&#19981;&#30495;&#23454;&#12290;&#36890;&#36807;&#23545;&#20854;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21435;&#38500;&#20559;&#32622;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21483;&#20570;score debiasing&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;2D&#25193;&#25955;&#27169;&#22411;&#24471;&#20986;&#30340;&#20998;&#25968;&#30340;&#25130;&#26029;&#20540;&#65292;&#26469;&#36798;&#21040;&#21435;&#38500;&#20559;&#32622;&#30340;&#25928;&#26524;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#21483;&#20570;prompt debiasing&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#29992;&#25143;&#25552;&#31034;&#21644;&#35270;&#35282;&#25552;&#31034;&#20043;&#38388;&#30340;&#30683;&#30462;&#35789;&#35821;&#65292;&#24182;&#35843;&#25972;&#35270;&#35282;&#25552;&#31034;&#21644;&#29289;&#20307;&#31354;&#38388;&#25668;&#20687;&#26426;&#23039;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#20266;&#24433;&#65292;&#25552;&#39640;&#20102;&#30495;&#23454;&#24863;&#65292;&#24182;&#22312;&#36136;&#37327;&#19982;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The view inconsistency problem in score-distilling text-to-3D generation, also known as the Janus problem, arises from the intrinsic bias of 2D diffusion models, which leads to the unrealistic generation of 3D objects. In this work, we explore score-distilling text-to-3D generation and identify the main causes of the Janus problem. Based on these findings, we propose two approaches to debias the score-distillation frameworks for robust text-to-3D generation. Our first approach, called score debiasing, involves gradually increasing the truncation value for the score estimated by 2D diffusion models throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts utilizing a language model and adjusts the discrepancy between view prompts and object-space camera poses. Our experimental results show that our methods improve realism by significantly reducing artifacts and achieve a good trade-off between fa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.00196</link><description>&lt;p&gt;
&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#21487;&#20197;&#24110;&#21161;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#22238;&#31572;&#20102;&#36716;&#25442;&#30340;&#20302;&#31209;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#22810;&#36890;&#36947;&#25968;&#25454;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36716;&#25442;&#22495;&#20013;&#30340;&#20302;&#31209;&#24615;&#65292;&#21363;&#36716;&#25442;&#30340;&#20302;&#31209;&#24615;&#65292;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#22312;&#22810;&#36890;&#36947;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#24182;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#20989;&#25968;&#34920;&#31034;&#65292;&#22914;&#20855;&#26377;t-&#20056;&#31215;&#23618;&#65288;t-NNs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;t-SVD&#29702;&#35770;&#19978;&#22914;&#20309;&#24433;&#21709;t-NNs&#30340;&#23398;&#20064;&#34892;&#20026;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#36890;&#36807;&#25512;&#23548;&#26631;&#20934;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;t-NNs&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#36716;&#25442;&#20302;&#31209;&#21442;&#25968;&#21270;&#21387;&#32553;&#30340;t-NNs&#21487;&#20197;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#23545;&#25239;&#27867;&#21270;&#19978;&#30028;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23613;&#31649;t-NNs&#24456;&#23569;&#20855;&#26377;&#23436;&#20840;&#36716;&#25442;&#30340;&#20302;&#31209;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#27969;&#65288;GF&#65289;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;t-NNs&#20855;&#26377;ReLU
&lt;/p&gt;
&lt;p&gt;
Achieving efficient and robust multi-channel data learning is a challenging task in data science. By exploiting low-rankness in the transformed domain, i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has achieved extensive success in multi-channel data representation and has recently been extended to function representation such as Neural Networks with t-product layers (t-NNs). However, it still remains unclear how t-SVD theoretically affects the learning behavior of t-NNs. This paper is the first to answer this question by deriving the upper bounds of the generalization error of both standard and adversarially trained t-NNs. It reveals that the t-NNs compressed by exact transformed low-rank parameterization can achieve a sharper adversarial generalization bound. In practice, although t-NNs rarely have exactly transformed low-rank weights, our analysis further shows that by adversarial training with gradient flow (GF), the over-parameterized t-NNs with ReLU 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.03713</link><description>&lt;p&gt;
&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#30340;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing. (arXiv:2301.03713v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#30340;&#21628;&#21560;&#39057;&#29575;&#21644;&#21628;&#21560;&#27169;&#24335;&#20256;&#36798;&#20102;&#20851;&#20110;&#20027;&#20307;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#24322;&#24120;&#21628;&#21560;&#21487;&#33021;&#34920;&#26126;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#20351;&#29992;&#38750;&#30456;&#24178;&#32418;&#22806;&#20809;&#30340;&#26080;&#32447;&#20809;&#27874;&#20256;&#24863;&#65288;LWS&#65289;&#22312;&#19981;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#31034;&#20102;&#23433;&#20840;&#12289;&#38544;&#34109;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#30340;&#28508;&#21147;&#12290;&#21628;&#21560;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#21628;&#21560;&#24322;&#24120;&#12290;&#35813;&#31995;&#32479;&#36824;&#24517;&#39035;&#39564;&#35777;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#26159;&#21542;&#20026;&#21628;&#21560;&#27874;&#24418;&#65292;&#20002;&#24323;&#30001;&#22806;&#37096;&#24178;&#25200;&#12289;&#29992;&#25143;&#31227;&#21160;&#25110;&#31995;&#32479;&#25925;&#38556;&#24341;&#36215;&#30340;&#20219;&#20309;&#38169;&#35823;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#27169;&#25311;&#20154;&#31867;&#21628;&#21560;&#27169;&#24335;&#30340;&#26426;&#22120;&#20154;&#65292;&#27169;&#25311;&#20102;&#27491;&#24120;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#21628;&#21560;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#25910;&#38598;&#20102;&#26102;&#38388;&#24207;&#21015;&#21628;&#21560;&#25968;&#25454;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and non-invasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies.The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.06370</link><description>&lt;p&gt;
&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31934;&#24230;&#36136;&#37327;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#23398;&#20064;&#22522;&#20110;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#32780;&#38750;&#21482;&#25552;&#20379;&#20256;&#32479;&#30340;&#30446;&#26631;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#20197;&#21450;&#25552;&#39640;&#35206;&#30422;&#27010;&#29575;&#26469;&#25552;&#39640;PI&#30340;&#36136;&#37327;&#21644;&#31934;&#24230;&#65292;&#19988;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#24212;&#35813;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#39044;&#27979;&#20043;&#22806;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;(PIs)&#12290;&#21482;&#35201;Pis&#36275;&#22815;&#31364;&#32780;&#19988;&#25429;&#33719;&#20102;&#22823;&#37096;&#20998;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36825;&#20123;Pis&#23601;&#26159;&#26377;&#29992;&#30340;&#25110;"&#39640;&#36136;&#37327;"&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#21306;&#38388;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#30446;&#26631;&#39044;&#27979;&#20043;&#22806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20004;&#20010;&#20276;&#20387;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#20351;&#29992;&#19968;&#20010;&#36755;&#20986;&#65292;&#30446;&#26631;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#36755;&#20986;&#65292;&#30456;&#24212;PI&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#29983;&#25104;PI&#30340;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#32771;&#34385;&#20102;&#30446;&#26631;&#20272;&#35745;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#20004;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#20943;&#23567;&#24179;&#22343;&#39044;&#27979;&#21306;&#38388;&#23485;&#24230;&#21644;&#25552;&#39640;Pis&#30340;&#36136;&#37327;(&#36890;&#36807;&#20854;&#35206;&#30422;&#27010;&#29575;&#36827;&#34892;&#27979;&#37327;)&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#19988;&#36136;&#37327;&#26356;&#39640;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10525</link><description>&lt;p&gt;
&#21487;&#24494;&#38750;&#26631;&#23450;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Differentiable Uncalibrated Imaging. (arXiv:2211.10525v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a differentiable imaging framework to address uncertainty in measurement coordinates, using implicit neural networks and differentiable spline interpolators. The method is applied to 2D and 3D computed tomography and produces improved reconstructions.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#65288;&#22914;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#25237;&#24433;&#35282;&#24230;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#22312;&#26410;&#30693;&#33410;&#28857;&#22788;&#30340;&#27979;&#37327;&#25554;&#20540;&#65292;&#36890;&#36807;&#27491;&#21521;&#31639;&#23376;&#36827;&#34892;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#20063;&#31216;&#20026;&#31070;&#32463;&#22330;&#65292;&#23427;&#20204;&#22312;&#36755;&#20837;&#22352;&#26631;&#26041;&#38754;&#33258;&#28982;&#21487;&#24494;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#65292;&#20854;&#24615;&#33021;&#19982;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#22909;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#20248;&#21270;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#12290;&#21487;&#24494;&#24615;&#26159;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#20849;&#21516;&#25311;&#21512;&#27979;&#37327;&#34920;&#31034;&#65292;&#20248;&#21270;&#19981;&#30830;&#23450;&#30340;&#27979;&#37327;&#22352;&#26631;&#65292;&#24182;&#25191;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#20174;&#32780;&#30830;&#20445;&#19968;&#33268;&#30340;&#26631;&#23450;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#19981;&#32771;&#34385;&#32570;&#20047;&#26631;&#23450;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
We propose a differentiable imaging framework to address uncertainty in measurement coordinates such as sensor locations and projection angles. We formulate the problem as measurement interpolation at unknown nodes supervised through the forward operator. To solve it we apply implicit neural networks, also known as neural fields, which are naturally differentiable with respect to the input coordinates. We also develop differentiable spline interpolators which perform as well as neural networks, require less time to optimize and have well-understood properties. Differentiability is key as it allows us to jointly fit a measurement representation, optimize over the uncertain measurement coordinates, and perform image reconstruction which in turn ensures consistent calibration. We apply our approach to 2D and 3D computed tomography and show that it produces improved reconstructions compared to baselines that do not account for the lack of calibration. The flexibility of the proposed framew
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.04587</link><description>&lt;p&gt;
&#22810;&#28857;-BAX: &#19968;&#31181;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#21457;&#23556;&#24230;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v4 [physics.acc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#28857;-BAX&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34394;&#25311;&#30446;&#26631;&#26469;&#39640;&#25928;&#35843;&#25972;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21457;&#23556;&#24230;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#20351;&#29992;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#36827;&#34892;&#32531;&#24930;&#32780;&#20302;&#25928;&#30340;&#22810;&#28857;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#21457;&#23556;&#24230;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#20013;&#26368;&#23567;&#21270;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26463;&#21457;&#23556;&#24230;&#23545;&#20110;&#39640;&#20142;&#24230;&#21152;&#36895;&#22120;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20248;&#21270;&#36890;&#24120;&#20250;&#21463;&#21040;&#26102;&#38388;&#38480;&#21046;&#65292;&#22240;&#20026;&#21457;&#23556;&#24230;&#35745;&#31639;&#36890;&#24120;&#26159;&#36890;&#36807;&#22235;&#26497;&#25195;&#25551;&#23436;&#25104;&#30340;&#65292;&#32780;&#22235;&#26497;&#25195;&#25551;&#36890;&#24120;&#36739;&#24930;&#12290;&#36825;&#31181;&#35745;&#31639;&#26159;&#19968;&#31181;&#22810;&#28857;&#26597;&#35810;&#65292;&#21363;&#27599;&#20010;&#26597;&#35810;&#37117;&#38656;&#35201;&#22810;&#20010;&#36741;&#21161;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#22788;&#29702;&#36825;&#26679;&#30340;&#30446;&#26631;&#26102;&#36895;&#24230;&#24930;&#19988;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#33719;&#21462;&#23436;&#25972;&#30340;&#27979;&#37327;&#24207;&#21015;&#65292;&#20294;&#27599;&#20010;&#26597;&#35810;&#20165;&#36820;&#22238;&#21457;&#23556;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36125;&#21494;&#26031;&#31639;&#27861;&#25191;&#34892;(BAX)&#24212;&#29992;&#20110;&#26597;&#35810;&#21644;&#24314;&#27169;&#21333;&#20010;&#26463;&#27969;&#23610;&#23544;&#27979;&#37327;&#12290;BAX&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#21152;&#36895;&#22120;&#20013;&#33719;&#21462;&#21457;&#23556;&#24230;&#25351;&#26631;&#26469;&#36991;&#20813;&#22312;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#32531;&#24930;&#30340;&#22810;&#28857;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;BAX&#26469;&#26368;&#23567;&#21270;Linac&#30456;&#24178;&#20809;&#28304;(LCLS)&#21644;Facility for Adv&#30340;&#21457;&#23556;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of $\textit{multi-point query}$, i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose applying Bayesian Algorithm Execution (BAX) to instead query and model individual beam-size measurements. BAX avoids the slow multi-point query on the accelerator by acquiring points through a $\textit{virtual objective}$, i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. Here, we use BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;CMD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2202.06152</link><description>&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#21453;&#23556;&#19979;&#38477;&#20998;&#26512;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Analysis of Dual-Based PID Controllers through Convolutional Mirror Descent. (arXiv:2202.06152v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;CMD&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20840;&#23616;&#32422;&#26463;&#30340;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#20013;&#30340;&#39044;&#31639;&#33410;&#22863;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#22120;&#22312;&#23454;&#36341;&#20013;&#20197;&#21551;&#21457;&#24335;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#19988;&#23545;&#20854;&#24615;&#33021;&#27809;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#20998;&#37197;&#38382;&#39064;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#21452;&#22522;&#30784;PID&#25511;&#21046;&#22120;&#19982;&#19968;&#31181;&#21517;&#20026;&#8220;&#21367;&#31215;&#21453;&#23556;&#19979;&#38477;&#8221;&#65288;Convolutional Mirror Descent&#65292;CMD&#65289;&#30340;&#26032;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#19968;&#38454;&#31639;&#27861;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#12290;CMD&#26681;&#25454;&#36807;&#21435;&#26799;&#24230;&#30340;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#26356;&#26032;&#36845;&#20195;&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;CMD&#24674;&#22797;&#20102;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#19982;&#21160;&#37327;&#21644;&#20048;&#35266;&#38236;&#20687;&#19979;&#38477;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;CMD&#21487;&#23454;&#29616;&#23545;&#24102;&#26377;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#19968;&#33324;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20302;&#36951;&#25022;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#26032;&#32467;&#26524;&#32473;&#20986;&#20102;&#20851;&#20110;&#32593;&#32476;&#24191;&#21578;&#20013;&#39044;&#31639;&#33410;&#22863;&#38382;&#39064;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual-based proportional-integral-derivative (PID) controllers are often employed in practice to solve online allocation problems with global constraints, such as budget pacing in online advertising. However, controllers are used in a heuristic fashion and come with no provable guarantees on their performance. This paper provides the first regret bounds on the performance of dual-based PID controllers for online allocation problems. We do so by first establishing a fundamental connection between dual-based PID controllers and a new first-order algorithm for online convex optimization called \emph{Convolutional Mirror Descent} (CMD), which updates iterates based on a weighted moving average of past gradients. CMD recovers, in a special case, online mirror descent with momentum and optimistic mirror descent. We establish sufficient conditions under which CMD attains low regret for general online convex optimization problems with adversarial inputs. We leverage this new result to give the 
&lt;/p&gt;</description></item></channel></rss>