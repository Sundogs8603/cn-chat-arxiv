<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>MaGNAS&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;MPSoC&#37096;&#32626;&#30340;&#26144;&#23556;&#24863;&#30693;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#35270;&#35273;&#22270;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2307.08065</link><description>&lt;p&gt;
MaGNAS:&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;MPSoC&#37096;&#32626;&#30340;&#26144;&#23556;&#24863;&#30693;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment. (arXiv:2307.08065v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08065
&lt;/p&gt;
&lt;p&gt;
MaGNAS&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;MPSoC&#37096;&#32626;&#30340;&#26144;&#23556;&#24863;&#30693;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#35270;&#35273;&#22270;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#20687;&#24103;&#30340;&#21508;&#20010;&#37096;&#20998;&#20043;&#38388;&#24314;&#27169;&#32467;&#26500;&#21644;&#19978;&#19979;&#25991;&#20851;&#31995;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24322;&#26500;&#22810;&#22788;&#29702;&#22120;&#33455;&#29255;&#31995;&#32479;&#65288;MPSoCs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#36793;&#32536;&#19978;&#30340;&#28145;&#24230;&#35270;&#35273;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#23454;&#26102;&#20005;&#26684;&#30340;&#25191;&#34892;&#35201;&#27714;&#36827;&#34892;&#25512;&#29702;&#12290;&#25193;&#23637;&#32780;&#35328;&#65292;&#29992;&#20110;&#35270;&#35273;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24517;&#39035;&#36981;&#23432;&#21516;&#26679;&#30340;&#25191;&#34892;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#19982;&#20856;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#21453;&#65292;&#22270;&#23398;&#20064;&#25805;&#20316;&#30340;&#19981;&#35268;&#21017;&#27969;&#31243;&#23545;&#22312;&#36825;&#31181;&#24322;&#26500;MPSoC&#24179;&#21488;&#19978;&#36816;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#35774;&#35745;&#26144;&#23556;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;MPSoC&#24179;&#21488;&#19978;&#39640;&#25928;&#22788;&#29702;&#35270;&#35273;&#22270;&#31070;&#32463;&#32593;&#32476;&#24037;&#20316;&#36127;&#36733;&#12290; &#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;MaGNAS&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNA
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08045</link><description>&lt;p&gt;
&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#29992;&#20110;&#27880;&#24847;&#21147;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Fast Quantum Algorithm for Attention Computation. (arXiv:2307.08045v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24322;&#24120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30001;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#39537;&#21160;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#22312;&#21508;&#31181;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;LLMs &#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20204;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#35821;&#35328;&#27169;&#24335;&#12289;&#29702;&#35299;&#32972;&#26223;&#12289;&#29983;&#25104;&#36830;&#36143;&#19988;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#26696;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#26550;&#26500;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#26159;&#19968;&#20010;&#22522;&#26412;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21152;&#24555;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#26696;&#30340;&#36895;&#24230;&#26159;&#21152;&#36895;LLMs&#35745;&#31639;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. These models, powered by advanced deep learning techniques, have revolutionized the field of natural language processing (NLP) and have achieved remarkable results in various language-related tasks.  LLMs have excelled in tasks such as machine translation, sentiment analysis, question answering, text generation, text classification, language modeling, and more. They have proven to be highly effective in capturing complex linguistic patterns, understanding context, and generating coherent and contextually relevant text. The attention scheme plays a crucial role in the architecture of large language models (LLMs). It is a fundamental component that enables the model to capture and utilize contextual information during language processing tasks effectively. Making the attention scheme computation faster is one of the central questions to speed up the LLMs computation. It is well-known that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.08044</link><description>&lt;p&gt;
&#26580;&#24615;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65306;&#36890;&#36807;&#25490;&#21517;&#22238;&#24402;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#24314;&#27169;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#24182;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20107;&#20214;&#20998;&#26512;&#65292;&#20063;&#34987;&#31216;&#20026;&#29983;&#23384;&#20998;&#26512;&#65292;&#26088;&#22312;&#26681;&#25454;&#19968;&#32452;&#29305;&#24449;&#39044;&#27979;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#34987;&#25130;&#23614;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20351;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#22797;&#26434;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21644;&#21152;&#36895;&#22833;&#25928;&#26102;&#38388;&#65288;AFT&#65289;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#27604;&#20363;&#39118;&#38505;&#21644;&#32447;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;AFT&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#30340;&#21442;&#25968;&#20998;&#24067;&#20551;&#35774;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#36731;&#20005;&#26684;&#30340;&#20551;&#35774;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21361;&#38505;&#27169;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#20013;&#23545;&#20110;AFT&#30340;&#34920;&#31034;&#23398;&#20064;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20197;&#21361;&#38505;&#20026;&#37325;&#28857;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#26356;&#21152;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;AFT&#25490;&#21517;&#22238;&#24402;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#22330;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#30446;&#26631;&#21644;&#38556;&#30861;&#29289;&#35270;&#20026;&#27704;&#20037;&#30913;&#38081;&#65292;&#24182;&#26681;&#25454;&#30913;&#22330;&#24378;&#24230;&#20540;&#24314;&#31435;&#22870;&#21169;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24212;&#29992;&#25928;&#26524;&#19981;&#22909;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08033</link><description>&lt;p&gt;
&#22522;&#20110;&#30913;&#22330;&#30340;&#22870;&#21169;&#22609;&#24418;&#29992;&#20110;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning. (arXiv:2307.08033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#22330;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#30446;&#26631;&#21644;&#38556;&#30861;&#29289;&#35270;&#20026;&#27704;&#20037;&#30913;&#38081;&#65292;&#24182;&#26681;&#25454;&#30913;&#22330;&#24378;&#24230;&#20540;&#24314;&#31435;&#22870;&#21169;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24212;&#29992;&#25928;&#26524;&#19981;&#22909;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26159;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#36259;&#25193;&#23637;&#65292;&#21160;&#24577;&#29615;&#22659;&#21644;&#22870;&#21169;&#31232;&#30095;&#24615;&#21487;&#33021;&#23548;&#33268;&#20256;&#32479;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#12290;&#22870;&#21169;&#22609;&#24418;&#26159;&#36890;&#36807;&#23558;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#29616;&#26377;&#22522;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#36890;&#24120;&#24314;&#31435;&#22312;&#32447;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#20998;&#24067;&#30340;&#36317;&#31163;&#24230;&#37327;&#19978;&#65292;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#39640;&#22797;&#26434;&#24230;&#29615;&#22659;&#30340;&#20805;&#20998;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30913;&#22330;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65288;MFRS&#65289;&#65292;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#30446;&#26631;&#21644;&#38556;&#30861;&#29289;&#30340;&#20219;&#21153;&#12290;&#21463;&#21040;&#30913;&#38081;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#21644;&#38556;&#30861;&#29289;&#35270;&#20026;&#27704;&#20037;&#30913;&#38081;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#30913;&#38081;&#20135;&#29983;&#30340;&#30913;&#22330;&#24378;&#24230;&#20540;&#26469;&#24314;&#31435;&#22870;&#21169;&#20989;&#25968;&#12290;&#22870;&#21169;&#20989;&#25968;&#20855;&#26377;&#38750;&#32447;&#24615;&#21644;&#21508;&#21521;&#24322;&#24615;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#24863;&#30693;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#36807;&#22122;&#22768;&#20998;&#31867;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#26469;&#22686;&#24378;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08029</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#22122;&#22768;&#24863;&#30693;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Noise-aware Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2307.08029v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#24863;&#30693;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#36807;&#22122;&#22768;&#20998;&#31867;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#26469;&#22686;&#24378;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29983;&#25104;&#24335;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#22240;&#20854;&#23545;&#26410;&#30693;&#27979;&#35797;&#22122;&#22768;&#30340;&#24040;&#22823;&#28508;&#21147;&#32780;&#21463;&#21040;&#20102;&#22823;&#37327;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28165;&#26224;&#35821;&#38899;&#30340;&#22266;&#26377;&#29305;&#24615;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#21464;&#21270;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35821;&#38899;&#22686;&#24378;&#65288;NASE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#21462;&#22122;&#22768;&#29305;&#23450;&#20449;&#24687;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21521;&#22788;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22122;&#22768;&#20998;&#31867;&#65288;NC&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20135;&#29983;&#22768;&#23398;&#23884;&#20837;&#20316;&#20026;&#22122;&#22768;&#35843;&#33410;&#22120;&#26469;&#25351;&#23548;&#36870;&#21521;&#38477;&#22122;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#20849;&#21516;&#20248;&#21270;SE&#21644;NC&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#30340;&#22122;&#22768;&#35843;&#33410;&#22120;&#30340;&#22122;&#22768;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;NASE&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#25193;&#25955;SE&#27169;&#22411;&#12290;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;NASE&#36798;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;DEQ&#21464;&#20307;&#26356;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#31232;&#30095;&#25513;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08013</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38544;&#24335;&#27169;&#22411;&#65306;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#30340;&#31232;&#30095;&#24230;&#26435;&#34913;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#27604;DEQ&#21464;&#20307;&#26356;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#31232;&#30095;&#25513;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27169;&#22411;&#22914;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;Deep Equilibrium Models, DEQs&#65289;&#22240;&#20854;&#33021;&#22815;&#29992;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#24658;&#23450;&#30340;&#20869;&#23384;&#21344;&#29992;&#35757;&#32451;&#26080;&#38480;&#23618;&#27169;&#22411;&#32780;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#20960;&#27425;&#23581;&#35797;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#27169;&#22411;&#20302;&#25928;&#21644;&#20248;&#21270;&#19981;&#31283;&#23450;&#24615;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35270;&#35273;&#20219;&#21153;&#65292;&#32570;&#20047;&#30456;&#20851;&#26041;&#27861;&#30340;&#20844;&#24179;&#22522;&#20934;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#38544;&#24335;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#36861;&#28335;&#21040;&#21407;&#22987;&#30340;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;DEQ&#21464;&#20307;&#30456;&#27604;&#65292;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#26356;&#21152;&#26377;&#25928;&#12289;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290;&#36890;&#36807;&#36825;&#20123;&#31616;&#21333;&#32780;&#28165;&#26224;&#30340;&#26435;&#37325;&#32465;&#23450;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#22411;&#23481;&#37327;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#31232;&#30095;&#25513;&#27169;&#26469;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#20851;&#20110;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#31232;&#30095;&#24230;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#31995;&#32479;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#30452;&#25509;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#35299;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.08004</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder. (arXiv:2307.08004v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26497;&#21270;&#35299;&#30721;&#22120;&#30340;&#21333;&#27425;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#31995;&#32479;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#30452;&#25509;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#35299;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#21270;&#30721;&#30340;&#21333;&#27425;&#35299;&#30721;&#12290;&#22312;&#36825;&#31181;&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#20449;&#24687;&#20301;&#21521;&#37327;&#20316;&#20026;&#26631;&#31614;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#65292;&#32780;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26497;&#21270;&#30721;&#30340;&#29983;&#25104;&#30697;&#38453;&#26469;&#35757;&#32451;NN&#20197;&#23454;&#29616;&#26377;&#30028;&#36317;&#31163;&#35299;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#22686;&#24378;&#20102;&#30452;&#25509;&#22312;&#36890;&#20449;&#31995;&#32479;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#24212;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#35745;&#31639;&#26426;&#27169;&#25311;&#34920;&#26126;&#65292;&#65288;i&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#35823;&#27604;&#29305;&#29575;&#65288;BER&#65289;&#21644;&#22359;&#38169;&#35823;&#29575;&#65288;BLER&#65289;&#24615;&#33021;&#22312;&#38750;&#24120;&#30701;&#30340;&#25968;&#25454;&#21253;&#19978;&#21487;&#20197;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#65288;ii&#65289;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;NN&#35299;&#30721;&#22120;&#23637;&#31034;&#20102;&#26356;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised deep learning-based decoding scheme that enables one-shot decoding of polar codes. In the proposed scheme, rather than using the information bit vectors as labels for training the neural network (NN) through supervised learning as the conventional scheme did, the NN is trained to function as a bounded distance decoder by leveraging the generator matrix of polar codes through self-supervised learning. This approach eliminates the reliance on predefined labels, empowering the potential to train directly on the actual data within communication systems and thereby enhancing the applicability. Furthermore, computer simulations demonstrate that (i) the bit error rate (BER) and block error rate (BLER) performances of the proposed scheme can approach those of the maximum a posteriori (MAP) decoder for very short packets and (ii) the proposed NN decoder exhibits much superior generalization ability compared to the conventional one.
&lt;/p&gt;</description></item><item><title>LUCYD&#26159;&#19968;&#31181;&#29305;&#24449;&#39537;&#21160;&#30340;&#35299;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;&#28145;&#23618;&#29305;&#24449;&#21644;Richardson-Lucy&#20844;&#24335;&#65292;&#25552;&#39640;&#20102;&#20307;&#31215;&#26174;&#24494;&#22270;&#20687;&#30340;&#24674;&#22797;&#36136;&#37327;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07998</link><description>&lt;p&gt;
LUCYD: &#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#39537;&#21160;&#30340;Richardson-Lucy&#35299;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network. (arXiv:2307.07998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07998
&lt;/p&gt;
&lt;p&gt;
LUCYD&#26159;&#19968;&#31181;&#29305;&#24449;&#39537;&#21160;&#30340;&#35299;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;&#28145;&#23618;&#29305;&#24449;&#21644;Richardson-Lucy&#20844;&#24335;&#65292;&#25552;&#39640;&#20102;&#20307;&#31215;&#26174;&#24494;&#22270;&#20687;&#30340;&#24674;&#22797;&#36136;&#37327;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#65292;&#33719;&#21462;&#26174;&#24494;&#22270;&#20687;&#24120;&#24120;&#20250;&#23548;&#33268;&#22270;&#20687;&#36864;&#21270;&#21644;&#25439;&#22351;&#65292;&#34920;&#29616;&#20026;&#22122;&#22768;&#21644;&#27169;&#31946;&#65292;&#36825;&#32473;&#20934;&#30830;&#20998;&#26512;&#21644;&#35299;&#37322;&#33719;&#21462;&#30340;&#25968;&#25454;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LUCYD&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#24674;&#22797;&#20307;&#31215;&#26174;&#24494;&#22270;&#20687;&#65292;&#23427;&#23558;Richardson-Lucy&#35299;&#21367;&#31215;&#20844;&#24335;&#21644;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#33719;&#24471;&#30340;&#28145;&#23618;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#12290;&#36890;&#36807;&#23558;&#22270;&#20687;&#24418;&#25104;&#36807;&#31243;&#25972;&#21512;&#21040;&#29305;&#24449;&#39537;&#21160;&#30340;&#24674;&#22797;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24674;&#22797;&#21518;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#20445;&#25345;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LUCYD&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#26174;&#24494;&#22270;&#20687;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#26174;&#24494;&#38236;&#27169;&#24335;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a feature-driven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and differ
&lt;/p&gt;</description></item><item><title>MargCTGAN&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#25913;&#21892;&#20102;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07997</link><description>&lt;p&gt;
MargCTGAN: "&#36793;&#38469;&#21270;" &#26356;&#22909;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;CTGAN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MargCTGAN: A "Marginally'' Better CTGAN for the Low Sample Regime. (arXiv:2307.07997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07997
&lt;/p&gt;
&lt;p&gt;
MargCTGAN&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#25913;&#21892;&#20102;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#32780;&#26377;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#32479;&#35745;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#30095;&#24573;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20276;&#38543;&#30528;&#36825;&#20123;&#32479;&#35745;&#25351;&#26631;&#30340;&#36805;&#36895;&#24694;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#36793;&#38469;&#20998;&#24067;&#12289;&#21015;&#23545;&#30456;&#20851;&#24615;&#12289;&#32852;&#21512;&#20998;&#24067;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#29992;&#24615;&#34920;&#29616;&#31561;&#26041;&#38754;&#22312;&#39640;&#21040;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#35780;&#20272;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27969;&#34892;&#30340;CTGAN&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MargCTGAN&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#29305;&#24449;&#21305;&#37197;&#30340;&#21435;&#30456;&#20851;&#36793;&#38469;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#23454;&#29992;&#24615;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#32479;&#35745;&#23646;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of realistic and useful synthetic data is significant. However, current evaluation methods for synthetic tabular data generation predominantly focus on downstream task usefulness, often neglecting the importance of statistical properties. This oversight becomes particularly prominent in low sample scenarios, accompanied by a swift deterioration of these statistical measures. In this paper, we address this issue by conducting an evaluation of three state-of-the-art synthetic tabular data generators based on their marginal distribution, column-pair correlation, joint distribution and downstream task utility performance across high to low sample regimes. The popular CTGAN model shows strong utility, but underperforms in low sample settings in terms of utility. To overcome this limitation, we propose MargCTGAN that adds feature matching of de-correlated marginals, which results in a consistent improvement in downstream utility as well as statistical properties of the syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;Transformer&#25512;&#29702;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.07982</link><description>&lt;p&gt;
&#20248;&#21270;Transformer&#25512;&#29702;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;Transformer&#25512;&#29702;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#21253;&#25324;BERT&#12289;GPT&#21644;ViT&#22312;&#20869;&#30340;Transformer&#32593;&#32476;&#23478;&#26063;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#23637;&#29616;&#20102;&#25928;&#26524;&#12290;ChatGPT&#31561;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#20063;&#24433;&#21709;&#20102;&#26222;&#36890;&#20154;&#30340;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#36861;&#27714;&#39640;&#39044;&#27979;&#24615;&#33021;&#23548;&#33268;&#20102;Transformer&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#28040;&#32791;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22312;&#25277;&#35937;&#32423;&#21035;&#30340;Transformer&#25512;&#29702;&#20248;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;Transformer&#32593;&#32476;&#25512;&#29702;&#38454;&#27573;&#30340;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#23618;&#38754;&#19978;&#35843;&#26597;&#20102;&#30693;&#35782;&#33976;&#39311;&#12289;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#36731;&#37327;&#32423;&#32593;&#32476;&#35774;&#35745;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2307.07980</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#65306;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#24212;&#23545;&#23545;&#25163;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#36890;&#36807;&#65288;&#23545;&#25163;&#30340;&#65289;&#36951;&#25022;&#26469;&#35780;&#20272;&#65292;&#22312;&#29615;&#22659;&#25552;&#20379;&#23545;&#25163;&#25439;&#22833;&#26102;&#35780;&#20272;&#19968;&#27493;&#20915;&#31574;&#30340;&#36136;&#37327;&#65292;&#32780;&#26399;&#26395;&#24471;&#21040;&#19968;&#20010;&#27425;&#32447;&#24615;&#30340;&#19978;&#30028;&#12290;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65292;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#21644;&#23384;&#22312;&#25308;&#21344;&#24237;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#30340;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#65292;&#36825;&#26159;&#32039;&#23494;&#30340;&#12290;&#36825;&#26159;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24517;&#28982;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#21487;&#20197;&#23558;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#25511;&#21046;&#22312;&#21512;&#29702;&#30340;&#27700;&#24179;&#19978;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#29615;&#22659;&#19981;&#26159;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#65292;&#21363;&#35802;&#23454;&#21442;&#19982;&#32773;&#30340;&#25439;&#22833;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#21069;&#38754;&#25552;&#21040;&#30340;&#23545;&#25163;&#36951;&#25022;&#30456;&#21453;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07975</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;: &#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21512;&#29702;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27169;&#25311;&#21487;&#21464;&#24418;&#32447;&#24615;&#29289;&#20307;&#65288;DLO&#65289;&#30340;&#21160;&#21147;&#23398;&#22312;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#34987;&#20154;&#35299;&#35835;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21018;&#24615;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;R-FEM&#65289;&#65292;&#23558;DLO&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21018;&#20307;&#38142;&#65292;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#20197;&#26102;&#38388;&#23637;&#24320;&#12290;&#30001;&#20110;&#35813;&#29366;&#24577;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#21160;&#21147;&#23398;&#32593;&#32476;&#19982;&#19968;&#20010;&#29289;&#29702;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#35757;&#32451;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21464;&#37327;&#26144;&#23556;&#21040;&#21018;&#20307;&#38142;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#20419;&#20351;&#29366;&#24577;&#33719;&#24471;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;R-FEM&#27169;&#22411;&#30340;&#27491;&#36816;&#21160;&#23398;&#65288;FK&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#34987;&#31216;&#20026;&#8220;&#26377;&#38480;&#20803;&#21551;&#21457;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;DLO&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#24471;&#20986;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65288;HOST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27491;&#24577;&#24615;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#22122;&#22768;&#19979;&#24674;&#22797;&#19968;&#20010;&#26377;&#25928;&#30340;&#22240;&#26524;&#25490;&#24207;&#65292;&#20174;&#32780;&#21807;&#19968;&#30830;&#23450;&#22240;&#26524;DAG&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#37117;&#20197;&#22810;&#39033;&#24335;&#35268;&#27169;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.07973</link><description>&lt;p&gt;
&#24322;&#26041;&#24046;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heteroscedastic Causal Structure Learning. (arXiv:2307.07973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65288;HOST&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#27491;&#24577;&#24615;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#22122;&#22768;&#19979;&#24674;&#22797;&#19968;&#20010;&#26377;&#25928;&#30340;&#22240;&#26524;&#25490;&#24207;&#65292;&#20174;&#32780;&#21807;&#19968;&#30830;&#23450;&#22240;&#26524;DAG&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#37117;&#20197;&#22810;&#39033;&#24335;&#35268;&#27169;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23398;&#20064;&#32534;&#30721;&#35266;&#23519;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36235;&#21183;&#34920;&#26126;&#65292;&#22312;&#30456;&#31561;&#26041;&#24046;&#20551;&#35774;&#19979;&#65292;&#21487;&#20197;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#24674;&#22797;DAGs&#12290;&#28982;&#32780;&#65292;&#36825;&#31105;&#27490;&#20102;&#22122;&#22768;&#30340;&#24322;&#26041;&#24046;&#24615;&#65292;&#36825;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#24322;&#26041;&#24046;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#30340;&#27491;&#24577;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#19968;&#20010;&#26377;&#25928;&#30340;&#22240;&#26524;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#29992;&#19968;&#31995;&#21015;&#26465;&#20214;&#29420;&#31435;&#26816;&#39564;&#21807;&#19968;&#22320;&#30830;&#35748;&#22240;&#26524;DAG&#12290;&#32467;&#26524;&#26159;HOST&#65288;&#24322;&#26041;&#24046;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#37117;&#20197;&#22810;&#39033;&#24335;&#35268;&#27169;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heretofore, learning the directed acyclic graphs (DAGs) that encode the cause-effect relationships embedded in observational data is a computationally challenging problem. A recent trend of studies has shown that it is possible to recover the DAGs with polynomial time complexity under the equal variances assumption. However, this prohibits the heteroscedasticity of the noise, which allows for more flexible modeling capabilities, but at the same time is substantially more challenging to handle. In this study, we tackle the heteroscedastic causal structure learning problem under Gaussian noises. By exploiting the normality of the causal mechanisms, we can recover a valid causal ordering, which can uniquely identify the causal DAG using a series of conditional independence tests. The result is HOST (Heteroscedastic causal STructure learning), a simple yet effective causal structure learning algorithm that scales polynomially in both sample size and dimensionality. In addition, via extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#32534;&#30721;&#29702;&#35770;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07963</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#24418;&#24577;&#26041;&#27861;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#20272;&#35745;&#30340;&#33021;&#25928;&#19982;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach. (arXiv:2307.07963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#32534;&#30721;&#29702;&#35770;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#21644;&#21487;&#38752;&#24615;&#19968;&#30452;&#26159;&#20445;&#35777;&#33258;&#20027;&#31995;&#32479;&#35745;&#31639;&#26426;&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#23433;&#20840;&#20219;&#21153;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#30528;&#31354;&#38388;&#26426;&#22120;&#20154;&#21644;&#20808;&#36827;&#31354;&#20013;&#26426;&#21160;&#24615;&#31561;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23545;&#36825;&#20123;&#20302;&#23610;&#23544;&#12289;&#37325;&#37327;&#12289;&#21151;&#32791;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24341;&#20837;&#22522;&#20110;&#33033;&#20914;&#32534;&#30721;&#29702;&#35770;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26426;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#24050;&#23450;&#20041;&#33391;&#22909;&#30340;&#32447;&#24615;&#31995;&#32479;&#30340;&#26681;&#26412;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25913;&#36827;&#30340;&#28369;&#21160;&#21019;&#26032;&#28388;&#27874;&#22120;&#65288;MSIF&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;SNN-MSIF&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#26681;&#25454;&#31995;&#32479;&#27169;&#22411;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#23398;&#20064;&#30340;&#38656;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31639;&#27861;&#23545;&#24212;&#29289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy efficiency and reliability have long been crucial factors for ensuring cost-effective and safe missions in autonomous systems computers. With the rapid evolution of industries such as space robotics and advanced air mobility, the demand for these low size, weight, and power (SWaP) computers has grown significantly. This study focuses on introducing an estimation framework based on spike coding theories and spiking neural networks (SNN), leveraging the efficiency and scalability of neuromorphic computers. Therefore, we propose an SNN-based Kalman filter (KF), a fundamental and widely adopted optimal strategy for well-defined linear systems. Furthermore, based on the modified sliding innovation filter (MSIF) we present a robust strategy called SNN-MSIF. Notably, the weight matrices of the networks are designed according to the system model, eliminating the need for learning. To evaluate the effectiveness of the proposed strategies, we compare them to their algorithmic counterparts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;Auto-Polynomial&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07956</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Polynomial Filter Learning for Graph Neural Networks. (arXiv:2307.07956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;Auto-Polynomial&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#20316;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#25351;&#23548;&#21407;&#21017;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#22312;&#24314;&#27169;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#34920;&#36798;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#21019;&#26032;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-Polynomial&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#33258;&#21160;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#26356;&#22909;&#30340;&#28388;&#27874;&#22120;&#65292;&#36866;&#24212;&#21508;&#31181;&#22797;&#26434;&#30340;&#22270;&#20449;&#21495;&#12290;&#32508;&#21512;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#22312;&#32771;&#34385;&#22810;&#31181;&#26631;&#31614;&#27604;&#20363;&#30340;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polynomial graph filters have been widely used as guiding principles in the design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the polynomial graph filters has demonstrated promising performance for modeling graph signals on both homophilic and heterophilic graphs, owning to their flexibility and expressiveness. In this work, we conduct a novel preliminary study to explore the potential and limitations of polynomial graph filter learning approaches, revealing a severe overfitting issue. To improve the effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel and general automated polynomial graph filter learning framework that efficiently learns better filters capable of adapting to various complex graph signals. Comprehensive experiments and ablation studies demonstrate significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#65292;&#36798;&#21040;&#20102;&#19982;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07950</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#21152;&#36895;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#21516;&#27493;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#65292;&#36798;&#21040;&#20102;&#19982;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21516;&#26102;&#22312;&#22810;&#20010;&#24037;&#20316;&#32773;&#19978;&#21551;&#21160;&#65292;&#24182;&#20351;&#29992;&#25209;&#37327;&#21516;&#27493;&#24182;&#34892;&#65288;BSP&#65289;&#35757;&#32451;&#20013;&#30340;&#27599;&#20010;&#27493;&#39588;&#32858;&#21512;&#20854;&#26412;&#22320;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32858;&#21512;&#30340;&#36890;&#20449;&#25104;&#26412;&#36739;&#39640;&#65292;BSP&#26080;&#27861;&#32447;&#24615;&#25193;&#23637;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24320;&#38144;&#65292;FedAvg&#21644;SSP&#31561;&#26367;&#20195;&#26041;&#26696;&#35201;&#20040;&#38477;&#20302;&#21516;&#27493;&#39057;&#29575;&#65292;&#35201;&#20040;&#23436;&#20840;&#28040;&#38500;&#21516;&#27493;&#65292;&#36890;&#24120;&#20197;&#38477;&#20302;&#26368;&#32456;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelSync&#30340;&#23454;&#29992;&#12289;&#20302;&#24320;&#38144;&#30340;DNN&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#26681;&#25454;&#27599;&#19968;&#27493;&#30340;&#37325;&#35201;&#24615;&#21160;&#24577;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#36890;&#20449;&#12290;&#20316;&#20026;\texttt{SelSync}&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;\textit {&#21322;&#21516;&#27493;}&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#26368;&#22810;14&#65285;&#30340;&#21516;&#26102;&#65292;&#36798;&#21040;&#19982;BSP&#30456;&#21516;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#65292;&#20197;&#26368;&#23567;&#21270;&#26631;&#27880;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20943;&#36731;LiDAR&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#27880;&#37322;&#36127;&#25285;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07942</link><description>&lt;p&gt;
KECOR:&#29992;&#20110;&#20027;&#21160;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#65292;&#20197;&#26368;&#23567;&#21270;&#26631;&#27880;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#20943;&#36731;LiDAR&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#27880;&#37322;&#36127;&#25285;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#21487;&#38752;&#30340;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20854;&#25104;&#21151;&#21462;&#20915;&#20110;&#33719;&#21462;&#22823;&#37327;&#31934;&#30830;&#30340;3D&#27880;&#37322;&#12290;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#21644;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#30340;&#31639;&#27861;&#26469;&#20943;&#36731;&#27880;&#37322;&#36127;&#25285;&#12290;&#23613;&#31649;AL&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#26410;&#26631;&#35760;&#28857;&#20113;&#65292;&#23548;&#33268;&#36873;&#25321;&#26356;&#22810;&#23454;&#20363;&#36827;&#34892;&#26631;&#35760;&#24182;&#38477;&#20302;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#26680;&#32534;&#30721;&#29575;&#26368;&#22823;&#21270;&#65288;KECOR&#65289;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#30830;&#23450;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#28857;&#20113;&#20197;&#33719;&#21462;&#26631;&#31614;&#12290;&#36138;&#23146;&#25628;&#32034;&#34987;&#24212;&#29992;&#20110;&#23547;&#25214;&#33021;&#22815;&#26368;&#22823;&#21270;&#32534;&#30721;&#28508;&#22312;&#29305;&#24449;&#25152;&#38656;&#30340;&#26368;&#23567;&#27604;&#29305;&#25968;&#30340;&#26399;&#26395;&#28857;&#20113;&#12290;&#20026;&#20102;&#30830;&#23450;&#25152;&#36873;&#26679;&#26412;&#30340;&#29420;&#29305;&#24615;&#21644;&#20449;&#24687;&#37327;&#65292;&#20174;&#27169;&#22411;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#22833;&#30495;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07941</link><description>&lt;p&gt;
&#39640;&#22833;&#30495;&#26465;&#20214;&#19979;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#30340;&#26368;&#20248;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Optimal Compression of Unit Norm Vectors in the High Distortion Regime. (arXiv:2307.07941v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#22833;&#30495;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#38656;&#27714;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#21333;&#20301;&#33539;&#25968;&#21521;&#37327;&#21387;&#32553;&#21040;&#26368;&#23569;&#27604;&#29305;&#25968;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#19968;&#23450;&#31243;&#24230;&#30340;&#22833;&#30495;&#24674;&#22797;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#36895;&#29575;-&#22833;&#30495;/&#35206;&#30422;&#32534;&#30721;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#30740;&#31350;&#36807;&#65292;&#20294;&#25105;&#20204;&#30340;&#37325;&#28857;&#20165;&#38480;&#20110;&#8220;&#39640;&#22833;&#30495;&#8221;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#32771;&#34385;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#21521;&#37327;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#20294;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21387;&#32553;&#26144;&#23556;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#20559;&#21644;&#26080;&#20559;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#21387;&#32553;&#27604;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21387;&#32553;&#26041;&#26696;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#34429;&#28982;&#32467;&#26524;&#26159;&#26032;&#26087;&#38382;&#39064;&#30340;&#28151;&#21512;&#65292;&#20294;&#20026;&#20102;&#23436;&#25972;&#36215;&#35265;&#65292;&#23427;&#20204;&#22312;&#26412;&#25991;&#20013;&#20104;&#20197;&#25972;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the "high-distortion" regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65292;&#21033;&#29992;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#26469;&#25552;&#21462;&#30456;&#20284;&#30340;&#32467;&#26500;&#36827;&#34892;&#21435;&#22122;&#65292;&#27169;&#22411;&#20860;&#39038;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.07932</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#30340;&#25130;&#26029;&#33539;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising. (arXiv:2307.07932v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07932
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65292;&#21033;&#29992;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#26469;&#25552;&#21462;&#30456;&#20284;&#30340;&#32467;&#26500;&#36827;&#34892;&#21435;&#22122;&#65292;&#27169;&#22411;&#20860;&#39038;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20302;&#31209;&#36924;&#36817;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#22312;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#24573;&#30053;&#20102;&#36328;&#36890;&#36947;&#24046;&#24322;&#25110;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21152;&#26435;&#25130;&#26029;&#26680;&#33539;&#25968;&#20943;&#21435;&#25130;&#26029;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#65288;DtNFM&#65289;&#30340;&#26041;&#27861;&#26469;&#21435;&#22122;&#24425;&#33394;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22122;&#22768;&#22270;&#20687;&#30340;&#38750;&#23616;&#37096;&#33258;&#30456;&#20284;&#24615;&#65292;&#30456;&#20284;&#32467;&#26500;&#34987;&#25910;&#38598;&#24182;&#26500;&#36896;&#20102;&#19968;&#31995;&#21015;&#30456;&#20284;&#30340;&#22359;&#30697;&#38453;&#12290;&#23545;&#20110;&#27599;&#20010;&#20998;&#32452;&#65292;&#23545;&#20854;&#36827;&#34892;DtNFM&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#21435;&#22122;&#29256;&#26412;&#12290;&#36890;&#36807;&#36830;&#25509;&#25152;&#26377;&#21435;&#22122;&#22359;&#30697;&#38453;&#65292;&#24471;&#21040;&#21435;&#22122;&#22270;&#20687;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;DtNFM&#27169;&#22411;&#26377;&#20004;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#36328;&#36890;&#36947;&#24046;&#24322;&#21644;&#22122;&#22768;&#30340;&#31354;&#38388;&#21464;&#21270;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;&#36825;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high flexibility and remarkable performance, low-rank approximation methods has been widely studied for color image denoising. However, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real world color image denoising. To overcome those drawbacks, this paper is proposed to denoise color images with a double-weighted truncated nuclear norm minus truncated Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal self-similarity of the noisy image, the similar structures are gathered and a series of similar patch matrices are constructed. For each group, the DtNFM model is conducted for estimating its denoised version. The denoised image would be obtained by concatenating all the denoised patch matrices. The proposed DtNFM model has two merits. First, it models and utilizes both the cross-channel difference and the spatial variation of noise. This provides sufficient flexibility 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#21463;&#20449;&#20219;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20165;&#21521;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#65292;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25932;&#23545;&#25915;&#20987;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2307.07916</link><description>&lt;p&gt;
&#35770;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#21463;&#20449;&#20219;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20165;&#21521;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#65292;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25932;&#23545;&#25915;&#20987;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#21644;&#27169;&#22411;&#32454;&#33410;&#65288;&#21363;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20165;&#25345;&#26377;&#37096;&#20998;&#23376;&#32593;&#32476;&#24182;&#20132;&#25442;&#20013;&#38388;&#35745;&#31639;&#65289;&#26469;&#23454;&#29616;&#21327;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#27169;&#22411;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#39564;&#20854;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#38752;&#24615;&#65292;&#23545;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#29978;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25506;&#32034;&#23436;&#25972;&#27169;&#22411;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21457;&#36215;&#25932;&#23545;&#25915;&#20987;&#65292;&#32780;&#20998;&#35010;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20165;&#21521;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#20844;&#24320;&#37096;&#20998;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#31181;&#20005;&#37325;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#20998;&#35010;&#23398;&#20064;&#23545;&#25239;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#25932;&#23545;&#25915;&#20987;&#22823;&#22810;&#38598;&#20013;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#32780;&#38750;&#21327;&#21516;&#24335;&#29615;&#22659;&#65292;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#35010;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a ta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;FPGA&#25216;&#26415;&#25552;&#20986;&#20102;&#39640;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#24352;&#37327;&#35745;&#31639;&#21333;&#20803;&#21152;&#36895;&#22120;&#23545;&#29983;&#29289;&#21307;&#23398;&#30340;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#25552;&#39640;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07914</link><description>&lt;p&gt;
&#21033;&#29992;FPGA&#21152;&#36895;&#25216;&#26415;&#30340;&#29983;&#29289;&#21307;&#23398;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting FPGA Capabilities for Accelerated Biomedical Computing. (arXiv:2307.07914v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;FPGA&#25216;&#26415;&#25552;&#20986;&#20102;&#39640;&#32423;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#24352;&#37327;&#35745;&#31639;&#21333;&#20803;&#21152;&#36895;&#22120;&#23545;&#29983;&#29289;&#21307;&#23398;&#30340;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#25552;&#39640;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28145;&#24230;&#32622;&#20449;&#32593;&#32476;&#65288;DBN&#65289;&#65292;&#21033;&#29992;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#26469;&#22686;&#24378;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#24182;&#24341;&#20837;&#39640;&#26031;&#22122;&#22768;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#23454;&#29616;&#30340;&#27169;&#22411;&#20855;&#26377;&#21508;&#31181;&#23618;&#65292;&#29992;&#20110;&#19981;&#21516;&#30340;&#22788;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#26089;&#20572;&#21644;&#20002;&#24323;&#23618;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25506;&#32034;&#20102;&#20026;PYNQ Z1&#26495;&#30740;&#21457;&#33258;&#23450;&#20041;&#30340;&#24352;&#37327;&#35745;&#31639;&#21333;&#20803;&#65288;TCU&#65289;&#21152;&#36895;&#22120;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;FPGA&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#27493;&#39588;&#65292;&#21253;&#25324;&#22312;Docker&#20013;&#35774;&#32622;Tensil&#24320;&#21457;&#24037;&#20855;&#38142;&#65292;&#36873;&#25321;&#26500;&#26550;&#65292;&#37197;&#32622;PS-PL&#20197;&#21450;&#32534;&#35793;&#21644;&#25191;&#34892;&#27169;&#22411;&#12290;&#24615;&#33021;&#25351;&#26631;&#22914;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#20063;&#24471;&#21040;&#20102;&#35745;&#31639;&#65292;&#20197;&#36827;&#34892;&#23454;&#38469;&#30340;&#30740;&#31350;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents advanced neural network architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for enhanced ECG signal analysis using Field Programmable Gate Arrays (FPGAs). We utilize the MIT-BIH Arrhythmia Database for training and validation, introducing Gaussian noise to improve algorithm robustness. The implemented models feature various layers for distinct processing and classification tasks and techniques like EarlyStopping callback and Dropout layer are used to mitigate overfitting. Our work also explores the development of a custom Tensor Compute Unit (TCU) accelerator for the PYNQ Z1 board, offering comprehensive steps for FPGA-based machine learning, including setting up the Tensil toolchain in Docker, selecting architecture, configuring PS-PL, and compiling and executing models. Performance metrics such as latency and throughput are calculated for practical in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#23618;&#21512;&#25104;&#26377;&#38480;&#20803;&#27169;&#22411;&#27169;&#25311;&#65292;&#36890;&#36807;&#22522;&#20110;AI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30899;&#32435;&#31859;&#31649;&#65288;CNT&#65289;&#22270;&#20687;&#30340;&#21147;&#23398;&#24615;&#33021;&#12290;&#23558;2D&#21512;&#25104;&#22270;&#20687;&#28151;&#21512;&#29983;&#25104;&#26356;&#25509;&#36817;&#20110;&#30495;&#23454;&#22270;&#20687;&#30340;MLS&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#20272;&#35745;&#20854;&#21147;&#23398;&#24615;&#36136;&#12290;&#25552;&#20986;&#30340;CNTNeXt&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#36807;ResNeXt&#29305;&#24449;&#34920;&#31034;&#21644;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#20272;&#35745;&#22120;&#25913;&#36827;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07912</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#23618;&#21512;&#25104;&#26377;&#38480;&#20803;&#27169;&#22411;&#27169;&#25311;&#26469;&#39044;&#27979;&#30899;&#32435;&#31859;&#31649;&#65288;CNT&#65289;&#22270;&#20687;&#30340;&#21147;&#23398;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations. (arXiv:2307.07912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07912
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#23618;&#21512;&#25104;&#26377;&#38480;&#20803;&#27169;&#22411;&#27169;&#25311;&#65292;&#36890;&#36807;&#22522;&#20110;AI&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30899;&#32435;&#31859;&#31649;&#65288;CNT&#65289;&#22270;&#20687;&#30340;&#21147;&#23398;&#24615;&#33021;&#12290;&#23558;2D&#21512;&#25104;&#22270;&#20687;&#28151;&#21512;&#29983;&#25104;&#26356;&#25509;&#36817;&#20110;&#30495;&#23454;&#22270;&#20687;&#30340;MLS&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#20272;&#35745;&#20854;&#21147;&#23398;&#24615;&#36136;&#12290;&#25552;&#20986;&#30340;CNTNeXt&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#36807;ResNeXt&#29305;&#24449;&#34920;&#31034;&#21644;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#20272;&#35745;&#22120;&#25913;&#36827;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26448;&#26009;&#21457;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#22402;&#30452;&#23450;&#21521;&#30899;&#32435;&#31859;&#31649;&#65288;CNT&#65289;&#26862;&#26519;&#22270;&#20687;&#30340;&#21147;&#23398;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#22810;&#23618;&#21512;&#25104;&#65288;MLS&#65289;&#25110;&#20934;2.5D&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#26159;&#36890;&#36807;&#28151;&#21512;2D&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#12290;MLS&#22270;&#20687;&#26356;&#25509;&#36817;&#20110;3D&#21512;&#25104;&#21644;&#30495;&#23454;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#65292;&#20294;&#19981;&#38656;&#35201;&#25191;&#34892;&#26114;&#36149;&#30340;3D&#27169;&#25311;&#25110;&#23454;&#39564;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#20272;&#35745;&#20102;MLS&#22270;&#20687;&#30340;&#21018;&#24230;&#21644;&#23624;&#26354;&#33655;&#36733;&#12290;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;CNTNeXt&#22312;&#25105;&#20204;&#20808;&#21069;&#30340;CNTNet&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#20351;&#29992;&#20102;ResNeXt&#29305;&#24449;&#34920;&#31034;&#65292;&#38543;&#21518;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#38598;&#21512;&#26469;&#39044;&#27979;CNT&#30340;&#29289;&#29702;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a pipeline for predicting mechanical properties of vertically-oriented carbon nanotube (CNT) forest images using a deep learning model for artificial intelligence (AI)-based materials discovery. Our approach incorporates an innovative data augmentation technique that involves the use of multi-layer synthetic (MLS) or quasi-2.5D images which are generated by blending 2D synthetic images. The MLS images more closely resemble 3D synthetic and real scanning electron microscopy (SEM) images of CNTs but without the computational cost of performing expensive 3D simulations or experiments. Mechanical properties such as stiffness and buckling load for the MLS images are estimated using a physics-based model. The proposed deep learning architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a ResNeXt feature representation followed by random forest regression estimator. Our machine learning approach for predicting CNT physical properties by utilizing a blended set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;MESOB&#21450;&#30456;&#24212;&#30340;MESOB-OMO&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#27425;&#12289;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#30340;&#31454;&#20105;&#21644;&#21512;&#20316;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#22312;&#25293;&#21334;&#20986;&#20215;&#25512;&#33616;&#31561;&#38382;&#39064;&#20013;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07911</link><description>&lt;p&gt;
MESOB: &#24179;&#34913;&#22343;&#34913;&#19982;&#31038;&#20250;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
MESOB: Balancing Equilibria &amp; Social Optimality. (arXiv:2307.07911v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;MESOB&#21450;&#30456;&#24212;&#30340;MESOB-OMO&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#27425;&#12289;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#20013;&#30340;&#31454;&#20105;&#21644;&#21512;&#20316;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#22312;&#25293;&#21334;&#20986;&#20215;&#25512;&#33616;&#31561;&#38382;&#39064;&#20013;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#20986;&#20215;&#25512;&#33616;&#21551;&#21457;&#65292;&#32771;&#34385;&#20102;&#19968;&#31867;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#30340;&#22810;&#23618;&#27425;&#21644;&#22810;&#26234;&#33021;&#20307;&#21338;&#24328;&#65306;&#19968;&#20010;&#26159;&#22823;&#37327;&#30340;&#21311;&#21517;&#26234;&#33021;&#20307;&#65292;&#21478;&#19968;&#20010;&#26159;&#31454;&#20105;&#21644;&#21512;&#20316;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#22797;&#26434;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25805;&#20316;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MESOB&#65288;Mean-field Equilibria &amp; Social Optimality Balancing&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20851;&#30340;&#32844;&#19994;&#24230;&#37327;&#20248;&#21270;&#65288;OMO&#65289;&#26041;&#27861;MESOB-OMO&#26469;&#35299;&#20915;&#23427;&#12290;MESOB-OMO&#33021;&#22815;&#22522;&#20110;MESOB&#20013;&#30340;&#31454;&#20105;&#21644;&#21512;&#20316;&#30340;&#21452;&#30446;&#26631;&#33719;&#21462;&#36817;&#20284;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#35299;&#65292;&#29305;&#21035;&#26159;&#20801;&#35768;&#28176;&#36817;&#22320;&#36873;&#25321;&#32435;&#20160;&#22343;&#34913;&#21644;&#31038;&#20250;&#22343;&#34913;&#21270;&#12290;&#25105;&#20204;&#23558;MESOB-OMO&#24212;&#29992;&#20110;&#27169;&#25311;&#30340;&#25353;&#28857;&#20987;&#20184;&#36153;&#24191;&#21578;&#25293;&#21334;&#30340;&#20986;&#20215;&#25512;&#33616;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by bid recommendation in online ad auctions, this paper considers a general class of multi-level and multi-agent games, with two major characteristics: one is a large number of anonymous agents, and the other is the intricate interplay between competition and cooperation. To model such complex systems, we propose a novel and tractable bi-objective optimization formulation with mean-field approximation, called MESOB (Mean-field Equilibria &amp; Social Optimality Balancing), as well as an associated occupation measure optimization (OMO) method called MESOB-OMO to solve it. MESOB-OMO enables obtaining approximately Pareto efficient solutions in terms of the dual objectives of competition and cooperation in MESOB, and in particular allows for Nash equilibrium selection and social equalization in an asymptotic manner. We apply MESOB-OMO to bid recommendation in a simulated pay-per-click ad auction. Experiments demonstrate its efficacy in balancing the interests of different parties an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#34394;&#20551;&#30456;&#20851;&#24615;&#26159;&#30001;&#20110;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#26222;&#36941;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.07907</link><description>&lt;p&gt;
&#35270;&#32780;&#19981;&#35265;&#65306;&#38024;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#31181;&#34394;&#20551;&#30456;&#20851;&#24615;&#26159;&#30001;&#20110;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#26222;&#36941;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#24418;&#24335;&#65292;&#22914;&#38543;&#26426;&#25200;&#21160;&#12289;&#32597;&#35265;&#20107;&#20214;&#21644;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#30340;&#19981;&#21516;&#37096;&#20998;&#27809;&#26377;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#21364;&#23384;&#22312;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#24341;&#36215;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#34394;&#20551;&#30456;&#20851;&#24615;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#36890;&#24120;&#22312;&#30333;&#22825;&#35266;&#23519;&#21040;&#20132;&#36890;&#25317;&#22581;&#65292;&#22812;&#26202;&#35266;&#23519;&#21040;&#20132;&#36890;&#36731;&#26494;&#65292;&#21407;&#22240;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#36825;&#31181;&#26080;&#29992;&#29978;&#33267;&#26377;&#23475;&#30456;&#20851;&#24615;&#26102;&#65292;&#24403;&#27979;&#35797;&#26696;&#20363;&#30340;&#28151;&#26434;&#22240;&#32032;&#20559;&#31163;&#35757;&#32451;&#26102;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#28151;&#26434;&#22240;&#32032;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#32467;&#26500;&#24418;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#24456;&#38590;&#36827;&#34892;&#34920;&#24449;&#21644;&#35782;&#21035;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07893</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#25968;&#25454;&#26377;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;(AFP)&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#26679;&#26412;&#65292;&#32780;&#36825;&#20123;&#26679;&#26412;&#24456;&#38590;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20174;&#22522;&#30784;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#27491;&#24120;&#26679;&#26412;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#32420;&#32500;&#23618;&#29255;&#65288;tow&#65289;&#23545;&#32420;&#32500;&#38138;&#35774;&#34920;&#38754;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20998;&#21106;&#25104;&#23567;&#31383;&#21475;&#12290;&#20854;&#20013;&#19981;&#21253;&#21547;&#24322;&#24120;&#30340;&#31383;&#21475;&#23376;&#38598;&#20256;&#36882;&#32473;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#36755;&#20837;&#12290;&#22240;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;&#29992;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#36825;&#20123;&#26679;&#26412;&#65292;&#23427;&#20135;&#29983;&#30340;&#37325;&#26500;&#27604;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#26356;&#31934;&#30830;&#12290;&#22240;&#27492;&#65292;&#37325;&#26500;&#35823;&#24046;&#30340;&#20540;&#34987;&#29992;&#20316;&#19968;&#20010;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#28508;&#22312;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;RABASAR&#21644;&#31616;&#21270;&#30340;GLR&#26041;&#27861;&#65292;&#22312;&#22810;&#26102;&#30456;SAR&#22270;&#20687;&#20013;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#21644;&#21487;&#35270;&#21270;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#30340;&#21464;&#21270;&#24133;&#24230;&#25351;&#25968;&#26041;&#27861;&#21644;&#25913;&#36827;&#30340;&#21464;&#21270;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07892</link><description>&lt;p&gt;
&#22810;&#26102;&#30456;SAR&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#21644;&#21487;&#35270;&#21270;&#20351;&#29992;RABASAR&#21644;&#31616;&#21270;GLR
&lt;/p&gt;
&lt;p&gt;
Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR. (arXiv:2307.07892v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07892
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;RABASAR&#21644;&#31616;&#21270;&#30340;GLR&#26041;&#27861;&#65292;&#22312;&#22810;&#26102;&#30456;SAR&#22270;&#20687;&#20013;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#21644;&#21487;&#35270;&#21270;&#65292;&#24182;&#24320;&#21457;&#20102;&#26032;&#30340;&#21464;&#21270;&#24133;&#24230;&#25351;&#25968;&#26041;&#27861;&#21644;&#25913;&#36827;&#30340;&#21464;&#21270;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#21464;&#21270;&#21306;&#22495;&#30340;&#29366;&#24577;&#38656;&#35201;&#25552;&#20379;&#20851;&#20110;&#21464;&#21270;&#30340;&#31934;&#30830;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#21270;&#23545;&#20110;&#22320;&#34920;&#30417;&#27979;&#26159;&#37325;&#35201;&#30340;&#12290;SAR&#20256;&#24863;&#22120;&#38750;&#24120;&#36866;&#21512;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20840;&#22825;&#20505;&#21644;&#20840;&#22825;&#26102;&#38388;&#30340;&#33021;&#21147;&#65292;&#19988;&#22312;&#24133;&#24230;&#25968;&#25454;&#20013;&#19981;&#21463;&#22823;&#27668;&#25104;&#20998;&#24433;&#21709;&#30340;&#20934;&#30830;&#30340;&#33719;&#21462;&#20960;&#20309;&#21644;&#31561;&#25928;&#26679;&#26412;&#25968;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#24191;&#20041;&#20284;&#28982;&#27604; ($S_{GLR}$) &#26041;&#27861;&#65292;&#20551;&#35774;&#30456;&#24212;&#30340;&#26102;&#22495;&#20687;&#32032;&#20855;&#26377;&#30456;&#21516;&#30340;&#31561;&#25928;&#26679;&#26412;&#25968;&#37327; (ENL)&#12290;&#20511;&#21161;&#20110;&#30001;&#22522;&#20110;&#27604;&#29575;&#30340;&#22810;&#26102;&#30456;SAR&#22270;&#20687;&#21435;&#22122;&#26041;&#27861; (RABASAR) &#25552;&#20379;&#30340;&#21435;&#22122;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;&#36825;&#31181;&#30456;&#20284;&#24615;&#27979;&#35797;&#26041;&#27861;&#26469;&#35745;&#31639;&#21464;&#21270;&#21306;&#22495;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#21270;&#24133;&#24230;&#25351;&#25968;&#26041;&#27861;&#21644;&#25913;&#36827;&#30340;&#22522;&#20110;&#20809;&#35889;&#32858;&#31867;&#30340;&#21464;&#21270;&#20998;&#31867;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#30340;&#24191;&#20041;&#20284;&#28982;&#27604;&#24212;&#29992;&#20110;&#26816;&#27979;&#26368;&#22823;&#21464;&#21270;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the state of changed areas requires that precise information be given about the changes. Thus, detecting different kinds of changes is important for land surface monitoring. SAR sensors are ideal to fulfil this task, because of their all-time and all-weather capabilities, with good accuracy of the acquisition geometry and without effects of atmospheric constituents for amplitude data. In this study, we propose a simplified generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding temporal pixels have the same equivalent number of looks (ENL). Thanks to the denoised data provided by a ratio-based multitemporal SAR image denoising method (RABASAR), we successfully applied this similarity test approach to compute the change areas. A new change magnitude index method and an improved spectral clustering-based change classification method are also developed. In addition, we apply the simplified generalized likelihood ratio to detect the maximum change magnitud
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;ODE&#36827;&#34892;&#31995;&#32479;&#35782;&#21035;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;EKI&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;ODE&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36136;&#37327;&#19982;&#24120;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.07882</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#30340;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#29992;&#20110;&#31995;&#32479;&#35782;&#21035;&#21644;&#25511;&#21046;&#30340;&#31070;&#32463;ODE&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion. (arXiv:2307.07882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#26041;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;ODE&#36827;&#34892;&#31995;&#32479;&#35782;&#21035;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;EKI&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;ODE&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#36136;&#37327;&#19982;&#24120;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#65288;EKI&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#27714;&#35299;&#21453;&#38382;&#39064;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#19982;&#21453;&#21521;&#20256;&#25773;&#19981;&#21516;&#65292;EKI&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#20013;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;EKI&#22312;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;ODE&#65289;&#30340;&#31995;&#32479;&#35782;&#21035;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;EKI&#24212;&#29992;&#20110;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#21253;&#21547;Tikhonov&#22411;&#27491;&#21017;&#21270;&#39033;&#30340;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31995;&#32479;&#35782;&#21035;&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;EKI&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;ODE&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#21644;&#35299;&#30340;&#36136;&#37327;&#19982;&#24120;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble Kalman inversion (EKI) is a sequential Monte Carlo method used to solve inverse problems within a Bayesian framework. Unlike backpropagation, EKI is a gradient-free optimization method that only necessitates the evaluation of artificial neural networks in forward passes. In this study, we examine the effectiveness of EKI in training neural ordinary differential equations (neural ODEs) for system identification and control tasks. To apply EKI to optimal control problems, we formulate inverse problems that incorporate a Tikhonov-type regularization term. Our numerical results demonstrate that EKI is an efficient method for training neural ODEs in system identification and optimal control problems, with runtime and quality of solutions that are competitive with commonly used gradient-based optimizers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#22270;&#23884;&#20837;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07881</link><description>&lt;p&gt;
&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning. (arXiv:2307.07881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07881
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#22270;&#23884;&#20837;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#65292;&#22312;&#23545;&#23569;&#25968;&#31867;&#21035;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#26041;&#38754;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#20110;&#22810;&#25968;&#31867;&#21035;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#20808;&#32771;&#34385;&#65292;&#20174;&#32780;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#30340;&#20195;&#34920;&#19981;&#36275;&#12290;&#38543;&#26426;&#21521;&#37327;&#20989;&#25968;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#26377;&#25928;&#30340;&#20998;&#31867;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20854;&#36895;&#24230;&#21644;&#25928;&#29575;&#39640;&#32780;&#21463;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#23427;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#65288;GE-IFRVFL-CIL&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#26469;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;GE-IFRVFL-CIL&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#65306;&#65288;i&#65289;&#23427;&#21033;&#29992;&#22270;&#23884;&#20837;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#65288;ii&#65289;&#23427;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#38598;&#26469;&#36827;&#34892;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain of machine learning is confronted with a crucial research area known as class imbalance learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely-used and effective learning model for classification due to its speed and efficiency. However, it suffers from low accuracy when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a plethora of benefits, such as $(i)$ it leverages graph embedding to extract semantically rich information from the dataset, $(ii)$ it uses intuitionistic fuzzy sets to ha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32570;&#20047;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.07872</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#21542;&#20250;&#21457;&#29983;&#21452;&#19979;&#38477;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Double Descent Occur in Self-Supervised Learning?. (arXiv:2307.07872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07872
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#32570;&#20047;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#30417;&#30563;&#27169;&#22411;&#19978;&#65292;&#32780;&#23545;&#20110;&#33258;&#30417;&#30563;&#35774;&#32622;&#30340;&#30740;&#31350;&#21364;&#21457;&#29616;&#36825;&#31181;&#29616;&#35937;&#30340;&#32570;&#22833;&#20196;&#20154;&#24778;&#35766;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#21644;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#26469;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#27979;&#35797;&#25439;&#22833;&#35201;&#20040;&#21576;&#29616;&#32463;&#20856;&#30340;U&#22411;&#26354;&#32447;&#65292;&#35201;&#20040;&#21333;&#35843;&#36882;&#20943;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#21452;&#19979;&#38477;&#26354;&#32447;&#12290;&#25105;&#20204;&#24076;&#26395;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#33021;&#22815;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically using a standard and linear autoencoder, two previously unstudied settings. The test loss is found to have either a classical U-shape or to monotonically decrease instead of exhibiting a double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.07871</link><description>&lt;p&gt;
&#31038;&#20250;AI&#23398;&#26657;&#65306;&#20174;&#21457;&#23637;&#24515;&#29702;&#23398;&#21040;&#20154;&#24037;&#31038;&#20250;&#25991;&#21270;&#20195;&#29702;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#30830;&#31435;&#20102;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#22312;&#20154;&#31867;&#26234;&#21147;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#20837;&#12289;&#21442;&#19982;&#21644;&#20174;&#20154;&#31867;&#25991;&#21270;&#20013;&#21463;&#30410;&#12290;&#31038;&#20250;&#20132;&#20114;&#20195;&#29702;&#30340;AI&#30740;&#31350;&#22823;&#22810;&#20851;&#27880;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25991;&#21270;&#30340;&#20986;&#29616;&#65288;&#36890;&#24120;&#27809;&#26377;&#24378;&#28872;&#30340;&#21457;&#23637;&#24515;&#29702;&#23398;&#22522;&#30784;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Michael Tomasello&#21644;Jerome Bruner&#30340;&#29702;&#35770;&#65292;&#20171;&#32461;&#20102;&#20182;&#20204;&#30340;&#19968;&#20123;&#27010;&#24565;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#21644;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#8212;&#8212;&#19968;&#20010;&#21253;&#25324;&#23450;&#21046;&#21442;&#25968;&#21270;&#29615;&#22659;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;RL&#20195;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27492;&#31867;&#23454;&#39564;&#30340;&#31034;&#20363;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21560;&#24341;AI&#31038;&#21306;&#22260;&#32469;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#35843;&#33410;&#30340;&#21453;STDP&#23398;&#20064;&#30340;&#33258;&#23450;&#20041;DNN&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#26102;&#38388;&#27169;&#24335;&#35782;&#21035;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#30701;&#26102;&#38388;&#38388;&#38548;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#27169;&#24335;&#12290;&#22312;&#22797;&#26434;&#30340;&#35821;&#38899;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07869</link><description>&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#35843;&#33410;&#30340;&#21453;STDP&#23398;&#20064;&#26500;&#24314;&#33258;&#23450;&#20041;DNN&#36827;&#34892;&#26102;&#38388;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition. (arXiv:2307.07869v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#35843;&#33410;&#30340;&#21453;STDP&#23398;&#20064;&#30340;&#33258;&#23450;&#20041;DNN&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#26102;&#38388;&#27169;&#24335;&#35782;&#21035;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#30701;&#26102;&#38388;&#38388;&#38548;&#30340;&#21160;&#24577;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#27169;&#24335;&#12290;&#22312;&#22797;&#26434;&#30340;&#35821;&#38899;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#33033;&#20914;&#35782;&#21035;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#31070;&#32463;&#31185;&#23398;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#30340;&#26102;&#38388;&#33033;&#20914;&#27169;&#24335;&#35782;&#21035;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#22870;&#21169;&#35843;&#33410;&#12289;Hebbian&#21644;&#21453;Hebbian&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#30701;&#26102;&#38388;&#38388;&#38548;&#35757;&#32451;&#20013;&#21160;&#24577;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#12290;&#31639;&#27861;&#39318;&#20808;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#29702;&#24615;&#21270;&#24182;&#36716;&#21270;&#20026;&#29305;&#24449;&#20016;&#23500;&#19988;&#31232;&#30095;&#30340;&#33033;&#20914;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#32447;&#24615;&#21069;&#39304;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#24050;&#35757;&#32451;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#19979;&#19968;&#23618;&#36827;&#34892;&#21152;&#26435;&#26816;&#26597;&#65292;&#20197;&#30830;&#20445;&#27491;&#30830;&#30340;&#27169;&#24335;&#24050;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23427;&#22312;&#21253;&#21547;&#33033;&#20914;&#20449;&#24687;&#30340;&#35821;&#38899;&#25968;&#23383;&#30340;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal spike recognition plays a crucial role in various domains, including anomaly detection, keyword spotting and neuroscience. This paper presents a novel algorithm for efficient temporal spike pattern recognition on sparse event series data. The algorithm leverages a combination of reward-modulatory behavior, Hebbian and anti-Hebbian based learning methods to identify patterns in dynamic datasets with short intervals of training. The algorithm begins with a preprocessing step, where the input data is rationalized and translated to a feature-rich yet sparse spike time series data. Next, a linear feed forward spiking neural network processes this data to identify a trained pattern. Finally, the next layer performs a weighted check to ensure the correct pattern has been detected.To evaluate the performance of the proposed algorithm, it was trained on a complex dataset containing spoken digits with spike information and its output compared to state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#31867;&#22411;&#30340;LSTM&#27169;&#22411;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23547;&#27714;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#20844;&#21496;&#30340;&#39044;&#27979;&#21644;&#34892;&#19994;&#34920;&#29616;&#26469;&#27491;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07868</link><description>&lt;p&gt;
&#21033;&#29992;&#24773;&#24863;&#20998;&#26512;&#36741;&#21161;&#30340;&#21508;&#31181;LSTM&#27169;&#22411;&#23545;&#27604;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#27169;&#22411;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis. (arXiv:2307.07868v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07868
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#31867;&#22411;&#30340;LSTM&#27169;&#22411;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23547;&#27714;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#20844;&#21496;&#30340;&#39044;&#27979;&#21644;&#34892;&#19994;&#34920;&#29616;&#26469;&#27491;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#27169;&#22411;&#65292;&#21033;&#29992;&#20844;&#21496;&#30340;&#39044;&#27979;&#21644;&#34892;&#19994;&#34920;&#29616;&#65292;&#28982;&#21518;&#26681;&#25454;&#32473;&#23450;&#20844;&#21496;&#30340;&#24773;&#20917;&#65292;&#27491;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research aims to find the best model that uses companies projections and sector performances and how the given company fares accordingly to correctly predict equity share prices for both short and long term goals.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;SVM&#26680;&#20989;&#25968;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;RBF SVM&#26680;&#24515;&#31639;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.07863</link><description>&lt;p&gt;
&#26816;&#27979;&#35910;&#31867;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#26680;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans. (arXiv:2307.07863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;SVM&#26680;&#20989;&#25968;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;RBF SVM&#26680;&#24515;&#31639;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#32946;&#31181;&#24072;&#21644;&#20892;&#19994;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#35910;&#31867;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#29702;&#24819;&#29305;&#24449;&#12289;&#25239;&#30149;&#24615;&#21644;&#33829;&#20859;&#21547;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#31639;&#27861;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#65292;&#20197;&#21450;&#20854;&#20182;&#27969;&#34892;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#20998;&#26512;&#26159;&#22312;&#35910;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#20027;&#35201;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#20934;&#30830;&#29575;&#65292;&#32780;RBF SVM&#26680;&#24515;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;93.34%&#65292;&#31934;&#30830;&#29575;92.61%&#65292;&#21484;&#22238;&#29575;92.35%&#21644;F1&#24471;&#20998;91.40%&#12290;&#38500;&#20102;&#29087;&#32451;&#30340;&#21487;&#35270;&#21270;&#21644;&#32463;&#39564;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#35843;&#32771;&#34385;&#19981;&#21516;&#30340;SVM&#31639;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plant breeders and agricultural researchers can increase crop productivity by identifying desirable features, disease resistance, and nutritional content by analysing the Dry Bean dataset. This study analyses and compares different Support Vector Machine (SVM) classification algorithms, namely linear, polynomial, and radial basis function (RBF), along with other popular classification algorithms. The analysis is performed on the Dry Bean Dataset, with PCA (Principal Component Analysis) conducted as a preprocessing step for dimensionality reduction. The primary evaluation metric used is accuracy, and the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept visualization and empirical analysis, this study offers valuable guidance by emphasizing the importance of considering different SVM algorithms for complex and non-linear structured datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#35780;&#20998;&#21305;&#37197;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#31639;&#27861;&#23558;&#21464;&#20998;&#36817;&#20284;&#19982;&#31934;&#30830;&#21518;&#39564;&#30340;&#35780;&#20998;&#21305;&#37197;&#12290;&#24403;&#21464;&#20998;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#26102;&#65292;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#26377;&#38381;&#24335;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.07849</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#35780;&#20998;&#21305;&#37197;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Gaussian Score Matching. (arXiv:2307.07849v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#35780;&#20998;&#21305;&#37197;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#31639;&#27861;&#23558;&#21464;&#20998;&#36817;&#20284;&#19982;&#31934;&#30830;&#21518;&#39564;&#30340;&#35780;&#20998;&#21305;&#37197;&#12290;&#24403;&#21464;&#20998;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#26102;&#65292;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#26377;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#29702;&#65288;VI&#65289;&#26159;&#19968;&#31181;&#36924;&#36817;&#36125;&#21494;&#26031;&#32479;&#35745;&#20013;&#30340;&#35745;&#31639;&#22256;&#38590;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;VI&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#65288;&#20363;&#22914;&#35777;&#25454;&#19979;&#30028;ELBO&#65289;&#23558;&#31616;&#21333;&#30340;&#21442;&#25968;&#20998;&#24067;&#25311;&#21512;&#21040;&#30446;&#26631;&#21518;&#39564;&#20998;&#24067;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#21305;&#37197;&#21407;&#29702;&#30340;&#26032;&#22411;VI&#26041;&#27861;&#65292;&#21363;&#22914;&#26524;&#20004;&#20010;&#20998;&#24067;&#30456;&#31561;&#65292;&#21017;&#23427;&#20204;&#30340;&#35780;&#20998;&#20989;&#25968;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#22312;&#20854;&#25903;&#25345;&#38598;&#30340;&#27599;&#20010;&#28857;&#19978;&#37117;&#30456;&#31561;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#29702;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35780;&#20998;&#21305;&#37197;VI&#65292;&#36825;&#26159;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#26088;&#22312;&#21305;&#37197;&#21464;&#20998;&#36817;&#20284;&#19982;&#31934;&#30830;&#21518;&#39564;&#20043;&#38388;&#30340;&#35780;&#20998;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#35780;&#20998;&#21305;&#37197;VI&#35299;&#20915;&#20102;&#19968;&#20010;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#35843;&#25972;&#24403;&#21069;&#21464;&#20998;&#20272;&#35745;&#65292;&#20351;&#20854;&#19982;&#26032;&#25277;&#21462;&#30340;&#28508;&#21464;&#37327;&#20540;&#22788;&#30340;&#35780;&#20998;&#21305;&#37197;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#21464;&#20998;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#26102;&#65292;&#36825;&#20010;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#26377;&#19968;&#20010;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to the target posterior by minimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI based on the principle of score matching, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this, we develop score matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed form solution, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07847</link><description>&lt;p&gt;
&#20113;&#28216;&#25103;&#20013;&#30340;&#31070;&#32463;&#35270;&#39057;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#30340;&#35270;&#39057;&#24103;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#21644;&#37096;&#20998;&#35299;&#30721;&#24103;&#26469;&#25552;&#39640;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28216;&#25103;&#26159;&#19968;&#20010;&#20215;&#20540;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#12290;&#22312;&#20113;&#28216;&#25103;&#20013;&#65292;&#23458;&#25143;&#31471;&#23558;&#33258;&#24049;&#30340;&#31227;&#21160;&#21457;&#36865;&#21040;&#20114;&#32852;&#32593;&#19978;&#30340;&#28216;&#25103;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#28210;&#26579;&#24182;&#20256;&#36755;&#32467;&#26524;&#35270;&#39057;&#22238;&#26469;&#12290;&#20026;&#20102;&#25552;&#20379;&#33391;&#22909;&#30340;&#28216;&#25103;&#20307;&#39564;&#65292;&#38656;&#35201;&#20302;&#20110;80&#27627;&#31186;&#30340;&#24310;&#36831;&#12290;&#36825;&#24847;&#21619;&#30528;&#35270;&#39057;&#30340;&#28210;&#26579;&#12289;&#32534;&#30721;&#12289;&#20256;&#36755;&#12289;&#35299;&#30721;&#21644;&#26174;&#31034;&#24517;&#39035;&#22312;&#36825;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#65292;&#30001;&#20110;&#26381;&#21153;&#22120;&#36807;&#36733;&#12289;&#32593;&#32476;&#25317;&#22622;&#21644;&#20002;&#21253;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#28857;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20113;&#28216;&#25103;&#20013;&#24674;&#22797;&#20002;&#22833;&#25110;&#25439;&#22351;&#35270;&#39057;&#24103;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35270;&#39057;&#24103;&#24674;&#22797;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28216;&#25103;&#29366;&#24577;&#26174;&#33879;&#25552;&#21319;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#37096;&#20998;&#35299;&#30721;&#30340;&#24103;&#26469;&#24674;&#22797;&#20002;&#22833;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;(i)&#39640;&#25928;&#25552;&#21462;&#28216;&#25103;&#29366;&#24577;&#65292;(ii)&#20462;&#25913; H.264 &#35270;&#39057;&#35299;&#30721;&#22120;&#29983;&#25104;&#19968;&#20010;&#25351;&#31034;&#38656;&#35201;&#24674;&#22797;&#35270;&#39057;&#24103;&#21738;&#20123;&#37096;&#20998;&#30340;&#25513;&#30721;&#65292;&#21644; (iii)&#35774;&#35745;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#39057;&#24103;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
&lt;/p&gt;</description></item><item><title>Transformers&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#65292;&#24182;&#19988;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.07843</link><description>&lt;p&gt;
Transformers&#26159;&#36890;&#29992;&#30340;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07843
&lt;/p&gt;
&lt;p&gt;
Transformers&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#65292;&#24182;&#19988;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25214;&#21040;&#20102;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#20449;&#24687;&#29702;&#35770;&#24847;&#20041;&#19978;&#20855;&#26377;&#36890;&#29992;&#30340;&#39044;&#27979;&#24615;&#36136;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#22312;&#38750;&#28176;&#36817;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;Transformer&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#22312;&#25968;&#25454;&#39640;&#25928;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;MixupExplainer&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21644;&#22270;mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07832</link><description>&lt;p&gt;
MixupExplainer:&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36890;&#29992;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;MixupExplainer&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21644;&#22270;mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#35299;&#37322;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#24448;&#24448;&#19981;&#21487;&#35299;&#37322;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20107;&#21518;&#23454;&#20363;&#32423;&#35299;&#37322;&#26041;&#27861;&#26469;&#29702;&#35299;GNN&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21457;&#29616;&#35299;&#37322;&#35757;&#32451;&#36807;&#30340;GNN&#39044;&#27979;&#34892;&#20026;&#30340;&#23376;&#32467;&#26500;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#20013;&#29305;&#21035;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#20005;&#26684;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#29420;&#31435;&#20110;&#26631;&#31614;&#30340;&#22270;&#21464;&#37327;&#30340;&#24191;&#20041;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#24418;&#24335;&#65292;&#31561;&#20215;&#20110;&#20256;&#32479;&#30340;GIB&#12290;&#21463;&#24191;&#20041;GIB&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;mixup&#26041;&#27861;&#65292;MixupExplainer&#65292;&#20855;&#26377;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;Mean-KL&#65292;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07816</link><description>&lt;p&gt;
&#24102;&#26377;Mean-KL&#21442;&#25968;&#21270;&#30340;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;Mean-KL&#65292;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#23567;&#38543;&#26426;&#32534;&#30721;&#23398;&#20064;&#65288;MIRACLE&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#22312;&#21387;&#32553;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#24615;&#34892;&#20026;&#21644;&#40065;&#26834;&#24615;&#12290;MIRACLE&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#26465;&#20214;&#39640;&#26031;&#21464;&#20998;&#36817;&#20284;&#26435;&#37325;&#21518;&#39564;$Q_{\mathbf{w}}$&#65292;&#24182;&#20351;&#29992;&#30456;&#23545;&#29109;&#32534;&#30721;&#26469;&#21387;&#32553;&#20174;&#21518;&#39564;&#20013;&#25277;&#26679;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#39640;&#26031;&#32534;&#30721;&#20998;&#24067;$P_{\mathbf{w}}$&#12290;&#20026;&#20102;&#36798;&#21040;&#25152;&#38656;&#30340;&#21387;&#32553;&#29575;&#65292;&#24517;&#39035;&#23545;$Q_{\mathbf{w}} \Vert P_{\mathbf{w}}$&#36827;&#34892;&#32422;&#26463;&#65292;&#36825;&#38656;&#35201;&#22312;&#20256;&#32479;&#30340;&#22343;&#20540;-&#26041;&#24046;&#65288;Mean-Var&#65289;&#21442;&#25968;&#21270;&#19979;&#36827;&#34892;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#36864;&#28779;&#36807;&#31243;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#20854;&#24179;&#22343;&#20540;&#21644;KL&#25955;&#24230;&#26469;&#21442;&#25968;&#21270;$Q_{\mathbf{w}}$&#65292;&#20197;&#36890;&#36807;&#26500;&#36896;&#23558;&#21387;&#32553;&#25104;&#26412;&#32422;&#26463;&#20026;&#25152;&#38656;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;Mean-KL&#21442;&#25968;&#21270;&#30340;&#21464;&#20998;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#26159;&#20256;&#32479;&#26041;&#27861;&#30340;&#20004;&#20493;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21518;&#20445;&#25345;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian neural networks. MIRACLE implements a powerful, conditionally Gaussian variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for $Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-KL parameterization converges twice as fast and maintains predictive performance after 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.07810</link><description>&lt;p&gt;
&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Automorphism Group Equivariant Neural Networks. (arXiv:2307.07810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22270;&#33258;&#21516;&#24577;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;n&#20010;&#39030;&#28857;&#21644;&#20854;&#33258;&#21516;&#24577;&#32676;Aut(G)&#30340;&#22270;G&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;Aut(G)-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#65292;&#20854;&#23618;&#26159;n&#32500;&#23454;&#25968;&#24352;&#37327;&#30340;&#26576;&#20123;&#24352;&#37327;&#24130;&#27425;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;n&#32500;&#23454;&#25968;&#31354;&#38388;&#30340;&#26631;&#20934;&#22522;&#19979;&#25214;&#21040;&#20102;&#21487;&#23398;&#20064;&#30340;&#12289;&#32447;&#24615;&#30340;Aut(G)-&#31561;&#21464;&#23618;&#20989;&#25968;&#20043;&#38388;&#30340;&#30697;&#38453;&#30340;&#29983;&#25104;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
For any graph $G$ having $n$ vertices and its automorphism group $\textrm{Aut}(G)$, we provide a full characterisation of all of the possible $\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a spanning set of matrices for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25554;&#20540;&#20449;&#24687;&#20934;&#21017;&#65292;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#36125;&#21494;&#26031;&#23545;&#20598;&#24418;&#24335;&#65292;&#35813;&#20934;&#21017;&#23558;&#20808;&#39564;&#36873;&#25321;&#32435;&#20837;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#20102;&#20808;&#39564;&#35823;&#35774;&#12289;&#27169;&#22411;&#30340;&#20960;&#20309;&#21644;&#35889;&#29305;&#24615;&#12290;&#35813;&#20934;&#21017;&#22312;&#23454;&#35777;&#21644;&#29702;&#35770;&#34892;&#20026;&#26041;&#38754;&#19982;&#24050;&#30693;&#32467;&#26524;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.07785</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#25554;&#20540;&#20449;&#24687;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Interpolating Information Criterion for Overparameterized Models. (arXiv:2307.07785v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25554;&#20540;&#20449;&#24687;&#20934;&#21017;&#65292;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#31435;&#36125;&#21494;&#26031;&#23545;&#20598;&#24418;&#24335;&#65292;&#35813;&#20934;&#21017;&#23558;&#20808;&#39564;&#36873;&#25321;&#32435;&#20837;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#20102;&#20808;&#39564;&#35823;&#35774;&#12289;&#27169;&#22411;&#30340;&#20960;&#20309;&#21644;&#35889;&#29305;&#24615;&#12290;&#35813;&#20934;&#21017;&#22312;&#23454;&#35777;&#21644;&#29702;&#35770;&#34892;&#20026;&#26041;&#38754;&#19982;&#24050;&#30693;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#36807;&#21442;&#25968;&#21270;&#20272;&#35745;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#36229;&#36807;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#20256;&#32479;&#30340;&#20449;&#24687;&#20934;&#21017;&#36890;&#24120;&#32771;&#34385;&#22823;&#25968;&#25454;&#26497;&#38480;&#65292;&#23545;&#27169;&#22411;&#22823;&#23567;&#36827;&#34892;&#24809;&#32602;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#20934;&#21017;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#33391;&#22909;&#12290;&#23545;&#20110;&#20219;&#20309;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#23545;&#20598;&#30340;&#27424;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#36793;&#32536;&#20284;&#28982;&#24615;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#36125;&#21494;&#26031;&#23545;&#20598;&#24418;&#24335;&#12290;&#36825;&#20351;&#24471;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#21487;&#20197;&#20351;&#29992;&#26356;&#22810;&#32463;&#20856;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#25554;&#20540;&#20449;&#24687;&#20934;&#21017;&#65292;&#19968;&#31181;&#33258;&#28982;&#22320;&#23558;&#20808;&#39564;&#36873;&#25321;&#32435;&#20837;&#27169;&#22411;&#36873;&#25321;&#30340;&#27169;&#22411;&#36136;&#37327;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26032;&#20449;&#24687;&#20934;&#21017;&#32771;&#34385;&#20102;&#20808;&#39564;&#35823;&#35774;&#12289;&#27169;&#22411;&#30340;&#20960;&#20309;&#21644;&#35889;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#35813;&#21306;&#22495;&#19982;&#24050;&#30693;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#34892;&#20026;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of model selection is considered for the setting of interpolating estimators, where the number of model parameters exceeds the size of the dataset. Classical information criteria typically consider the large-data limit, penalizing model size. However, these criteria are not appropriate in modern settings where overparameterized models tend to perform well. For any overparameterized model, we show that there exists a dual underparameterized model that possesses the same marginal likelihood, thus establishing a form of Bayesian duality. This enables more classical methods to be used in the overparameterized setting, revealing the Interpolating Information Criterion, a measure of model quality that naturally incorporates the choice of prior into the model selection. Our new information criterion accounts for prior misspecification, geometric and spectral properties of the model, and is numerically consistent with known empirical and theoretical behavior in this regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2307.07771</link><description>&lt;p&gt;
CatBoost&#23545;&#27604;XGBoost&#21644;LightGBM&#65306;&#24320;&#21457;&#22686;&#24378;&#30340;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;CatBoost&#12289;XGBoost&#21644;LightGBM&#19977;&#31181;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#22312;&#22788;&#29702;&#38646;&#33192;&#32960;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36130;&#20135;&#21644;&#24847;&#22806;&#20107;&#25925;&#20445;&#38505;&#34892;&#19994;&#20013;&#65292;&#30001;&#20110;&#27491;&#21521;&#29702;&#36180;&#25968;&#25454;&#20855;&#26377;&#39640;&#24230;&#21491;&#20559;&#20998;&#24067;&#21644;&#36807;&#37327;&#30340;&#38646;&#20540;&#65292;&#26500;&#24314;&#29702;&#36180;&#39044;&#27979;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#65292;&#22914;&#27850;&#26494;&#25110;&#36127;&#20108;&#39033;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;(GLM)&#65292;&#32463;&#24120;&#22312;&#22788;&#29702;&#36807;&#37327;&#38646;&#20540;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#31934;&#31639;&#31185;&#23398;&#30340;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#37319;&#29992;&#20102;&#8220;&#38646;&#33192;&#32960;&#8221;&#27169;&#22411;&#65292;&#23558;&#20256;&#32479;&#35745;&#25968;&#27169;&#22411;&#21644;&#20108;&#20803;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#25552;&#21319;&#31639;&#27861;&#26469;&#22788;&#29702;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#65292;&#21253;&#25324;&#38646;&#33192;&#32960;&#30340;&#36965;&#27979;&#25968;&#25454;&#65292;&#20197;&#26500;&#24314;&#29702;&#36180;&#39057;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#19977;&#20010;&#27969;&#34892;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211; - XGBoost&#12289;LightGBM&#21644;CatBoost&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#21512;&#35757;&#32451;&#20445;&#38505;&#29702;&#36180;&#25968;&#25454;&#21644;&#25311;&#21512;&#31934;&#31639;&#39057;&#29575;&#27169;&#22411;&#30340;&#24211;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CatBoost&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
&lt;/p&gt;</description></item><item><title>randomHAR&#26159;&#19968;&#31181;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#36873;&#25321;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20102;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07770</link><description>&lt;p&gt;
randomHAR&#65306;&#21033;&#29992;&#20256;&#24863;&#22120;&#36873;&#25321;&#21644;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning. (arXiv:2307.07770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07770
&lt;/p&gt;
&lt;p&gt;
randomHAR&#26159;&#19968;&#31181;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#36873;&#25321;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20102;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#39046;&#22495;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#20854;&#20182;&#38656;&#35201;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#30340;&#26550;&#26500;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;HAR&#25968;&#25454;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#22914;&#22122;&#22768;&#25968;&#25454;&#12289;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#31867;&#38388;&#30456;&#20284;&#24615;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;randomHAR&#12290;randomHAR&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#35757;&#32451;&#19968;&#31995;&#21015;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#19968;&#20010;Agent&#26469;&#30830;&#23450;&#22312;&#36816;&#34892;&#26102;&#39044;&#27979;&#20013;&#20351;&#29992;&#30340;&#26368;&#20248;&#23376;&#27169;&#22411;&#38598;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#30340;&#26159;&#38598;&#25104;&#36807;&#31243;&#32780;&#19981;&#26159;&#32452;&#25104;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20004;&#31181;HAR&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#24403;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#20845;&#20010;HAR&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has proven to be an effective approach in the field of Human activity recognition (HAR), outperforming other architectures that require manual feature engineering. Despite recent advancements, challenges inherent to HAR data, such as noisy data, intra-class variability and inter-class similarity, remain. To address these challenges, we propose an ensemble method, called randomHAR. The general idea behind randomHAR is training a series of deep learning models with the same architecture on randomly selected sensor data from the given dataset. Besides, an agent is trained with the reinforcement learning algorithm to identify the optimal subset of the trained models that are utilized for runtime prediction. In contrast to existing work, this approach optimizes the ensemble process rather than the architecture of the constituent models. To assess the performance of the approach, we compare it against two HAR algorithms, including the current state of the art, on six HAR benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;5G NSA&#21152;&#23494;&#25968;&#25454;&#27969;&#30340;&#23454;&#26102;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#36947;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#30340;&#20915;&#31574;&#26641;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26102;&#20998;&#26512;&#21152;&#23494;&#27969;&#37327;&#65292;&#21487;&#29992;&#20110;QoS&#31649;&#29702;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.07756</link><description>&lt;p&gt;
5G NSA&#21152;&#23494;&#25968;&#25454;&#27969;&#30340;&#23454;&#26102;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#21450;&#20854;&#29289;&#29702;&#20449;&#36947;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records. (arXiv:2307.07756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;5G NSA&#21152;&#23494;&#25968;&#25454;&#27969;&#30340;&#23454;&#26102;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#36947;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#30340;&#20915;&#31574;&#26641;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26102;&#20998;&#26512;&#21152;&#23494;&#27969;&#37327;&#65292;&#21487;&#29992;&#20110;QoS&#31649;&#29702;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31532;&#20116;&#20195;&#26032;&#26080;&#32447;&#30005;&#65288;5G-NR&#65289;&#31227;&#21160;&#32593;&#32476;&#27969;&#37327;&#30340;&#20998;&#31867;&#26159;&#30005;&#20449;&#39046;&#22495;&#30340;&#19968;&#20010;&#26032;&#20852;&#35805;&#39064;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#31649;&#29702;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#22914;&#28145;&#24230;&#25968;&#25454;&#21253;&#26816;&#26597;&#65288;DPI&#65289;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#21152;&#23494;&#25968;&#25454;&#27969;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#30740;&#31350;&#26032;&#30340;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#31639;&#27861;&#26469;&#22788;&#29702;&#21160;&#24577;&#20256;&#36755;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29289;&#29702;&#20449;&#36947;&#35760;&#24405;&#65292;&#20351;&#29992;&#26032;&#30340;&#23454;&#26102;&#21152;&#23494;5G Non-Standalone&#65288;NSA&#65289;&#24212;&#29992;&#32423;&#27969;&#37327;&#20998;&#31867;&#26041;&#27861;&#12290;&#30001;&#20110;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#24212;&#29992;&#27969;&#37327;&#30340;&#21463;&#22122;&#22768;&#38480;&#21046;&#30340;5G NSA&#36861;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#27969;&#31243;&#65292;&#23558;&#29289;&#29702;&#20449;&#36947;&#35760;&#24405;&#24207;&#21015;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#32452;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36827;&#34892;&#27969;&#37327;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of fifth-generation New-Radio (5G-NR) mobile network traffic is an emerging topic in the field of telecommunications. It can be utilized for quality of service (QoS) management and dynamic resource allocation. However, traditional approaches such as Deep Packet Inspection (DPI) can not be directly applied to encrypted data flows. Therefore, new real-time encrypted traffic classification algorithms need to be investigated to handle dynamic transmission. In this study, we examine the real-time encrypted 5G Non-Standalone (NSA) application-level traffic classification using physical channel records. Due to the vastness of their features, decision-tree-based gradient boosting algorithms are a viable approach for classification. We generate a noise-limited 5G NSA trace dataset with traffic from multiple applications. We develop a new pipeline to convert sequences of physical channel records into numerical vectors. A set of machine learning models are tested, and we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#25512;&#24191;&#30340;&#20449;&#24687;&#20808;&#39564;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07753</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#24615;&#20808;&#39564;&#65292;&#25552;&#39640;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks. (arXiv:2307.07753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#25512;&#24191;&#30340;&#20449;&#24687;&#20808;&#39564;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#24182;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25512;&#24191;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#21487;&#25193;&#23637;&#21644;&#32467;&#26500;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20316;&#20026;&#20855;&#26377;&#25512;&#24191;&#20445;&#35777;&#30340;&#20449;&#24687;&#20808;&#39564;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#22312;&#22823;&#35268;&#27169;&#19978;&#25552;&#20379;&#20102;&#34920;&#36798;&#24615;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#31867;&#20284;&#20110;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#65292;&#24182;&#36827;&#19968;&#27493;&#20135;&#29983;&#20102;&#38750;&#31354;&#25512;&#24191;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20010;&#24819;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#26377;&#21033;&#29305;&#24615;&#26159;&#21487;&#21462;&#30340;&#12290;&#20027;&#35201;&#30340;&#25512;&#21160;&#22240;&#32032;&#26159;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;(1) Kronecker&#31215;&#27714;&#21644;&#30340;&#35745;&#31639;&#65292;(2) &#25512;&#23548;&#21644;&#20248;&#21270;&#21487;&#22788;&#29702;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#25913;&#36827;&#30340;&#25512;&#24191;&#30028;&#38480;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35814;&#23613;&#22320;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#25512;&#24191;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Our learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. We also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. Major enablers are our technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, we exhaustively show the effectiveness of this method for uncertainty estimation and generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#23545;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36827;&#34892;&#25968;&#25454;&#20272;&#35745;&#26102;&#30340;&#25928;&#29992;&#25552;&#21319;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;IBU&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#30697;&#38453;&#27714;&#36870;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07744</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#23545;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#25928;&#29992;&#25552;&#21319;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms. (arXiv:2307.07744v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#23545;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36827;&#34892;&#25968;&#25454;&#20272;&#35745;&#26102;&#30340;&#25928;&#29992;&#25552;&#21319;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;IBU&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#30697;&#38453;&#27714;&#36870;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#65288;IBU&#65289;&#23545;&#20351;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#26426;&#21046;&#27169;&#31946;&#21270;&#30340;&#25968;&#25454;&#36827;&#34892;&#31169;&#26377;&#31163;&#25955;&#20998;&#24067;&#20272;&#35745;&#30340;&#25928;&#29992;&#25552;&#21319;&#12290;&#25105;&#20204;&#23558;IBU&#30340;&#24615;&#33021;&#19982;&#30697;&#38453;&#27714;&#36870;&#65288;MI&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;MI&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#20272;&#35745;&#25216;&#26415;&#65292;&#29992;&#20110;&#19971;&#31181;&#29992;&#20110;&#19968;&#27425;&#25968;&#25454;&#25910;&#38598;&#30340;LDP&#26426;&#21046;&#21644;&#20854;&#20182;&#19971;&#31181;&#29992;&#20110;&#22810;&#27425;&#25968;&#25454;&#25910;&#38598;&#65288;&#20363;&#22914;RAPPOR&#65289;&#30340;LDP&#26426;&#21046;&#12290;&#20026;&#20102;&#25193;&#22823;&#30740;&#31350;&#33539;&#22260;&#65292;&#25105;&#20204;&#36824;&#21464;&#21270;&#20102;&#25928;&#29992;&#24230;&#37327;&#26631;&#20934;&#12289;&#29992;&#25143;&#25968;n&#12289;&#22495;&#22823;&#23567;k&#21644;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;IBU&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#29992;&#30340;&#21518;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;LDP&#26426;&#21046;&#30340;&#25928;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#39640;&#38544;&#31169;&#24773;&#20917;&#19979;&#65288;&#21363;&#949;&#24456;&#23567;&#65289;IBU&#21487;&#20197;&#25552;&#20379;&#27604;MI&#26356;&#22909;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the utility gain of using Iterative Bayesian Update (IBU) for private discrete distribution estimation using data obfuscated with Locally Differentially Private (LDP) mechanisms. We compare the performance of IBU to Matrix Inversion (MI), a standard estimation technique, for seven LDP mechanisms designed for one-time data collection and for other seven LDP mechanisms designed for multiple data collections (e.g., RAPPOR). To broaden the scope of our study, we also varied the utility metric, the number of users n, the domain size k, and the privacy parameter {\epsilon}, using both synthetic and real-world data. Our results suggest that IBU can be a useful post-processing tool for improving the utility of LDP mechanisms in different scenarios without any additional privacy cost. For instance, our experiments show that IBU can provide better utility than MI, especially in high privacy regimes (i.e., when {\epsilon} is small). Our paper provides insights for practiti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#30456;&#20851;&#24615;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#37327;&#21270;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#20854;&#23545;&#32593;&#32476;&#20960;&#20309;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07738</link><description>&lt;p&gt;
&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#30340;&#36127;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Negative probabilities in Gene Regulatory Networks. (arXiv:2307.07738v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#30456;&#20851;&#24615;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#37327;&#21270;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#20854;&#23545;&#32593;&#32476;&#20960;&#20309;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#30693;&#34920;&#36798;&#21644;&#32473;&#23450;&#30456;&#20851;&#24615;&#31526;&#21495;&#26469;&#35782;&#21035;&#22522;&#22240;&#20043;&#38388;&#23384;&#22312;&#30340;&#31526;&#21495;&#19981;&#30830;&#23450;&#20849;&#34920;&#36798;&#30340;&#33258;&#28982;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22522;&#22240;&#20043;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#20449;&#24687;&#65288;&#21363;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#30340;&#36830;&#25509;&#24615;&#65289;&#21644;&#23427;&#20204;&#20419;&#36827;/&#25233;&#21046;&#20849;&#34920;&#36798;&#34507;&#30333;&#20135;&#29983;&#30340;&#30693;&#35782;&#65292;&#24182;&#23547;&#27714;&#33021;&#22815;&#35299;&#37322;&#34507;&#30333;&#36136;&#24179;&#34913;&#20998;&#24067;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#23558;&#23427;&#20204;&#30340;&#8220;&#20419;&#36827; vs. &#25233;&#21046;&#8221;&#21151;&#33021;&#23553;&#35013;&#22312;&#19968;&#20010;&#31526;&#21495;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#36716;&#31227;&#30697;&#38453;&#20013;&#65292;&#35813;&#30697;&#38453;&#30340;&#34892;&#21644;&#20026;&#19968;&#65292;&#20294;&#26159;&#38500;&#27492;&#20043;&#22806;&#31526;&#21495;&#19981;&#30830;&#23450;&#12290;&#26500;&#24314;&#20855;&#26377;&#31526;&#21495;&#19981;&#30830;&#23450;&#36129;&#29486;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#34920;&#31034;&#30340;&#30446;&#30340;&#26159;&#37327;&#21270;&#21508;&#31181;&#36830;&#25509;&#30340;&#32467;&#26500;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#35299;&#37322;&#36825;&#20123;&#36830;&#25509;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31361;&#26174;&#35843;&#25511;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a natural framework to identify sign-indefinite co-expressions between genes based on the known expressions and given the sign of their respective correlations. Specifically, given information concerning the affinity among genes (i.e., connectivity in the gene regulatory network) and knowledge whether they promote/inhibit co-expression of the respective protein production, we seek rates that may explain the observed stationary distributions at the level of proteins. We propose to encapsulate their ``promotion vs.\ inhibition'' functionality in a sign-indefinite probability transition matrix--a matrix whose row-sums equal to one, but is otherwise sign indefinite. The purpose of constructing such a representation for the interaction network with sign-indefinite contributions in protein regulation, is to quantify the structure and significance of various links, and to explain how these may affect the geometry of the network, highlighting the significance of the regulatory fun
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20108;&#27425;&#35268;&#21010;&#36755;&#20837;&#35268;&#27169;&#21644;&#35299;&#20915;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07735</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nearly-Linear Time Algorithm for Structured Support Vector Machines. (arXiv:2307.07735v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20108;&#27425;&#35268;&#21010;&#36755;&#20837;&#35268;&#27169;&#21644;&#35299;&#20915;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#35268;&#21010;&#26159;&#20984;&#20248;&#21270;&#39046;&#22495;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#21487;&#20197;&#34920;&#31034;&#20026;&#20108;&#27425;&#35268;&#21010;&#65292;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30427;&#34892;&#20043;&#21069;&#65292;&#32447;&#24615;SVM&#26159;&#36807;&#21435;&#19977;&#21313;&#24180;&#26469;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#30340;&#36755;&#20837;&#35268;&#27169;&#20026;&#920;(n^2)&#65288;&#20854;&#20013;n&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#65289;&#65292;&#22240;&#27492;&#35299;&#20915;&#35813;&#38382;&#39064;&#38656;&#35201;&#937;(n^2)&#30340;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;SVM&#20135;&#29983;&#30340;&#20108;&#27425;&#35268;&#21010;&#30340;&#36755;&#20837;&#35268;&#27169;&#20026;O(n)&#65292;&#36825;&#20351;&#24471;&#35774;&#35745;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#20004;&#20010;&#37325;&#35201;&#30340;SVM&#31867;&#21035;&#26159;&#20855;&#26377;&#20302;&#31209;&#26680;&#22240;&#24335;&#20998;&#35299;&#21644;&#20302;&#26641;&#23485;&#35268;&#27169;&#30340;&#31243;&#24207;&#12290;&#20302;&#26641;&#23485;&#20984;&#20248;&#21270;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65288;&#20363;&#22914;&#32447;&#24615;&#35268;&#21010;[Dong, Lee and Ye 2021]&#21644;&#21322;&#23450;&#35268;&#21010;[Gu and Song 2022]&#65289;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#26159;&#26159;&#21542;&#23384;&#22312;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating.  In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#30340;&#26679;&#26412;&#25286;&#20998;&#26041;&#27861;&#30340;&#22885;&#31192;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.07726</link><description>&lt;p&gt;
&#36808;&#21521;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#65306;&#26679;&#26412;&#25286;&#20998;&#22312;&#36229;&#21442;&#25968;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection. (arXiv:2307.07726v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#30340;&#26679;&#26412;&#25286;&#20998;&#26041;&#27861;&#30340;&#22885;&#31192;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#23454;&#36341;&#25104;&#21151;&#26102;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#22914;&#36924;&#36817;&#33021;&#21147;&#12289;&#32479;&#35745;&#24615;&#36136;&#21644;&#27867;&#21270;&#24615;&#33021;&#31561;&#30340;&#30740;&#31350;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26500;&#24314;&#20013;&#19968;&#31181;&#24120;&#35265;&#23454;&#36341;&#32972;&#21518;&#30340;&#22885;&#31192;&#65306;&#26679;&#26412;&#25286;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#65292;&#20174;&#26679;&#26412;&#25286;&#20998;&#20013;&#24471;&#21040;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#28176;&#36827;&#22320;&#26368;&#23567;&#21270;&#39044;&#27979;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#32593;&#32476;&#32467;&#26500;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When artificial neural networks have demonstrated exceptional practical success in a variety of domains, investigations into their theoretical characteristics, such as their approximation power, statistical properties, and generalization performance, have made significant strides. In this paper, we construct a novel theory for understanding the effectiveness of neural networks by discovering the mystery underlying a common practice during neural network model construction: sample splitting. Our theory demonstrates that, the optimal hyperparameters derived from sample splitting can enable a neural network model that asymptotically minimizes the prediction risk. We conduct extensive experiments across different application scenarios and network architectures, and the results manifest our theory's effectiveness.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#26469;&#21033;&#29992;&#21487;&#35270;&#21270;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#24182;&#20174;&#25968;&#25454;&#35270;&#35282;&#24635;&#32467;&#20102;&#36825;&#20123;&#20316;&#21697;&#12290;&#35770;&#25991;&#23558;ML&#27169;&#22411;&#22788;&#29702;&#30340;&#24120;&#35265;&#25968;&#25454;&#20998;&#20026;&#20116;&#31867;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#25797;&#38271;&#23398;&#20064;&#36825;&#20123;&#25968;&#25454;&#30340;ML&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#25805;&#20316;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#30340;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#23545;VIS4ML&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.07712</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35273;&#20998;&#26512;:&#25968;&#25454;&#35270;&#35282;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visual Analytics For Machine Learning: A Data Perspective Survey. (arXiv:2307.07712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07712
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#26469;&#21033;&#29992;&#21487;&#35270;&#21270;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#24182;&#20174;&#25968;&#25454;&#35270;&#35282;&#24635;&#32467;&#20102;&#36825;&#20123;&#20316;&#21697;&#12290;&#35770;&#25991;&#23558;ML&#27169;&#22411;&#22788;&#29702;&#30340;&#24120;&#35265;&#25968;&#25454;&#20998;&#20026;&#20116;&#31867;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#25797;&#38271;&#23398;&#20064;&#36825;&#20123;&#25968;&#25454;&#30340;ML&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#25805;&#20316;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#30340;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#23545;VIS4ML&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#21033;&#29992;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24037;&#20316;&#12290;&#30456;&#24212;&#30340;&#30740;&#31350;&#20027;&#39064;VIS4ML&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#32452;&#32455;&#36825;&#20123;&#20316;&#21697;&#24182;&#25581;&#31034;VIS4ML&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#25105;&#20204;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#25552;&#20379;&#23545;&#36825;&#20123;&#20316;&#21697;&#30340;&#31995;&#32479;&#22238;&#39038;&#12290;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#19987;&#38376;&#20174;&#25968;&#25454;&#35270;&#35282;&#24635;&#32467;&#20102;VIS4ML&#30340;&#20316;&#21697;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;ML&#27169;&#22411;&#22788;&#29702;&#30340;&#24120;&#35265;&#25968;&#25454;&#20998;&#20026;&#20116;&#31867;&#65292;&#35299;&#37322;&#20102;&#27599;&#31867;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#25797;&#38271;&#20174;&#20013;&#23398;&#20064;&#30340;ML&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#20174;&#20247;&#22810;&#30340;VIS4ML&#20316;&#21697;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20986;&#22312;ML&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#25805;&#20316;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#65288;&#21363;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#65289;&#30340;&#20845;&#20010;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#12289;&#35786;&#26029;&#21644;&#25913;&#36827;ML&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30740;&#31350;143&#31687;&#35843;&#26597;&#35770;&#25991;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#23545;VIS4ML&#30740;&#31350;&#30340;&#28909;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective. First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24615;&#21644;&#38750;&#38543;&#26426;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#25552;&#21462;&#29305;&#24449;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.07703</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#38543;&#26426;&#24615;&#35782;&#21035;&#65306;&#24212;&#29992;&#20110;&#40657;&#27934;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data. (arXiv:2307.07703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24615;&#21644;&#38750;&#38543;&#26426;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#25552;&#21462;&#29305;&#24449;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20026;&#38543;&#26426;&#65288;&#22122;&#22768;&#26679;&#65289;&#25110;&#38750;&#38543;&#26426;&#65288;&#32467;&#26500;&#21270;&#65289;&#26377;&#21161;&#20110;&#29702;&#35299;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#36275;&#30697;&#38453;&#20998;&#35299;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20998;&#26512;&#37096;&#20998;&#65292;&#25105;&#20204;&#23545;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#30340;&#22855;&#24322;&#21521;&#37327;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#65288;Betti&#25968;&#65289;&#65292;&#24471;&#21040;SVD&#26631;&#31614;&#12290;&#21516;&#26102;&#65292;&#36827;&#34892;&#26102;&#38388;&#39034;&#24207;&#26080;&#20851;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#24182;&#35745;&#31639;&#25152;&#25552;&#20986;&#30340;PCA&#34893;&#29983;&#29305;&#24449;&#12290;&#35266;&#23519;&#21040;&#36825;&#20123;&#29305;&#24449;&#20174;&#20004;&#20010;&#26631;&#31614;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#26144;&#23556;&#21040;&#32447;&#24615;&#21487;&#20998;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#29983;&#25104;PCA&#26631;&#31614;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#65292;&#21253;&#25324;41&#20010;&#30333;&#22122;&#22768;&#12289;&#31881;&#32418;&#22122;&#22768;&#65288;&#38543;&#26426;&#24615;&#65289;&#30340;&#23454;&#39564;&#65292;&#29983;&#38271;&#29575;&#20026;4&#30340;Logistic&#26144;&#23556;&#21644;Lorentz&#31995;&#32479;&#65288;&#38750;&#38543;&#26426;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timeseries classification as stochastic (noise-like) or non-stochastic (structured), helps understand the underlying dynamics, in several domains. Here we propose a two-legged matrix decomposition-based algorithm utilizing two complementary techniques for classification. In Singular Value Decomposition (SVD) based analysis leg, we perform topological analysis (Betti numbers) on singular vectors containing temporal information, leading to SVD-label. Parallely, temporal-ordering agnostic Principal Component Analysis (PCA) is performed, and the proposed PCA-derived features are computed. These features, extracted from synthetic timeseries of the two labels, are observed to map the timeseries to a linearly separable feature space. Support Vector Machine (SVM) is used to produce PCA-label. The proposed methods have been applied to synthetic data, comprising 41 realisations of white-noise, pink-noise (stochastic), Logistic-map at growth-rate 4 and Lorentz-system (non-stochastic), as proof-of
&lt;/p&gt;</description></item><item><title>NeurASP&#26159;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;Answer Set Programming&#20013;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#65292;NeurASP&#33021;&#22815;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#31526;&#21495;&#25512;&#29702;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20174;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07700</link><description>&lt;p&gt;
NeurASP&#65306;&#23558;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#21040;Answer Set Programming &#20013;
&lt;/p&gt;
&lt;p&gt;
NeurASP: Embracing Neural Networks into Answer Set Programming. (arXiv:2307.07700v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07700
&lt;/p&gt;
&lt;p&gt;
NeurASP&#26159;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;Answer Set Programming&#20013;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#20998;&#24067;&#30340;&#24418;&#24335;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#65292;NeurASP&#33021;&#22815;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#31526;&#21495;&#25512;&#29702;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20174;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NeurASP&#65292;&#23427;&#26159;&#23545;Answer Set Programs&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#36890;&#36807;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;Answer Set Programs&#20013;&#21407;&#23376;&#20107;&#23454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;NeurASP&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#23558;&#23376;&#31526;&#21495;&#21644;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NeurASP&#22914;&#20309;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;Answer Set Programming&#20013;&#30340;&#31526;&#21495;&#25512;&#29702;&#26469;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#24863;&#30693;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;NeurASP&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;ASP&#35268;&#21017;&#36827;&#34892;&#35757;&#32451;&#26469;&#26356;&#22909;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#19981;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#36824;&#20174;&#35268;&#21017;&#25152;&#34920;&#31034;&#30340;&#26174;&#24335;&#22797;&#26434;&#35821;&#20041;&#32422;&#26463;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#28369;&#27979;&#35797;&#21521;&#37327;&#65292;&#38477;&#20302;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#20013;&#38750;Galerkin&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#25972;&#20307;AMG&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07695</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38477;&#20302;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#20013;&#30340;&#25805;&#20316;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches. (arXiv:2307.07695v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#28369;&#27979;&#35797;&#21521;&#37327;&#65292;&#38477;&#20302;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#20013;&#38750;Galerkin&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#25972;&#20307;AMG&#25910;&#25947;&#24615;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#65288;AMG&#65289;&#26041;&#27861;&#20013;&#30340;&#38750;Galerkin&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#65292;&#35299;&#20915;&#20102;&#22686;&#21152;&#25805;&#20316;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;AMG&#29702;&#35770;&#20013;&#20851;&#20110;&#20809;&#35889;&#31561;&#25928;&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;ML&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#32467;&#21512;&#26469;&#33258;&#22810;&#37325;&#32593;&#26684;&#29305;&#24449;&#20540;&#38382;&#39064;&#30340;&#24179;&#28369;&#27979;&#35797;&#21521;&#37327;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#30340;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#35299;&#20915;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#38382;&#39064;&#30340;&#25972;&#20307;AMG&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#25552;&#20379;&#23545;&#21508;&#21521;&#24322;&#24615;&#26059;&#36716;&#30340;Laplacian&#21644;&#32447;&#24615;&#24377;&#24615;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#23637;&#31034;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#29992;&#20110;&#35745;&#31639;&#38750;Galerkin&#31895;&#32593;&#26684;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a data-driven and machine-learning-based approach to compute non-Galerkin coarse-grid operators in algebraic multigrid (AMG) methods, addressing the well-known issue of increasing operator complexity. Guided by the AMG theory on spectrally equivalent coarse-grid operators, we have developed novel ML algorithms that utilize neural networks (NNs) combined with smooth test vectors from multigrid eigenvalue problems. The proposed method demonstrates promise in reducing the complexity of coarse-grid operators while maintaining overall AMG convergence for solving parametric partial differential equation (PDE) problems. Numerical experiments on anisotropic rotated Laplacian and linear elasticity problems are provided to showcase the performance and compare with existing methods for computing non-Galerkin coarse-grid operators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;Utopia&#26631;&#31614;&#20998;&#24067;&#36924;&#36817;&#65288;ULDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#26631;&#31614;&#20998;&#24067;&#19982;&#39640;&#26031;&#26680;&#36827;&#34892;&#21367;&#31215;&#26469;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07682</link><description>&lt;p&gt;
&#36890;&#36807;Utopia&#26631;&#31614;&#20998;&#24067;&#36924;&#36817;&#23398;&#20064;&#20027;&#35266;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning Subjective Time-Series Data via Utopia Label Distribution Approximation. (arXiv:2307.07682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;Utopia&#26631;&#31614;&#20998;&#24067;&#36924;&#36817;&#65288;ULDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#26631;&#31614;&#20998;&#24067;&#19982;&#39640;&#26031;&#26680;&#36827;&#34892;&#21367;&#31215;&#26469;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20027;&#35266;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#65288;STR&#65289;&#20219;&#21153;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;STR&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#20559;&#20506;&#30340;&#27169;&#22411;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#21644;&#28145;&#24230;&#20272;&#35745;&#31561;&#19981;&#24179;&#34913;&#22238;&#24402;&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#38598;&#30340;&#20808;&#39564;&#26631;&#31614;&#20998;&#24067;&#26159;&#22343;&#21248;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;STR&#20219;&#21153;&#20013;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#26631;&#31614;&#20998;&#24067;&#24456;&#21487;&#33021;&#26082;&#19981;&#22343;&#21248;&#20063;&#19981;&#30456;&#21516;&#12290;&#36825;&#19968;&#29420;&#29305;&#30340;&#29305;&#24449;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26356;&#21512;&#29702;&#30340;&#20998;&#24067;&#20197;&#35757;&#32451;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;Utopia&#26631;&#31614;&#20998;&#24067;&#36924;&#36817;&#65288;ULDA&#65289;&#26041;&#27861;&#65292;&#20351;&#35757;&#32451;&#26631;&#31614;&#20998;&#24067;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#20294;&#26410;&#30693;&#30340;&#65288;utopia&#65289;&#26631;&#31614;&#20998;&#24067;&#12290;&#36825;&#23558;&#22686;&#24378;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ULDA&#39318;&#20808;&#23558;&#35757;&#32451;&#26631;&#31614;&#20998;&#24067;&#19982;&#39640;&#26031;&#26680;&#36827;&#34892;&#21367;&#31215;&#12290;&#21367;&#31215;&#21518;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#22788;&#65292;&#26679;&#26412;&#25968;&#37327;&#38656;&#35201;&#36890;&#36807;&#21152;&#26435;&#37325;&#26032;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subjective time-series regression (STR) tasks have gained increasing attention recently. However, most existing methods overlook the label distribution bias in STR data, which results in biased models. Emerging studies on imbalanced regression tasks, such as age estimation and depth estimation, hypothesize that the prior label distribution of the dataset is uniform. However, we observe that the label distributions of training and test sets in STR tasks are likely to be neither uniform nor identical. This distinct feature calls for new approaches that estimate more reasonable distributions to train a fair model. In this work, we propose Utopia Label Distribution Approximation (ULDA) for time-series data, which makes the training label distribution closer to real-world but unknown (utopia) label distribution. This would enhance the model's fairness. Specifically, ULDA first convolves the training label distribution by a Gaussian kernel. After convolution, the required sample quantity at 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#20135;&#21697;&#30340;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;ODD&#21442;&#25968;&#30340;&#32500;&#24230;&#21644;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#30830;&#23450;&#20102;&#20854;&#23545;&#31995;&#32479;&#23618;&#38754;&#30340;&#24433;&#21709;&#12290;&#31034;&#20363;&#20013;&#20197;&#39134;&#26426;&#39134;&#34892;&#21253;&#32447;&#20026;&#20363;&#35828;&#26126;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.07681</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#33322;&#31354;&#20135;&#21697;&#20013;&#30340;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products. (arXiv:2307.07681v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07681
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#20135;&#21697;&#30340;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;ODD&#21442;&#25968;&#30340;&#32500;&#24230;&#21644;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#30830;&#23450;&#20102;&#20854;&#23545;&#31995;&#32479;&#23618;&#38754;&#30340;&#24433;&#21709;&#12290;&#31034;&#20363;&#20013;&#20197;&#39134;&#26426;&#39134;&#34892;&#21253;&#32447;&#20026;&#20363;&#35828;&#26126;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33322;&#31354;&#20135;&#21697;&#30340;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODD&#65289;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29305;&#24449;&#21270;&#12290;&#19982;&#20854;&#20182;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#36947;&#36335;&#36710;&#36742;&#65289;&#30340;ODD&#24320;&#21457;&#22522;&#20110;&#22330;&#26223;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#25968;&#25454;&#30340;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#26126;&#30830;&#25429;&#25417;&#23450;&#20041;ODD&#30340;&#21442;&#25968;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#23545;ML-based&#24212;&#29992;&#22312;&#25805;&#20316;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#21516;&#26102;&#30830;&#23450;&#20102;&#23427;&#20204;&#23545;&#31995;&#32479;&#23618;&#38754;&#30340;&#30456;&#20851;&#24615;&#21644;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#25968;&#25454;&#31867;&#21035;&#22914;&#20309;&#26377;&#21161;&#20110;&#30830;&#23450;&#65306;&#39537;&#21160;ML&#27169;&#22411;&#65288;MLM&#65289;&#35774;&#35745;&#25152;&#38656;&#30340;&#35201;&#27714;&#65307;&#23545;MLM&#21644;&#31995;&#32479;&#23618;&#32423;&#30340;&#28508;&#22312;&#24433;&#21709;&#65307;&#21487;&#33021;&#38656;&#35201;&#30340;&#23398;&#20064;&#20445;&#35777;&#36807;&#31243;&#21644;&#31995;&#32479;&#26550;&#26500;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#39134;&#26426;&#39134;&#34892;&#21253;&#32447;&#30340;&#31034;&#20363;&#26469;&#35828;&#26126;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a first rigorous characterization of Operational Design Domains (ODDs) for Machine Learning (ML)-based aeronautical products. Unlike in other application sectors (such as self-driving road vehicles) where ODD development is scenario-based, our approach is data-centric: we propose the dimensions along which the parameters that define an ODD can be explicitly captured, together with a categorization of the data that ML-based applications can encounter in operation, whilst identifying their system-level relevance and impact. Specifically, we discuss how those data categories are useful to determine: the requirements necessary to drive the design of ML Models (MLMs); the potential effects on MLMs and higher levels of the system hierarchy; the learning assurance processes that may be needed, and system architectural considerations. We illustrate the underlying concepts with an example of an aircraft flight envelope.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.07679</link><description>&lt;p&gt;
&#21305;&#37197;&#36861;&#36394;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21363;&#36890;&#36807;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#36817;&#20284;&#30446;&#26631;&#20989;&#25968;&#30340;&#32431;&#36138;&#23146;&#31639;&#27861;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21253;&#21547;&#22312;&#23545;&#24212;&#20110;&#23383;&#20856;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#26102;&#65292;&#35768;&#22810;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#30740;&#31350;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#33719;&#24471;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#33719;&#24471;&#21305;&#37197;&#36861;&#36394;&#24615;&#33021;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#19979;&#30028;&#20197;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19978;&#30028;&#19981;&#33021;&#25913;&#36827;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#36138;&#23146;&#31639;&#27861;&#21464;&#20307;&#19981;&#21516;&#65292;&#25910;&#25947;&#36895;&#24230;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#30001;&#35299;&#26576;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20915;&#23450;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20219;&#24847;&#31243;&#24230;&#30340;&#25910;&#32553;&#37117;&#20250;&#25913;&#21892;&#21305;&#37197;&#36861;&#36394;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#22810;Agent&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#21046;&#20013;&#65292;&#26368;&#31361;&#20986;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;$\epsilon$-greedy&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#21516;&#26102;&#23384;&#22312;&#30340;&#28608;&#21169;&#22240;&#32032;&#12289;&#19978;&#19979;&#25991;&#21644;&#25439;&#22351;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.07675</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#33410;&#28857;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#21046;&#20013;Epoch-Greedy&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms. (arXiv:2307.07675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#22810;Agent&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#21046;&#20013;&#65292;&#26368;&#31361;&#20986;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;$\epsilon$-greedy&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#21516;&#26102;&#23384;&#22312;&#30340;&#28608;&#21169;&#22240;&#32032;&#12289;&#19978;&#19979;&#25991;&#21644;&#25439;&#22351;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#28857;&#20987;&#20184;&#36153;(Pay-Per-Click)&#25293;&#21334;&#36825;&#26679;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#21046;&#20013;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#19977;&#20010;&#25361;&#25112;&#65306;1)&#24341;&#23548;&#30495;&#23454;&#20986;&#20215;&#34892;&#20026;(&#28608;&#21169;&#22240;&#32032;)&#65292;2)&#22312;&#29992;&#25143;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#20010;&#24615;&#21270;(&#19978;&#19979;&#25991;)&#65292;3)&#35268;&#36991;&#28857;&#20987;&#27169;&#24335;&#20013;&#30340;&#25805;&#32437;(&#25439;&#22351;&#34892;&#20026;)&#12290;&#36807;&#21435;&#25991;&#29486;&#20013;&#27599;&#20010;&#25361;&#25112;&#37117;&#34987;&#29420;&#31435;&#30740;&#31350;&#36807;&#65307;&#28608;&#21169;&#22240;&#32032;&#24050;&#22312;&#19968;&#31995;&#21015;&#30740;&#31350;&#20013;&#24471;&#21040;&#35299;&#20915;&#65292;&#19978;&#19979;&#25991;&#38382;&#39064;&#24050;&#36890;&#36807;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#24471;&#21040;&#24191;&#27867;&#35299;&#20915;&#65292;&#32780;&#25439;&#22351;&#38382;&#39064;&#21017;&#36890;&#36807;&#26368;&#36817;&#30340;&#20851;&#20110;&#20855;&#26377;&#23545;&#25239;&#24615;&#25439;&#22351;&#30340;&#36172;&#21338;&#26426;&#21046;&#24037;&#20316;&#36827;&#34892;&#35752;&#35770;&#12290;&#30001;&#20110;&#36825;&#20123;&#25361;&#25112;&#21516;&#26102;&#23384;&#22312;&#65292;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#27599;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#20854;&#20182;&#25361;&#25112;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20379;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25152;&#26377;&#25361;&#25112;&#30340;&#31639;&#27861;&#65292;&#24182;&#31361;&#20986;&#36825;&#31181;&#32452;&#21512;&#20013;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#31361;&#20986;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;$\epsilon$-greedy&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#21516;&#26102;&#23384;&#22312;&#30340;&#28608;&#21169;&#22240;&#32032;&#12289;&#19978;&#19979;&#25991;&#21644;&#25439;&#22351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient learning in multi-armed bandit mechanisms such as pay-per-click (PPC) auctions typically involves three challenges: 1) inducing truthful bidding behavior (incentives), 2) using personalization in the users (context), and 3) circumventing manipulations in click patterns (corruptions). Each of these challenges has been studied orthogonally in the literature; incentives have been addressed by a line of work on truthful multi-armed bandit mechanisms, context has been extensively tackled by contextual bandit algorithms, while corruptions have been discussed via a recent line of work on bandits with adversarial corruptions. Since these challenges co-exist, it is important to understand the robustness of each of these approaches in addressing the other challenges, provide algorithms that can handle all simultaneously, and highlight inherent limitations in this combination. In this work, we show that the most prominent contextual bandit algorithm, $\epsilon$-greedy can be extended to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07674</link><description>&lt;p&gt;
&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#27169;&#24335;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;GFlowNets&#20013;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#21160;&#20316;&#20174;&#32780;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22238;&#25253;$R&#65288;x&#65289;$&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;GFlowNets&#26159;&#19968;&#31867;&#29305;&#27530;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#20110;$R&#65288;x&#65289;$&#30340;&#27010;&#29575;&#37319;&#26679;&#31574;&#30053;&#65292;&#20174;&#31163;&#25955;&#38598;&#21512;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#20505;&#36873;&#26679;&#26412;$x$&#12290;&#19982;&#20256;&#32479;&#30340;RL&#31639;&#27861;&#30456;&#27604;&#65292;GFlowNets&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27169;&#24335;&#21457;&#29616;&#33021;&#21147;&#65292;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#21644;&#32452;&#21512;&#25628;&#32034;&#31561;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GFlowNets&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#31639;&#27861;&#31867;&#21035;&#65292;&#35768;&#22810;&#22312;RL&#20013;&#26377;&#29992;&#30340;&#25216;&#26415;&#23578;&#26410;&#19982;&#20854;&#20851;&#32852;&#36215;&#26469;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;GFlowNets&#20013;&#21033;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#21508;&#31181;&#22238;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#25216;&#26415;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;&#27169;&#24335;&#21457;&#29616;&#36895;&#24230;&#21644;&#21457;&#29616;&#30340;&#27169;&#24335;&#36136;&#37327;&#12290;&#22312;Hypergrid&#27169;&#25311;&#29615;&#22659;&#21644;&#20998;&#23376;&#21512;&#25104;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39318;&#20808;&#23637;&#31034;&#20102;&#21333;&#29420;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.07670</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning. (arXiv:2307.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#22312;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39318;&#20808;&#23637;&#31034;&#20102;&#21333;&#29420;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#20102;&#35299;&#23545;MARL&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#24212;&#29992;&#35813;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;MARL&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#19968;&#20010;&#22806;&#37096;&#25915;&#20987;&#32773;&#65292;&#20182;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25509;&#25910;&#21040;&#22870;&#21169;&#20043;&#21069;&#20462;&#25913;&#22870;&#21169;&#65292;&#25110;&#22312;&#29615;&#22659;&#25509;&#25910;&#21040;&#21160;&#20316;&#20043;&#21069;&#25805;&#32437;&#21160;&#20316;&#12290;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#26159;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#24341;&#23548;&#21040;&#30446;&#26631;&#31574;&#30053;&#65292;&#25110;&#22312;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26576;&#20010;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#19979;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21453;&#39304;&#21644;&#21160;&#20316;&#30340;&#25805;&#32437;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#21482;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#25915;&#20987;&#21644;&#21482;&#36827;&#34892;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21516;&#26102;&#36827;&#34892;&#21160;&#20316;&#27745;&#26579;&#21644;&#22870;&#21169;&#27745;&#26579;&#30340;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#28151;&#21512;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#39640;&#25928;&#22320;&#25915;&#20987;MARL&#26234;&#33021;&#20307;&#65292;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.07666</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#30830;&#23450;&#24615;&#38754;&#21069;&#25214;&#21040;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#26426;&#22120;&#19981;&#24635;&#26159;&#25353;&#29031;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#36827;&#34892;&#65292;&#32780;&#26159;&#20197;&#27010;&#29575;$1-\rho$&#25191;&#34892;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#65292;&#20197;&#27010;&#29575;$\rho$&#25191;&#34892;&#26367;&#20195;&#30340;&#23545;&#25239;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#20854;&#30340;&#34892;&#21160;&#40065;&#26834;&#36125;&#23572;&#26364;&#26368;&#20248;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35777;&#20070;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;(ARRLC)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ARRLC&#20248;&#20110;&#38750;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#40065;&#26834;TD&#31639;&#27861;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#39640;&#36895;&#20844;&#36335;&#32593;&#32476;&#32467;&#26500;&#22312;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07657</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;&#65306;&#23545;&#32593;&#32476;&#32467;&#26500;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine learning for option pricing: an empirical investigation of network architectures. (arXiv:2307.07657v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07657
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#39640;&#36895;&#20844;&#36335;&#32593;&#32476;&#32467;&#26500;&#22312;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#36866;&#24403;&#30340;&#36755;&#20837;&#25968;&#25454;&#65288;&#27169;&#22411;&#21442;&#25968;&#65289;&#21644;&#30456;&#24212;&#36755;&#20986;&#25968;&#25454;&#65288;&#26399;&#26435;&#20215;&#26684;&#25110;&#38544;&#21547;&#27874;&#21160;&#29575;&#65289;&#26469;&#23398;&#20064;&#26399;&#26435;&#20215;&#26684;&#25110;&#38544;&#21547;&#27874;&#21160;&#29575;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#30456;&#20851;&#25991;&#29486;&#37117;&#20351;&#29992;&#65288;&#26222;&#36890;&#30340;&#65289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#36830;&#25509;&#29992;&#20110;&#23398;&#20064;&#23558;&#36755;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#21644;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26469;&#25506;&#31350;&#32593;&#32476;&#32467;&#26500;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31934;&#30830;&#24230;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;Black-Scholes&#21644;Heston&#27169;&#22411;&#65292;&#24191;&#20041;&#39640;&#36895;&#20844;&#36335;&#32593;&#32476;&#32467;&#26500;&#30456;&#36739;&#20110;&#20854;&#20182;&#21464;&#20307;&#22312;&#22343;&#26041;&#35823;&#24046;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;&#35745;&#31639;&#38544;&#21547;&#27874;&#21160;&#29575;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that for option pricing problems, where we focus on the Black--Scholes and the Heston model, the generalized highway network architecture outperforms all other variants, when considering the mean squared error and the training time as criteria. Moreover, for the computation of the implied volatility, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIGEST&#30340;&#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#25955;&#23398;&#20064;&#20013;&#36739;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07652</link><description>&lt;p&gt;
DIGEST: &#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#23398;&#20064;&#19982;&#26412;&#22320;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIGEST&#30340;&#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#25955;&#23398;&#20064;&#20013;&#36739;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#24191;&#27867;&#32771;&#34385;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#26159;Gossip&#21644;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#23398;&#20064;&#12290;Gossip&#31639;&#27861;&#65288;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#65289;&#23384;&#22312;&#36739;&#39640;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#32780;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#23398;&#20064;&#21017;&#20250;&#22686;&#21152;&#25910;&#25947;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#36890;&#20449;&#26377;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;DIGEST&#65292;&#21033;&#29992;&#20102;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;DIGEST&#26159;&#19968;&#20010;&#22522;&#20110;&#26412;&#22320;SGD&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#25955;&#31639;&#27861;&#65292;&#23427;&#26368;&#21021;&#26159;&#20026;&#36890;&#20449;&#39640;&#25928;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#27969;&#21644;&#22810;&#27969;&#30340;DIGEST&#65292;&#24403;&#27969;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#36890;&#20449;&#24320;&#38144;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#24182;&#19988;&#26377;&#19968;&#31181;&#25910;&#25947;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#26435;&#34913;&#21487;&#20197;&#21033;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#27969;&#21644;&#22810;&#27969;DIGEST&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#31181;&#31639;&#27861;&#37117;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima
&lt;/p&gt;</description></item><item><title>SALC&#26159;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#36866;&#24212;&#26102;&#21464;&#23460;&#20869;&#29615;&#22659;&#65292;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07650</link><description>&lt;p&gt;
SALC&#65306;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#29992;&#20110;&#26102;&#21464;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization. (arXiv:2307.07650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07650
&lt;/p&gt;
&lt;p&gt;
SALC&#26159;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#30340;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#36866;&#24212;&#26102;&#21464;&#23460;&#20869;&#29615;&#22659;&#65292;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#32447;&#23460;&#20869;&#23450;&#20301;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#20351;&#29992;&#20174;WiFi&#35775;&#38382;&#28857;&#65288;AP&#65289;&#33719;&#21462;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#24314;&#31435;&#25351;&#32441;&#25968;&#25454;&#24211;&#26159;&#23460;&#20869;&#23450;&#20301;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#23545;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#30340;&#26102;&#21464;&#38382;&#39064;&#30740;&#31350;&#19981;&#20805;&#20998;&#12290;&#19982;&#20256;&#32479;&#30340;&#38745;&#24577;&#25351;&#32441;&#30456;&#27604;&#65292;&#21160;&#24577;&#37325;&#24314;&#30340;&#25968;&#25454;&#24211;&#21487;&#20197;&#36866;&#24212;&#39640;&#24230;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#20301;&#20934;&#30830;&#24615;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#21464;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#36741;&#21161;&#23398;&#20064;&#32858;&#31867;&#23450;&#20301;&#65288;SALC&#65289;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;RSS&#23548;&#21521;&#30340;&#22320;&#22270;&#36741;&#21161;&#32858;&#31867;&#65288;ROMAC&#65289;&#12289;&#22522;&#20110;&#32858;&#31867;&#30340;&#22312;&#32447;&#25968;&#25454;&#24211;&#24314;&#31435;&#65288;CODE&#65289;&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#23450;&#20301;&#20272;&#35745;&#65288;CsLE&#65289;&#12290;SALC&#26041;&#26696;&#21516;&#26102;&#32771;&#34385;&#20102;&#22522;&#20110;&#39592;&#26550;&#26368;&#30701;&#36335;&#24452;&#65288;SSP&#65289;&#21644;&#21442;&#32771;&#28857;&#65288;RPs&#65289;&#20043;&#38388;&#30340;&#26102;&#21464;RSS&#27979;&#37327;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamicallyreconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs)
&lt;/p&gt;</description></item><item><title>DistTGL&#26159;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;GPU&#38598;&#32676;&#19978;&#35757;&#32451;&#20869;&#23384;&#21270;TGNN&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07649</link><description>&lt;p&gt;
DistTGL&#65306;&#22522;&#20110;&#20998;&#24067;&#24335;&#20869;&#23384;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training. (arXiv:2307.07649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07649
&lt;/p&gt;
&lt;p&gt;
DistTGL&#26159;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;GPU&#38598;&#32676;&#19978;&#35757;&#32451;&#20869;&#23384;&#21270;TGNN&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23384;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#33410;&#28857;&#20869;&#23384;&#26356;&#36866;&#21512;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20197;&#25429;&#25417;&#22270;&#20107;&#20214;&#20013;&#30340;&#26356;&#22810;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#25152;&#26377;&#35757;&#32451;&#22120;&#20043;&#38388;&#21516;&#27493;&#32500;&#25252;&#12290;&#22240;&#27492;&#65292;&#24403;&#25193;&#23637;&#21040;&#22810;&#20010;GPU&#26102;&#65292;&#29616;&#26377;&#26694;&#26550;&#20250;&#20986;&#29616;&#31934;&#24230;&#25439;&#22833;&#30340;&#38382;&#39064;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#21516;&#27493;&#33410;&#28857;&#20869;&#23384;&#30340;&#24040;&#22823;&#24320;&#38144;&#20351;&#24471;&#23558;&#20854;&#37096;&#32626;&#21040;&#20998;&#24067;&#24335;GPU&#38598;&#32676;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistTGL - &#19968;&#31181;&#22312;&#20998;&#24067;&#24335;GPU&#38598;&#32676;&#19978;&#35757;&#32451;&#22522;&#20110;&#20869;&#23384;&#30340;TGNN&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;DistTGL&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26377;&#19977;&#20010;&#25913;&#36827;&#65306;&#22686;&#24378;&#30340;TGNN&#27169;&#22411;&#65292;&#26032;&#39062;&#30340;&#35757;&#32451;&#31639;&#27861;&#21644;&#20248;&#21270;&#30340;&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DistTGL&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#30340;&#25910;&#25947;&#21152;&#36895;&#65292;&#31934;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#21333;&#26426;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#65292;&#35757;&#32451;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;10.17&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;MBI&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#35745;&#31639;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24490;&#29615;&#27880;&#24847;&#27169;&#22411;&#65288;RAM&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23384;&#20648;&#38190;&#20540;&#23545;&#30340;&#34920;&#26469;&#36991;&#20813;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.07631</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#22823;&#23567;&#19981;&#21487;&#30693;&#12289;&#26080;&#38656;&#35745;&#31639;&#12289;&#22522;&#20110;&#35760;&#24518;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning. (arXiv:2307.07631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;MBI&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#35745;&#31639;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24490;&#29615;&#27880;&#24847;&#27169;&#22411;&#65288;RAM&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#23384;&#20648;&#38190;&#20540;&#23545;&#30340;&#34920;&#26469;&#36991;&#20813;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#22914;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21644;&#21442;&#25968;&#25968;&#37327;&#20063;&#22686;&#21152;&#65292;&#20351;&#24471;&#38590;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;MBI&#65289;&#65292;&#23427;&#19981;&#38656;&#35201;&#35745;&#31639;&#65292;&#21482;&#38656;&#35201;&#26597;&#25214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#24490;&#29615;&#27880;&#24847;&#27169;&#22411;&#65288;RAM&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#23567;&#31383;&#21475;&#30340;&#36755;&#20837;&#22495;&#65288;glance&#65289;&#22312;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#19988;&#26469;&#33258;&#22810;&#20010;glance&#30340;&#36755;&#20986;&#36890;&#36807;&#38544;&#34255;&#21521;&#37327;&#32452;&#21512;&#26469;&#30830;&#23450;&#38382;&#39064;&#30340;&#25972;&#20307;&#20998;&#31867;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;glance&#30340;&#20302;&#32500;&#24615;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#23558;&#30001;&#21253;&#21547;glance&#20301;&#32622;&#12289;&#34917;&#19969;&#21521;&#37327;&#31561;&#30340;&#38190;&#20540;&#23545;&#23384;&#20648;&#22312;&#19968;&#24352;&#34920;&#20013;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#34920;&#26469;&#35835;&#21462;&#38190;&#20540;&#23545;&#65292;&#21487;&#20197;&#36991;&#20813;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of deep neural networks has significantly improved various tasks, such as image and speech recognition. However, as the complexity of these models increases, so does the computational cost and the number of parameters, making it difficult to deploy them on resource-constrained devices. This paper proposes a novel memorization-based inference (MBI) that is compute free and only requires lookups. Specifically, our work capitalizes on the inference mechanism of the recurrent attention model (RAM), where only a small window of input domain (glimpse) is processed in a one time step, and the outputs from multiple glimpses are combined through a hidden vector to determine the overall classification output of the problem. By leveraging the low-dimensionality of glimpse, our inference procedure stores key value pairs comprising of glimpse location, patch vector, etc. in a table. The computations are obviated during inference by utilizing the table to read out key-value pai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#23454;&#20307;&#20197;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.07620</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#23454;&#29616;&#36890;&#29992;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#25209;&#27425;&#24230;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#23454;&#20307;&#20197;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#24179;&#22343;&#27719;&#32858;&#65288;GAP&#65289;&#26159;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#20013;&#24120;&#29992;&#30340;&#32452;&#20214;&#65292;&#29992;&#20110;&#32858;&#21512;&#29305;&#24449;&#12290;&#20854;&#26377;&#25928;&#24615;&#36890;&#24120;&#24402;&#22240;&#20110;&#23558;&#27599;&#20010;&#29305;&#24449;&#21521;&#37327;&#35270;&#20026;&#29420;&#31435;&#30340;&#35821;&#20041;&#23454;&#20307;&#65292;&#24182;&#23558;GAP&#35270;&#20026;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#32463;&#36807;&#35777;&#23454;&#65292;&#20294;&#36825;&#31181;&#35299;&#37322;&#22312;&#23398;&#20064;&#21487;&#29992;&#20110;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36890;&#29992;&#23454;&#20307;&#30340;&#31639;&#27861;&#24847;&#20041;&#19978;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#26159;DML&#30340;&#20851;&#38190;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GAP&#23450;&#20041;&#20026;&#21487;&#23398;&#20064;&#21407;&#22411;&#30340;&#20984;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22411;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#23558;&#32447;&#24615;&#39044;&#27979;&#22120;&#25311;&#21512;&#21040;&#19968;&#25209;&#26679;&#26412;&#30340;&#36882;&#24402;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#20004;&#20010;&#19981;&#30456;&#20132;&#31867;&#21035;&#30340;&#25209;&#27425;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36866;&#24212;&#20110;&#21478;&#19968;&#20010;&#25209;&#27425;&#30340;&#21407;&#22411;&#26469;&#35268;&#33539;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;4&#20010;&#28909;&#38376;DML&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#21644;&#24377;&#24615;&#20108;&#20803;&#27491;&#21017;&#21270;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26377;&#25928;&#22320;&#20998;&#35299;&#24067;&#23572;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#21644;&#20934;&#30830;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#32467;&#26524;&#26131;&#20110;&#35299;&#37322;&#21644;&#35821;&#20041;&#26377;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.07615</link><description>&lt;p&gt;
&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26377;&#25928;&#22320;&#20998;&#35299;&#24067;&#23572;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent. (arXiv:2307.07615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07615
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#21644;&#24377;&#24615;&#20108;&#20803;&#27491;&#21017;&#21270;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26377;&#25928;&#22320;&#20998;&#35299;&#24067;&#23572;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#21644;&#20934;&#30830;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#32467;&#26524;&#26131;&#20110;&#35299;&#37322;&#21644;&#35821;&#20041;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#22312;&#24067;&#23572;&#25968;&#25454;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#65288;BMF&#65289;&#20351;&#29992;&#24067;&#23572;&#20195;&#25968;&#23558;&#36755;&#20837;&#20998;&#35299;&#20026;&#20302;&#31209;&#24067;&#23572;&#22240;&#23376;&#30697;&#38453;&#12290;&#36825;&#20123;&#30697;&#38453;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;NP&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;BMF&#30340;&#26032;&#22411;&#24377;&#24615;&#20108;&#20803;&#27491;&#21017;&#21270;&#22120;&#65292;&#20174;&#20013;&#25512;&#23548;&#20986;&#19968;&#31181;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65306;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#24555;&#36895;&#25910;&#25947;&#65292;&#31934;&#30830;&#24674;&#22797;&#20102;&#30495;&#23454;&#20540;&#65292;&#24182;&#20934;&#30830;&#20272;&#35745;&#20102;&#27169;&#25311;&#31209;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#22312;&#21484;&#22238;&#29575;&#12289;&#25439;&#22833;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#19988;&#26469;&#33258;&#21307;&#23398;&#39046;&#22495;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#26131;&#20110;&#35299;&#37322;&#21644;&#35821;&#20041;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the interpretability problem of NMF on Boolean data, Boolean Matrix Factorization (BMF) uses Boolean algebra to decompose the input into low-rank Boolean factor matrices. These matrices are highly interpretable and very useful in practice, but they come at the high computational cost of solving an NP-hard combinatorial optimization problem. To reduce the computational burden, we propose to relax BMF continuously using a novel elastic-binary regularizer, from which we derive a proximal gradient algorithm. Through an extensive set of experiments, we demonstrate that our method works well in practice: On synthetic data, we show that it converges quickly, recovers the ground truth precisely, and estimates the simulated rank exactly. On real-world data, we improve upon the state of the art in recall, loss, and runtime, and a case study from the medical domain confirms that our results are easily interpretable and semantically meaningful.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#32447;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#38382;&#39064;&#30340;&#35268;&#27169;&#25193;&#23637;&#38590;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#35770;&#22363;&#24086;&#23376;&#30340;&#32039;&#24613;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#32473;&#25945;&#24072;&#27880;&#24847;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#39044;&#27979;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07614</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35752;&#35770;&#35770;&#22363;&#24086;&#23376;&#32039;&#24613;&#24615;&#30340;&#36890;&#29992;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#32447;&#35838;&#31243;&#35752;&#35770;&#35770;&#22363;&#38382;&#39064;&#30340;&#35268;&#27169;&#25193;&#23637;&#38590;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#33258;&#21160;&#30830;&#23450;&#35770;&#22363;&#24086;&#23376;&#30340;&#32039;&#24613;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#32473;&#25945;&#24072;&#27880;&#24847;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#39044;&#27979;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#32423;&#21035;&#65292;&#23454;&#29616;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#22312;&#32447;&#35838;&#31243;&#65288;&#22914;MOOC&#65289;&#30340;&#23398;&#29983;&#20250;&#20351;&#29992;&#35838;&#31243;&#30340;&#35752;&#35770;&#35770;&#22363;&#65292;&#22312;&#36935;&#21040;&#38382;&#39064;&#26102;&#21521;&#25945;&#24072;&#25552;&#38382;&#25110;&#23547;&#27714;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#32771;&#34385;&#27599;&#26465;&#28040;&#24687;&#30340;&#26102;&#38388;&#65292;&#38405;&#35835;&#21644;&#22238;&#22797;&#23398;&#29983;&#30340;&#38382;&#39064;&#38590;&#20197;&#25193;&#23637;&#12290;&#32467;&#26524;&#65292;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#21487;&#33021;&#24471;&#19981;&#21040;&#35299;&#20915;&#65292;&#23398;&#29983;&#21487;&#33021;&#22833;&#21435;&#32487;&#32493;&#23398;&#20064;&#30340;&#21160;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33021;&#22815;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#35770;&#22363;&#24086;&#23376;&#32039;&#24613;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#20415;&#21487;&#20197;&#23558;&#36825;&#20123;&#24086;&#23376;&#24102;&#32473;&#25945;&#24072;&#30340;&#27880;&#24847;&#12290;&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#19981;&#20165;&#20108;&#36827;&#21046;&#20915;&#31574;&#20999;&#21106;&#28857;&#65292;&#32780;&#19988;&#36824;&#39044;&#27979;&#20102;&#24086;&#23376;&#22312;7&#20998;&#21046;&#32039;&#24613;&#31243;&#24230;&#19978;&#30340;&#32423;&#21035;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#23486;&#22805;&#27861;&#23612;&#20122;&#22823;&#23398;&#30340;MOOCs&#30340;3503&#26465;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#20132;&#21449;&#39564;&#35777;&#20102;&#22810;&#20010;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#30830;&#23450;&#25105;&#20204;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;29,604&#26465;&#24086;&#23376;&#30340;&#20808;&#21069;&#21457;&#34920;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts fro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22797;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#26368;&#20302;&#22797;&#26434;&#24230;&#19979;&#30028;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;IPG&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07605</link><description>&lt;p&gt;
&#29992;&#20110;&#20223;&#23556;&#32422;&#26463;&#32452;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#65306;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods. (arXiv:2307.07605v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07605
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22797;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#26368;&#20302;&#22797;&#26434;&#24230;&#19979;&#30028;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;IPG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#20851;&#20110;&#19968;&#38454;&#26041;&#27861;(FOMs)&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20855;&#26377;&#32447;&#24615;&#21644;/&#25110;&#38750;&#32447;&#24615;&#20989;&#25968;&#32422;&#26463;&#30340;&#22797;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#19978;&#12290;&#24050;&#32463;&#20026;&#36825;&#20123;&#26041;&#27861;&#24314;&#31435;&#20102;&#19978;&#38480;(&#25110;&#26368;&#22351;&#24773;&#20917;)&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#30340;&#20809;&#28369;&#38750;&#20984;&#24773;&#20917;&#22806;&#65292;&#23427;&#20204;&#30340;&#26368;&#20248;&#24615;&#20960;&#20046;&#19981;&#33021;&#24471;&#21040;&#20445;&#35777;&#65292;&#22240;&#20026;&#21482;&#26377;&#24456;&#23569;&#30340;&#19979;&#30028;&#26159;&#24050;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24314;&#31435;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#20855;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#22797;&#21512;&#38750;&#20984;&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;FOMs&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#12290;&#20551;&#35774;&#20004;&#20010;&#19981;&#21516;&#30340;&#19968;&#38454;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FOMs&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65288;&#21450;&#20854;&#25913;&#20889;&#65289;&#30340;(&#25509;&#36817;)$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;&#32771;&#34385;&#38382;&#39064;&#31867;&#21035;&#30340;&#20219;&#20309;&#32473;&#23450;&#23481;&#24046;$\epsilon&gt;0$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#39044;&#27979;&#26426;&#20013;&#26356;&#23485;&#26494;&#30340;&#19968;&#20010;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#30340;&#36817;&#31471;&#26799;&#24230;(IPG)&#26041;&#27861;&#12290;&#35813;IPG&#30340;&#39044;&#35328;&#26426;&#22797;&#26434;&#24230;&#65292;&#29992;&#20110;&#25214;&#21040;&#19968;&#20010;(&#25509;&#36817;)&#12290;epsilon-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent studies on first-order methods (FOMs) focus on \emph{composite non-convex non-smooth} optimization with linear and/or nonlinear function constraints. Upper (or worst-case) complexity bounds have been established for these methods. However, little can be claimed about their optimality as no lower bound is known, except for a few special \emph{smooth non-convex} cases. In this paper, we make the first attempt to establish lower complexity bounds of FOMs for solving a class of composite non-convex non-smooth optimization with linear constraints. Assuming two different first-order oracles, we establish lower complexity bounds of FOMs to produce a (near) $\epsilon$-stationary point of a problem (and its reformulation) in the considered problem class, for any given tolerance $\epsilon&gt;0$. In addition, we present an inexact proximal gradient (IPG) method by using the more relaxed one of the two assumed first-order oracles. The oracle complexity of the proposed IPG, to find a (near
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.</title><link>http://arxiv.org/abs/2307.07604</link><description>&lt;p&gt;
&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#32534;&#30721;&#26041;&#27861;&#26159;&#26368;&#24191;&#27867;&#29992;&#20110;&#30830;&#23450;&#32422;&#26463;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25110;&#38169;&#35823;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#36866;&#24403;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#25105;&#20204;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#19979;&#30028;&#20063;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35823;&#24046;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#21464;&#24471;&#26080;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22635;&#20805;&#21644;&#32622;&#25442;&#36716;&#25442;&#24212;&#29992;&#20110;&#25351;&#32441;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22256;&#38590;&#23454;&#20363;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25552;&#20379;&#26032;&#30340;&#19979;&#30028;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;1. &#20302;&#20934;&#30830;&#24230;&#24773;&#26223;&#19979;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#30340;&#32039;&#23494;&#19979;&#30028;&#65292;&#36825;&#23588;&#20854;&#24847;&#21619;&#30528;&#26032;&#30340;&#31169;&#26377;1&#31751;&#38382;&#39064;&#30340;&#19979;&#30028; 2. &#36817;&#20284;k
&lt;/p&gt;
&lt;p&gt;
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33021;&#37327;&#24046;&#24322;&#35757;&#32451;&#31163;&#25955;&#33021;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#28857;&#21450;&#20854;&#25200;&#21160;&#23545;&#24212;&#28857;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#20026;&#21508;&#31181;&#25200;&#21160;&#36807;&#31243;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07595</link><description>&lt;p&gt;
&#29992;&#33021;&#37327;&#24046;&#24322;&#35757;&#32451;&#31163;&#25955;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Discrete Energy-Based Models with Energy Discrepancy. (arXiv:2307.07595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07595
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33021;&#37327;&#24046;&#24322;&#35757;&#32451;&#31163;&#25955;&#33021;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#28857;&#21450;&#20854;&#25200;&#21160;&#23545;&#24212;&#28857;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#23454;&#29616;&#65292;&#33021;&#22815;&#20026;&#21508;&#31181;&#25200;&#21160;&#36807;&#31243;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#25955;&#31354;&#38388;&#19978;&#35757;&#32451;&#33021;&#37327;&#27169;&#22411;&#65288;EBMs&#65289;&#20805;&#28385;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#36825;&#26679;&#30340;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33021;&#37327;&#24046;&#24322;&#65288;ED&#65289;&#26469;&#35757;&#32451;&#31163;&#25955;EBMs&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#21482;&#38656;&#35201;&#35780;&#20272;&#25968;&#25454;&#28857;&#21450;&#20854;&#25200;&#21160;&#23545;&#24212;&#28857;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#22240;&#27492;&#19981;&#20381;&#36182;&#20110;&#20687;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#36825;&#26679;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#33021;&#37327;&#24046;&#24322;&#20026;&#19968;&#31867;&#24191;&#27867;&#30340;&#25200;&#21160;&#36807;&#31243;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#25200;&#21160;&#65306;&#22522;&#20110;&#20271;&#21162;&#21033;&#22122;&#22768;&#30340;&#25200;&#21160;&#65292;&#22522;&#20110;&#30830;&#23450;&#24615;&#21464;&#25442;&#30340;&#25200;&#21160;&#65292;&#20197;&#21450;&#22522;&#20110;&#37051;&#22495;&#32467;&#26500;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#26230;&#26684;&#20234;&#36763;&#27169;&#22411;&#12289;&#20108;&#20540;&#21512;&#25104;&#25968;&#25454;&#21644;&#31163;&#25955;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) on discrete spaces is challenging because sampling over such spaces can be difficult. We propose to train discrete EBMs with energy discrepancy (ED), a novel type of contrastive loss functional which only requires the evaluation of the energy function at data points and their perturbed counter parts, thus not relying on sampling strategies like Markov chain Monte Carlo (MCMC). Energy discrepancy offers theoretical guarantees for a broad class of perturbation processes of which we investigate three types: perturbations based on Bernoulli noise, based on deterministic transforms, and based on neighbourhood structures. We demonstrate their relative performance on lattice Ising models, binary synthetic data, and discrete image data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20266;&#26680;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20998;&#26512;&#21644;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#35745;&#21010;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24182;&#21457;&#22810;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.07575</link><description>&lt;p&gt;
&#19968;&#20010;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks. (arXiv:2307.07575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20266;&#26680;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20998;&#26512;&#21644;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#35745;&#21010;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24182;&#21457;&#22810;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;&#21253;&#25324;&#29983;&#29289;&#21644;&#20154;&#24037;&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#23427;&#20204;&#22914;&#20309;&#23398;&#20064;&#26469;&#34920;&#31034;&#21644;&#22788;&#29702;&#36755;&#20837;&#20449;&#24687;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#34920;&#31034;&#21487;&#33021;&#36866;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#35782;&#21035;&#21644;&#29702;&#35299;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#26159;&#29702;&#35299;&#21644;&#35774;&#35745;&#26377;&#29992;&#32593;&#32476;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#26680;&#30340;&#24037;&#20855;&#65292;&#20165;&#22522;&#20110;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#21644;&#35757;&#32451;&#35745;&#21010;&#26469;&#20998;&#26512;&#21644;&#39044;&#27979;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#31616;&#21333;&#27979;&#35797;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#28982;&#21518;&#22312;&#20851;&#20110;&#34920;&#31034;&#23398;&#20064;&#23545;&#39034;&#24207;&#21333;&#20219;&#21153;&#19982;&#24182;&#21457;&#22810;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#30340;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#35268;&#27169;&#21644;&#35757;&#32451;&#35745;&#21010;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#24182;&#21457;&#22810;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key property of neural networks (both biological and artificial) is how they learn to represent and manipulate input information in order to solve a task. Different types of representations may be suited to different types of tasks, making identifying and understanding learned representations a critical part of understanding and designing useful networks. In this paper, we introduce a new pseudo-kernel based tool for analyzing and predicting learned representations, based only on the initial conditions of the network and the training curriculum. We validate the method on a simple test case, before demonstrating its use on a question about the effects of representational learning on sequential single versus concurrent multitask performance. We show that our method can be used to predict the effects of the scale of weight initialization and training curriculum on representational learning and downstream concurrent multitasking performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.07572</link><description>&lt;p&gt;
Harpa: &#39640;&#36895;&#29575;&#19979;&#30340;&#30456;&#20301;&#20851;&#32852;&#19982;&#36208;&#26102;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#20851;&#32852;&#26159;&#26681;&#25454;&#20854;&#36215;&#28304;&#22320;&#38663;&#20998;&#32452;&#22320;&#38663;&#27874;&#21040;&#36798;&#30340;&#20219;&#21153;&#12290;&#23427;&#26159;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#20123;&#20107;&#20214;&#25658;&#24102;&#26377;&#20851;&#22320;&#38663;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#24120;&#20551;&#23450;&#19981;&#20934;&#30830;&#30340;&#27874;&#36895;&#27169;&#22411;&#19979;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#20851;&#32852;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21457;&#29983;&#29575;&#36739;&#20302;&#19988;&#23481;&#26131;&#20851;&#32852;&#30340;&#36739;&#22823;&#20107;&#20214;&#65292;&#23613;&#31649;&#24494;&#22320;&#38663;&#27963;&#21160;&#25552;&#20379;&#20102;&#20117;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#20197;&#27604;&#20197;&#21069;&#25253;&#21578;&#30340;&#26356;&#39640;&#30340;&#36895;&#29575;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Harpa&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#39318;&#20808;&#35299;&#20915;&#32852;&#21512;&#26102;&#31354;&#28304;&#23450;&#20301;&#21644;&#27874;&#36895;&#24674;&#22797;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20301;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#39044;&#27979;&#36825;&#19968;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#30028;&#30452;&#25509;&#23398;&#20064;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#36793;&#32536;&#21270;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29609;&#20855;&#20363;&#23376;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07568</link><description>&lt;p&gt;
&#21464;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Variational Prediction. (arXiv:2307.07568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#39044;&#27979;&#36825;&#19968;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#30028;&#30452;&#25509;&#23398;&#20064;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#36793;&#32536;&#21270;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29609;&#20855;&#20363;&#23376;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#27604;&#26368;&#22823;&#20284;&#28982;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20063;&#20276;&#38543;&#30528;&#35745;&#31639;&#25104;&#26412;&#12290;&#35745;&#31639;&#21518;&#39564;&#36890;&#24120;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#32780;&#23558;&#21518;&#39564;&#36793;&#32536;&#21270;&#24418;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21464;&#20998;&#39044;&#27979;&#65292;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#30028;&#30452;&#25509;&#23398;&#20064;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27979;&#35797;&#26102;&#38388;&#36793;&#32536;&#21270;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#33391;&#22909;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35828;&#26126;&#24615;&#30340;&#29609;&#20855;&#20363;&#23376;&#19978;&#28436;&#31034;&#20102;&#21464;&#20998;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference offers benefits over maximum likelihood, but it also comes with computational costs. Computing the posterior is typically intractable, as is marginalizing that posterior to form the posterior predictive distribution. In this paper, we present variational prediction, a technique for directly learning a variational approximation to the posterior predictive distribution using a variational bound. This approach can provide good predictive distributions without test time marginalization costs. We demonstrate Variational Prediction on an illustrative toy example.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#24515;&#25615;&#22270;&#25104;&#20998;&#20013;&#39044;&#27979;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;</title><link>http://arxiv.org/abs/2307.07566</link><description>&lt;p&gt;
&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#25104;&#20998;&#37325;&#24314;&#19977;&#36724;&#24515;&#25615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#30340;&#24515;&#25615;&#22270;&#25104;&#20998;&#20013;&#39044;&#27979;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20174;&#21491;&#21040;&#24038;&#21644;&#22836;&#21040;&#33050;&#26041;&#21521;&#30340;&#24515;&#25615;&#22270;&#65288;SCG&#65289;&#20449;&#21495;&#22312;&#32972;&#33145;&#26041;&#21521;&#30340;&#20449;&#21495;&#65288;SCG_x&#21644;SCG_y&#65289;&#12290;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;15&#21517;&#20581;&#24247;&#25104;&#24180;&#20154;&#12290;&#20351;&#29992;&#19977;&#36724;&#21152;&#36895;&#24230;&#35745;&#23558;SCG&#20449;&#21495;&#35760;&#24405;&#22312;&#27599;&#20010;&#34987;&#35797;&#30340;&#33016;&#37096;&#19978;&#12290;&#28982;&#21518;&#20351;&#29992;&#24515;&#30005;&#22270;R&#27874;&#36827;&#34892;&#20998;&#27573;&#65292;&#23558;&#29255;&#27573;&#36827;&#34892;&#38477;&#37319;&#26679;&#12289;&#24402;&#19968;&#21270;&#21644;&#38646;&#23621;&#20013;&#22788;&#29702;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35757;&#32451;&#21644;&#39564;&#35777;&#19968;&#20010;&#20855;&#26377;&#20004;&#23618;&#21644;&#19968;&#20010;&#36991;&#20813;&#36807;&#25311;&#21512;&#30340;dropout&#23618;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#12290;&#32593;&#32476;&#20197;100&#20010;&#26102;&#38388;&#27493;&#38271;&#30340;SCG_x&#21644;SCG_y&#20316;&#20026;&#36755;&#20837;&#65292;&#34920;&#31034;&#19968;&#20010;&#24515;&#33039;&#21608;&#26399;&#65292;&#36755;&#20986;&#26144;&#23556;&#21040;&#30446;&#26631;&#21464;&#37327;&#30340;&#21521;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LSTM&#27169;&#22411;&#30340;&#22343;&#26041;&#35823;&#24046;&#20026;0.09&#12290;
&lt;/p&gt;
&lt;p&gt;
This pilot study aims to develop a deep learning model for predicting seismocardiogram (SCG) signals in the dorsoventral direction from the SCG signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and $\textrm{SCG}_y$). The dataset used for the training and validation of the model was obtained from 15 healthy adult subjects. The SCG signals were recorded using tri-axial accelerometers placed on the chest of each subject. The signals were then segmented using electrocardiogram R waves, and the segments were downsampled, normalized, and centered around zero. The resulting dataset was used to train and validate a long short-term memory (LSTM) network with two layers and a dropout layer to prevent overfitting. The network took as input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one cardiac cycle, and outputted a vector that mapped to the target variable being predicted. The results showed that the LSTM model had a mean square error of 0.09 b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;MAPU&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#34109;&#21644;&#26102;&#38388;&#25554;&#34917;&#30340;&#26041;&#24335;&#65292;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07542</link><description>&lt;p&gt;
&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#19982;&#26102;&#38388;&#25554;&#34917;&#30340;&#26102;&#24207;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Temporal Imputation for Time Series Data. (arXiv:2307.07542v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;MAPU&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#34109;&#21644;&#26102;&#38388;&#25554;&#34917;&#30340;&#26041;&#24335;&#65292;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#24182;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#65288;SFDA&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#39046;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20174;&#24050;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#20445;&#25345;&#28304;&#39046;&#22495;&#30340;&#38544;&#31169;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#26159;&#22312;&#26102;&#24207;&#24212;&#29992;&#20013;&#65292;SFDA&#36824;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#35270;&#35273;&#24212;&#29992;&#30340;SFDA&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#20174;&#32780;&#23548;&#33268;&#33258;&#36866;&#24212;&#24615;&#33021;&#21463;&#25439;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#30340;&#26102;&#24207;&#25968;&#25454;&#26041;&#27861;&#65292;&#21363;MAsk and imPUte (MAPU)&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#25429;&#25417;&#28304;&#39046;&#22495;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#26102;&#24207;&#20449;&#21495;&#36827;&#34892;&#38543;&#26426;&#25513;&#34109;&#65292;&#21516;&#26102;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25554;&#34917;&#22120;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20174;&#25513;&#34109;&#29256;&#26412;&#20013;&#24674;&#22797;&#21407;&#22987;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#22312;&#36866;&#24212;&#27493;&#39588;&#20013;&#65292;&#25554;&#34917;&#22120;&#32593;&#32476;&#34987;&#21033;&#29992;&#26469;&#24341;&#23548;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;GP-UCB&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#35299;&#20915;&#20102;&#20851;&#20110;&#36951;&#25022;&#20998;&#26512;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07539</link><description>&lt;p&gt;
&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#25913;&#36827;&#33258;&#26631;&#20934;&#21270;&#27987;&#24230;&#65306;&#23545;GP-UCB&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;GP-UCB&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#35299;&#20915;&#20102;&#20851;&#20110;&#36951;&#25022;&#20998;&#26512;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#21270;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#22120;&#26088;&#22312;&#36890;&#36807;&#20165;&#22312;&#39034;&#24207;&#36873;&#25321;&#30340;&#28857;&#22788;&#36827;&#34892;&#22122;&#22768;&#35780;&#20272;&#65292;&#39034;&#24207;&#35745;&#31639;&#20301;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#23398;&#20064;&#22120;&#26088;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#36951;&#25022;&#26159;&#25152;&#20570;&#36873;&#25321;&#30340;&#27425;&#20248;&#24615;&#24230;&#37327;&#12290;&#21487;&#20197;&#35828;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#26159;&#39640;&#26031;&#36807;&#31243;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#65288;GP-UCB&#65289;&#31639;&#27861;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#26410;&#30693;&#20989;&#25968;&#30340;&#31616;&#21333;&#32447;&#24615;&#20272;&#35745;&#22120;&#36827;&#34892;&#34892;&#21160;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#29616;&#26377;&#30340;GP-UCB&#36951;&#25022;&#20998;&#26512;&#32473;&#20986;&#20102;&#27425;&#20248;&#36951;&#25022;&#29575;&#65292;&#23545;&#20110;&#35768;&#22810;&#24120;&#29992;&#30340;&#20869;&#26680;&#65288;&#22914;Mat&#233;rn&#20869;&#26680;&#65289;&#32780;&#35328;&#65292;&#36951;&#25022;&#29575;&#24182;&#19981;&#27425;&#32447;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;GP-UCB&#36951;&#25022;&#20998;&#26512;&#26159;&#21542;&#32039;&#23494;&#65292;&#25110;&#32773;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#20998;&#26512;&#25216;&#26415;&#25913;&#36827;&#30028;&#38480;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;GP-UCB&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#30452;&#25509;&#26263;&#31034;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Mat\'ern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results directly imply sublinear regret rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DAG&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07529</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;DAG&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32422;&#26463;&#19979;&#23398;&#20064;&#22810;&#20010;&#21327;&#35843;&#20195;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#21033;&#29992;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;DAG&#32467;&#26500;&#65292;&#20197;&#36798;&#21040;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#22870;&#21169;&#30340;MARL&#27169;&#22411;&#30340;&#26032;&#22411;&#20195;&#29702;&#20540;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#22312;&#35745;&#31639;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#39046;&#23548;&#32773;&#20195;&#29702;&#21644;&#22870;&#21169;&#29983;&#25104;&#21644;&#20998;&#21457;&#20195;&#29702;&#30340;&#26032;&#27010;&#24565;&#65292;&#24341;&#23548;&#20998;&#35299;&#30340;&#20174;&#23646;&#20195;&#29702;&#22312;&#20855;&#26377;DAG&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#26356;&#22909;&#22320;&#25506;&#32034;&#21442;&#25968;&#31354;&#38388;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#22235;&#20010;DAG&#29615;&#22659;&#65292;&#21253;&#25324;Intel&#39640;&#20135;&#37327;&#25171;&#21253;&#21644;&#27979;&#35797;&#24037;&#21378;&#30340;&#23454;&#38469;&#35843;&#24230;&#65292;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#20854;&#20182;&#38750;DAG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#24182;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#30740;&#31350;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.07527</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;: &#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Autonomous Vehicle's Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions. (arXiv:2307.07527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#30740;&#31350;&#35843;&#26597;&#20102;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#24182;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#30740;&#31350;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742; (AVs) &#36890;&#36807;&#20195;&#26367;&#20154;&#24037;&#39550;&#39542;&#21592;&#32780;&#37319;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#20915;&#31574;&#31995;&#32479;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33021;&#22815;&#26377;&#25928;&#22320;&#39550;&#39542;&#36947;&#36335;&#65292;AVs &#24517;&#39035;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#39044;&#27979;&#39550;&#39542;&#33021;&#21147;&#65292;&#21363;&#39044;&#27979;&#21608;&#22260;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#20511;&#37492;&#29616;&#26377;&#25991;&#29486;&#23545;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#26159;&#25512;&#36827;&#35813;&#39046;&#22495;&#21644;&#21457;&#23637;&#20840;&#38754;&#29702;&#35299;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;AVs&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#19982;AVs&#36712;&#36857;&#39044;&#27979;&#30456;&#20851;&#30340;&#20004;&#30334;&#20313;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#30340;&#19968;&#33324;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25509;&#30528;&#25105;&#20204;&#35752;&#35770;&#20102;&#30446;&#21069;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have emerged as a promising solution by replacing human drivers with advanced computer-aided decision-making systems. However, for AVs to effectively navigate the road, they must possess the capability to predict the future behavior of nearby traffic participants, similar to the predictive driving abilities of human drivers. Building upon existing literature is crucial to advance the field and develop a comprehensive understanding of trajectory prediction methods in the context of automated driving. To address this need, we have undertaken a comprehensive review that focuses on trajectory prediction methods for AVs, with a particular emphasis on machine learning techniques including deep learning and reinforcement learning-based approaches. We have extensively examined over two hundred studies related to trajectory prediction in the context of AVs. The paper begins with an introduction to the general problem of predicting vehicle trajectories and provides an o
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.</title><link>http://arxiv.org/abs/2307.07514</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#26159;&#28216;&#25103;&#12290;(arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07514
&lt;/p&gt;
&lt;p&gt;
Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#20154;&#31867;&#20915;&#31574;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;XAI&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#35828;&#26126;Shapley&#20540;&#21487;&#33021;&#20250;&#32473;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#35823;&#23548;&#65292;&#20351;&#20854;&#20026;&#39044;&#27979;&#20013;&#26080;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#26356;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#23545;&#19982;&#39044;&#27979;&#26377;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#36739;&#20302;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#24847;&#20041;&#22312;&#20110;&#23427;&#20204;&#26377;&#25928;&#22320;&#25361;&#25112;&#20102;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#22810;&#31181;&#25552;&#35758;&#29992;&#27861;&#65292;&#36825;&#20123;&#29992;&#27861;&#27491;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;</title><link>http://arxiv.org/abs/2307.07513</link><description>&lt;p&gt;
&#20351;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#25913;&#21892;ICU&#27515;&#20129;&#29575;&#39044;&#27979;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25918;&#23556;&#23398;&#25253;&#21578;&#21644;&#22270;&#20687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#27515;&#20129;&#29575;&#65292;&#24182;&#22312;MIMIC-IV&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#30340;&#35780;&#20998;&#31995;&#32479;&#22312;ICU&#31649;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#39044;&#27979;&#37325;&#35201;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#27515;&#20129;&#29575;&#12290;&#35768;&#22810;&#35780;&#20998;&#31995;&#32479;&#24050;&#32463;&#22312;ICU&#20013;&#24320;&#21457;&#21644;&#20351;&#29992;&#12290;&#36825;&#20123;&#35780;&#20998;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#20250;&#20007;&#22833;&#21465;&#36848;&#21644;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#20020;&#24202;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;ICU&#27515;&#20129;&#29575;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22235;&#32452;&#29305;&#24449;&#65306;&#65288;1&#65289;&#31616;&#21270;&#24613;&#24615;&#29983;&#29702;&#23398;&#35780;&#20998;&#65288;SAPS&#65289;II&#30340;&#29983;&#29702;&#27979;&#37327;&#12289;&#65288;2&#65289;&#25918;&#23556;&#19987;&#23478;&#39044;&#23450;&#20041;&#30340;&#24120;&#35265;&#33016;&#37096;&#30142;&#30149;&#12289;&#65288;3&#65289;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#65288;4&#65289;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;Medical Information Mart for Intensive Care IV&#65288;MIMIC-IV&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;0.7829&#30340;&#24179;&#22343;C-index&#65288;95%&#30340;&#32622;&#20449;&#21306;&#38388;&#65289;
&lt;/p&gt;
&lt;p&gt;
Background: The predictive Intensive Care Unit (ICU) scoring system plays an important role in ICU management because it predicts important outcomes, especially mortality. Many scoring systems have been developed and used in the ICU. These scoring systems are primarily based on the structured clinical data in the electronic health record (EHR), which may suffer the loss of important clinical information in the narratives and images. Methods: In this work, we build a deep learning based survival prediction model with multi-modality data to predict ICU mortality. Four sets of features are investigated: (1) physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2) common thorax diseases pre-defined by radiologists, (3) BERT-based text representations, and (4) chest X-ray image features. We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the proposed model. Results: Our model achieves the average C-index of 0.7829 (95% confidence i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20998;&#25351;&#26631;&#12289;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#24471;SCIP&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;4%&#12290;</title><link>http://arxiv.org/abs/2307.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20998;&#25351;&#26631;&#12289;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#24471;SCIP&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#30340;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#33258;&#20854;&#21019;&#24314;&#20197;&#26469;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#20999;&#21106;&#35780;&#20998;&#25351;&#26631;&#12289;&#20999;&#21106;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#20351;SCIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
The current cut selection algorithm used in mixed-integer programming solvers has remained largely unchanged since its creation. In this paper, we propose a set of new cut scoring measures, cut filtering techniques, and stopping criteria, extending the current state-of-the-art algorithm and obtaining a 4\% performance improvement for SCIP over the MIPLIB 2017 benchmark set.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;</title><link>http://arxiv.org/abs/2307.07264</link><description>&lt;p&gt;
&#20851;&#20110;&#25554;&#20540;&#19987;&#23478;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07264
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#23427;&#20204;&#22312;&#27599;&#19968;&#36718;&#35266;&#23519;&#20449;&#24687;&#30340;&#26041;&#24335;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#23545;&#20110;&#21521;&#37327;$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$&#65292;$\mathbf{m}$-MAB&#30340;&#19968;&#20010;&#23454;&#20363;&#34920;&#31034;&#23558;&#33218;&#20998;&#25104;$K$&#32452;&#65292;&#31532;$i$&#32452;&#21253;&#21547;$m_i$&#20010;&#33218;&#12290;&#19968;&#26086;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#21516;&#19968;&#32452;&#20013;&#25152;&#26377;&#33218;&#30340;&#25439;&#22833;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#32039;&#33268;&#26497;&#23567;&#21518;&#24724;&#30028;&#65292;&#24182;&#20026;&#20854;&#32431;&#25506;&#32034;&#29256;&#26412;$\mathbf{m}$-BAI&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;PAC&#31639;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#26469;&#35782;&#21035;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#26159;$\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$&#65292;&#23545;&#20110;&#19968;&#20010;$(\epsilon,0.05)$-PAC&#31639;&#27861;&#30340;$\mathbf{m}$-BAI&#65292;&#25289;&#21160;&#33218;&#30340;&#26368;&#23567;&#27425;&#25968;&#26159;$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is $\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;KoBo&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07246</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#65306;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;KoBo&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#22522;&#30784;&#27169;&#22411;&#20174;&#29702;&#35770;&#19978;&#21040;&#23454;&#36341;&#24212;&#29992;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#34892;&#24615;&#65292;&#20351;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#21307;&#23398;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#35786;&#26029;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#20449;&#24687;&#26469;&#25351;&#23548;&#34920;&#24449;&#23398;&#20064;&#65292;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#21463;&#21046;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#22686;&#24378;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KoBo&#65289;&#65292;&#35813;&#26694;&#26550;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#26080;&#20559;&#30340;&#12289;&#24320;&#38598;&#26679;&#26412;&#32423;&#30693;&#35782;&#34920;&#31034;&#26469;&#34913;&#37327;&#36127;&#26679;&#26412;&#22122;&#22768;&#65292;&#24182;&#34917;&#20805;&#35270;&#35273;&#35821;&#35328;&#20114;&#20449;&#24687;&#19982;&#20020;&#24202;&#30693;&#35782;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowl
&lt;/p&gt;</description></item><item><title>DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07119</link><description>&lt;p&gt;
DataAssist:&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07119
&lt;/p&gt;
&lt;p&gt;
DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#27169;&#22411;&#36873;&#25321;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#28165;&#27927;&#21644;&#25972;&#29702;&#25152;&#21344;&#25454;&#30340;&#22823;&#37096;&#20998;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DataAssist&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#20934;&#22791;&#21644;&#28165;&#27927;&#24179;&#21488;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DataAssist&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#28165;&#27927;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#20026;&#29992;&#25143;&#36873;&#25321;&#30340;&#21464;&#37327;&#29983;&#25104;&#21487;&#35270;&#21270;&#65292;&#32479;&#19968;&#25968;&#25454;&#27880;&#37322;&#65292;&#25552;&#20379;&#24322;&#24120;&#20540;&#21024;&#38500;&#24314;&#35758;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23548;&#20986;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25110;&#29992;&#25143;&#25351;&#23450;&#30340;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;&#24037;&#20855;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32463;&#27982;&#23398;&#12289;&#21830;&#19994;&#21644;&#39044;&#27979;&#24212;&#29992;&#65292;&#21487;&#33410;&#30465;&#36229;&#36807;50\%&#30340;&#25968;&#25454;&#28165;&#29702;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.06324</link><description>&lt;p&gt;
&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#19979;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#26426;&#36741;&#21161;&#20998;&#26512;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#32463;&#36807;&#38271;&#36317;&#31163;&#27493;&#39588;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#38750;&#24120;&#25968;&#27493;&#38271;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#27425;&#36845;&#20195;&#30340;&#25972;&#20307;&#25928;&#26524;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#19968;&#27425;&#36845;&#20195;&#24402;&#32435;&#20351;&#29992;&#30340;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#30772;&#22351;&#19979;&#38477;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38271;&#36317;&#31163;&#27493;&#39588;&#65292;&#21487;&#33021;&#22312;&#30701;&#26399;&#20869;&#22686;&#21152;&#30446;&#26631;&#20540;&#65292;&#20294;&#22312;&#38271;&#26399;&#20869;&#24102;&#26469;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#29468;&#24819;&#65292;&#24182;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05949</link><description>&lt;p&gt;
&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Newell's theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Newell&#29702;&#35770;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#20351;&#29992;&#21367;&#31215;&#25110;&#22270;&#21367;&#31215;&#36807;&#28388;&#22120;&#20197;&#21450;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#20123;&#27169;&#22411;, &#22914;CNN-LSTM, &#21033;&#29992;&#37051;&#36817;&#26816;&#27979;&#31449;&#30340;&#20132;&#36890;&#27969;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#27969;&#37327;&#12290;&#28982;&#32780;, &#36825;&#20123;&#27169;&#22411;&#22312;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#21160;&#24577;&#26041;&#38754;&#20855;&#26377;&#23616;&#38480;&#24615;, &#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#29305;&#23450;&#20110;&#26816;&#27979;&#37197;&#32622;&#21644;&#30446;&#26631;&#20301;&#32622;&#20132;&#36890;&#29305;&#24449;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;, &#24403;&#22312;&#26032;&#30340;&#20301;&#32622;&#32570;&#23569;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;, &#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#36890;&#27969;&#29289;&#29702;&#23398;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target loc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.05422</link><description>&lt;p&gt;
&#40657;&#30418;DNN&#21518;&#38376;&#26816;&#27979;&#30340;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#30340;&#24046;&#24322;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#38376;&#25915;&#20987;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#40657;&#30418;&#24773;&#20917;&#19979;&#65292;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#36335;&#26159;&#65292;&#19982;&#20854;&#20182;&#33391;&#24615;&#29305;&#24449;&#30456;&#27604;&#65292;&#19982;&#35302;&#21457;&#22120;&#30456;&#20851;&#30340;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#20102;&#23450;&#37327;&#34913;&#37327;&#35302;&#21457;&#22120;&#21644;&#33391;&#24615;&#29305;&#24449;&#23545;&#30830;&#23450;&#21518;&#38376;&#32593;&#32476;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20116;&#20010;&#24230;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#35745;&#31639;&#32473;&#23450;&#36755;&#20837;&#30340;&#20116;&#20010;&#24230;&#37327;&#20540;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#36755;&#20837;&#30340;&#37096;&#20998;&#20869;&#23481;&#27880;&#20837;&#21040;&#24178;&#20928;&#30340;&#39564;&#35777;&#26679;&#26412;&#20013;&#29983;&#25104;&#20960;&#20010;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#24212;&#21512;&#25104;&#26679;&#26412;&#30340;&#36755;&#20986;&#26631;&#31614;&#35745;&#31639;&#20986;&#20116;&#20010;&#24230;&#37327;&#25351;&#26631;&#12290;&#26412;&#25991;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#20351;&#29992;&#20102;&#19968;&#20010;&#23567;&#22411;&#30340;&#24178;&#20928;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#20986;&#20116;&#20010;&#24230;&#37327;&#20540;&#21518;&#65292;&#25105;&#20204;&#20174;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20986;&#20116;&#20010;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#12290;&#19968;&#20010;&#20803;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#23558;&#20116;&#20010;&#35757;&#32451;&#22909;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#30340;&#36755;&#20986;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.05358</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#35843;&#33410;&#22120;&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;FedDure&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#21644;&#32454;&#35843;&#33410;&#22120;&#23545;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#36827;&#34892;&#35268;&#33539;&#65292;&#20197;&#21450;&#23398;&#20064;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#24322;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#25955;&#23458;&#25143;&#31471;&#19978;&#26631;&#31614;&#31232;&#32570;&#65292;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#20986;&#29616;&#20197;&#20174;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FSSL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#25968;&#25454;&#29420;&#31435;&#19988;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FSSL&#30340;&#26356;&#23454;&#38469;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#20165;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#21516;&#65292;&#22312;&#23458;&#25143;&#31471;&#20869;&#37096;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#20063;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21452;&#35843;&#33410;&#22120;&#30340;&#26032;&#22411;FSSL&#26694;&#26550;&#65292;FedDure&#12290;FedDure&#36890;&#36807;&#31895;&#35843;&#33410;&#22120;&#65288;C-reg&#65289;&#21644;&#32454;&#35843;&#33410;&#22120;&#65288;F-reg&#65289;&#35299;&#38500;&#20102;&#20197;&#21069;&#30340;&#20551;&#35774;&#65306;C-reg&#36890;&#36807;&#36319;&#36394;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#25928;&#26524;&#26469;&#35268;&#33539;&#26412;&#22320;&#27169;&#22411;&#30340;&#26356;&#26032;&#65307;F-reg&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#24615;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#36866;&#24212;&#23458;&#25143;&#31471;&#20869;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#24212;&#29992;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05217</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#36136;&#24615;&#30340;&#26377;&#30417;&#30563;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#24212;&#29992;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#22270;&#19978;&#23398;&#20064;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#21464;&#31181;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATs&#65289;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#22312;GAT&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#20026;&#20854;&#37051;&#23621;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20854;&#20182;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;GAT&#32858;&#21512;&#26469;&#33258;&#23646;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#20135;&#29983;&#30340;&#33410;&#28857;&#34920;&#31034;&#22312;&#19981;&#21516;&#31867;&#21035;&#26041;&#38754;&#19981;&#22815;&#26126;&#30830;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#21040;&#20219;&#20309;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#65292;&#20197;&#40723;&#21169;&#20849;&#20139;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#33410;&#28857;&#20043;&#38388;&#33719;&#24471;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26631;&#20934;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04954</link><description>&lt;p&gt;
&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#30701;&#26399;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#36825;&#31181;&#27169;&#22411;&#32467;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#25429;&#25417;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20132;&#36890;&#21464;&#37327;&#30340;&#30701;&#26399;&#21644;&#36817;&#30701;&#26399;&#26410;&#26469;&#26041;&#38754;&#24050;&#32463;&#20248;&#20110;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#21382;&#21490;&#24179;&#22343;&#12289;ARIMA&#21644;&#20854;&#21464;&#20307;&#65292;&#36825;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21450;&#20854;&#21464;&#20307;&#65288;&#20363;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#34987;&#35774;&#35745;&#29992;&#20110;&#20445;&#30041;&#38271;&#26399;&#26102;&#24207;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22810;&#21046;&#24230;&#27169;&#22411;&#20551;&#35774;&#20132;&#36890;&#31995;&#32479;&#20197;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#20010;&#29366;&#24577;&#65288;&#20363;&#22914;&#30021;&#36890;&#12289;&#25317;&#22581;&#65289;&#28436;&#21464;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#19981;&#21516;&#27169;&#22411;&#20197;&#34920;&#24449;&#27599;&#20010;&#21046;&#24230;&#20869;&#30340;&#20132;&#36890;&#21160;&#24577;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#36827;&#34892;&#21046;&#24230;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20999;&#25442;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#21160;&#24577;&#27169;&#24335;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;LSTM&#37117;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20174;&#19968;&#32452;&#28508;&#22312;&#30340;&#25110;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20013;&#30340;&#35266;&#23519;&#24207;&#21015;&#12290;&#22312;LSTM&#20013;&#65292;&#28508;&#22312;&#21464;&#37327;&#21487;&#20197;&#20174;&#19978;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#38544;&#34255;&#29366;&#24577;&#21464;&#37327;&#20256;&#36882;&#36807;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31751;&#35825;&#23548;&#33945;&#29256;&#21464;&#25442;&#22120;&#65292;&#22312;&#38750;&#23545;&#27604;CT&#25195;&#25551;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32963;&#30284;&#31579;&#26597;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#27880;&#24847;&#21644;&#20132;&#21449;&#27880;&#24847;&#19982;&#21367;&#31215;&#29305;&#24449;&#20132;&#20114;&#65292;&#20197;&#22810;&#20219;&#21153;&#26041;&#24335;&#20998;&#21106;&#32959;&#30244;&#24182;&#20998;&#31867;&#24322;&#24120;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20445;&#30041;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;85.0%&#30340;&#25935;&#24863;&#24615;&#21644;92.6%&#30340;&#29305;&#24322;&#24615;&#65292;&#30456;&#27604;&#20004;&#21517;&#25918;&#23556;&#31185;&#21307;&#29983;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04525</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#23545;&#27604;CT&#25195;&#25551;&#36827;&#34892;&#32963;&#30284;&#26377;&#25928;&#31579;&#26597;&#30340;&#31751;&#35825;&#23548;&#33945;&#29256;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans. (arXiv:2307.04525v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31751;&#35825;&#23548;&#33945;&#29256;&#21464;&#25442;&#22120;&#65292;&#22312;&#38750;&#23545;&#27604;CT&#25195;&#25551;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32963;&#30284;&#31579;&#26597;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#27880;&#24847;&#21644;&#20132;&#21449;&#27880;&#24847;&#19982;&#21367;&#31215;&#29305;&#24449;&#20132;&#20114;&#65292;&#20197;&#22810;&#20219;&#21153;&#26041;&#24335;&#20998;&#21106;&#32959;&#30244;&#24182;&#20998;&#31867;&#24322;&#24120;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20445;&#30041;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;85.0%&#30340;&#25935;&#24863;&#24615;&#21644;92.6%&#30340;&#29305;&#24322;&#24615;&#65292;&#30456;&#27604;&#20004;&#21517;&#25918;&#23556;&#31185;&#21307;&#29983;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32963;&#30284;&#26159;&#20840;&#29699;&#32959;&#30244;&#30456;&#20851;&#27515;&#20129;&#30340;&#31532;&#19977;&#22823;&#21407;&#22240;&#65292;&#28982;&#32780;&#30446;&#21069;&#27809;&#26377;&#35268;&#33539;&#25512;&#33616;&#30340;&#31579;&#26597;&#27979;&#35797;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20405;&#20837;&#24615;&#24378;&#12289;&#26114;&#36149;&#19988;&#23545;&#26089;&#26399;&#32963;&#30284;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38750;&#23545;&#27604;CT&#25195;&#25551;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32963;&#30284;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31751;&#35825;&#23548;&#33945;&#29256;&#21464;&#25442;&#22120;&#65292;&#20197;&#22810;&#20219;&#21153;&#26041;&#24335;&#21516;&#26102;&#20998;&#21106;&#32959;&#30244;&#24182;&#20998;&#31867;&#24322;&#24120;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#21487;&#23398;&#20064;&#30340;&#31751;&#65292;&#29992;&#20110;&#32534;&#30721;&#32963;&#30284;&#30340;&#32441;&#29702;&#21644;&#24418;&#29366;&#21407;&#22411;&#65292;&#24182;&#21033;&#29992;&#33258;&#27880;&#24847;&#21644;&#20132;&#21449;&#27880;&#24847;&#19982;&#21367;&#31215;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30001;100&#21517;&#30284;&#30151;&#24739;&#32773;&#21644;148&#21517;&#27491;&#24120;&#20154;&#32452;&#25104;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;85.0%&#30340;&#25935;&#24863;&#24615;&#21644;92.6%&#30340;&#29305;&#24322;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20004;&#21517;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24179;&#22343;&#25935;&#24863;&#24615;&#20026;73.5%&#65292;&#29305;&#24322;&#24615;&#20026;84.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24037;&#20855;ECS&#65292;&#29992;&#20110;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04368</link><description>&lt;p&gt;
ECS -- &#29992;&#20110;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#30340;&#20132;&#20114;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
ECS -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.04368v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24037;&#20855;ECS&#65292;&#29992;&#20110;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#21450;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#35777;&#26041;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#25968;&#23398;&#22522;&#30784;&#65292;&#28982;&#21518;&#36890;&#36807;&#22810;&#20010;&#31034;&#20363;&#20171;&#32461;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#20986;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.04251</link><description>&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#24212;&#29992;&#65306;&#19968;&#20221;&#31616;&#27905;&#30340;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04251
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20844;&#20247;&#22823;&#35268;&#27169;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20114;&#21160;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20063;&#24341;&#21457;&#20102;&#31867;&#20284;&#25216;&#26415;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#24212;&#29992;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#21019;&#24314;&#30340;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32463;&#36807;&#31934;&#24515;&#35757;&#32451;&#24182;&#20351;&#29992;&#20102;&#22823;&#37327;&#25968;&#25454;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#38761;&#65292;&#24182;&#25512;&#21160;&#20102;LLM&#33021;&#21147;&#30340;&#36793;&#30028;&#12290;ChatGPT&#22312;&#22823;&#35268;&#27169;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26222;&#36941;&#20844;&#20247;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#20114;&#21160;&#65292;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#36824;&#24341;&#21457;&#20102;&#24320;&#21457;&#31867;&#20284;&#25216;&#26415;&#21644;&#30740;&#31350;&#20854;&#24212;&#29992;&#21644;&#24433;&#21709;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21450;&#20854;&#28436;&#21270;&#30340;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#31616;&#26126;&#35843;&#26597;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;ChatGPT&#30340;&#29627;&#29827;&#30418;&#21644;&#40657;&#30418;&#35270;&#35282;&#65292;&#21253;&#25324;&#25216;&#26415;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#22522;&#26412;&#35201;&#32032;&#65292;&#20197;&#21450;&#20854;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#24433;&#21709;&#12290;&#29627;&#29827;&#30418;&#26041;&#27861;&#30528;&#37325;&#20110;&#29702;&#35299;&#25216;&#26415;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#32780;&#40657;&#30418;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#65292;&#22240;&#27492;&#30740;&#31350;&#20854;&#36755;&#20837;&#65292;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25193;&#23637;&#20102;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65292;&#39318;&#20808;&#22312;IMDb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#27425;&#24341;&#20837;&#20102;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#26469;&#25913;&#36827;&#25439;&#22833;&#38408;&#20540;&#65292;&#26368;&#21518;&#36890;&#36807;&#21442;&#25968;&#21487;&#35270;&#21270;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2307.04205</link><description>&lt;p&gt;
&#25193;&#23637;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extending the Forward Forward Algorithm. (arXiv:2307.04205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25193;&#23637;&#20102;&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#65292;&#39318;&#20808;&#22312;IMDb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#27425;&#24341;&#20837;&#20102;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#26469;&#25913;&#36827;&#25439;&#22833;&#38408;&#20540;&#65292;&#26368;&#21518;&#36890;&#36807;&#21442;&#25968;&#21487;&#35270;&#21270;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#21069;&#21521;&#31639;&#27861;&#26159;Geoffrey Hinton&#20110;2022&#24180;11&#26376;&#25552;&#20986;&#30340;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20316;&#20026;&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#22797;&#21046;&#20102;Hinton&#30340;&#23454;&#39564;&#65292;&#24182;&#38543;&#21518;&#36890;&#36807;&#20004;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#30340;&#33539;&#22260;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#21069;&#21521;&#21069;&#21521;&#32593;&#32476;&#22312;IMDb&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26631;&#24535;&#30528;&#35813;&#31639;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#22806;&#30340;&#39318;&#27425;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37329;&#23383;&#22612;&#20248;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#25439;&#22833;&#38408;&#20540;&#65292;&#36825;&#26159;&#21069;&#21521;&#21069;&#21521;&#26041;&#27861;&#29305;&#26377;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#37329;&#23383;&#22612;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#20010;&#22909;&#30340;&#38408;&#20540;&#31574;&#30053;&#20250;&#23548;&#33268;&#27979;&#35797;&#38169;&#35823;&#29575;&#30340;&#24046;&#24322;&#39640;&#36798;8%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#21442;&#25968;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#20363;&#22914;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#20102;10-20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022, is a novel method for training neural networks as an alternative to backpropagation. In this project, we replicate Hinton's experiments on the MNIST dataset, and subsequently extend the scope of the method with two significant contributions. First, we establish a baseline performance for the Forward Forward network on the IMDb movie reviews dataset. As far as we know, our results on this sentiment analysis task marks the first instance of the algorithm's extension beyond computer vision. Second, we introduce a novel pyramidal optimization strategy for the loss threshold - a hyperparameter specific to the Forward Forward method. Our pyramidal approach shows that a good thresholding strategy causes a difference of upto 8% in test error. 1 Lastly, we perform visualizations of the trained parameters and derived several significant insights, such as a notably larger (10-20x) mean and variance in the weights acquire
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#26031;&#36807;&#31243;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#19988;&#26126;&#30830;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#30340;&#25351;&#23548;&#65292;&#35813;&#26694;&#26550;&#22312;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03093</link><description>&lt;p&gt;
&#36229;&#36234;&#30452;&#35273;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#26031;&#36807;&#31243;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#19988;&#26126;&#30830;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#30340;&#25351;&#23548;&#65292;&#35813;&#26694;&#26550;&#22312;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#23567;&#22411;&#12289;&#32467;&#26500;&#21270;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#22238;&#24402;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#21463;&#21040;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#23545;&#20110;&#22914;&#20309;&#23558;GPs&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#30830;&#23450;GPs&#22312;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#22914;&#20309;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#19988;&#26126;&#30830;&#30340;GP&#27169;&#22411;&#12290;&#25351;&#23548;&#26041;&#38024;&#24418;&#24335;&#21270;&#20102;&#32463;&#39564;&#20016;&#23500;&#30340;GP&#23454;&#36341;&#32773;&#30340;&#20915;&#31574;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#26680;&#20989;&#25968;&#35774;&#35745;&#21644;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20912;&#24029;&#39640;&#31243;&#21464;&#21270;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#22312;&#27979;&#35797;&#26102;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.02631</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#20197;&#25903;&#25345;AML&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;AML&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#19987;&#23478;&#38656;&#27714;&#39069;&#22806;&#27979;&#35797;&#21644;&#20998;&#26512;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#26159;&#19968;&#31181;&#26368;&#20855;&#20405;&#30053;&#24615;&#30340;&#34880;&#28082;&#32959;&#30244;&#12290;&#20026;&#20102;&#25903;&#25345;&#19987;&#23478;&#20851;&#20110;&#21512;&#36866;&#27835;&#30103;&#30340;&#20915;&#31574;&#65292;AML&#24739;&#32773;&#26681;&#25454;&#20854;&#32454;&#32990;&#36951;&#20256;&#21644;&#20998;&#23376;&#29305;&#24449;&#33719;&#24471;&#39044;&#21518;&#20449;&#24687;&#65292;&#36890;&#24120;&#20998;&#20026;&#26377;&#21033;&#12289;&#20013;&#31561;&#21644;&#19981;&#21033;&#19977;&#20010;&#39118;&#38505;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#20998;&#31867;&#23384;&#22312;&#24050;&#30693;&#38382;&#39064;&#65292;&#22914;&#21516;&#19968;&#39118;&#38505;&#32452;&#20013;&#24739;&#32773;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#20013;&#39118;&#38505;&#31867;&#21035;&#30340;&#28165;&#26224;&#23450;&#20041;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;AML&#24739;&#32773;&#34987;&#24402;&#20026;&#20013;&#39118;&#38505;&#20998;&#31867;&#65292;&#19987;&#23478;&#24120;&#38656;&#36827;&#34892;&#20854;&#20182;&#27979;&#35797;&#21644;&#20998;&#26512;&#65292;&#23548;&#33268;&#27835;&#30103;&#24310;&#36831;&#21644;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#25454;&#20998;&#26512;&#21644;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#26681;&#25454;&#24739;&#32773;&#29983;&#23384;&#39044;&#27979;&#30830;&#23450;&#26368;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#22810;&#21151;&#33021;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;PIGNet2&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#29289;&#29702;&#21407;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#35780;&#20998;&#21644;&#39640;&#25928;&#31579;&#36873;&#12290;</title><link>http://arxiv.org/abs/2307.01066</link><description>&lt;p&gt;
PIGNet2&#65306;&#19968;&#31181;&#29992;&#20110;&#32467;&#21512;&#20146;&#21644;&#21147;&#35780;&#20998;&#21644;&#34394;&#25311;&#31579;&#36873;&#30340;&#22810;&#21151;&#33021;&#28145;&#24230;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening. (arXiv:2307.01066v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#22810;&#21151;&#33021;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;PIGNet2&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#29289;&#29702;&#21407;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#35780;&#20998;&#21644;&#39640;&#25928;&#31579;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#65288;PLI&#65289;&#30340;&#39044;&#27979;&#22312;&#25351;&#23548;&#26377;&#25928;&#32467;&#21512;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#30340;&#37492;&#23450;&#21644;&#20248;&#21270;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;PLI&#39044;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#20934;&#30830;&#35780;&#20998;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#36827;&#34892;&#39640;&#25928;&#34394;&#25311;&#31579;&#36873;&#30340;&#36890;&#29992;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26500;-&#20146;&#21644;&#21147;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#27169;&#22411;&#22312;&#35780;&#20998;&#21644;&#31579;&#36873;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#36317;&#31163;&#30340;&#29616;&#26377;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of protein-ligand interactions (PLI) plays a crucial role in drug discovery as it guides the identification and optimization of molecules that effectively bind to target proteins. Despite remarkable advances in deep learning-based PLI prediction, the development of a versatile model capable of accurately scoring binding affinity and conducting efficient virtual screening remains a challenge. The main obstacle in achieving this lies in the scarcity of experimental structure-affinity data, which limits the generalization ability of existing models. Here, we propose a viable solution to address this challenge by introducing a novel data augmentation strategy combined with a physics-informed graph neural network. The model showed significant improvements in both scoring and screening, outperforming task-specific deep learning models in various tests including derivative benchmarks, and notably achieving results comparable to the state-of-the-art performance based on distance lik
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#34892;&#20026;&#20943;&#37325;&#24178;&#39044;&#20013;&#20010;&#24615;&#21270;&#37329;&#34701;&#28608;&#21169;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#12290;&#38382;&#39064;&#20027;&#35201;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#20998;&#37197;&#26377;&#38480;&#30340;&#24178;&#39044;&#39044;&#31639;&#21644;&#39640;&#26114;&#30340;&#36164;&#28304;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#28608;&#21169;&#32467;&#26500;&#26469;&#20419;&#36827;&#21442;&#19982;&#32773;&#30340;&#20381;&#20174;&#24615;&#21644;&#34892;&#20026;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.00444</link><description>&lt;p&gt;
&#31227;&#21160;&#34892;&#20026;&#20943;&#37325;&#24178;&#39044;&#30340;&#20010;&#24615;&#21270;&#37329;&#34701;&#28608;&#21169;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions. (arXiv:2307.00444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#34892;&#20026;&#20943;&#37325;&#24178;&#39044;&#20013;&#20010;&#24615;&#21270;&#37329;&#34701;&#28608;&#21169;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#12290;&#38382;&#39064;&#20027;&#35201;&#22312;&#20110;&#22914;&#20309;&#26377;&#25928;&#22320;&#20998;&#37197;&#26377;&#38480;&#30340;&#24178;&#39044;&#39044;&#31639;&#21644;&#39640;&#26114;&#30340;&#36164;&#28304;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#30340;&#28608;&#21169;&#32467;&#26500;&#26469;&#20419;&#36827;&#21442;&#19982;&#32773;&#30340;&#20381;&#20174;&#24615;&#21644;&#34892;&#20026;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32933;&#32982;&#26159;&#24433;&#21709;&#32654;&#22269;&#30340;&#20851;&#38190;&#21307;&#30103;&#38382;&#39064;&#12290;&#23545;&#20110;&#32933;&#32982;&#30151;&#65292;&#26368;&#20302;&#39118;&#38505;&#30340;&#27835;&#30103;&#26041;&#27861;&#26159;&#34892;&#20026;&#24178;&#39044;&#65292;&#26088;&#22312;&#20419;&#36827;&#39278;&#39135;&#21644;&#36816;&#21160;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#21253;&#25324;&#31227;&#21160;&#32452;&#20214;&#65292;&#20801;&#35768;&#24178;&#39044;&#32773;&#25910;&#38598;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#24182;&#20026;&#21442;&#19982;&#32773;&#25552;&#20379;&#28608;&#21169;&#21644;&#30446;&#26631;&#65292;&#20197;&#20419;&#36827;&#38271;&#26399;&#34892;&#20026;&#25913;&#21464;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#30452;&#25509;&#32463;&#27982;&#28608;&#21169;&#20419;&#36827;&#34892;&#20026;&#25913;&#21464;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#20381;&#20174;&#24615;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#19981;&#21516;&#30340;&#28608;&#21169;&#32467;&#26500;&#21644;&#37329;&#39069;&#20250;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#32771;&#34385;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;&#20010;&#24615;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#20020;&#24202;&#21307;&#29983;&#20107;&#20808;&#19981;&#30693;&#36947;&#22914;&#20309;&#26368;&#22909;&#22320;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#28608;&#21169;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#24178;&#39044;&#39044;&#31639;&#19979;&#22914;&#20309;&#39640;&#25928;&#22320;&#20351;&#29992;&#26114;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#35774;&#35745;&#20010;&#24615;&#21270;&#20943;&#37325;&#24178;&#39044;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obesity is a critical healthcare issue affecting the United States. The least risky treatments available for obesity are behavioral interventions meant to promote diet and exercise. Often these interventions contain a mobile component that allows interventionists to collect participants level data and provide participants with incentives and goals to promote long term behavioral change. Recently, there has been interest in using direct financial incentives to promote behavior change. However, adherence is challenging in these interventions, as each participant will react differently to different incentive structure and amounts, leading researchers to consider personalized interventions. The key challenge for personalization, is that the clinicians do not know a priori how best to administer incentives to participants, and given finite intervention budgets how to disburse costly resources efficiently. In this paper, we consider this challenge of designing personalized weight loss interv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;Stackelberg&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#38450;&#24481;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.13800</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#38454;Meta Stackelberg&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A First Order Meta Stackelberg Method for Robust Federated Learning. (arXiv:2306.13800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;Stackelberg&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#38450;&#24481;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#38754;&#20020;&#30528;&#21508;&#31181;&#23433;&#20840;&#39118;&#38505;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#22810;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26159;&#38750;&#33258;&#36866;&#24212;&#30340;&#65292;&#21482;&#38024;&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#20174;&#32780;&#26080;&#27861;&#25269;&#24481;&#19981;&#21487;&#39044;&#27979;&#25110;&#33258;&#36866;&#24212;&#30340;&#23041;&#32961;&#12290;&#26412;&#30740;&#31350;&#23558;&#23545;&#25239;&#24615;&#32852;&#37030;&#23398;&#20064;&#24314;&#27169;&#20026;&#36125;&#21494;&#26031;Stackelberg&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;(BSMG)&#20197;&#25429;&#25417;&#38450;&#24481;&#32773;&#23545;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;Stackelberg&#23398;&#20064;(meta-SL)&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;BSMG&#20013;&#30340;&#22343;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;FL&#38450;&#24481;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;meta-SL&#22312;$O(\varepsilon^{-2})$&#26799;&#24230;&#36845;&#20195;&#20013;&#25910;&#25947;&#20110;&#19968;&#38454;$\varepsilon$-&#22343;&#34913;&#28857;&#65292;&#27599;&#27425;&#36845;&#20195;&#38656;&#35201;$O(\varepsilon^{-4})$&#20010;&#26679;&#26412;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20803;Stackelberg&#26694;&#26550;&#22312;&#24378;&#22823;&#30340;&#27169;&#22411;&#27745;&#26579;&#21644;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;</title><link>http://arxiv.org/abs/2306.13253</link><description>&lt;p&gt;
&#25552;&#21069;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65306;&#30740;&#31350;&#25484;&#25569;&#25216;&#33021;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#21363;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#26469;&#21028;&#26029;&#21518;&#32493;&#26159;&#21542;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#20351;&#29992;&#27874;&#24418;&#25391;&#33633;&#21644;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#39044;&#27979;&#65292;&#35813;&#29616;&#35937;&#26159;&#23436;&#32654;&#27010;&#25324;&#22312;&#20986;&#29616;&#36807;&#25311;&#21512;&#25110;&#35760;&#24518;&#36857;&#35937;&#20043;&#21518;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#25165;&#20986;&#29616;&#12290;&#25253;&#21578;&#31216;&#65292;&#21482;&#26377;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#19979;&#25165;&#33021;&#35266;&#23519;&#21040;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#36825;&#20351;&#24471;&#30830;&#23450;&#23548;&#33268;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29702;&#35299;&#21069;&#28010;&#28526;&#38656;&#35201;&#22823;&#37327;&#30340;&#36845;&#20195;&#36718;&#25968;&#65292;&#22240;&#27492;&#23547;&#25214;&#23548;&#33268;&#23427;&#30340;&#36229;&#21442;&#25968;&#26159;&#24456;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#26041;&#27861;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22823;&#37327;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21069;&#20960;&#36718;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#22312;&#21069;&#20960;&#36718;&#20013;&#20986;&#29616;&#26576;&#20123;&#25391;&#33633;&#65292;&#37027;&#20040;&#21487;&#20197;&#26399;&#26395;&#22312;&#27169;&#22411;&#35757;&#32451;&#26356;&#38271;&#26102;&#38388;&#21518;&#20986;&#29616;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23398;&#20064;&#26354;&#32447;&#30340;&#39057;&#35889;&#29305;&#24449;&#20540;&#26469;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29702;&#35299;&#21069;&#28010;&#28526;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#33410;&#28857;&#21644;&#36793;&#20844;&#24179;&#24863;&#30693;&#30340;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#33410;&#28857;&#24179;&#34913;&#21644;&#36793;&#24179;&#34913;&#26469;&#23454;&#29616;&#20844;&#24179;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#21327;&#23884;&#20837;&#26694;&#26550;&#23398;&#20064;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#33410;&#28857;&#21644;&#36793;&#26041;&#38754;&#22343;&#20855;&#26377;&#24179;&#34913;&#20998;&#21106;&#21644;&#33391;&#22909;&#25928;&#29992;&#65292;&#24182;&#21487;&#29992;&#20316;&#20266;&#26631;&#31614;&#26469;&#20419;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.10123</link><description>&lt;p&gt;
&#21452;&#33410;&#28857;&#21644;&#36793;&#20844;&#24179;&#24863;&#30693;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Dual Node and Edge Fairness-Aware Graph Partition. (arXiv:2306.10123v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#33410;&#28857;&#21644;&#36793;&#20844;&#24179;&#24863;&#30693;&#30340;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#33410;&#28857;&#24179;&#34913;&#21644;&#36793;&#24179;&#34913;&#26469;&#23454;&#29616;&#20844;&#24179;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#21327;&#23884;&#20837;&#26694;&#26550;&#23398;&#20064;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#33410;&#28857;&#21644;&#36793;&#26041;&#38754;&#22343;&#20855;&#26377;&#24179;&#34913;&#20998;&#21106;&#21644;&#33391;&#22909;&#25928;&#29992;&#65292;&#24182;&#21487;&#29992;&#20316;&#20266;&#26631;&#31614;&#26469;&#20419;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#30340;&#22270;&#20998;&#21106;&#26159;&#30830;&#20445;&#26080;&#30417;&#30563;&#29992;&#25143;&#20998;&#26512;&#20013;&#30340;&#20844;&#27491;&#21644;&#38750;&#27495;&#35270;&#24453;&#36935;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#20844;&#24179;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#33410;&#28857;&#24179;&#34913;&#65292;&#21363;&#36861;&#27714;&#21508;&#20010;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#33410;&#28857;&#25968;&#37327;&#30340;&#27604;&#20363;&#24179;&#34913;&#65292;&#20294;&#24573;&#35270;&#20102;&#27599;&#20010;&#32858;&#31867;&#20013;&#19981;&#24179;&#34913;&#36793;&#25152;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#36830;&#25509;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#36793;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#33410;&#28857;&#24179;&#34913;&#21644;&#36793;&#24179;&#34913;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#36890;&#36807;&#32447;&#22270;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#23884;&#20837;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#23545;&#22270;&#20998;&#21106;&#20855;&#26377;&#21452;&#37325;&#33410;&#28857;&#21644;&#36793;&#20844;&#24179;&#24863;&#30693;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#33410;&#28857;&#21644;&#36793;&#26041;&#38754;&#37117;&#20855;&#26377;&#24179;&#34913;&#20998;&#21106;&#21644;&#33391;&#22909;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20844;&#24179;&#20998;&#21106;&#21487;&#20197;&#29992;&#20316;&#20266;&#26631;&#31614;&#65292;&#20197;&#20419;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair graph partition of social networks is a crucial step toward ensuring fair and non-discriminatory treatments in unsupervised user analysis. Current fair partition methods typically consider node balance, a notion pursuing a proportionally balanced number of nodes from all demographic groups, but ignore the bias induced by imbalanced edges in each cluster. To address this gap, we propose a notion edge balance to measure the proportion of edges connecting different demographic groups in clusters. We analyze the relations between node balance and edge balance, then with line graph transformations, we propose a co-embedding framework to learn dual node and edge fairness-aware representations for graph partition. We validate our framework through several social network datasets and observe balanced partition in terms of both nodes and edges along with good utility. Moreover, we demonstrate our fair partition can be used as pseudo labels to facilitate graph neural networks to behave fair
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08044</link><description>&lt;p&gt;
&#21098;&#26525;&#26041;&#24335;&#25552;&#39640;&#21487;&#38752;&#31574;&#30053;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#37325;&#30151;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#20915;&#31574;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#26377;&#26395;&#21046;&#23450;&#31934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20027;&#35201;&#22522;&#20110;&#27515;&#20129;&#29575;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#31163;&#32447;&#20272;&#35745;&#30340;&#31283;&#23450;&#24615;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#23558;&#30456;&#20851;&#20294;&#22024;&#26434;&#30340;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65288;&#20363;&#22914;&#24739;&#32773;&#29983;&#23384;&#29575;&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#26681;&#25454;&#25152;&#26377;&#21487;&#29992;&#22870;&#21169;&#23545;&#21160;&#20316;&#38598;&#36827;&#34892;&#21098;&#26525;&#65292;&#28982;&#21518;&#22522;&#20110;&#31232;&#30095;&#20027;&#35201;&#22870;&#21169;&#65292;&#20351;&#29992;&#21463;&#38480;&#21160;&#20316;&#38598;&#36827;&#34892;&#26368;&#32456;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#35299;&#31163;&#20934;&#30830;&#21644;&#36817;&#20284;&#22870;&#21169;&#26469;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#30340;&#28508;&#22312;&#25197;&#26354;&#65292;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#29256;&#26412;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#20984;&#20989;&#25968;&#26102;&#19981;&#20877;&#20381;&#36182;&#20110;&#21442;&#25968;&#955;&#21644;&#23398;&#20064;&#29575;&#951;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#19987;&#23478;&#24182;&#23558;&#20182;&#20204;&#30340;&#39044;&#27979;&#32467;&#26524;&#21512;&#24182;&#21040;&#20027;&#31639;&#27861;&#20013;&#65292;&#36825;&#20351;&#24471;&#35813;&#31639;&#27861;&#20855;&#26377;O(d log T)&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.06613</link><description>&lt;p&gt;
&#24378;&#20984;&#20989;&#25968;&#30340;&#26080;&#21442;&#25968;&#29256;&#26412;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-free version of Adaptive Gradient Methods for Strongly-Convex Functions. (arXiv:2306.06613v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#29256;&#26412;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#20984;&#20989;&#25968;&#26102;&#19981;&#20877;&#20381;&#36182;&#20110;&#21442;&#25968;&#955;&#21644;&#23398;&#20064;&#29575;&#951;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#19987;&#23478;&#24182;&#23558;&#20182;&#20204;&#30340;&#39044;&#27979;&#32467;&#26524;&#21512;&#24182;&#21040;&#20027;&#31639;&#27861;&#20013;&#65292;&#36825;&#20351;&#24471;&#35813;&#31639;&#27861;&#20855;&#26377;O(d log T)&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#20984;&#20989;&#25968;&#26102;&#30340;&#26368;&#20248;&#23398;&#20064;&#29575;&#20381;&#36182;&#20110;&#21442;&#25968;&#955;&#21644;&#23398;&#20064;&#29575;&#951;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;Metagrad&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#25670;&#33073;&#23545;&#955;&#21644;&#951;&#30340;&#20381;&#36182;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#19987;&#23478;&#65292;&#24182;&#23558;&#20182;&#20204;&#30340;&#39044;&#27979;&#32467;&#26524;&#21512;&#24182;&#21040;&#20027;&#31639;&#27861;&#20013;&#12290;&#35813;&#20027;&#31639;&#27861;&#20855;&#26377;O(d log T)&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal learning rate for adaptive gradient methods applied to {\lambda}-strongly convex functions relies on the parameters {\lambda} and learning rate {\eta}. In this paper, we adapt a universal algorithm along the lines of Metagrad, to get rid of this dependence on {\lambda} and {\eta}. The main idea is to concurrently run multiple experts and combine their predictions to a master algorithm. This master enjoys O(d log T) regret bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.06534</link><description>&lt;p&gt;
K-Tensors&#65306;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-Tensors&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#32467;&#26500;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;&#30001;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#21487;&#20197;&#22312; p&#8805;2 &#30340;&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#26925;&#29699;&#20307;&#65292;&#22240;&#27492;&#20445;&#25345;&#23427;&#20204;&#30340;&#32467;&#26500;&#20449;&#24687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#32858;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30697;&#38453;&#32858;&#31867;&#31639;&#27861;&#24120;&#24120;&#28041;&#21450;&#23558;&#30697;&#38453;&#21521;&#37327;&#21270;&#65292;&#23548;&#33268;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#32467;&#26500;&#20449;&#24687;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#36825;&#31181;&#36317;&#31163;&#24230;&#37327;&#20351;&#24471;&#32858;&#31867;&#31639;&#27861;&#33021;&#22815;&#32771;&#34385;&#27491;&#21322;&#23450;&#30697;&#38453;&#19982;&#23427;&#20204;&#22312;&#30001;&#19968;&#32452;&#27491;&#21322;&#23450;&#30697;&#38453;&#23450;&#20041;&#30340;&#27491;&#20132;&#21521;&#37327;&#24352;&#25104;&#30340;&#20849;&#21516;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#31181;&#31867;&#30340;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;</title><link>http://arxiv.org/abs/2306.06206</link><description>&lt;p&gt;
&#8220;PotatoPestNet&#65306;&#19968;&#31181;&#22522;&#20110;CTInceptionV3-RS&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;&#8221;(arXiv&#65306;2306.06206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#21516;&#31181;&#31867;&#30340;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#38083;&#34223;&#26159;&#20840;&#29699;&#31532;&#19977;&#22823;&#39135;&#21697;&#20316;&#29289;&#65292;&#20294;&#30001;&#20110;&#20405;&#34989;&#24615;&#23475;&#34411;&#30340;&#22256;&#25200;&#65292;&#20854;&#20135;&#37327;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35843;&#26597;&#36825;&#20123;&#23475;&#34411;&#30340;&#21508;&#31181;&#31867;&#22411;&#21644;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;AI&#30340;&#33258;&#21160;&#39532;&#38083;&#34223;&#23475;&#34411;&#35782;&#21035;&#31995;&#32479;PotatoPestNet&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31579;&#36873;&#20102;&#21253;&#25324;&#20843;&#31181;&#39532;&#38083;&#34223;&#23475;&#34411;&#30340;&#21487;&#38752;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#20116;&#20010;&#32463;&#36807;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65306;CMobileNetV2&#12289;CNASLargeNet&#12289;CXception&#12289;CDenseNet201&#21644;CInceptionV3&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;PotatoPestNet&#27169;&#22411;&#26469;&#20934;&#30830;&#20998;&#31867;&#39532;&#38083;&#34223;&#23475;&#34411;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21152;&#20837;&#19968;&#20010;&#20840;&#23616;&#22343;&#20540;&#27744;&#21270;&#23618;&#65292;&#24182;&#23454;&#26045;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#25628;&#32034;&#65288;RS&#65289;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potatoes are the third-largest food crop globally, but their production frequently encounters difficulties because of aggressive pest infestations. The aim of this study is to investigate the various types and characteristics of these pests and propose an efficient PotatoPestNet AI-based automatic potato pest identification system. To accomplish this, we curated a reliable dataset consisting of eight types of potato pests. We leveraged the power of transfer learning by employing five customized, pre-trained transfer learning models: CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in proposing a robust PotatoPestNet model to accurately classify potato pests. To improve the models' performance, we applied various augmentation techniques, incorporated a global average pooling layer, and implemented proper regularization methods. To further enhance the performance of the models, we utilized random search (RS) optimization for hyperparameter tuning. This optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36951;&#25022;&#30340;&#24230;&#37327;&#65292;&#20197;&#34913;&#37327;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#35813;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38797;&#28857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26377;&#24847;&#20041;&#30340;&#26354;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26080;&#30028;&#39046;&#22495;&#20013;&#25910;&#25947;&#20110;&#23545;&#20598;&#38388;&#38553;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#21160;&#24577;&#36951;&#25022;&#65292;&#20197;&#21450;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.04923</link><description>&lt;p&gt;
&#26080;&#32422;&#26463;&#22312;&#32447;&#23398;&#20064;&#21644;&#26080;&#30028;&#25439;&#22833;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36951;&#25022;&#30340;&#24230;&#37327;&#65292;&#20197;&#34913;&#37327;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#35813;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38797;&#28857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26377;&#24847;&#20041;&#30340;&#26354;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26080;&#30028;&#39046;&#22495;&#20013;&#25910;&#25947;&#20110;&#23545;&#20598;&#38388;&#38553;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#21160;&#24577;&#36951;&#25022;&#65292;&#20197;&#21450;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#25110;&#22810;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#65306;&#21363;&#22495;&#26159;&#26377;&#30028;&#30340;&#65292;&#25439;&#22833;&#26159;Lipschitz&#30340;&#25110;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20855;&#26377;&#26080;&#30028;&#22495;&#21644;&#38750;Lipschitz&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#35813;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20219;&#20309;&#28385;&#36275;&#23376;&#26799;&#24230;&#28385;&#36275;$\|g_{t}\|\le G+L\|w_{t}\|$&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#36951;&#25022;&#30340;&#24230;&#37327;&#20540;$R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$&#65292;&#24182;&#19988;&#34920;&#26126;&#38500;&#38750;&#26377;&#36827;&#19968;&#27493; &#20551;&#35774;&#65292;&#21542;&#21017;&#35813;&#30028;&#38480;&#26159;&#19981;&#33021;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02797</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#20154;&#31867;&#31867;&#20154;&#27010;&#24565;&#23398;&#20064;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#24182;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#27169;&#25311;&#23545;&#25277;&#35937;&#31526;&#21495;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#39640;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#35758;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#20808;&#39564;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#32773;&#65292;&#24182;&#22312;&#29983;&#25104;&#24615;&#21644;&#36923;&#36753;&#24615;&#27010;&#24565;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02520</link><description>&lt;p&gt;
&#20132;&#36890;&#29702;&#35299;&#30340;&#24773;&#22659;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#22659;&#20915;&#31574;&#12289;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#21644;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#19981;&#21516;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#30417;&#25511;(ITMo)&#25216;&#26415;&#26377;&#28508;&#21147;&#25913;&#21892;&#36947;&#36335;&#23433;&#20840;/&#23433;&#20840;&#24615;&#65292;&#23454;&#29616;&#26234;&#33021;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#12290;&#20102;&#35299;&#20132;&#36890;&#24773;&#20917;&#38656;&#35201;&#23558;&#24863;&#30693;&#20449;&#24687;&#19982;&#39046;&#22495;&#29305;&#23450;&#21644;&#22240;&#26524;&#24120;&#35782;&#30693;&#35782;&#22797;&#26434;&#34701;&#21512;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;&#20132;&#36890;&#30417;&#25511;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#20294;&#27169;&#22411;&#33021;&#21542;&#26377;&#25928;&#22320;&#23545;&#40784;&#36825;&#20123;&#20449;&#24687;&#26469;&#28304;&#24182;&#22312;&#26032;&#22330;&#26223;&#20013;&#25512;&#29702;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#35780;&#20272;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#29992;&#20110;&#20132;&#36890;&#39046;&#22495;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25991;&#26412;&#20219;&#21153;&#65306;i) BDD-QA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;(LMs)&#25191;&#34892;&#24773;&#22659;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;ii) TV-QA&#65292;&#35780;&#20272;LMs&#25512;&#29702;&#22797;&#26434;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;iii) HDT-QA&#65292;&#35780;&#20272;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#39550;&#39542;&#32771;&#35797;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36890;&#29992;&#24615;&#30340;&#22235;&#31181;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DWT-CompCNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#23545;&#20351;&#29992;HTJ2K&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01359</link><description>&lt;p&gt;
DWT-CompCNN&#65306;&#29992;&#20110;&#39640;&#21534;&#21520;&#37327;JPEG 2000&#21387;&#32553;&#25991;&#26723;&#30340;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DWT-CompCNN&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#23545;&#20351;&#29992;HTJ2K&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#21253;&#21547;&#25991;&#26723;&#22270;&#20687;&#30340;&#25968;&#23383;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#26816;&#32034;&#65292;&#25991;&#26723;&#22270;&#20687;&#30340;&#20998;&#31867;&#25104;&#20026;&#24517;&#35201;&#30340;&#38454;&#27573;&#12290;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25991;&#26723;&#30340;&#23436;&#25972;&#29256;&#26412;&#65292;&#21363;&#26410;&#21387;&#32553;&#30340;&#25991;&#26723;&#22270;&#20687;&#26500;&#25104;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#22240;&#25968;&#25454;&#37327;&#22823;&#32780;&#24102;&#26469;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#25991;&#26723;&#30340;&#21387;&#32553;&#34920;&#31034;&#65288;&#22312;&#37096;&#20998;&#35299;&#21387;&#32553;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#30452;&#25509;&#23436;&#25104;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20197;&#20351;&#25972;&#20010;&#36807;&#31243;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#65292;&#37027;&#23558;&#20250;&#26159;&#19968;&#39033;&#21019;&#26032;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;DWT-CompCNN&#65292;&#29992;&#20110;&#20351;&#29992;&#39640;&#21534;&#21520;&#37327;JPEG 2000&#65288;HTJ2K&#65289;&#31639;&#27861;&#21387;&#32553;&#30340;&#25991;&#26723;&#30340;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;DWT-CompCNN&#21253;&#25324;&#20116;&#20010;&#21367;&#31215;&#23618;&#65292;&#21367;&#31215;&#26680;&#22823;&#23567;&#20998;&#21035;&#20026;16&#12289;32&#12289;64&#12289;128&#21644;256&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23567;&#27874;&#31995;&#25968;&#20013;&#25552;&#39640;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model, DWT CompCNN is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01007</link><description>&lt;p&gt;
&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#20844;&#24179;&#35299;&#32544;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#25353;&#26102;&#38388;&#39034;&#24207;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#25509;&#25910;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#24067;&#20551;&#35774;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#23545;&#21160;&#24577;&#36951;&#25022;&#25110;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#20005;&#26684;&#30028;&#38480;&#26469;&#23637;&#31034;&#20854;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23436;&#20840;&#24573;&#30053;&#20102;&#24102;&#26377;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#23398;&#20064;&#65292;&#20854;&#23450;&#20041;&#20026;&#36328;&#19981;&#21516;&#23376;&#26063;&#32676;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#32773;&#38656;&#35201;&#20351;&#29992;&#20840;&#23616;&#26356;&#25913;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#20302;&#25928;&#30340;&#12290;&#21463;&#21040;&#31232;&#30095;&#26426;&#21046;&#36716;&#31227;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22768;&#31216;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#21464;&#21270;&#29615;&#22659;&#21487;&#20197;&#24402;&#22240;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#37096;&#20998;&#23398;&#20064;&#21442;&#25968;&#30340;&#37096;&#20998;&#21464;&#21270;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#22312;&#20551;&#35774;&#20174;&#19981;&#21516;&#23376;&#20154;&#32676;&#25910;&#38598;&#30340;&#25968;&#25454;&#20855;&#26377;&#20844;&#24179;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27599;&#20010;&#23376;&#20154;&#32676;&#27169;&#22411;&#34920;&#31034;&#20844;&#27491;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19779</link><description>&lt;p&gt;
&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#20197;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#65306;&#20197;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;aggVAE&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21644;MCMC&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26144;&#23556;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#25968;&#25454;&#65292;&#24182;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#30142;&#30149;&#26144;&#23556;&#26159;&#20844;&#20849;&#21355;&#29983;&#21644;&#30142;&#30149;&#30417;&#27979;&#20013;&#22522;&#26412;&#30340;&#25919;&#31574;&#20449;&#24687;&#24037;&#20855;&#65292;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#24403;&#22788;&#29702;&#21306;&#22495;&#25968;&#25454;&#65292;&#22914;&#34892;&#25919;&#21306;&#21010;&#21333;&#20301;&#65288;&#20363;&#22914;&#21439;&#25110;&#30465;&#65289;&#30340;&#32858;&#21512;&#25968;&#25454;&#26102;&#65292;&#24120;&#29992;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#21306;&#22495;&#21333;&#20803;&#30340;&#30456;&#37051;&#32467;&#26500;&#20197;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#30142;&#30149;&#30417;&#27979;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#38543;&#26102;&#38388;&#36319;&#36394;&#30142;&#30149;&#32467;&#26524;&#65292;&#20294;&#22312;&#21361;&#26426;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#25919;&#27835;&#21464;&#21270;&#23548;&#33268;&#34892;&#25919;&#36793;&#30028;&#26356;&#25913;&#65289;&#65292;&#36825;&#23558;&#24102;&#26469;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#21644;&#26131;&#20110;&#23454;&#26045;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#32452;&#21512;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#21644;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#29616;&#26377;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE) &#24037;&#20316;&#19978;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#32858;&#21512;VAE(aggVAE)&#20307;&#31995;&#32467;&#26500;&#21487;&#29992;&#20110;&#22312;&#20197;&#21439;&#20026;&#23618;&#32423;&#30340;&#32858;&#21512;&#32423;&#21035;&#22788;&#29702;&#25968;&#25454;&#65292;&#20197;&#26144;&#23556;&#32943;&#23612;&#20122;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#32771;&#34385;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30456;&#37051;&#24615;&#20551;&#35774;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#34892;&#25919;&#36793;&#30028;&#30340;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#30111;&#30142;&#24739;&#30149;&#29575;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;Gboard&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#22312;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;DP&#20445;&#35777;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#12290;&#22312;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;LMs&#20197;&#23454;&#29616;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18465</link><description>&lt;p&gt;
&#12298;&#24102;&#24046;&#20998;&#38544;&#31169;&#30340;Gboard&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;Gboard&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#22312;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335;DP&#20445;&#35777;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#12290;&#22312;&#23545;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;LMs&#20197;&#23454;&#29616;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35895;&#27468;&#38190;&#30424;(Gboard)&#20013;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LMs)&#12290;&#25105;&#20204;&#24212;&#29992;DP-FTRL&#31639;&#27861;&#65292;&#22312;&#19981;&#35201;&#27714;&#23545;&#23458;&#25143;&#35774;&#22791;&#36827;&#34892;&#22343;&#21248;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#24847;&#20041;&#30340;&#24418;&#24335; DP &#20445;&#35777;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#21033;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20132;&#25442;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#23458;&#25143;&#21442;&#19982;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#30340;&#37197;&#32622;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#21098;&#20999;&#20272;&#35745;&#19982;DP-FTRL&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#36873;&#25321;&#21098;&#20999;&#33539;&#25968;&#25110;&#20943;&#23569;&#36229;&#21442;&#25968;&#35843;&#25972;&#20197;&#20934;&#22791;&#35757;&#32451;&#12290;&#20511;&#21161;&#20110;&#23545;&#20844;&#20849;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#37096;&#32626;&#20102;&#36229;&#36807;20&#20010;Gboard LM&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;$\rho \in (0.2, 2)$&#19979;&#23454;&#29616;&#20102;&#39640;&#25928;&#29992;&#21644;$\rho-$zCDP&#38544;&#31169;&#20445;&#35777;&#65292;&#20854;&#20013;&#20004;&#20010;&#27169;&#22411;&#36824;&#20351;&#29992;&#20102;&#23433;&#20840;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models additionally trained with secure aggregation~\citep{bonawitz2017practical}. We are happy to announce tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#30340;&#26032;&#30340; emergent behaviors&#12290;&#36890;&#36807;&#23398;&#20064;&#32676;&#20307;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#32467;&#21512;&#26032;&#22855;&#25628;&#32034;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#33258;&#21160;&#21457;&#29616;&#21487;&#33021;&#20986;&#29616;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#21516;&#26102;&#24341;&#20837;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.16148</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#22312;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#21457;&#29616;&#26032;&#30340; emergent behaviors
&lt;/p&gt;
&lt;p&gt;
Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in Robot Swarms. (arXiv:2305.16148v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#30340;&#26032;&#30340; emergent behaviors&#12290;&#36890;&#36807;&#23398;&#20064;&#32676;&#20307;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#32467;&#21512;&#26032;&#22855;&#25628;&#32034;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#33258;&#21160;&#21457;&#29616;&#21487;&#33021;&#20986;&#29616;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#21516;&#26102;&#24341;&#20837;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#32676;&#20307;&#32463;&#24120;&#34920;&#29616;&#20986;&#20196;&#20154;&#30528;&#36855;&#30340; emergent behaviors&#65292;&#28982;&#32780;&#24456;&#38590;&#39044;&#27979;&#22312;&#29305;&#23450;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#19979;&#20250;&#20986;&#29616;&#21738;&#20123;&#32676;&#20307;&#34892;&#20026;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#26469;&#33258;&#21160;&#21457;&#29616;&#21487;&#33021;&#20135;&#29983;&#30340; collective behaviors &#30340;&#20998;&#31867;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#20107;&#20808;&#30693;&#36947;&#21738;&#20123;&#34892;&#20026;&#26159;&#26377;&#36259;&#25110;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20154;&#26426;&#20132;&#20114;&#26597;&#35810;&#23398;&#20064;&#20102;&#19968;&#20010;&#32676;&#20307;&#34892;&#20026;&#30340;&#30456;&#20284;&#24230;&#31354;&#38388;&#65292;&#26469;&#36866;&#24212;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#19982;&#26032;&#22855;&#25628;&#32034;&#21644;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#24182;&#20998;&#31867;&#21487;&#33021;&#30340;&#32676;&#20307;&#34892;&#20026;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#36890;&#29992;&#21551;&#21457;&#24335;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#36259; emergent behaviors &#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#25552;&#39640;&#20102;&#26032;&#22855;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20223;&#30495;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot swarms often exhibit emergent behaviors that are fascinating to observe; however, it is often difficult to predict what swarm behaviors can emerge under a given set of agent capabilities. We seek to efficiently leverage human input to automatically discover a taxonomy of collective behaviors that can emerge from a particular multi-agent system, without requiring the human to know beforehand what behaviors are interesting or even possible. Our proposed approach adapts to user preferences by learning a similarity space over swarm collective behaviors using self-supervised learning and human-in-the-loop queries. We combine our learned similarity metric with novelty search and clustering to explore and categorize the space of possible swarm behaviors. We also propose several general-purpose heuristics that improve the efficiency of our novelty search by prioritizing robot controllers that are likely to lead to interesting emergent behaviors. We test our approach in simulation on two 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#19988;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14442</link><description>&lt;p&gt;
&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimal Preconditioning and Fisher Adaptive Langevin Sampling. (arXiv:2305.14442v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#39044;&#26465;&#20214;&#21644;&#36153;&#33293;&#23572;&#33258;&#36866;&#24212; Langevin &#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#19988;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26368;&#22823;&#21270;&#39044;&#26399;&#24179;&#26041;&#36339;&#36291;&#36317;&#31163;&#65292;&#20026; Langevin &#25193;&#25955;&#23450;&#20041;&#20102;&#26368;&#20248;&#39044;&#26465;&#20214;&#12290;&#36825;&#23548;&#33268;&#26368;&#20248;&#39044;&#26465;&#20214;&#20026;&#21453;&#36153;&#33293;&#23572;&#20449;&#24687;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20854;&#20013;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#22312;&#30446;&#26631;&#19979;&#24179;&#22343;&#23545;&#25968;&#30446;&#26631;&#26799;&#24230;&#30340;&#22806;&#31215;&#12290;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#24212;&#29992;&#20110; Metropolis &#35843;&#25972; Langevin &#31639;&#27861; (MALA)&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#20174;&#31639;&#27861;&#36816;&#34892;&#20135;&#29983;&#30340;&#26799;&#24230;&#21382;&#21490;&#20013;&#23398;&#20064;&#39044;&#26465;&#20214;&#30340;&#35745;&#31639;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212; MCMC &#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39640;&#32500;&#20013;&#38750;&#24120;&#24378;&#20581;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26631;&#20934;&#33258;&#36866;&#24212; MCMC &#23398;&#20064;&#39044;&#26465;&#20214;&#21644;&#20301;&#32622;&#30456;&#20851;&#30340; Riemann &#27969;&#24418; MALA &#37319;&#26679;&#22120;&#30340;&#23494;&#20999;&#30456;&#20851;&#30340;&#33258;&#36866;&#24212; MALA &#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define an optimal preconditioning for the Langevin diffusion by analytically maximizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36807;&#21435;22&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#25345;&#32493;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#35782;&#21035;&#21644;&#25551;&#36848;&#20102;&#30456;&#20851;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#24037;&#31243;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#31561;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#38598;&#25104;&#27979;&#35797;&#30340;&#38454;&#27573;&#12289;&#25968;&#25454;&#26469;&#28304;&#21644;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.12695</link><description>&lt;p&gt;
&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#25345;&#32493;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Systematic Literature Review on Application of Machine Learning in Continuous Integration. (arXiv:2305.12695v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36807;&#21435;22&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#25345;&#32493;&#38598;&#25104;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#35782;&#21035;&#21644;&#25551;&#36848;&#20102;&#30456;&#20851;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#24037;&#31243;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#31561;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#38598;&#25104;&#27979;&#35797;&#30340;&#38454;&#27573;&#12289;&#25968;&#25454;&#26469;&#28304;&#21644;&#29305;&#24449;&#31867;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#36807;&#21435;22&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25345;&#32493;&#38598;&#25104;&#39046;&#22495;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#21644;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#20998;&#26512;&#25968;&#25454;&#24037;&#31243;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#25345;&#32493;&#38598;&#25104;&#27979;&#35797;&#30340;&#21508;&#20010;&#38454;&#27573;&#12289;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#20197;&#21450;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#20351;&#29992;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#20061;&#31181;&#25968;&#25454;&#26469;&#28304;&#21644;&#36873;&#23450;&#30740;&#31350;&#20013;&#25968;&#25454;&#20934;&#22791;&#30340;&#22235;&#20010;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30740;&#31350;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#22235;&#31181;&#29305;&#24449;&#31867;&#22411;&#21644;&#20061;&#20010;&#25968;&#25454;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#20116;&#31181;&#36873;&#25321;&#21644;&#35843;&#20248;&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35782;&#21035;&#20986;&#20102;&#21313;&#20116;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research conducted a systematic review of the literature on machine learning (ML)-based methods in the context of Continuous Integration (CI) over the past 22 years. The study aimed to identify and describe the techniques used in ML-based solutions for CI and analyzed various aspects such as data engineering, feature engineering, hyper-parameter tuning, ML models, evaluation methods, and metrics. In this paper, we have depicted the phases of CI testing, the connection between them, and the employed techniques in training the ML method phases. We presented nine types of data sources and four taken steps in the selected studies for preparing the data. Also, we identified four feature types and nine subsets of data features through thematic analysis of the selected studies. Besides, five methods for selecting and tuning the hyper-parameters are shown. In addition, we summarised the evaluation methods used in the literature and identified fifteen different metrics. The most commonly u
&lt;/p&gt;</description></item><item><title>Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.12032</link><description>&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12032
&lt;/p&gt;
&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;(WOSAC)&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#20223;&#30495;&#26159;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;WOSAC&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#30340;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#20219;&#21153;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#28608;&#21457;&#35774;&#35745;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#20852;&#36259;&#65292;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06124</link><description>&lt;p&gt;
FedDWA: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21517;&#20026;FedDWA&#65292;&#37319;&#29992;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33021;&#22815;&#35757;&#32451;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#19981;&#21516;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#33021;&#22815;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#29420;&#29305;&#38656;&#27714;&#26469;&#35757;&#32451;&#23450;&#21046;&#21270;&#27169;&#22411;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#37319;&#29992;&#19968;&#31181;&#21152;&#26435;&#32858;&#21512;&#26041;&#27861;&#26469;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#30001;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25439;&#22833;&#20540;&#25110;&#27169;&#22411;&#21442;&#25968;&#30830;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#23458;&#25143;&#31471;&#19979;&#36733;&#20854;&#20182;&#27169;&#22411;&#65292;&#19981;&#20165;&#22686;&#21152;&#20102;&#36890;&#20449;&#27969;&#37327;&#65292;&#32780;&#19988;&#21487;&#33021;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PFL&#31639;&#27861;&#65292;&#31216;&#20026;FedDWA&#65288;&#24102;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#26681;&#25454;&#20174;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#27169;&#22411;&#35745;&#31639;&#20010;&#24615;&#21270;&#32858;&#21512;&#26435;&#37325;&#12290;&#36825;&#26679;&#65292;FedDWA&#21487;&#20197;&#20197;&#26356;&#23569;&#30340;&#36890;&#20449;&#24320;&#38144;&#25429;&#25417;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23558;PFL&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26435;&#37325;&#35843;&#25972;&#26426;&#21046;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;FedDWA&#33021;&#22815;&#23398;&#20064;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#32508;&#21512;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedDWA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#39640;&#26680;&#30005;&#31449;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.05812</link><description>&lt;p&gt;
&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization. (arXiv:2305.05812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26680;&#30005;&#31449;&#29123;&#26009;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#25552;&#39640;&#26680;&#30005;&#31449;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21830;&#19994;&#26680;&#33021;&#20135;&#19994;&#35806;&#29983;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#26680;&#29123;&#26009;&#35013;&#36733;&#27169;&#24335;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#20505;&#36873;&#27169;&#24335;&#25968;&#37327;&#38750;&#24120;&#39640;&#65292;&#22240;&#27492;&#26080;&#27861;&#26126;&#30830;&#35299;&#20915;&#12290;&#19981;&#21516;&#30340;&#26680;&#33021;&#20844;&#29992;&#20107;&#19994;&#21644;&#20379;&#24212;&#21830;&#20351;&#29992;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#26469;&#25191;&#34892;&#29123;&#26009;&#24490;&#29615;&#37325;&#35013;&#35774;&#35745;&#65292;&#20294;&#25163;&#21160;&#35774;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#29366;&#24577;&#30340;&#29123;&#26009;&#24490;&#29615;&#37325;&#35013;&#27169;&#24335;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#31181;&#23613;&#21487;&#33021;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31526;&#21512;&#35774;&#35745;&#24072;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#30446;&#26631;&#12290;&#20026;&#20102;&#24110;&#21161;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#29305;&#21035;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#12290;&#26368;&#36817;&#65292;RL&#22312;&#28216;&#25103;&#20013;&#30340;&#25104;&#21151;&#32473;&#23427;&#24102;&#26469;&#20102;&#24378;&#21170;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#35758;&#30740;&#31350;&#24433;&#21709;RL&#31639;&#27861;&#24615;&#33021;&#30340;&#20960;&#20010;&#36229;&#21442;&#25968;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nuclear fuel loading pattern optimization problem has been studied since the dawn of the commercial nuclear energy industry. It is characterized by multiple objectives and constraints, with a very high number of candidate patterns, which makes it impossible to solve explicitly. Stochastic optimization methodologies are used by different nuclear utilities and vendors to perform fuel cycle reload design. Nevertheless, hand-designed solutions continue to be the prevalent method in the industry. To improve the state-of-the-art core reload patterns, we aim to create a method as scalable as possible, that agrees with the designer's goal of performance and safety. To help in this task Deep Reinforcement Learning (RL), in particular, Proximal Policy Optimization is leveraged. RL has recently experienced a strong impetus from its successes applied to games. This paper lays out the foundation of this method and proposes to study the behavior of several hyper-parameters that influence the RL 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#65292;&#24182;&#35777;&#26126;&#20102;GCV&#22312;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13016</link><description>&lt;p&gt;
&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65306;&#31561;&#25928;&#24615;&#21644;&#24191;&#20041;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation. (arXiv:2304.13016v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13016
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#65292;&#24182;&#35777;&#26126;&#20102;GCV&#22312;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#20854;&#20013;&#29305;&#24449;&#22823;&#23567;&#19982;&#26679;&#26412;&#22823;&#23567;&#25104;&#27604;&#20363;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#27604;&#29575;&#25910;&#25947;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#24179;&#26041;&#39044;&#27979;&#39118;&#38505;&#20316;&#20026;&#26174;&#24335;&#24809;&#32602;$\lambda$&#21644;&#26497;&#38480;&#23376;&#26679;&#26412;&#26041;&#38754;&#27604;$\phi_s$&#65288;&#29305;&#24449;&#22823;&#23567;&#19982;&#23376;&#26679;&#26412;&#22823;&#23567;&#30340;&#27604;&#29575;&#65289;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#20219;&#20309;&#21487;&#36798;&#39118;&#38505;&#19979;&#30340;$(\lambda, \phi_s)$-&#24179;&#38754;&#19978;&#30340;&#36718;&#24275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#65288;&#36866;&#21512;&#20110;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#26679;&#26412;&#65289;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#65292;&#22522;&#20110;&#24191;&#20041;&#20132;&#21449;&#39564;&#35777;&#65288;GCV&#65289;&#30340;&#23376;&#26679;&#26412;&#22823;&#23567;&#24378;&#19968;&#33268;&#24615;&#12290;&#36825;&#20801;&#35768;&#26080;&#38656;&#26679;&#26412;&#25286;&#20998;&#22522;&#20110;GCV&#20248;&#21270;&#20840;&#23616;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#22238;&#24402;&#39118;&#38505;&#30456;&#21305;&#37197;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\lambda$ and the limiting subsample aspect ratio $\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\lambda, \phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24615;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03935</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#20844;&#24179;&#24494;&#35843;&#31616;&#21333;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks. (arXiv:2304.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24615;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#31181;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26045;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#21363;&#36827;&#34892;&#22788;&#29702;&#20844;&#24179;&#24615;&#35757;&#32451;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22240;&#20026;&#19982;&#21518;&#22788;&#29702;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26399;&#38388;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#12290;&#34429;&#28982;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23545;&#26045;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26234;&#24935;&#21644;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#35757;&#32451;&#20844;&#24179;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#28385;&#36275;&#24863;&#20852;&#36259;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#65292;&#20351;&#29992;&#19981;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25439;&#22833;&#24494;&#35843;&#24050;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#27700;&#24179;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#29306;&#29298;&#26631;&#20934;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#32435;&#20837;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning has been deployed ubiquitously across applications in modern data science, algorithmic fairness has become a great concern and varieties of fairness criteria have been proposed. Among them, imposing fairness constraints during learning, i.e. in-processing fair training, has been a popular type of training method because they don't require accessing sensitive attributes during test time in contrast to post-processing methods. Although imposing fairness constraints have been studied extensively for classical machine learning models, the effect these techniques have on deep neural networks is still unclear. Recent research has shown that adding fairness constraints to the objective function leads to severe over-fitting to fairness criteria in large models, and how to solve this challenge is an important open question. To address this challenge, we leverage the wisdom and power of pre-training and fine-tuning and develop a simple but novel framework to train fair neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01906</link><description>&lt;p&gt;
Torch-Choice: &#29992;Python&#23454;&#29616;&#22823;&#35268;&#27169;&#36873;&#25321;&#24314;&#27169;&#30340;PyTorch&#21253;
&lt;/p&gt;
&lt;p&gt;
Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026; Torch-Choice &#30340; PyTorch &#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#31649;&#29702;&#25968;&#25454;&#24211;&#12289;&#26500;&#24314;&#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;GPU&#21152;&#36895;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$\texttt{torch-choice}$ &#26159;&#19968;&#27454;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#20351;&#29992;Python&#21644;PyTorch&#23454;&#29616;&#28789;&#27963;&#12289;&#24555;&#36895;&#30340;&#36873;&#25321;&#24314;&#27169;&#12290;&#23427;&#25552;&#20379;&#20102; $\texttt{ChoiceDataset}$ &#25968;&#25454;&#32467;&#26500;&#65292;&#20197;&#20415;&#28789;&#27963;&#32780;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#24211;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#20174;&#21508;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314; $\texttt{ChoiceDataset}$&#65292;&#24182;&#23637;&#31034;&#20102; $\texttt{ChoiceDataset}$ &#30340;&#21508;&#31181;&#21151;&#33021;&#12290;&#35813;&#36719;&#20214;&#21253;&#23454;&#29616;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;: &#22810;&#39033;&#24335;Logit&#21644;&#23884;&#22871;Logit&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#27169;&#22411;&#20272;&#35745;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#35813;&#36719;&#20214;&#21253;&#36824;&#25903;&#25345;&#20351;&#29992;GPU&#36827;&#34892;&#20272;&#35745;&#65292;&#20351;&#20854;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32780;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;R&#39118;&#26684;&#30340;&#20844;&#24335;&#23383;&#31526;&#20018;&#25110;Python&#23383;&#20856;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102; $\texttt{torch-choice}$ &#21644; R&#20013;&#30340; $\texttt{mlogit}$ &#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#30340;&#35745;&#31639;&#25928;&#29575;: (1) &#35266;&#27979;&#25968;&#22686;&#21152;&#26102;&#65292;(2) &#21327;&#21464;&#37327;&#20010;&#25968;&#22686;&#21152;&#26102;&#65292; (3) &#27979;&#35797;&#25968;&#21319;&#39640;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.17765</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20284;&#30340;&#32447;&#24615;&#34920;&#31034;&#65306;&#36866;&#24212;&#24615;&#12289;&#26497;&#23567;&#21270;&#12289;&#20197;&#21450;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#27424;&#32570;&#12290;&#26412;&#25991;&#26088;&#22312;&#29702;&#35299;&#20174;&#20855;&#26377;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#32447;&#24615;&#34920;&#31034;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#24120;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#20219;&#21153;&#25110;&#20165;&#30446;&#26631;&#23398;&#20064;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.11369</link><description>&lt;p&gt;
&#12298;&#26725;&#25509;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20048;&#35266;&#30340;&#25925;&#20107;&#12299;
&lt;/p&gt;
&lt;p&gt;
Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#19981;&#23436;&#32654;&#19987;&#23478;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#65292;&#26368;&#22909;&#30340;&#26041;&#24335;&#26159;&#20160;&#20040;&#26469;&#21033;&#29992;&#23427;&#26469;&#24341;&#23548; MDP &#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#24773;&#21518;&#39564;&#37319;&#26679;&#30340; RL&#65288;iPSRL&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#20449;&#24687;&#26469;&#29983;&#25104;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#33021;&#24178;&#65292;&#21017;&#20854;&#32047;&#31215;&#36125;&#21494;&#26031;&#36951;&#25022;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22823;&#23567; N &#19979;&#20250;&#25351;&#25968;&#24555;&#36895;&#19979;&#38477;&#21040;&#38646;&#12290;&#30001;&#20110;&#35813;&#31639;&#27861;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102; iRLSVI &#31639;&#27861;&#65292;&#21487;&#30475;&#20316;&#26159;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#30340; RLSVI &#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20004;&#20010;&#22522;&#20934;&#65288;&#27809;&#26377;&#31163;&#32447;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20294;&#19981;&#21033;&#29992;&#29983;&#25104;&#31574;&#30053;&#20449;&#24687;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; iRLSVI &#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#35745;&#31639;&#39044;&#31639;&#30340;&#38480;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;&#20256;&#32479;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#12289;&#33976;&#39311;&#25439;&#22833;&#21644;&#37096;&#20998;&#24494;&#35843;&#26041;&#27861;&#22312;&#25968;&#25454;&#36882;&#22686;&#12289;&#31867;&#36882;&#22686;&#21644;&#26102;&#38388;&#36882;&#22686;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11165</link><description>&lt;p&gt;
&#35745;&#31639;&#39044;&#31639;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#20160;&#20040;&#25165;&#26159;&#37325;&#35201;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#35745;&#31639;&#39044;&#31639;&#30340;&#38480;&#21046;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;&#20256;&#32479;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#12289;&#33976;&#39311;&#25439;&#22833;&#21644;&#37096;&#20998;&#24494;&#35843;&#26041;&#27861;&#22312;&#25968;&#25454;&#36882;&#22686;&#12289;&#31867;&#36882;&#22686;&#21644;&#26102;&#38388;&#36882;&#22686;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20445;&#25345;&#20197;&#21069;&#30340;&#30693;&#35782;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#23545;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#36827;&#34892;&#39034;&#24207;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#25968;&#25454;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#12290;&#24403;&#21069;&#30340;&#25345;&#32493;&#23398;&#20064;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#23545;&#20197;&#21069;&#30340;&#25968;&#25454;&#26377;&#38480;&#35775;&#38382;&#30340;&#38382;&#39064;&#65292;&#32780;&#23545;&#35757;&#32451;&#30340;&#35745;&#31639;&#39044;&#31639;&#27809;&#26377;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#12290;&#36825;&#22312;&#37326;&#22806;&#24212;&#29992;&#20013;&#26159;&#19981;&#21512;&#29702;&#30340;&#65292;&#22240;&#20026;&#31995;&#32479;&#20027;&#35201;&#21463;&#21040;&#35745;&#31639;&#21644;&#26102;&#38388;&#39044;&#31639;&#30340;&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#23384;&#20648;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20256;&#32479;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#30001;&#20110;&#26377;&#38480;&#30340;&#35745;&#31639;&#65292;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26377;&#25928;&#20869;&#23384;&#26679;&#26412;&#21487;&#20197;&#34987;&#38544;&#24335;&#38480;&#21046;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;ImageNet2K&#21644;Continual Google Landmarks V2&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#12289;&#33976;&#39311;&#25439;&#22833;&#21644;&#37096;&#20998;&#24494;&#35843;&#65292;&#22312;&#25968;&#25454;&#36882;&#22686;&#12289;&#31867;&#36882;&#22686;&#21644;&#26102;&#38388;&#36882;&#22686;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#25193;&#23637;&#23454;&#39564;&#35780;&#20215;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#12289;&#33976;&#39311;&#25439;&#22833;&#21644;&#37096;&#20998;&#24494;&#35843;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensi
&lt;/p&gt;</description></item><item><title>Ref-NeuS&#36890;&#36807;&#20943;&#23569;&#21453;&#23556;&#34920;&#38754;&#30340;&#24433;&#21709;&#20197;&#38477;&#20302;&#27495;&#20041;&#65292;&#22312;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.10840</link><description>&lt;p&gt;
Ref-NeuS: &#20943;&#23569;&#27495;&#20041;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#38754;&#23398;&#20064;&#65292;&#29992;&#20110;&#24102;&#26377;&#21453;&#23556;&#30340;&#22810;&#35270;&#35282;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection. (arXiv:2303.10840v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10840
&lt;/p&gt;
&lt;p&gt;
Ref-NeuS&#36890;&#36807;&#20943;&#23569;&#21453;&#23556;&#34920;&#38754;&#30340;&#24433;&#21709;&#20197;&#38477;&#20302;&#27495;&#20041;&#65292;&#22312;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#34920;&#38754;&#23398;&#20064;&#22312;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20854;&#20013;&#23545;&#35937;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#34920;&#31034;&#65292;&#25552;&#20379;&#36830;&#32493;&#30340;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#21644;&#35270;&#35282;&#30456;&#20851;&#36752;&#23556;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#20934;&#30830;&#37325;&#24314;&#21453;&#23556;&#34920;&#38754;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#27495;&#20041;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ref-NeuS&#65292;&#26088;&#22312;&#36890;&#36807;&#21066;&#24369;&#21453;&#23556;&#34920;&#38754;&#30340;&#24433;&#21709;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#20272;&#35745;&#19968;&#20010;&#26174;&#24335;&#30340;&#21453;&#23556;&#20998;&#25968;&#65292;&#24182;&#22312;&#22810;&#35270;&#35282;&#19978;&#19979;&#25991;&#30340;&#25351;&#23548;&#19979;&#23450;&#20301;&#21453;&#23556;&#34920;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#23556;&#24863;&#30693;&#30340;&#20809;&#24230;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#28210;&#26579;&#30340;&#39068;&#33394;&#24314;&#27169;&#20026;&#39640;&#26031;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#20943;&#23569;&#27495;&#20041;&#65292;&#20854;&#20013;&#21453;&#23556;&#20998;&#25968;&#20195;&#34920;&#26041;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#32467;&#21512;&#21453;&#23556;&#26041;&#21521;&#30456;&#20851;&#30340;&#36752;&#23556;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#21453;&#23556;&#34920;&#38754;&#19978;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#34920;&#38754;&#37325;&#24314;&#65292;&#24182;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit surface learning has shown significant progress in multi-view 3D reconstruction, where an object is represented by multilayer perceptrons that provide continuous implicit surface representation and view-dependent radiance. However, current methods often fail to accurately reconstruct reflective surfaces, leading to severe ambiguity. To overcome this issue, we propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of reflective surfaces. Specifically, we utilize an anomaly detector to estimate an explicit reflection score with the guidance of multi-view context to localize reflective surfaces. Afterward, we design a reflection-aware photometric loss that adaptively reduces ambiguity by modeling rendered color as a Gaussian distribution, with the reflection score representing the variance. We show that together with a reflection direction-dependent radiance, our model achieves high-quality surface reconstruction on reflective surfaces and outperforms t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.09705</link><description>&lt;p&gt;
&#22312;&#20803;&#26641;&#19978;&#25209;&#37327;&#26356;&#26032;&#21518;&#39564;&#26641;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#26641;&#21644;&#19968;&#20010;&#24207;&#21015;&#26356;&#26032;&#26041;&#27861;&#34920;&#31034;&#30340;&#27010;&#29575;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#19968;&#32452;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#38598;&#21512;&#31216;&#20026;&#20803;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previously, we proposed a probabilistic data generation model represented by an unobservable tree and a sequential updating method to calculate a posterior distribution over a set of trees. The set is called a meta-tree. In this paper, we propose a more efficient batch updating method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.08691</link><description>&lt;p&gt;
&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SSBM&#65292;&#23427;&#21482;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25506;&#32034;&#20102;&#20174;&#19981;&#23436;&#25972;&#30340;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#36825;&#20026;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#24674;&#22797;&#20449;&#21495;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SSBM&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#20165;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#23398;&#20064;&#20449;&#21495;&#37325;&#26500;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#21644;&#31185;&#23398;&#25104;&#20687;&#20197;&#21450;&#20256;&#24863;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#32463;&#24120;&#31232;&#32570;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#37327;&#19981;&#20165;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#65292;&#32780;&#19988;&#36824;&#34987;&#37327;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20174;&#20108;&#36827;&#21046;&#35266;&#23519;&#20013;&#23398;&#20064;&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20174;&#19981;&#23436;&#25972;&#20108;&#36827;&#21046;&#25968;&#25454;&#20013;&#35782;&#21035;&#19968;&#32452;&#20449;&#21495;&#25152;&#38656;&#30340;&#27979;&#37327;&#25968;&#37327;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#20174;&#20108;&#36827;&#21046;&#27979;&#37327;&#20013;&#20449;&#21495;&#24674;&#22797;&#29616;&#26377;&#30028;&#38480;&#30340;&#34917;&#20805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;SSBM&#8221;&#65292;&#23427;&#20165;&#38656;&#35201;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;SSBM&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#24182;&#20248;&#20110;&#31232;&#30095;&#37325;&#26500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in unsupervised learning have highlighted the possibility of learning to reconstruct signals from noisy and incomplete linear measurements alone. These methods play a key role in medical and scientific imaging and sensing, where ground truth data is often scarce or difficult to obtain. However, in practice, measurements are not only noisy and incomplete but also quantized. Here we explore the extreme case of learning from binary observations and provide necessary and sufficient conditions on the number of measurements required for identifying a set of signals from incomplete binary data. Our results are complementary to existing bounds on signal recovery from binary measurements. Furthermore, we introduce a novel self-supervised learning approach, which we name SSBM, that only requires binary data for training. We demonstrate in a series of experiments with real datasets that SSBM performs on par with supervised learning and outperforms sparse reconstruction methods wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Brain-Score&#35780;&#20272;64&#20010;CNN&#27169;&#22411;&#20013;&#19982;&#22270;&#20687;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.07679</link><description>&lt;p&gt;
&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#25152;&#38656;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Feature representations useful for predicting image memorability. (arXiv:2303.07679v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Brain-Score&#35780;&#20272;64&#20010;CNN&#27169;&#22411;&#20013;&#19982;&#22270;&#20687;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#24050;&#32463;&#21560;&#24341;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#24050;&#32463;&#25509;&#36817;&#22522;&#20110;&#20154;&#31867;&#19968;&#33268;&#24615;&#20272;&#35745;&#30340;&#32463;&#39564;&#19978;&#38480;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#23884;&#20837;&#22312;CNN&#27169;&#22411;&#20013;&#30340;&#21738;&#20123;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#35760;&#24518;&#21147;&#30340;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#36127;&#36131;&#20173;&#28982;&#26159;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#33041;&#37096;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;CNN&#27169;&#22411;&#20013;&#19982;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;Brain-Score&#35780;&#20272;&#20102;&#22312;64&#20010;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#30340;CNN&#27169;&#22411;&#30340;16,860&#23618;&#20013;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#33041;&#37096;&#30456;&#20284;&#24615;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#20998;&#26512;&#26174;&#31034;&#20986;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65292;&#21363;&#20855;&#26377;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#26368;&#39640;&#38454;&#27573;&#30340;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#23545;64&#20010;CNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;CNN&#27169;&#22411;&#24050;&#32463;&#21253;&#21547;&#20102;&#19982;&#35760;&#24518;&#21147;&#30456;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting image memorability has attracted interest in various fields. Consequently, prediction accuracy with convolutional neural network (CNN) models has been approaching the empirical upper bound estimated based on human consistency. However, identifying which feature representations embedded in CNN models are responsible for such high prediction accuracy of memorability remains an open question. To tackle this problem, this study sought to identify memorability-related feature representations in CNN models using brain similarity. Specifically, memorability prediction accuracy and brain similarity were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models pretrained for object recognition. A clear tendency was shown in this comprehensive analysis that layers with high memorability prediction accuracy had higher brain similarity with the inferior temporal (IT) cortex, which is the highest stage in the ventral visual pathway. Furthermore, fine-tuning the 64 CNN m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#27169;&#22411;&#29702;&#35770;&#30340;&#32771;&#34385;&#12290;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#21644;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#25552;&#20379;&#20102;&#26694;&#26550;&#21644;&#21487;&#35777;&#26126;&#24615;&#36136;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2303.04676</link><description>&lt;p&gt;
&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#27169;&#22411;&#29702;&#35770;&#30340;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Considerations on the Theory of Training Models with Differential Privacy. (arXiv:2303.04676v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#27169;&#22411;&#29702;&#35770;&#30340;&#32771;&#34385;&#12290;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#21644;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#25552;&#20379;&#20102;&#26694;&#26550;&#21644;&#21487;&#35777;&#26126;&#24615;&#36136;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#19968;&#32452;&#23458;&#25143;&#31471;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#24076;&#26395;&#25511;&#21046;&#20854;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#22914;&#20309;&#20445;&#25345;&#31169;&#23494;&#65311;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#38480;&#21046;&#38544;&#31169;&#27844;&#28431;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#26694;&#26550;&#21644;&#21487;&#35777;&#26126;&#24615;&#36136;&#30340;&#27010;&#36848;&#65292;&#37319;&#29992;&#26368;&#26032;&#30340;&#22522;&#20110;&#20551;&#35774;&#30340;&#23450;&#20041;&#65292;&#21363;&#39640;&#26031;&#24046;&#20998;&#38544;&#31169;&#25110;$f$-DP&#65292;&#24182;&#35752;&#35770;&#20102;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#12290;&#25105;&#20204;&#20445;&#25345;&#22312;&#20803;&#27700;&#24179;&#19978;&#65292;&#24182;&#23581;&#35797;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#35299;&#37322;&#21644;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning collaborative learning takes place by a set of clients who each want to remain in control of how their local training data is used, in particular, how can each client's local training data remain private? Differential privacy is one method to limit privacy leakage. We provide a general overview of its framework and provable properties, adopt the more recent hypothesis based definition called Gaussian DP or $f$-DP, and discuss Differentially Private Stochastic Gradient Descent (DP-SGD). We stay at a meta level and attempt intuitive explanations and insights \textit{in this book chapter}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;</title><link>http://arxiv.org/abs/2303.04185</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#19982;&#26080;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#65292;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20165;&#38656;&#20960;&#20998;&#38047;&#23601;&#33021;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#32780;&#20934;&#30830;&#24230;&#20165;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#25104;&#21151;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#24310;&#36831;&#12290;&#38543;&#30528;&#24320;&#21457;&#20154;&#21592;&#21644;&#31532;&#19977;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#23450;&#21046;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#25512;&#29702;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;&#35768;&#22810;&#21162;&#21147;&#23581;&#35797;&#36890;&#36807;&#21098;&#26525;&#21644;&#33976;&#39311;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#35201;&#20040;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#35201;&#20040;&#22240;&#20026;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#20197;&#24674;&#22797;&#20934;&#30830;&#24615;&#32780;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26080;&#26799;&#24230;&#32467;&#26500;&#21270;&#21098;&#26525;&#26694;&#26550;&#12290;&#20351;&#29992;BERT$_{BASE}$&#21644;DistilBERT&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#20165;&#38656;&#20960;&#20998;&#38047;&#65292;&#21363;&#21487;&#23558;&#21407;&#22987;FLOP&#35745;&#25968;&#30340;&#26368;&#39640;40%&#20943;&#23569;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#19981;&#36229;&#36807;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accur
&lt;/p&gt;</description></item><item><title>CleanCLIP&#26159;&#19968;&#20010;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.03323</link><description>&lt;p&gt;
&#28165;&#27905;CLIP: &#32531;&#35299;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03323
&lt;/p&gt;
&lt;p&gt;
CleanCLIP&#26159;&#19968;&#20010;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#29992;&#20110;&#22312;&#22823;&#37327;&#37197;&#23545;&#30340;&#22270;&#25991;&#25968;&#25454;&#19978;&#35757;&#32451;&#22810;&#27169;&#24577;&#34920;&#31034;&#27169;&#22411;&#65292;&#22914;CLIP&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#31867;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#22312;&#21547;&#26377;&#21518;&#38376;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;CLIP&#23398;&#20064;&#21040;&#20102;&#23884;&#20837;&#24335;&#21518;&#38376;&#35302;&#21457;&#22120;&#19982;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#22312;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#21363;&#20351;&#27880;&#20837;&#20102;&#23569;&#37327;&#30340;&#27602;&#21270;&#31034;&#20363;&#65292;&#20363;&#22914;&#22312;3000000&#20010;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#20102;75&#20010;&#31034;&#20363;&#65292;&#20063;&#33021;&#26174;&#33879;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20351;&#20854;&#38590;&#20197;&#26816;&#27979;&#25110;&#24536;&#35760;&#36825;&#31181;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CleanCLIP&#65292;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#37325;&#26032;&#23545;&#40784;&#20010;&#21035;&#27169;&#24577;&#30340;&#34920;&#31034;&#26469;&#21066;&#24369;&#21518;&#38376;&#25915;&#20987;&#24341;&#20837;&#30340;&#23398;&#20064;&#21040;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#21644;&#21333;&#27169;&#24577;&#33258;&#30417;&#30563;&#30340;&#32452;&#21512;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervi
&lt;/p&gt;</description></item><item><title>&#21464;&#35843;&#31070;&#32463;ODEs &#65288;MoNODEs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31070;&#32463;ODE&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19981;&#21464;&#30340;&#35843;&#21046;&#21464;&#37327;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#31561;&#26041;&#38754;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.13262</link><description>&lt;p&gt;
&#21464;&#35843;&#31070;&#32463;ODEs
&lt;/p&gt;
&lt;p&gt;
Modulated Neural ODEs. (arXiv:2302.13262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13262
&lt;/p&gt;
&lt;p&gt;
&#21464;&#35843;&#31070;&#32463;ODEs &#65288;MoNODEs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31070;&#32463;ODE&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19981;&#21464;&#30340;&#35843;&#21046;&#21464;&#37327;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#31561;&#26041;&#38754;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23398;&#20064;&#20219;&#24847;&#36712;&#36857;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NODE&#26041;&#27861;&#20165;&#36890;&#36807;&#21021;&#22987;&#29366;&#24577;&#20540;&#25110;&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;&#26356;&#26032;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#35843;&#31070;&#32463;ODEs&#65288;MoNODEs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#25913;&#36827;&#29616;&#26377;NODE&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#8220;&#26102;&#38388;&#19981;&#21464;&#35843;&#21046;&#21464;&#37327;&#8221;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#21040;&#22235;&#31181;&#29616;&#26377;&#30340;NODE&#21464;&#20307;&#20013;&#12290;&#25105;&#20204;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#19978;&#23545;MoNODE&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#27599;&#20010;&#36712;&#36857;&#37117;&#20855;&#26377;&#36712;&#36857;&#29305;&#23450;&#30340;&#35843;&#21046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22987;&#32456;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#26032;&#30340;&#21160;&#24577;&#21442;&#25968;&#21270;&#24182;&#36827;&#34892;&#36828;&#26399;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#35843;&#21046;&#21464;&#37327;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce $\textit{time-invariant modulator variables}$ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are infor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;-free &#30340;&#21160;&#24577; SGD &#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#65292; &#23427;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#65292;&#20294;&#26159;&#22312;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#20013;&#25317;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#26377;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340; SGD &#30456;&#24403;&#25509;&#36817;&#12290;</title><link>http://arxiv.org/abs/2302.12022</link><description>&lt;p&gt;
DoG&#26159;SGD&#26368;&#22909;&#30340;&#26379;&#21451;&#65306;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#30340;&#21160;&#24577;&#27493;&#38271;&#22823;&#23567;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12022
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;-free &#30340;&#21160;&#24577; SGD &#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#65292; &#23427;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#65292;&#20294;&#26159;&#22312;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#20013;&#25317;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19982;&#26377;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340; SGD &#30456;&#24403;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#30340;&#21160;&#24577;SGD&#27493;&#38271;&#20844;&#24335;&#65292;&#31216;&#20026;&#26799;&#24230;&#36317;&#31163;&#20844;&#24335;&#65288;DoG&#65289;&#12290;DoG&#27493;&#38271;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#32463;&#39564;&#37327;&#65288;&#21021;&#22987;&#28857;&#36317;&#31163;&#21644;&#26799;&#24230;&#33539;&#25968;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#8220;&#23398;&#20064;&#29575;&#8221;&#21442;&#25968;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DoG&#20844;&#24335;&#30340;&#19968;&#20010;&#30053;&#24494;&#21464;&#21270;&#21487;&#20197;&#20445;&#35777;&#20855;&#26377;&#24378;&#22823;&#30340;&#26080;&#21442;&#25968;&#25910;&#25947;&#24615;&#65292;&#20551;&#23450;&#21482;&#26377;&#23616;&#37096;&#26377;&#30028;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#26174;&#31034;DoG&#30340;&#24615;&#33021;&#25509;&#36817;&#20855;&#26377;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#30340;SGD&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#21464;&#37327;&#30340;DoG&#21464;&#20307;&#65292;&#36890;&#24120;&#20248;&#20110;&#35843;&#25972;&#30340;SGD&#65292;&#24182;&#25509;&#36817;&#35843;&#25972;&#30340;Adam&#30340;&#24615;&#33021;&#12290;PyTorch&#23454;&#29616;&#21487;&#22312;https://github.com/formll/dog&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.10893</link><description>&lt;p&gt;
&#20844;&#24179;&#25193;&#25955;&#65306;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#12290;&#20294;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#38543;&#26426;&#25277;&#21462;&#30340;&#21313;&#20159;&#32423;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#20250;&#21463;&#21040;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#21152;&#21095;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#19981;&#20165;&#25581;&#31034;&#32780;&#19988;&#23545;&#25239;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#20844;&#24179;&#25193;&#25955;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20154;&#31867;&#25351;&#23548;&#30340;&#20559;&#24046;&#36716;&#31227;&#65292;&#21487;&#22312;&#20219;&#20309;&#26041;&#21521;&#19978;&#20135;&#29983;&#20219;&#24847;&#26032;&#30340;&#27604;&#20363;&#65292;&#20363;&#22914;&#65292;&#36523;&#20221;&#32452;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#31034;&#65292;&#36825;&#31181;&#25511;&#21046;&#20351;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#33021;&#22815;&#25509;&#21463;&#25351;&#23548;&#65292;&#26080;&#38656;&#25968;&#25454;&#36807;&#28388;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30001;&#24418;&#24335;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#25512;&#26029;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.09921</link><description>&lt;p&gt;
&#33258;&#30001;&#24418;&#24335;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Free-Form Variational Inference for Gaussian Process State-Space Models. (arXiv:2302.09921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30001;&#24418;&#24335;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#25512;&#26029;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#20026;&#24314;&#27169;&#28508;&#22312;&#29366;&#24577;&#30340;&#21160;&#24577;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20284;&#28982;&#27169;&#22411;&#20197;&#31163;&#25955;&#26102;&#38388;&#28857;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#20013;&#28508;&#22312;&#21464;&#37327;&#30340;&#25968;&#37327;&#36739;&#22823;&#19988;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24378;&#26102;&#24207;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#22312; GPSSMs &#20013;&#36827;&#34892;&#25512;&#26029;&#26159;&#35745;&#31639;&#19978;&#21644;&#32479;&#35745;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031; GPSSMs &#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#36807;&#20110;&#31616;&#21270;&#30340;&#20551;&#35774;&#21644;&#39640;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#35825;&#23548;&#21464;&#37327;&#24418;&#24335;&#20027;&#20041;&#20869;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#21464;&#20998;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21464;&#20998;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25240;&#21472;&#25193;&#23637;&#26041;&#27861;&#65292;&#20854;&#20013;&#35825;&#23548;&#21464;&#37327;&#22312;&#35299;&#26512;&#19978;&#36827;&#34892;&#36793;&#38469;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#31890;&#23376; MCMC &#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;s&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#25512;&#26029;&#20934;&#30830;&#24615;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process state-space models (GPSSMs) provide a principled and flexible approach to modeling the dynamics of a latent state, which is observed at discrete-time points via a likelihood model. However, inference in GPSSMs is computationally and statistically challenging due to the large number of latent variables in the model and the strong temporal dependencies between them. In this paper, we propose a new method for inference in Bayesian GPSSMs, which overcomes the drawbacks of previous approaches, namely over-simplified assumptions, and high computational requirements. Our method is based on free-form variational inference via stochastic gradient Hamiltonian Monte Carlo within the inducing-variable formalism. Furthermore, by exploiting our proposed variational distribution, we provide a collapsed extension of our method where the inducing variables are marginalized analytically. We also showcase results when combining our framework with particle MCMC methods. We show that, on s
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#23545;&#40784;&#21644;&#34701;&#21512;&#26469;&#35299;&#20915;&#36755;&#20837;&#22270;&#20687;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;AFHQ-Cat&#19978;&#39564;&#35777;&#20102;&#20248;&#20110;&#29616;&#26377;2D&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06608</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#23545;&#40784;&#21644;&#34701;&#21512;&#26469;&#35299;&#20915;&#36755;&#20837;&#22270;&#20687;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;AFHQ-Cat&#19978;&#39564;&#35777;&#20102;&#20248;&#20110;&#29616;&#26377;2D&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#26088;&#22312;&#26080;&#32541;&#22320;&#21512;&#24182;&#22810;&#20010;&#22270;&#20687;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;2D&#30340;&#26041;&#27861;&#26469;&#35828;&#65292;&#22914;&#26524;&#36755;&#20837;&#22270;&#20687;&#30001;&#20110;3D&#30456;&#26426;&#23039;&#24577;&#21644;&#29289;&#20307;&#24418;&#29366;&#30340;&#24046;&#24322;&#32780;&#19981;&#23545;&#40784;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;3D&#24863;&#30693;&#23545;&#40784;&#21644;3D&#24863;&#30693;&#34701;&#21512;&#12290;&#23545;&#20110;3D&#24863;&#30693;&#23545;&#40784;&#65292;&#25105;&#20204;&#39318;&#20808;&#20272;&#35745;&#19982;&#29983;&#25104;&#24335;NeRF&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#30456;&#26426;&#23039;&#24577;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;3D&#23616;&#37096;&#23545;&#40784;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;3D&#24863;&#30693;&#30340;&#34701;&#21512;&#65292;&#23427;&#30452;&#25509;&#22312;NeRF&#30340;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#19978;&#36827;&#34892;&#22270;&#20687;&#34701;&#21512;&#65292;&#32780;&#19981;&#26159;&#22312;&#21407;&#22987;&#20687;&#32032;&#31354;&#38388;&#19978;&#36827;&#34892;&#12290;&#36890;&#36807;&#23545;FFHQ&#21644;AFHQ-Cat&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;2D&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image blending aims to combine multiple images seamlessly. It remains challenging for existing 2D-based methods, especially when input images are misaligned due to differences in 3D camera poses and object shapes. To tackle these issues, we propose a 3D-aware blending method using generative Neural Radiance Fields (NeRF), including two key components: 3D-aware alignment and 3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of the reference image with respect to generative NeRFs and then perform 3D local alignment for each part. To further leverage 3D information of the generative NeRF, we propose 3D-aware blending that directly blends images on the NeRF's latent representation space, rather than raw pixel space. Collectively, our method outperforms existing 2D baselines, as validated by extensive quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
&lt;/p&gt;</description></item><item><title>CholecTriplet2022&#26159;&#19968;&#20010;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25163;&#26415;&#27963;&#21160;&#24418;&#24335;&#21270;&#20026;&#24037;&#20855;&#12289;&#34892;&#21160;&#21644;&#30446;&#26631;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#20803;&#32452;&#26469;&#24110;&#21161;&#24320;&#21457;&#26356;&#22909;&#30340;&#22270;&#20687;&#24341;&#23548;&#25163;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#12290;&#26412;&#25361;&#25112;&#20174;&#35782;&#21035;&#25193;&#23637;&#21040;&#26816;&#27979;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#24314;&#27169;&#65292;&#24182;&#21253;&#25324;&#23545;&#27599;&#20010;&#24037;&#20855;&#30340;&#36793;&#30028;&#26694;&#23450;&#20301;&#21644;&#23545;&#24037;&#20855;-&#27963;&#21160;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2302.06294</link><description>&lt;p&gt;
CholecTriplet2022&#65306;&#23637;&#31034;&#32473;&#25105;&#19968;&#20010;&#24037;&#20855;&#65292;&#24182;&#21578;&#35785;&#25105;&#19977;&#20803;&#32452;&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#26816;&#27979;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection. (arXiv:2302.06294v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06294
&lt;/p&gt;
&lt;p&gt;
CholecTriplet2022&#26159;&#19968;&#20010;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25163;&#26415;&#27963;&#21160;&#24418;&#24335;&#21270;&#20026;&#24037;&#20855;&#12289;&#34892;&#21160;&#21644;&#30446;&#26631;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#20803;&#32452;&#26469;&#24110;&#21161;&#24320;&#21457;&#26356;&#22909;&#30340;&#22270;&#20687;&#24341;&#23548;&#25163;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#12290;&#26412;&#25361;&#25112;&#20174;&#35782;&#21035;&#25193;&#23637;&#21040;&#26816;&#27979;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#24314;&#27169;&#65292;&#24182;&#21253;&#25324;&#23545;&#27599;&#20010;&#24037;&#20855;&#30340;&#36793;&#30028;&#26694;&#23450;&#20301;&#21644;&#23545;&#24037;&#20855;-&#27963;&#21160;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25163;&#26415;&#27963;&#21160;&#24418;&#24335;&#21270;&#20026;&#25152;&#20351;&#29992;&#30340;&#24037;&#20855;&#12289;&#25191;&#34892;&#30340;&#34892;&#21160;&#21644;&#30446;&#26631;&#35299;&#21078;&#32467;&#26500;&#30340;&#19977;&#20803;&#32452;&#65292;&#24050;&#32463;&#25104;&#20026;&#25163;&#26415;&#27963;&#21160;&#24314;&#27169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26041;&#27861;&#12290;&#36825;&#31181;&#24418;&#24335;&#21270;&#26377;&#21161;&#20110;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#24037;&#20855;&#19982;&#32452;&#32455;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21487;&#20197;&#29992;&#26469;&#24320;&#21457;&#26356;&#22909;&#30340;&#22270;&#20687;&#24341;&#23548;&#25163;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#12290;&#26089;&#26399;&#30340;&#21162;&#21147;&#21644;2021&#24180;&#24341;&#20837;&#30340;CholecTriplet&#25361;&#25112;&#25552;&#20986;&#20102;&#26088;&#22312;&#35782;&#21035;&#25163;&#26415;&#24405;&#20687;&#20013;&#30340;&#36825;&#20123;&#19977;&#20803;&#32452;&#30340;&#25216;&#26415;&#12290;&#20272;&#35745;&#19977;&#20803;&#32452;&#30340;&#31354;&#38388;&#20301;&#32622;&#36824;&#23558;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#26415;&#20013;&#29615;&#22659;&#24863;&#30693;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CholecTriplet2022&#25361;&#25112;&#65292;&#20174;&#35782;&#21035;&#25193;&#23637;&#21040;&#26816;&#27979;&#25163;&#26415;&#34892;&#21160;&#19977;&#20803;&#32452;&#24314;&#27169;&#12290;&#23427;&#21253;&#25324;&#27599;&#20010;&#21487;&#35265;&#25163;&#26415;&#20202;&#22120;&#65288;&#25110;&#24037;&#20855;&#65289;&#30340;&#24369;&#30417;&#30563;&#36793;&#30028;&#26694;&#23450;&#20301;&#65292;&#20316;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#20197;&#21450;&#23545;&#27599;&#20010;&#24037;&#20855;-&#27963;&#21160;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formalizing surgical activities as triplets of the used instruments, actions performed, and target anatomies is becoming a gold standard approach for surgical activity modeling. The benefit is that this formalization helps to obtain a more detailed understanding of tool-tissue interaction which can be used to develop better Artificial Intelligence assistance for image-guided surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021 have put together techniques aimed at recognizing these triplets from surgical footage. Estimating also the spatial locations of the triplets would offer a more precise intraoperative context-aware decision support for computer-assisted intervention. This paper presents the CholecTriplet2022 challenge, which extends surgical action triplet modeling from recognition to detection. It includes weakly-supervised bounding box localization of every visible surgical instrument (or tool), as the key actors, and the modeling of each tool-activity in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21152;&#26435;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#38598;&#25104;&#26469;&#33258;&#21160;&#21270;&#25552;&#31034;&#24037;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06235</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21152;&#26435;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21152;&#26435;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#38598;&#25104;&#26469;&#33258;&#21160;&#21270;&#25552;&#31034;&#24037;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35757;&#32451;&#30340;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#65292;&#21363;&#23558;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#27169;&#22411;&#20174;&#26410;&#26126;&#30830;&#35757;&#32451;&#36807;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#38656;&#35201;&#25552;&#31034;&#24037;&#31243;&#26469;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#25552;&#31034;&#24037;&#31243;&#36890;&#24120;&#38656;&#35201;&#25163;&#24037;&#21019;&#24314;&#19968;&#32452;&#29992;&#20110;&#20010;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#31034;&#38598;&#25104;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#25552;&#31034;&#24037;&#31243;&#65292;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#32473;&#23450;&#22823;&#37327;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#25552;&#31034;&#24182;&#38598;&#25104;&#37027;&#20123;&#23545;&#29305;&#23450;&#19979;&#28216;&#25968;&#25454;&#38598;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26377;&#26631;&#31614;&#30340;&#39564;&#35777;&#25968;&#25454;&#65311;&#8221;&#25105;&#20204;&#35777;&#26126;&#36825;&#26159;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#22825;&#30495;&#30340;&#25552;&#31034;&#35780;&#20998;&#26041;&#27861;&#20013;&#30340;&#20960;&#20010;&#30149;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#20998;&#25968;&#24456;&#23481;&#26131;&#22240;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#32780;&#36807;&#20110;&#33258;&#20449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask "Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?". We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#24494;&#32467;&#26500;&#36827;&#34892;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#30340;&#28508;&#22312;&#24494;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.02521</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#24494;&#32467;&#26500;&#36827;&#34892;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation. (arXiv:2302.02521v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#24494;&#32467;&#26500;&#36827;&#34892;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#30340;&#28508;&#22312;&#24494;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#23545;&#20110;&#20174;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#33258;&#21160;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#21313;&#20998;&#20851;&#38190;&#12290;&#26126;&#30830;&#20248;&#21270;&#25152;&#26377;&#27169;&#24577;&#38388;&#20849;&#20139;&#30340;&#20849;&#21516;&#20449;&#24687;&#65288;&#20363;&#22914;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30456;&#20851;&#24615;&#65289;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#23376;&#38598;&#38388;&#20849;&#20139;&#30340;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#21364;&#26080;&#35270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35782;&#21035;&#36825;&#31181;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#22686;&#24378;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#25513;&#30721;&#65288;PCI-mask&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#20197;&#32454;&#31890;&#24230;&#22320;&#25551;&#36848;&#21738;&#20123;&#27169;&#24577;&#23376;&#38598;&#20043;&#38388;&#20849;&#20139;&#20102;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#12290;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#25513;&#30721;&#30456;&#20851;&#24615;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#26368;&#20248;&#30340;PCI-mask&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#37096;&#20998;&#20849;&#21516;&#20449;&#24687;&#30340;&#28508;&#22312;&#24494;&#32467;&#26500;&#65292;&#24182;&#22312;&#33041;&#32959;&#30244;&#20998;&#21106;&#20013;&#21033;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with multiple modalities is crucial for automated brain tumor segmentation from magnetic resonance imaging data. Explicitly optimizing the common information shared among all modalities (e.g., by maximizing the total correlation) has been shown to achieve better feature representations and thus enhance the segmentation performance. However, existing approaches are oblivious to partial common information shared by subsets of the modalities. In this paper, we show that identifying such partial common information can significantly boost the discriminative power of image segmentation models. In particular, we introduce a novel concept of partial common information mask (PCI-mask) to provide a fine-grained characterization of what partial common information is shared by which subsets of the modalities. By solving a masked correlation maximization and simultaneously learning an optimal PCI-mask, we identify the latent microstructure of partial common information and leverage it in a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26032;&#30340;&#29275;&#39039;&#26041;&#27861;&#21464;&#31181;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#26367;&#25442;&#26799;&#24230;&#21644;&#28023;&#26862;&#30697;&#38453;&#65292;&#35777;&#26126;&#20102;&#36830;&#32493;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#32676;&#27700;&#24179;&#26368;&#23567;&#21270;&#22120;&#21608;&#22260;&#23567;&#29699;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#30340;&#31639;&#27861;&#26469;&#33719;&#21462;&#40065;&#26834;&#29275;&#39039;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.13192</link><description>&lt;p&gt;
&#36890;&#36807;&#29275;&#39039;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust empirical risk minimization via Newton's method. (arXiv:2301.13192v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26032;&#30340;&#29275;&#39039;&#26041;&#27861;&#21464;&#31181;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#26469;&#26367;&#25442;&#26799;&#24230;&#21644;&#28023;&#26862;&#30697;&#38453;&#65292;&#35777;&#26126;&#20102;&#36830;&#32493;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#32676;&#27700;&#24179;&#26368;&#23567;&#21270;&#22120;&#21608;&#22260;&#23567;&#29699;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#30340;&#31639;&#27861;&#26469;&#33719;&#21462;&#40065;&#26834;&#29275;&#39039;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#29275;&#39039;&#26041;&#27861;&#21464;&#31181;&#65292;&#29992;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#22312;&#20248;&#21270;&#31639;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#21644;&#28023;&#26862;&#30697;&#38453;&#34987;&#26367;&#25442;&#20026;&#29616;&#26377;&#25991;&#29486;&#20013;&#38024;&#23545;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#35777;&#26126;&#20102;&#36830;&#32493;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#32676;&#27700;&#24179;&#26368;&#23567;&#21270;&#22120;&#21608;&#22260;&#23567;&#29699;&#30340;&#19968;&#33324;&#23450;&#29702;&#20043;&#21518;&#65292;&#30740;&#31350;&#20102;&#24403;&#25968;&#25454;&#26469;&#33258;Huber&#30340;epsilon&#27745;&#26579;&#27169;&#22411;&#21644;/&#25110;&#37325;&#23614;&#20998;&#24067;&#26102;&#65292;&#35813;&#29702;&#35770;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#21518;&#26524;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#33719;&#21462;&#40065;&#26834;&#29275;&#39039;&#26041;&#21521;&#30340;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26356;&#36866;&#29992;&#20110;&#39640;&#32500;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#32467;&#26524;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29468;&#24819;&#12290;&#19982;&#40065;&#26834;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new variant of Newton's method for empirical risk minimization is studied, where at each iteration of the optimization algorithm, the gradient and Hessian of the objective function are replaced by robust estimators taken from existing literature on robust mean estimation for multivariate data. After proving a general theorem about the convergence of successive iterates to a small ball around the population-level minimizer, consequences of the theory in generalized linear models are studied when data are generated from Huber's epsilon-contamination model and/or heavytailed distributions. An algorithm for obtaining robust Newton directions based on the conjugate gradient method is also proposed, which may be more appropriate for high-dimensional settings, and conjectures about the convergence of the resulting algorithm are offered. Compared to robust gradient descent, the proposed algorithm enjoys the faster rates of convergence for successive iterates often achieved by second-order al
&lt;/p&gt;</description></item><item><title>Composer's Assistant&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;REAPER&#38899;&#39057;&#24037;&#20316;&#31449;&#20013;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#21019;&#20316;&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;T5-like&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#36712;MIDI&#22635;&#20805;&#30340;&#20219;&#21153;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#32452;&#33050;&#26412;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36827;&#34892;&#20102;&#23458;&#35266;&#21644;&#20027;&#35266;&#30340;&#27979;&#35797;&#12290;&#23436;&#25972;&#30340;&#31995;&#32479;&#21253;&#25324;&#28304;&#20195;&#30721;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;REAPER&#33050;&#26412;&#12290;</title><link>http://arxiv.org/abs/2301.12525</link><description>&lt;p&gt;
Composer's Assistant&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#36712;MIDI&#22635;&#20805;&#30340;&#20132;&#20114;&#24335;Transformer
&lt;/p&gt;
&lt;p&gt;
Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling. (arXiv:2301.12525v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12525
&lt;/p&gt;
&lt;p&gt;
Composer's Assistant&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;REAPER&#38899;&#39057;&#24037;&#20316;&#31449;&#20013;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#21019;&#20316;&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;T5-like&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#36712;MIDI&#22635;&#20805;&#30340;&#20219;&#21153;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#32452;&#33050;&#26412;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36827;&#34892;&#20102;&#23458;&#35266;&#21644;&#20027;&#35266;&#30340;&#27979;&#35797;&#12290;&#23436;&#25972;&#30340;&#31995;&#32479;&#21253;&#25324;&#28304;&#20195;&#30721;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;REAPER&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Composer's Assistant&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;REAPER&#25968;&#23383;&#38899;&#39057;&#24037;&#20316;&#31449;&#20013;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#21019;&#20316;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#24403;&#20174;&#19968;&#20010;MIDI&#25991;&#20214;&#30340;&#36830;&#32493;&#23567;&#33410;&#20013;&#21024;&#38500;&#20102;&#20219;&#24847;&#36712;&#36947;&#23567;&#33410;&#26102;&#30340;&#22810;&#36712;MIDI&#22635;&#20805;&#20219;&#21153;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;T5&#30340;&#27169;&#22411;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;Composer's Assistant&#30001;&#36825;&#20010;&#27169;&#22411;&#21644;&#33021;&#22815;&#19982;REAPER&#20013;&#30340;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#30340;&#33050;&#26412;&#32452;&#25104;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23458;&#35266;&#21644;&#20027;&#35266;&#30340;&#27169;&#22411;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#23436;&#25972;&#31995;&#32479;&#65292;&#21253;&#25324;&#28304;&#20195;&#30721;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;REAPER&#33050;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21482;&#22312;&#25480;&#26435;&#35768;&#21487;&#30340;MIDI&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Composer's Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36880;&#19968;&#22788;&#29702;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#23588;&#20854;&#26159;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2301.07863</link><description>&lt;p&gt;
&#20174;&#27969;&#25968;&#25454;&#20013;&#22312;&#32447;&#21457;&#29616;&#28436;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online discovering governing differential equations of evolving systems from streaming data. (arXiv:2301.07863v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36880;&#19968;&#22788;&#29702;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#23588;&#20854;&#26159;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29616;&#26377;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#28436;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#26041;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65306;&#20174;&#27969;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#32508;&#21512;&#32771;&#34385;&#26679;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#26080;&#27861;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#27969;&#25968;&#25454;&#32780;&#38750;&#22788;&#29702;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#36880;&#19968;&#22788;&#29702;&#26679;&#26412;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#28436;&#21270;&#31995;&#32479;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#20854;&#29366;&#24577;&#20063;&#38543;&#20043;&#25913;&#21464;&#12290;&#22240;&#27492;&#25214;&#21040;&#31934;&#30830;&#30340;&#21464;&#21270;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35782;&#21035;&#20986;&#24046;&#24322;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#20013;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering the governing equations of evolving systems from available observations is essential and challenging. In this paper, we consider a new scenario: discovering governing equations from streaming data. Current methods struggle to discover governing differential equations with considering measurements as a whole, leading to failure to handle this task. We propose an online modeling method capable of handling samples one by one sequentially by modeling streaming data instead of processing the entire dataset. The proposed method performs well in discovering ordinary differential equations (ODEs) and partial differential equations (PDEs) from streaming data. Evolving systems are changing over time, which invariably changes with system status. Thus, finding the exact change points is critical. The measurement generated from a changed system is distributed dissimilarly to before; hence, the difference can be identified by the proposed method. Our proposal is competitive in identifyin
&lt;/p&gt;</description></item><item><title>StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.01947</link><description>&lt;p&gt;
StitchNet: &#20174;&#39044;&#35757;&#32451;&#29255;&#27573;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01947
&lt;/p&gt;
&lt;p&gt;
StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#33539;&#24335;StitchNet&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#65288;&#19968;&#20010;&#25110;&#22810;&#20010;&#36830;&#32493;&#30340;&#32593;&#32476;&#23618;&#65289;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;StitchNet&#20801;&#35768;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#20256;&#32479;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20316;&#20026;&#19968;&#31181;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#22320;&#25351;&#23548;&#36873;&#25321;&#36825;&#20123;&#29255;&#27573;&#65292;&#20197;&#32452;&#21512;&#36866;&#21512;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#30340;&#20219;&#21153;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29255;&#27573;&#21487;&#20197;&#34987;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#21019;&#24314;&#19982;&#20256;&#32479;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#20934;&#30830;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26032;&#33539;&#24335;&#25152;&#33021;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23398;&#20064;&#30340;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#24615;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31867;&#21035;&#29305;&#24449;&#21464;&#24322;&#23849;&#22604;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#29616;&#35937;&#20351;&#24471;&#22312;&#26032;&#31867;&#21035;&#19978;&#36890;&#36807;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#29305;&#24449;&#26144;&#23556;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.12532</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers. (arXiv:2212.12532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23398;&#20064;&#30340;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#24615;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#31867;&#21035;&#29305;&#24449;&#21464;&#24322;&#23849;&#22604;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#29616;&#35937;&#20351;&#24471;&#22312;&#26032;&#31867;&#21035;&#19978;&#36890;&#36807;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#29305;&#24449;&#26144;&#23556;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#23398;&#20064;&#21487;&#20256;&#36755;&#21040;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#20998;&#31867;&#34920;&#31034;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#19968;&#20010;&#21333;&#19968;&#30340;&#20998;&#31867;&#22120;&#23398;&#24471;&#30340;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#19978;&#19982;&#19987;&#38376;&#20026;&#36825;&#20123;&#38382;&#39064;&#35774;&#35745;&#30340;&#29305;&#27530;&#31639;&#27861;&#23398;&#24471;&#30340;&#34920;&#31034;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#22522;&#20110;&#26368;&#36817;&#21457;&#29616;&#30340;&#31867;&#21035;&#29305;&#24449;&#21464;&#24322;&#23849;&#22604;&#29616;&#35937;&#65292;&#21363;&#22312;&#28145;&#24230;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#29305;&#24449;&#23884;&#20837;&#20542;&#21521;&#20110;&#32858;&#38598;&#22312;&#23427;&#20204;&#30340;&#31867;&#21035;&#22343;&#20540;&#38468;&#36817;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31867;&#21035;&#29305;&#24449;&#21464;&#24322;&#23849;&#22604;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#24471;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26032;&#30340;&#31867;&#21035;&#19978;&#30340;&#23569;&#26679;&#26412;&#35823;&#24046;&#23567;&#65292;&#21363;&#20351;&#29992;&#20174;&#27599;&#20010;&#26032;&#31867;&#21035;&#30340;&#23569;&#37327;&#38543;&#26426;&#26679;&#26412;&#23398;&#24471;&#30340;&#20013;&#24515;&#26469;&#36827;&#34892;&#26368;&#36817;&#31867;&#21035;&#20013;&#24515;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#35823;&#24046;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. We offer a theoretical explanation for this behavior based on the recently discovered phenomenon of class-feature-variability collapse, that is, that during the training of deep classification networks the feature embeddings of samples belonging to the same class tend to concentrate around their class means. More specifically, we show that the few-shot error of the learned feature map on new classes (defined as the classification error of the nearest class-center classifier using centers learned from a small number of random samples from each new class) is small in case of class-feature-variability collapse, under the a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#36731;&#37327;&#32423;&#23545;&#27604;&#27169;&#22411;&#24314;&#31435;&#26356;&#24378;&#30340;&#22522;&#20934;&#32447;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#20351;&#29992;&#25928;&#29575;&#22411;&#32593;&#32476;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#37197;&#32622;&#21644;&#24341;&#20837;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.07158</link><description>&lt;p&gt;
&#20026;&#36731;&#37327;&#32423;&#23545;&#27604;&#27169;&#22411;&#24314;&#31435;&#26356;&#24378;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#36731;&#37327;&#32423;&#23545;&#27604;&#27169;&#22411;&#24314;&#31435;&#26356;&#24378;&#30340;&#22522;&#20934;&#32447;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#20351;&#29992;&#25928;&#29575;&#22411;&#32593;&#32476;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#20248;&#21270;&#35757;&#32451;&#37197;&#32622;&#21644;&#24341;&#20837;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#22312;&#35832;&#22914;MobileNet&#21644;EfficientNet&#31561;&#29305;&#23450;&#35774;&#35745;&#30340;&#39640;&#25928;&#32593;&#32476;&#20013;&#65292;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#20986;&#29616;&#19979;&#38477;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25945;&#24072;&#29983;&#25104;&#30340;&#33976;&#39311;&#20449;&#21495;&#26469;&#35757;&#32451;&#36731;&#37327;&#32423;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#24403;&#25945;&#24072;&#27169;&#22411;&#19981;&#21487;&#29992;&#26102;&#65292;&#39044;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#26159;&#19968;&#39033;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#22823;&#30340;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#20351;&#29992;&#39044;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20026;&#36731;&#37327;&#32423;&#23545;&#27604;&#27169;&#22411;&#24314;&#31435;&#26356;&#24378;&#30340;&#22522;&#20934;&#32447;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39640;&#25928;&#27169;&#22411;&#30340;&#26368;&#20339;&#35757;&#32451;&#37197;&#32622;&#19982;&#36739;&#22823;&#27169;&#22411;&#19981;&#21516;&#65292;&#20351;&#29992;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#19982;ResNet50&#30456;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#38382;&#39064;&#65292;&#21363;&#27491;&#25110;&#36127;&#35270;&#22270;&#20013;&#30340;&#19968;&#20010;&#21487;&#33021;&#20250;&#26377;&#22122;&#22768;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#30340;InfoNCE&#25439;&#22833;&#20989;&#25968;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36731;&#37327;&#32423;&#23545;&#27604;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has reported a performance degradation in self-supervised contrastive learning for specially designed efficient networks, such as MobileNet and EfficientNet. A common practice to address this problem is to introduce a pretrained contrastive teacher model and train the lightweight networks with distillation signals generated by the teacher. However, it is time and resource consuming to pretrain a teacher model when it is not available. In this work, we aim to establish a stronger baseline for lightweight contrastive models without using a pretrained teacher model. Specifically, we show that the optimal recipe for efficient models is different from that of larger models, and using the same training settings as ResNet50, as previous research does, is inappropriate. Additionally, we observe a common issu e in contrastive learning where either the positive or negative views can be noisy, and propose a smoothed version of InfoNCE loss to alleviate this problem. As a result, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#21644;&#21704;&#23494;&#39039;&#37327;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#23376;&#23384;&#20648;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#30340;&#25351;&#25968;&#32423;&#37327;&#23376;&#20248;&#21183;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064; Pauli &#20256;&#36755;&#30697;&#38453;&#21644;&#39044;&#27979;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27874;&#20989;&#25968;&#26399;&#26395;&#20540;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.04471</link><description>&lt;p&gt;
&#36890;&#36807; Pauli &#20256;&#36755;&#30697;&#38453;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#21644;&#21704;&#23494;&#39039;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Quantum Processes and Hamiltonians via the Pauli Transfer Matrix. (arXiv:2212.04471v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#21644;&#21704;&#23494;&#39039;&#37327;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#37327;&#23376;&#23384;&#20648;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#30340;&#25351;&#25968;&#32423;&#37327;&#23376;&#20248;&#21183;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064; Pauli &#20256;&#36755;&#30697;&#38453;&#21644;&#39044;&#27979;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27874;&#20989;&#25968;&#26399;&#26395;&#20540;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#22686;&#24378;&#23454;&#39564;&#65292;&#20381;&#38752;&#37327;&#23376;&#23384;&#20648;&#21644;&#37327;&#23376;&#22788;&#29702;&#26469;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#20248;&#20110;&#21482;&#26377;&#32463;&#20856;&#23384;&#20648;&#21644;&#22788;&#29702;&#30340;&#23454;&#39564;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#29366;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#30830;&#23450;&#20102;&#37327;&#23376;&#20248;&#21183;&#65292;&#20294;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#21482;&#26377;&#22312;&#38382;&#39064;&#24471;&#21040;&#20180;&#32454;&#23450;&#20041;&#24182;&#19988;&#29702;&#35299;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#36798;&#21040;&#30456;&#20284;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23398;&#20064;&#26410;&#30693;&#30340; n &#37327;&#23376;&#27604;&#29305;&#37327;&#23376;&#36807;&#31243; &#30340;&#25351;&#25968;&#32423;&#37327;&#23376;&#20248;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#37327;&#23376;&#23384;&#20648;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#20197;&#19979;&#20219;&#21153;&#65306;(a) &#23398;&#20064;&#20219;&#24847; $\mathcal{N}$ &#30340; Pauli &#20256;&#36755;&#30697;&#38453;&#65292;(b) &#22312;&#36755;&#20837; Pauli &#31232;&#30095;&#24577;&#26102;&#39044;&#27979;&#20219;&#24847; $\mathcal{N}$ &#36755;&#20986;&#22788;&#27979;&#37327;&#30340;&#26377;&#30028; Pauli &#31232;&#30095;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#20197;&#21450; (c) &#22312;&#26410;&#30693; $\mathcal{N}$ &#30340;&#36755;&#20986;&#19978;&#39044;&#27979;&#26377;&#30028;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#20854;&#20013; $\mathcal{N}$ &#30340; Pauli &#20256;&#36755;&#30697;&#38453;&#31232;&#30095;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning about physical systems from quantum-enhanced experiments, relying on a quantum memory and quantum processing, can outperform learning from experiments in which only classical memory and processing are available. Whereas quantum advantages have been established for a variety of state learning tasks, quantum process learning allows for comparable advantages only with a careful problem formulation and is less understood. We establish an exponential quantum advantage for learning an unknown $n$-qubit quantum process $\mathcal{N}$. We show that a quantum memory allows to efficiently solve the following tasks: (a) learning the Pauli transfer matrix of an arbitrary $\mathcal{N}$, (b) predicting expectation values of bounded Pauli-sparse observables measured on the output of an arbitrary $\mathcal{N}$ upon input of a Pauli-sparse state, and (c) predicting expectation values of arbitrary bounded observables measured on the output of an unknown $\mathcal{N}$ with sparse Pauli transfer m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01793</link><description>&lt;p&gt;
\{kappa}HGCN: &#36890;&#36807;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#23398;&#20064;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#29366;&#32467;&#26500;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#30456;&#27604;&#20110;&#24179;&#22374;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#26354;&#38754;&#21452;&#26354;&#31354;&#38388;&#25552;&#20379;&#20102;&#26356;&#26131;&#22788;&#29702;&#21644;&#23884;&#20837;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23637;&#29616;&#38544;&#21547;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#26641;&#29366;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#23637;&#31034;&#20986;&#26641;&#29366;&#12289;&#24179;&#22374;&#21644;&#22278;&#24418;&#21306;&#22495;&#30340;&#24322;&#36136;&#32452;&#25104;&#12290;&#23558;&#36825;&#26679;&#24322;&#36136;&#30340;&#32467;&#26500;&#30452;&#25509;&#23884;&#20837;&#19968;&#20010;&#21516;&#36136;&#21270;&#30340;&#23884;&#20837;&#31354;&#38388;&#65288;&#21363;&#21452;&#26354;&#31354;&#38388;&#65289;&#24517;&#28982;&#23548;&#33268;&#37325;&#22823;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#21452;&#26354;&#31354;&#38388;&#30340;&#26354;&#29575;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#20934;&#30830;&#22320;&#24314;&#27169;&#26641;&#29366;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;BERT-like&#22686;&#24378;&#23376;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23376;&#37492;&#23450;&#26041;&#27861;(iEnhancer-ELM)&#65292;&#36890;&#36807;&#25552;&#21462;&#20301;&#32622;&#30456;&#20851;&#30340;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#21407;&#22987;DNA&#24207;&#21015;&#30340;&#22686;&#24378;&#23376;&#37492;&#23450;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2212.01495</link><description>&lt;p&gt;
iEnhancer-ELM:&#22522;&#20110;&#22686;&#24378;&#23376;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20301;&#32622;&#30456;&#20851;&#30340;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25913;&#21892;&#22686;&#24378;&#23376;&#37492;&#23450;
&lt;/p&gt;
&lt;p&gt;
iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models. (arXiv:2212.01495v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;BERT-like&#22686;&#24378;&#23376;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23376;&#37492;&#23450;&#26041;&#27861;(iEnhancer-ELM)&#65292;&#36890;&#36807;&#25552;&#21462;&#20301;&#32622;&#30456;&#20851;&#30340;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#21407;&#22987;DNA&#24207;&#21015;&#30340;&#22686;&#24378;&#23376;&#37492;&#23450;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#22686;&#24378;&#23376;&#26159;&#37325;&#35201;&#30340;&#39034;&#24335;&#35843;&#25511;&#20803;&#20214;&#65292;&#21487;&#20197;&#35843;&#25511;&#21508;&#31181;&#29983;&#29289;&#21151;&#33021;&#24182;&#22686;&#24378;&#30446;&#26631;&#22522;&#22240;&#30340;&#36716;&#24405;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#26469;&#25913;&#21892;&#22686;&#24378;&#23376;&#37492;&#23450;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20174;&#21407;&#22987;DNA&#24207;&#21015;&#20013;&#23398;&#20064;&#20301;&#32622;&#30456;&#20851;&#30340;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT-like&#22686;&#24378;&#23376;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#22686;&#24378;&#23376;&#37492;&#23450;&#26041;&#27861;&#65288;iEnhancer-ELM&#65289;&#12290;iEnhancer-ELM&#20351;&#29992;&#22810;&#23610;&#24230;k-mers&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#24182;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#25552;&#21462;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#19981;&#21516;&#23610;&#24230;k-mers&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#39318;&#20808;&#35780;&#20272;&#19981;&#21516;&#23610;&#24230;k-mers&#30340;&#24615;&#33021;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#38598;&#25104;&#20197;&#25552;&#39640;&#22686;&#24378;&#23376;&#37492;&#23450;&#30340;&#24615;&#33021;&#12290;&#23545;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;iEnhancer&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Enhancers are important cis-regulatory elements that regulate a wide range of biological functions and enhance the transcription of target genes. Although many feature extraction methods have been proposed to improve the performance of enhancer identification, they cannot learn position-related multiscale contextual information from raw DNA sequences.  Results: In this article, we propose a novel enhancer identification method (iEnhancer-ELM) based on BERT-like enhancer language models. iEnhancer-ELM tokenizes DNA sequences with multi-scale k-mers and extracts contextual information of different scale k-mers related with their positions via an multi-head attention mechanism. We first evaluate the performance of different scale k-mers, then ensemble them to improve the performance of enhancer identification. The experimental results on two popular benchmark datasets show that our model outperforms stateof-the-art methods. We further illustrate the interpretability of iEnhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16162</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21253;&#21547;&#26680;&#24515;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21450;&#35774;&#22791;&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20551;&#35774;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#30456;&#21516;&#20219;&#21153;&#30340;&#38598;&#32676;&#36827;&#34892;&#21327;&#20316;&#12290;&#20026;&#20102;&#22312;&#26080;&#32447;&#38142;&#36335;&#19978;&#23454;&#29616;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#31751;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#34892;&#38142;&#36335;&#65292;&#21516;&#26102;&#37319;&#29992;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#29992;&#20110;&#19979;&#34892;&#38142;&#36335;&#65292;&#27599;&#20010;&#31639;&#27861;&#36845;&#20195;&#21482;&#38656;&#35201;&#19968;&#20010;&#36164;&#28304;&#22359;&#65292;&#19981;&#21463;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#35774;&#32622;&#38754;&#20020;&#30528;&#19978;&#34892;&#38142;&#36335;&#35774;&#22791;&#24178;&#25200;&#21644;&#19979;&#34892;&#38142;&#36335;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#35774;&#22791;&#24314;&#27169;&#20026;&#19968;&#20010;&#27850;&#26494;&#38598;&#32676;&#36807;&#31243;&#65292;&#22312;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#23545;&#30001;&#24178;&#25200;&#24341;&#36215;&#30340;&#19978;&#34892;&#38142;&#36335;&#21644;&#19979;&#34892;&#38142;&#36335;&#30340;&#35823;&#24046;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#23548;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;PAC-Bayes&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#30028;&#38480;&#30340;&#27010;&#36848;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;PAC-Bayes&#30028;&#38480;&#26159;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#31163;&#32447;Bandit&#31639;&#27861;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20294;&#22312;&#32447;Bandit&#31639;&#27861;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.16110</link><description>&lt;p&gt;
PAC-Bayes&#23450;&#29702;&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#19982;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. (arXiv:2211.16110v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16110
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;PAC-Bayes&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#30028;&#38480;&#30340;&#27010;&#36848;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;PAC-Bayes&#30028;&#38480;&#26159;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#31163;&#32447;Bandit&#31639;&#27861;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20294;&#22312;&#32447;Bandit&#31639;&#27861;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PAC-Bayes&#26368;&#36817;&#37325;&#26032;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#29992;&#26469;&#25512;&#23548;&#20986;&#20855;&#26377;&#32039;&#23494;&#24615;&#33021;&#20445;&#35777;&#30340;&#26377;&#21407;&#21017;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;PAC-Bayes&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#36739;&#23569;&#65292;&#36825;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#36951;&#25022;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#33258;&#28982;&#31185;&#23398;&#31561;&#35768;&#22810;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#37117;&#21487;&#20197;&#23558;&#20854;&#24314;&#27169;&#20026;Bandit&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#24102;&#26377;&#24378;&#22823;&#24615;&#33021;&#20445;&#35777;&#30340;&#26377;&#21407;&#21017;&#31639;&#27861;&#23558;&#20250;&#21463;&#21040;&#24456;&#39640;&#30340;&#36190;&#36175;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;Bandit&#38382;&#39064;&#30340;PAC-Bayes&#30028;&#38480;&#30340;&#27010;&#36848;&#65292;&#24182;&#36827;&#34892;&#20102;&#36825;&#20123;&#30028;&#38480;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;PAC-Bayes&#30028;&#38480;&#26159;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#31163;&#32447;Bandit&#31639;&#27861;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19968;&#31181;PAC-Bayesian&#31163;&#32447;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#24615;&#39044;&#26399;&#22870;&#21169;&#21644;&#38750;&#31354;&#24615;&#33021;&#20445;&#35777;&#30340;&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;PAC-Bayesian&#22312;&#32447;Bandit&#31639;&#27861;&#21017;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes has recently re-emerged as an effective theory with which one can derive principled learning algorithms with tight performance guarantees. However, applications of PAC-Bayes to bandit problems are relatively rare, which is a great misfortune. Many decision-making problems in healthcare, finance and natural sciences can be modelled as bandit problems. In many of these applications, principled algorithms with strong performance guarantees would be very much appreciated. This survey provides an overview of PAC-Bayes bounds for bandit problems and an experimental comparison of these bounds. On the one hand, we found that PAC-Bayes bounds are a useful tool for designing offline bandit algorithms with performance guarantees. In our experiments, a PAC-Bayesian offline contextual bandit algorithm was able to learn randomised neural network polices with competitive expected reward and non-vacuous performance guarantees. On the other hand, the PAC-Bayesian online bandit algorithms that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.14400</link><description>&lt;p&gt;
&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#19978;&#65292;&#20851;&#20110;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#36924;&#36817;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Sobolev&#21644;Besov&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#24590;&#26679;&#30340;&#21442;&#25968;&#25928;&#29575;&#36924;&#36817;&#20989;&#25968;&#65292;&#21253;&#25324;$L_p(\Omega)$&#33539;&#25968;&#19979;&#30340;&#35823;&#24046;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q \leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26469;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#31354;&#38388;$W^s(L_q(\Omega))$&#21644;Besov&#31354;&#38388;$B^s_r(L_q(\Omega))$&#20013;&#20197;$L_p(\Omega)$&#33539;&#25968;&#24230;&#37327;&#35823;&#24046;&#30340;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20110;&#22312;&#31185;&#23398;&#35745;&#31639;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#36807;&#21435;&#21482;&#26377;&#24403;$p=q=\infty$&#26102;&#25165;&#23436;&#20840;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25152;&#26377;$1\leq p,q\leq \infty$&#21644;$s&gt;0$&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#28176;&#36817;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#19968;&#31181;&#26032;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#31232;&#30095;&#21521;&#37327;&#30340;&#26368;&#20339;&#32534;&#30721;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;$p&gt;q$&#30340;&#38750;&#32447;&#24615;&#21306;&#22495;&#33719;&#24471;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#30340;$L_p$&#36924;&#36817;&#19979;&#30028;&#25512;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s &gt; 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p &gt; q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10773</link><description>&lt;p&gt;
&#19968;&#31181; $k$ &#26368;&#36817;&#37051;&#30340;&#20004;&#38454;&#27573;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#39318;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#33258;&#21160;&#36866;&#24212;&#20998;&#24067;&#27604;&#20363;&#21464;&#21270;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#38024;&#23545;&#33258;&#28982;&#20445;&#30041;&#36825;&#20123;&#20248;&#31168;&#29305;&#24615;&#30340;&#26412;&#22320;&#25237;&#31080;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#65292;&#35774;&#35745;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#27492; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22312;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#27425;&#20445;&#25345;&#20102; $k$ &#26368;&#36817;&#37051;&#25237;&#31080;&#27010;&#24565;&#30340;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#20026;&#36890;&#36807;&#25105;&#20204;&#26041;&#26696;&#33719;&#21462;&#30340;&#26679;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340; $k$ &#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#24403;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968; $\mathbb{P}(Y=y|X=x)$ &#36275;&#22815;&#24179;&#28369;&#24182;&#19988; Tsybakov &#22122;&#22768;&#26465;&#20214;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#35757;&#32451;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
$k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#20219;&#21153;&#31354;&#38388;&#30340;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65292;&#24182;&#19988;&#30417;&#30563;&#23398;&#20064;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#21487;&#20197;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20135;&#29983;&#19968;&#23450;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.17011</link><description>&lt;p&gt;
&#20856;&#22411;&#21487;&#23398;&#20064;&#20219;&#21153;&#31354;&#38388;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17011
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#20219;&#21153;&#31354;&#38388;&#30340;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65292;&#24182;&#19988;&#30417;&#30563;&#23398;&#20064;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#21487;&#20197;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20135;&#29983;&#19968;&#23450;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#26469;&#29702;&#35299;&#28145;&#24230;&#32593;&#32476;&#22312;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23398;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#20219;&#21153;&#31354;&#38388;&#32467;&#26500;&#30456;&#20851;&#30340;&#20197;&#19979;&#29616;&#35937;&#65306;(1)&#20351;&#29992;&#19981;&#21516;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27010;&#29575;&#27169;&#22411;&#27969;&#24418;&#23454;&#38469;&#19978;&#26159;&#20302;&#32500;&#30340;&#65307;(2)&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#21363;&#20351;&#22312;&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#26159;&#19981;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#20063;&#33021;&#21462;&#24471;&#20986;&#20046;&#24847;&#26009;&#30340;&#36827;&#23637;&#65307;&#22914;&#26524;&#35757;&#32451;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#30340;&#31867;&#21035;&#65292;&#21017;&#20854;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#26356;&#22823;&#65307;(3)&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#25152;&#25351;&#31034;&#30340;&#20219;&#21153;&#31354;&#38388;&#32467;&#26500;&#19982;Wordnet&#31995;&#32479;&#36827;&#21270;&#26641;&#20013;&#30340;&#26576;&#20123;&#37096;&#20998;&#19968;&#33268;&#65307;(4)&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24773;&#22659;&#20803;&#23398;&#20064;&#31639;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#36981;&#24490;&#19981;&#21516;&#30340;&#36712;&#36857;&#65292;&#20294;&#26368;&#32456;&#36866;&#24212;&#30456;&#20284;&#30340;&#27169;&#22411;&#65307;(5)&#23545;&#27604;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36981;&#24490;&#31867;&#20284;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;MTT&#65292;&#24182;&#22312;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;MTT&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;</title><link>http://arxiv.org/abs/2210.15539</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-Target Tracking with Transferable Convolutional Neural Networks. (arXiv:2210.15539v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36801;&#31227;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;MTT&#65292;&#24182;&#22312;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;MTT&#20219;&#21153;&#20013;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MTT&#65289;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#20174;&#22122;&#22768;&#20256;&#24863;&#22120;&#27979;&#37327;&#20013;&#20272;&#35745;&#20986;&#26410;&#30693;&#25968;&#37327;&#30340;&#31227;&#21160;&#30446;&#26631;&#30340;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;MTT&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#29366;&#24577;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#34920;&#31034;&#20026;&#22270;&#20687;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23567;&#33539;&#22260;&#30340;&#36319;&#36394;&#21306;&#22495;&#35757;&#32451;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#20855;&#26377;&#22823;&#37327;&#30446;&#26631;&#21644;&#20256;&#24863;&#22120;&#30340;&#26356;&#22823;&#33539;&#22260;&#12290;&#36825;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;MTT&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#35813;&#20998;&#26512;&#38480;&#21046;&#20102;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#21487;&#36801;&#31227;CNN&#26550;&#26500;&#22312;&#20855;&#26377;10&#20010;&#30446;&#26631;&#30340;MTT&#20219;&#21153;&#19978;&#20248;&#20110;&#38543;&#26426;&#26377;&#38480;&#38598;&#28388;&#27874;&#22120;&#65292;&#24182;&#19988;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#36801;&#31227;&#21040;&#20855;&#26377;250&#20010;&#30446;&#26631;&#30340;&#26356;&#22823;&#30340;MTT&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;29%&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-target tracking (MTT) is a classical signal processing task, where the goal is to estimate the states of an unknown number of moving targets from noisy sensor measurements. In this paper, we revisit MTT from a deep learning perspective and propose a convolutional neural network (CNN) architecture to tackle it. We represent the target states and sensor measurements as images and recast the problem as an image-to-image prediction task. Then we train a fully convolutional model at small tracking areas and transfer it to much larger areas with numerous targets and sensors. This transfer learning approach enables MTT at a large scale and is also theoretically supported by our novel analysis that bounds the generalization error. In practice, the proposed transferable CNN architecture outperforms random finite set filters on the MTT task with 10 targets and transfers without re-training to a larger MTT task with 250 targets with a 29% performance improvement.
&lt;/p&gt;</description></item><item><title>FocusedCleaner&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#25237;&#27602;&#25915;&#20987;&#30340;&#28165;&#29702;&#22120;&#65292;&#36890;&#36807;&#36870;&#36716;&#25915;&#20987;&#36807;&#31243;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25628;&#32034;&#21306;&#22495;&#65292;&#36880;&#27493;&#28165;&#29702;&#21463;&#27745;&#26579;&#30340;&#22270;&#24418;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13815</link><description>&lt;p&gt;
FocusedCleaner&#65306;&#29992;&#20110;&#31283;&#20581;GNN&#33410;&#28857;&#20998;&#31867;&#30340;&#28165;&#29702;&#21463;&#27745;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification. (arXiv:2210.13815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13815
&lt;/p&gt;
&lt;p&gt;
FocusedCleaner&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#25237;&#27602;&#25915;&#20987;&#30340;&#28165;&#29702;&#22120;&#65292;&#36890;&#36807;&#36870;&#36716;&#25915;&#20987;&#36807;&#31243;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25628;&#32034;&#21306;&#22495;&#65292;&#36880;&#27493;&#28165;&#29702;&#21463;&#27745;&#26579;&#30340;&#22270;&#24418;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26131;&#21463;&#25968;&#25454;&#25237;&#27602;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20250;&#29983;&#25104;&#19968;&#20010;&#34987;&#27745;&#26579;&#30340;&#22270;&#24418;&#20316;&#20026;GNN&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;FocusedCleaner&#20316;&#20026;&#19968;&#31181;&#27745;&#26579;&#22270;&#24418;&#28165;&#29702;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#25915;&#20987;&#32773;&#27880;&#20837;&#30340;&#27602;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FocusedCleaner&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#23618;&#32467;&#26500;&#23398;&#20064;&#21644;&#21463;&#23475;&#33410;&#28857;&#26816;&#27979;&#12290;&#20854;&#20013;&#65292;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#23558;&#36870;&#36716;&#25915;&#20987;&#36807;&#31243;&#65292;&#31283;&#23450;&#22320;&#28165;&#29702;&#22270;&#24418;&#65292;&#32780;&#26816;&#27979;&#27169;&#22359;&#25552;&#20379;&#20102;&#8220;&#28966;&#28857;&#8221;&#8212;&#8212;&#19968;&#20010;&#26356;&#31364;&#12289;&#26356;&#20934;&#30830;&#30340;&#25628;&#32034;&#21306;&#22495;&#8212;&#8212;&#32473;&#32467;&#26500;&#23398;&#20064;&#27169;&#22359;&#20351;&#29992;&#12290;&#36825;&#20004;&#20010;&#27169;&#22359;&#23558;&#36845;&#20195;&#36816;&#34892;&#65292;&#24182;&#30456;&#20114;&#22686;&#24378;&#65292;&#36880;&#27493;&#28165;&#29702;&#34987;&#27745;&#26579;&#30340;&#22270;&#24418;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28165;&#29702;&#21518;&#30340;&#22270;&#19978;&#35757;&#32451;&#30340;GNN&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FocusedCleaner&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which will generate a poisoned graph as the input to the GNN models. We present FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison injected by attackers. Specifically, FocusedCleaner provides a sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reverse the attack process to steadily sanitize the graph while the detection module provides ``the focus" -- a narrowed and more accurate search region -- to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. As an important application, we show that the adversarial robustness of GNNs trained over the sanitized graph for the node classification task is significantly improved. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13601</link><description>&lt;p&gt;
&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#23545;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#26377;&#26102;&#20063;&#34987;&#31216;&#20026;&#8220;&#23725;&#20989;&#25968;&#8221;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#24191;&#27867;&#26377;&#25928;&#22320;&#27169;&#25311;&#29289;&#29702;&#29616;&#35937;&#65292;&#24182;&#26500;&#24314;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20195;&#29702;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#65288;&#22914;ReLU&#20989;&#25968;&#12289;sigmoid&#20989;&#25968;&#12289;&#32477;&#23545;&#20540;&#20989;&#25968;&#12289;&#20302;&#27425;&#22810;&#39033;&#24335;&#20989;&#25968;&#31561;&#65289;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#24050;&#30693;&#30340;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#8220;&#32447;&#24615;&#20989;&#25968;&#8221;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#34987;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25104;&#21151;&#25915;&#20987;&#26159;&#30001;&#20110;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.10886</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#19982;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#34987;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#25104;&#21151;&#25915;&#20987;&#26159;&#30001;&#20110;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22522;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#20197;&#25903;&#25345;&#24320;&#25918;&#30740;&#31350;&#24182;&#22686;&#21152;&#21307;&#23398;&#25968;&#25454;&#38598;&#12290;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#65288;GANs&#65289;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#35757;&#32451;&#20013;&#22830;&#27169;&#22411;&#24182;&#20445;&#25345;&#26412;&#22320;&#21407;&#22987;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;FL&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#38598;&#20013;&#22312;&#20998;&#31867;&#27169;&#22411;&#21644;&#20013;&#24515;&#21270;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21542;&#24433;&#21709;GAN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#22914;&#26524;&#21487;&#20197;&#24433;&#21709;&#65292;&#22914;&#20309;&#22312;FL&#29615;&#22659;&#20013;&#36827;&#34892;&#38450;&#24481;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32852;&#37030;GANs&#65288;FedGANs&#65289;&#20013;&#21518;&#38376;&#25915;&#20987;&#36825;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#38543;&#21518;&#34987;&#30830;&#23450;&#20026;&#37096;&#20998;&#26412;&#22320;&#21028;&#21035;&#22120;&#23545;&#27602;&#32032;&#36807;&#24230;&#25311;&#21512;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poison
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#21518;&#32456;&#27490;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.04527</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A policy gradient approach for Finite Horizon Constrained Markov Decision Processes. (arXiv:2210.04527v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#21518;&#32456;&#27490;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#36890;&#24120;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#23548;&#33268;&#20135;&#29983;&#26368;&#20248;&#30340;&#22266;&#23450;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#26102;&#22495;&#25511;&#21046;&#38382;&#39064;&#26356;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#36890;&#24120;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26368;&#36817;&#65292;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35774;&#32622;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;&#20195;&#29702;&#21516;&#26102;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#28385;&#36275;&#26576;&#20123;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35774;&#32622;&#20165;&#22312;&#26080;&#38480;&#26102;&#22495;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#22266;&#23450;&#31574;&#30053;&#26159;&#26368;&#20248;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#26102;&#38388;&#21518;&#32456;&#27490;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#36825;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#25110;&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#21462;&#20915;&#20110;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinite horizon setting is widely adopted for problems of reinforcement learning (RL). These invariably result in stationary policies that are optimal. In many situations, finite horizon control problems are of interest and for such problems, the optimal policies are time-varying in general. Another setting that has become popular in recent times is of Constrained Reinforcement Learning, where the agent maximizes its rewards while it also aims to satisfy some given constraint criteria. However, this setting has only been studied in the context of infinite horizon MDPs where stationary policies are optimal. We present an algorithm for constrained RL in the Finite Horizon Setting where the horizon terminates after a fixed (finite) time. We use function approximation in our algorithm which is essential when the state and action spaces are large or continuous and use the policy gradient method to find the optimal policy. The optimal policy that we obtain depends on the stage and so is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;VIPS&#21644;iBayes-GMM&#36825;&#20004;&#31181;&#30446;&#21069;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#26356;&#26032;&#21508;&#20010;&#32452;&#20214;&#21644;&#26435;&#37325;&#26102;&#20351;&#29992;&#30340;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#26159;&#31561;&#20215;&#30340;&#65292;&#20294;&#20854;&#23454;&#29616;&#21644;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#24322;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#36873;&#25321;&#12289;&#33258;&#28982;&#26799;&#24230;&#20272;&#35745;&#12289;&#27493;&#38271;&#36866;&#24212;&#20197;&#21450;&#21487;&#20449;&#21306;&#22495;&#25110;&#32452;&#20214;&#25968;&#37327;&#30340;&#35843;&#25972;&#31561;&#35774;&#35745;&#36873;&#25321;&#19978;&#23384;&#22312;&#21306;&#21035;&#65292;&#23545;&#20110;&#23398;&#20064;&#36817;&#20284;&#30340;&#36136;&#37327;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.11533</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;VIPS&#21644;iBayes-GMM&#36825;&#20004;&#31181;&#30446;&#21069;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#26356;&#26032;&#21508;&#20010;&#32452;&#20214;&#21644;&#26435;&#37325;&#26102;&#20351;&#29992;&#30340;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#26159;&#31561;&#20215;&#30340;&#65292;&#20294;&#20854;&#23454;&#29616;&#21644;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#24322;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26679;&#26412;&#36873;&#25321;&#12289;&#33258;&#28982;&#26799;&#24230;&#20272;&#35745;&#12289;&#27493;&#38271;&#36866;&#24212;&#20197;&#21450;&#21487;&#20449;&#21306;&#22495;&#25110;&#32452;&#20214;&#25968;&#37327;&#30340;&#35843;&#25972;&#31561;&#35774;&#35745;&#36873;&#25321;&#19978;&#23384;&#22312;&#21306;&#21035;&#65292;&#23545;&#20110;&#23398;&#20064;&#36817;&#20284;&#30340;&#36136;&#37327;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#33021;&#22815;&#20197;&#39640;&#24230;&#21487;&#34892;&#20294;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#20855;&#26377;&#26368;&#22810;&#20960;&#30334;&#20010;&#32500;&#24230;&#12290;&#30446;&#21069;&#23545;&#20110;&#22522;&#20110;GMM&#30340;&#21464;&#20998;&#25512;&#26029;&#26469;&#35828;&#65292;VIPS&#21644;iBayes-GMM&#26159;&#26368;&#26377;&#25928;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#23427;&#20204;&#37117;&#20351;&#29992;&#29420;&#31435;&#30340;&#33258;&#28982;&#26799;&#24230;&#26356;&#26032;&#26469;&#26356;&#26032;&#21508;&#20010;&#32452;&#20214;&#21450;&#20854;&#26435;&#37325;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#23427;&#20204;&#27966;&#29983;&#30340;&#26356;&#26032;&#26159;&#31561;&#20215;&#30340;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#23454;&#38469;&#23454;&#29616;&#21644;&#29702;&#35770;&#20445;&#35777;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#21306;&#20998;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#26679;&#26412;&#36873;&#25321;&#12289;&#33258;&#28982;&#26799;&#24230;&#20272;&#35745;&#12289;&#27493;&#38271;&#36866;&#24212;&#20197;&#21450;&#26159;&#21542;&#24378;&#21046;&#23454;&#26045;&#21487;&#20449;&#21306;&#22495;&#25110;&#35843;&#25972;&#32452;&#20214;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25152;&#23398;&#36817;&#20284;&#30340;&#36136;&#37327;&#21487;&#33021;&#20250;&#21463;&#21040;&#30456;&#24212;&#35774;&#35745;&#36873;&#25321;&#30340;&#20005;&#37325;&#24433;&#21709;&#65306;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#26469;&#26356;&#26032;&#21508;&#20010;&#32452;&#20214;&#65292;iBayes-GMM&#30340;&#23398;&#20064;&#36817;&#20284;&#36136;&#37327;&#21487;&#33021;&#21463;&#21040;&#26356;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#27604;&#36739;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.10818</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#21463;&#21551;&#21457;&#20110;&#22823;&#33041;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented Graph Neural Networks: A Brain-Inspired Review. (arXiv:2209.10818v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#27604;&#36739;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#23545;&#29616;&#26377;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35760;&#24518;&#22686;&#24378;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27861;&#21644;&#19968;&#32452;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#35760;&#24518;&#26426;&#21046;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a comprehensive review of the existing literature on memory-augmented GNNs. We review these works through the lens of psychology and neuroscience, which has several established theories on how multiple memory systems and mechanisms operate in biological brains. We propose a taxonomy of memory-augmented GNNs and a set of criteria for comparing their memory mechanisms. We also provide critical discussions on the limitations of these works. Finally, we discuss the challenges and future directions for this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#19988;&#23569;&#37327;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#33258;&#21160;&#21270;&#35786;&#26029;&#33258;&#38381;&#30151;&#65292;&#20811;&#26381;&#20102;&#21487;&#29992;&#25968;&#25454;&#37327;&#23567;&#21644;&#26679;&#26412;&#21464;&#21270;&#22823;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.05379</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#29305;&#24449;&#23398;&#20064;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#26089;&#26399;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#19988;&#23569;&#37327;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#33258;&#21160;&#21270;&#35786;&#26029;&#33258;&#38381;&#30151;&#65292;&#20811;&#26381;&#20102;&#21487;&#29992;&#25968;&#25454;&#37327;&#23567;&#21644;&#26679;&#26412;&#21464;&#21270;&#22823;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#65292;&#20063;&#34987;&#31216;&#20026;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#65292;&#26159;&#19968;&#31181;&#31070;&#32463;&#24615;&#30142;&#30149;&#12290;&#20854;&#20027;&#35201;&#30151;&#29366;&#21253;&#25324;(&#21475;&#35821;&#21644;/&#25110;&#38750;&#21475;&#35821;)&#27807;&#36890;&#22256;&#38590;&#21644;&#20725;&#21270;/&#37325;&#22797;&#34892;&#20026;&#12290;&#36825;&#20123;&#30151;&#29366;&#24120;&#24120;&#38590;&#20197;&#19982;&#27491;&#24120;&#65288;&#23545;&#29031;&#65289;&#20010;&#20307;&#21306;&#20998;&#65292;&#22240;&#27492;&#36825;&#31181;&#38556;&#30861;&#22312;&#20799;&#31461;&#26089;&#26399;&#20173;&#28982;&#26410;&#34987;&#35786;&#26029;&#20986;&#26469;&#65292;&#20174;&#32780;&#23548;&#33268;&#24310;&#36831;&#27835;&#30103;&#12290;&#30001;&#20110;&#22312;&#26089;&#26399;&#24180;&#40836;&#27573;&#23398;&#20064;&#26354;&#32447;&#38497;&#23789;&#65292;&#26089;&#26399;&#35786;&#26029;&#33258;&#38381;&#30151;&#21487;&#20197;&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#37319;&#21462;&#36866;&#24403;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21487;&#33021;&#23545;&#33258;&#38381;&#30151;&#23401;&#23376;&#30340;&#25104;&#38271;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#33258;&#38381;&#30151;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#23601;&#35786;&#20110;&#19987;&#31185;&#31934;&#31070;&#31185;&#21307;&#29983;&#65292;&#28982;&#32780;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#19988;&#23567;&#30340;&#34987;&#35797;&#21160;&#20316;&#35270;&#39057;&#21098;&#36753;&#26469;&#33258;&#21160;&#21270;&#33258;&#38381;&#30151;&#35786;&#26029;&#12290;&#36825;&#39033;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#27880;&#37322;&#25968;&#25454;&#37327;&#24456;&#23567;&#65292;&#24182;&#19988;&#26679;&#26412;&#20043;&#38388;&#23384;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological disorder. Its main symptoms include difficulty in (verbal and/or non-verbal) communication, and rigid/repetitive behavior. These symptoms are often indistinguishable from a normal (control) individual, due to which this disorder remains undiagnosed in early childhood leading to delayed treatment. Since the learning curve is steep during the initial age, an early diagnosis of autism could allow to take adequate interventions at the right time, which might positively affect the growth of an autistic child. Further, the traditional methods of autism diagnosis require multiple visits to a specialized psychiatrist, however this process can be time-consuming. In this paper, we present a learning based approach to automate autism diagnosis using simple and small action video clips of subjects. This task is particularly challenging because the amount of annotated data available is small, and the variations among samples
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#38750;&#24179;&#31283;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#19978;&#28216;&#21360;&#24230;&#27827;&#27969;&#22495;&#30340;&#38477;&#27700;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#23545;&#35813;&#22320;&#21306;&#22797;&#26434;&#26102;&#31354;&#38477;&#27700;&#20998;&#24067;&#30340;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.04947</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#27668;&#20505;&#31185;&#23398;&#30340;&#26680;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Learning for Explainable Climate Science. (arXiv:2209.04947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#38750;&#24179;&#31283;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#19978;&#28216;&#21360;&#24230;&#27827;&#27969;&#22495;&#30340;&#38477;&#27700;&#27169;&#24335;&#65292;&#35299;&#20915;&#20102;&#23545;&#35813;&#22320;&#21306;&#22797;&#26434;&#26102;&#31354;&#38477;&#27700;&#20998;&#24067;&#30340;&#29702;&#35299;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21916;&#39532;&#25289;&#38597;&#23665;&#19978;&#28216;&#21360;&#24230;&#27827;&#27969;&#22495;&#20026;2.7&#20159;&#20154;&#21475;&#21644;&#26080;&#25968;&#29983;&#24577;&#31995;&#32479;&#25552;&#20379;&#27700;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38477;&#27700;&#20316;&#20026;&#27700;&#25991;&#27169;&#25311;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#36825;&#20010;&#22320;&#21306;&#30340;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22260;&#32469;&#22312;&#27827;&#27969;&#22495;&#30340;&#22797;&#26434;&#26102;&#31354;&#38477;&#27700;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#38750;&#24179;&#31283;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#19978;&#28216;&#21360;&#24230;&#27827;&#27969;&#22495;&#30340;&#38477;&#27700;&#27169;&#24335;&#12290;&#20197;&#24448;&#22312;&#21360;&#24230;&#21916;&#39532;&#25289;&#38597;&#23665;&#21306;&#37327;&#21270;&#25110;&#27169;&#25311;&#38477;&#27700;&#30340;&#23581;&#35797;&#24448;&#24448;&#26159;&#23450;&#24615;&#30340;&#65292;&#21253;&#25324;&#20102;&#31895;&#31961;&#30340;&#20551;&#35774;&#21644;&#31616;&#21270;&#65292;&#26080;&#27861;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#19979;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#20960;&#20046;&#27809;&#26377;&#32771;&#34385;&#35823;&#24046;&#20256;&#25773;&#12290;&#25105;&#20204;&#21033;&#29992;&#38750;&#24179;&#31283;&#30340;&#21513;&#24067;&#26031;&#26680;&#21644;&#20381;&#36182;&#36755;&#20837;&#30340;&#38271;&#24230;&#21442;&#25968;&#26469;&#32771;&#34385;&#38477;&#27700;&#30340;&#31354;&#38388;&#21464;&#21270;&#65292;&#20351;&#24471;&#21518;&#39564;&#20989;&#25968;&#26679;&#26412;&#33021;&#22815;&#36866;&#24212;&#36825;&#19968;&#21306;&#22495;&#22266;&#26377;&#30340;&#38477;&#27700;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Upper Indus Basin, Himalayas provides water for 270 million people and countless ecosystems. However, precipitation, a key component to hydrological modelling, is poorly understood in this area. A key challenge surrounding this uncertainty comes from the complex spatial-temporal distribution of precipitation across the basin. In this work we propose Gaussian processes with structured non-stationary kernels to model precipitation patterns in the UIB. Previous attempts to quantify or model precipitation in the Hindu Kush Karakoram Himalayan region have often been qualitative or include crude assumptions and simplifications which cannot be resolved at lower resolutions. This body of research also provides little to no error propagation. We account for the spatial variation in precipitation with a non-stationary Gibbs kernel parameterised with an input dependent lengthscale. This allows the posterior function samples to adapt to the varying precipitation patterns inherent in the distin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#27714;&#35299;&#30340;&#26368;&#20248;&#27491;&#21017;&#22312;&#32447;&#20998;&#37197;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20598;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#38750;&#20984;&#32047;&#31215;&#22870;&#21169;&#12289;&#30828;&#36164;&#28304;&#32422;&#26463;&#21644;&#38750;&#20998;&#31163;&#27491;&#21017;&#21270;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#28040;&#38500;&#20102;&#36951;&#25022;&#30028;&#20013;&#30340;&#23545;&#25968;&#23545;&#25968;&#22240;&#23376;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19981;&#32463;&#24120;&#37325;&#26032;&#27714;&#35299;&#30340;&#26041;&#26696;&#38477;&#20302;&#20102;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26368;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.00399</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37325;&#27714;&#35299;&#30340;&#26368;&#20248;&#27491;&#21017;&#22312;&#32447;&#20998;&#37197;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Regularized Online Allocation by Adaptive Re-Solving. (arXiv:2209.00399v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#27714;&#35299;&#30340;&#26368;&#20248;&#27491;&#21017;&#22312;&#32447;&#20998;&#37197;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20598;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#38750;&#20984;&#32047;&#31215;&#22870;&#21169;&#12289;&#30828;&#36164;&#28304;&#32422;&#26463;&#21644;&#38750;&#20998;&#31163;&#27491;&#21017;&#21270;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#28040;&#38500;&#20102;&#36951;&#25022;&#30028;&#20013;&#30340;&#23545;&#25968;&#23545;&#25968;&#22240;&#23376;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#19981;&#32463;&#24120;&#37325;&#26032;&#27714;&#35299;&#30340;&#26041;&#26696;&#38477;&#20302;&#20102;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26368;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28508;&#22312;&#38750;&#20984;&#32047;&#31215;&#22870;&#21169;&#12289;&#30828;&#36164;&#28304;&#32422;&#26463;&#21644;&#38750;&#20998;&#31163;&#27491;&#21017;&#21270;&#22120;&#30340;&#27491;&#21017;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#33258;&#36866;&#24212;&#26356;&#26032;&#36164;&#28304;&#32422;&#26463;&#30340;&#31574;&#30053;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20165;&#35201;&#27714;&#23545;&#32463;&#39564;&#23545;&#20598;&#38382;&#39064;&#36827;&#34892;&#36817;&#20284;&#35299;&#65292;&#20197;&#36798;&#21040;&#19968;&#23450;&#30340;&#31934;&#24230;&#65292;&#24182;&#22312;&#23616;&#37096;&#20108;&#38454;&#22686;&#38271;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#23545;&#25968;&#36951;&#25022;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;&#20598;&#30446;&#26631;&#20989;&#25968;&#30340;&#31934;&#32454;&#20998;&#26512;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#28040;&#38500;&#36951;&#25022;&#30028;&#20013;&#30340;&#33261;&#21517;&#26157;&#33879;&#30340;&#23545;&#25968;&#23545;&#25968;&#22240;&#23376;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#31435;&#21363;&#36866;&#29992;&#20110;&#33879;&#21517;&#30340;&#35745;&#31639;&#24555;&#36895;&#31639;&#27861;&#65292;&#20363;&#22914;&#23545;&#20598;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#32463;&#24120;&#37325;&#26032;&#27714;&#35299;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#65292;&#19988;&#19981;&#20250;&#24433;&#21709;&#26368;&#20248;&#36951;&#25022;&#24615;&#33021;&#12290;&#21516;&#26102;&#24314;&#31435;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24179;&#26041;&#26681;&#36951;&#25022;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially non-concave cumulative rewards, hard resource constraints, and a non-separable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, e.g., dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is establ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#24182;&#35774;&#35745;&#20102;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861; TABASCO&#65292;&#26377;&#25928;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#23614;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2208.09833</link><description>&lt;p&gt;
&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Label-Noise Learning with Intrinsically Long-Tailed Data. (arXiv:2208.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#24182;&#35774;&#35745;&#20102;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861; TABASCO&#65292;&#26377;&#25928;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#23614;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#30340;&#30495;&#23454;&#31867;&#21035;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#26159;&#19981;&#24179;&#34913;&#30340;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#21644;&#20869;&#22312;&#31867;&#21035;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#22312;&#20855;&#26377;&#26410;&#30693;&#20869;&#22312;&#31867;&#21035;&#20998;&#24067;&#30340;&#38271;&#23614;&#31867;&#19978;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#30340;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#65288;TABASCO&#65289;&#26469;&#26356;&#22909;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23614;&#37096;&#31867;&#21035;&#12290;TABASCO&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20998;&#31163;&#24230;&#37327;&#65292;&#30456;&#20114;&#34917;&#20805;&#65292;&#24357;&#34917;&#20102;&#22312;&#26679;&#26412;&#20998;&#31163;&#20013;&#20351;&#29992;&#21333;&#20010;&#24230;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#20855;&#26377;&#30495;&#23454;&#25968;&#25454;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is one of the key factors that lead to the poor generalization of deep learning models. Existing label-noise learning methods usually assume that the ground-truth classes of the training data are balanced. However, the real-world data is often imbalanced, leading to the inconsistency between observed and intrinsic class distribution with label noises. In this case, it is hard to distinguish clean samples from noisy samples on the intrinsic tail classes with the unknown intrinsic class distribution. In this paper, we propose a learning framework for label-noise learning with intrinsically long-tailed data. Specifically, we propose two-stage bi-dimensional sample selection (TABASCO) to better separate clean samples from noisy samples, especially for the tail classes. TABASCO consists of two new separation metrics that complement each other to compensate for the limitation of using a single metric in sample separation. Extensive experiments on benchmarks we proposed with real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;Bradley-Terry&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#36125;&#21494;&#26031;&#26041;&#27861;&#33021;&#25552;&#20379;&#26356;&#32454;&#33268;&#30340;&#31639;&#27861;&#20043;&#38388;&#24046;&#24322;&#25551;&#36848;&#65292;&#24182;&#20801;&#35768;&#23545;&#31561;&#25928;&#24615;&#36827;&#34892;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2208.04935</link><description>&lt;p&gt;
&#19968;&#31181;&#27604;&#36739;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36125;&#21494;&#26031;Bradley-Terry&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets. (arXiv:2208.04935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;Bradley-Terry&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#36125;&#21494;&#26031;&#26041;&#27861;&#33021;&#25552;&#20379;&#26356;&#32454;&#33268;&#30340;&#31639;&#27861;&#20043;&#38388;&#24046;&#24322;&#25551;&#36848;&#65292;&#24182;&#20801;&#35768;&#23545;&#31561;&#25928;&#24615;&#36827;&#34892;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;Bradley-Terry&#27169;&#22411;&#65292;&#32479;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#21478;&#19968;&#20010;&#31639;&#27861;&#30340;&#27425;&#25968;&#12290;&#19982;&#39057;&#29575;&#27966;&#26041;&#27861;&#65288;&#22914;Demsar&#65288;2006&#65289;&#30340;&#24179;&#22343;&#25490;&#21517;&#27604;&#36739;&#27979;&#35797;&#21644;Benavoli&#31561;&#20154;&#65288;2016&#65289;&#30340;&#22810;&#20010;&#37197;&#23545;Wilcoxon&#27979;&#35797;&#19982;p&#35843;&#25972;&#36807;&#31243;&#65289;&#30456;&#27604;&#65292;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;Bradley-Terry&#27169;&#22411;&#65288;BBT&#65289;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;&#36125;&#21494;&#26031;&#26041;&#27861;&#20801;&#35768;&#23545;&#31639;&#27861;&#36827;&#34892;&#26356;&#21152;&#32454;&#33268;&#30340;&#25551;&#36848;&#65292;&#32780;&#19981;&#20165;&#20165;&#22768;&#31216;&#24046;&#24322;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#36824;&#20801;&#35768;&#23450;&#20041;&#20004;&#20010;&#31639;&#27861;&#22312;&#23454;&#38469;&#30446;&#30340;&#19979;&#26159;&#21542;&#31561;&#25928;&#65292;&#25110;&#23454;&#38469;&#31561;&#25928;&#21306;&#22495;&#65288;ROPE&#65289;&#12290;&#19982;Benavoli&#31561;&#20154;&#65288;2017&#65289;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#31526;&#21495;&#31209;&#27604;&#36739;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#20123;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Bayesian model to compare multiple algorithms on multiple data sets, on any metric. The model is based on the Bradley-Terry model, that counts the number of times one algorithm performs better than another on different data sets. Because of its Bayesian foundations, the Bayesian Bradley Terry model (BBT) has different characteristics than frequentist approaches to comparing multiple algorithms on multiple data sets, such as Demsar (2006) tests on mean rank, and Benavoli et al. (2016) multiple pairwise Wilcoxon tests with p-adjustment procedures. In particular, a Bayesian approach allows for more nuanced statements regarding the algorithms beyond claiming that the difference is or it is not statistically significant. Bayesian approaches also allow to define when two algorithms are equivalent for practical purposes, or the region of practical equivalence (ROPE). Different than a Bayesian signed rank comparison procedure proposed by Benavoli et al. (2017), our approa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24739;&#32773;&#20154;&#21475;&#22270;&#19978;&#21033;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10603</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21307;&#30103;&#22270;&#35889;&#20013;&#30340;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24739;&#32773;&#20154;&#21475;&#22270;&#19978;&#21033;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#23567;&#22411;&#21307;&#38498;&#25110;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#30340;&#24773;&#20917;&#19979;&#65292;&#34429;&#28982;&#35760;&#24405;&#20102;&#22823;&#37327;&#30340;&#20020;&#24202;&#35760;&#24405;&#65292;&#20294;&#25968;&#25454;&#21644;&#26631;&#31614;&#20173;&#28982;&#21487;&#33021;&#31232;&#32570;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39044;&#35757;&#32451;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#29992;&#20110;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#28789;&#24863;&#26469;&#33258;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#65292;&#36890;&#36807;&#22312;&#24739;&#32773;&#20154;&#21475;&#22270;&#19978;&#21033;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21464;&#25442;&#22120;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#20020;&#24202;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#25513;&#30721;&#30340;&#39044;&#35757;&#32451;&#19982;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#22312;&#20854;&#20182;&#39046;&#22495;&#20013;&#22522;&#20110;&#25513;&#30721;&#30340;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training has shown success in different areas of machine learning, such as Computer Vision, Natural Language Processing (NLP), and medical imaging. However, it has not been fully explored for clinical data analysis. An immense amount of clinical records are recorded, but still, data and labels can be scarce for data collected in small hospitals or dealing with rare diseases. In such scenarios, pre-training on a larger set of unlabelled clinical data could improve performance. In this paper, we propose novel unsupervised pre-training techniques designed for heterogeneous, multi-modal clinical data for patient outcome prediction inspired by masked language modeling (MLM), by leveraging graph deep learning over population graphs. To this end, we further propose a graph-transformer-based network, designed to handle heterogeneous clinical data. By combining masking-based pre-training with a transformer-based network, we translate the success of masking-based pre-training in other domain
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21322;&#30417;&#30563;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#24212;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#39046;&#22495;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.06767</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#30340;&#36328;&#35821;&#35328;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised cross-lingual speech emotion recognition. (arXiv:2207.06767v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06767
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21322;&#30417;&#30563;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#24212;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#39046;&#22495;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#21333;&#35821;&#31181;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#31181;&#21644;&#30446;&#26631;&#35821;&#31181;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#20197;&#21450;&#30446;&#26631;&#35821;&#31181;&#20013;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#65292;&#36328;&#35821;&#35328;SER&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24403;&#30446;&#26631;&#35821;&#31181;&#65288;&#21363;&#26032;&#35821;&#35328;&#65289;&#20013;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#26679;&#26412;&#21487;&#29992;&#26102;&#30340;&#36328;&#35821;&#35328;&#24773;&#32490;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;&#35821;&#21477;&#19978;&#21033;&#29992;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30828;&#20266;&#26631;&#31614;&#21644;&#36719;&#20266;&#26631;&#31614;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29420;&#31435;&#35828;&#35805;&#20154;&#30340;&#35774;&#23450;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20004;&#20010;&#30446;&#26631;&#35821;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance in Speech Emotion Recognition (SER) on a single language has increased greatly in the last few years thanks to the use of deep learning techniques. However, cross-lingual SER remains a challenge in real-world applications due to two main factors: the first is the big gap among the source and the target domain distributions; the second factor is the major availability of unlabeled utterances in contrast to the labeled ones for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when only few labeled examples in the target domain (i.e. the new language) are available. Our method is based on a Transformer and it adapts to the new domain by exploiting a pseudo-labeling strategy on the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the proposed method in a speaker-independent setup on both 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#20998;&#31867;&#22120;&#36755;&#20986;&#22312;&#23616;&#37096;&#36716;&#25442;&#37051;&#22495;&#20013;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#20998;&#24067;&#25110;&#27169;&#22411;&#20551;&#35774;&#65292;&#21487;&#24212;&#29992;&#20110;&#22495;&#22806;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2207.02093</link><description>&lt;p&gt;
&#20351;&#29992;&#37051;&#22495;&#19981;&#21464;&#24615;&#39044;&#27979;&#22495;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Predicting Out-of-Domain Generalization with Neighborhood Invariance. (arXiv:2207.02093v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#20998;&#31867;&#22120;&#36755;&#20986;&#22312;&#23616;&#37096;&#36716;&#25442;&#37051;&#22495;&#20013;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#20998;&#24067;&#25110;&#27169;&#22411;&#20551;&#35774;&#65292;&#21487;&#24212;&#29992;&#20110;&#22495;&#22806;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22320;&#24320;&#21457;&#21644;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#20915;&#20110;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#22312;&#26032;&#29615;&#22659;&#20013;&#30340;&#29305;&#24449;&#21644;&#27604;&#36739;&#33021;&#21147;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21487;&#20197;&#30452;&#25509;&#39044;&#27979;&#25110;&#29702;&#35770;&#19978;&#38480;&#21046;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#20381;&#36182;&#20110;&#21305;&#37197;&#30340;&#35757;&#32451;/&#27979;&#35797;&#20998;&#24067;&#21644;&#35775;&#38382;&#27169;&#22411;&#26799;&#24230;&#31561;&#24378;&#20551;&#35774;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20551;&#35774;&#19981;&#28385;&#36275;&#26102;&#25551;&#36848;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37051;&#22495;&#19981;&#21464;&#24615;&#65292;&#19968;&#31181;&#20998;&#31867;&#22120;&#22312;&#23616;&#37096;&#36716;&#25442;&#37051;&#22495;&#20013;&#36755;&#20986;&#19981;&#21464;&#30340;&#24230;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#26679;&#19968;&#32452;&#36716;&#25442;&#65292;&#23545;&#20110;&#19968;&#20010;&#36755;&#20837;&#27979;&#35797;&#28857;&#65292;&#35745;&#31639;&#19981;&#21464;&#24615;&#20316;&#20026;&#34987;&#20998;&#31867;&#20026;&#21516;&#19968;&#31867;&#21035;&#30340;&#36716;&#25442;&#28857;&#30340;&#26368;&#22823;&#27604;&#20363;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#31616;&#21333;&#26131;&#35745;&#31639;&#65292;&#19981;&#20381;&#36182;&#20110;&#27979;&#35797;&#28857;&#30340;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#25110;&#27169;&#22411;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#22495;&#22806;&#29615;&#22659;&#19979;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) setti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2206.05794</link><description>&lt;p&gt;
SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#35777;&#26126;&#20250;&#24341;&#20837;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05794
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#26102;&#23398;&#20064;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#30340;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#30340;&#26435;&#37325;&#34928;&#20943;&#26102;&#65292;&#36825;&#31181;&#20559;&#24046;&#26356;&#21152;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#27979;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26435;&#37325;&#34928;&#20943;&#26159;&#23454;&#29616;&#36825;&#31181;&#20559;&#24046;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#19982;&#20808;&#21069;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#12289;&#25910;&#25947;&#24615;&#25110;&#26435;&#37325;&#30697;&#38453;&#20248;&#21270;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#36866;&#29992;&#20110;&#20219;&#24847;&#23485;&#24230;&#25110;&#28145;&#24230;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#27745;&#26579;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#32473;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2205.14842</link><description>&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39640;&#25928;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (arXiv:2205.14842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#27745;&#26579;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#32473;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#23545;&#26234;&#33021;&#20307;&#20351;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#29615;&#22659;&#21160;&#24577;&#19968;&#26080;&#25152;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;&#23545;&#25239;MDP&#25915;&#20987;&#30340;&#36890;&#29992;&#40657;&#30418;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#23454;&#20363;&#21270;&#20026;&#20004;&#31181;&#26032;&#25915;&#20987;&#65292;&#21482;&#30772;&#22351;&#20102;&#24635;&#35757;&#32451;&#26102;&#38388;&#27493;&#25968;&#30340;&#23569;&#37327;&#22870;&#21169;&#65292;&#24182;&#20351;&#20195;&#29702;&#23398;&#20064;&#20302;&#25928;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#25915;&#20987;&#25928;&#29575;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#33021;&#26377;&#25928;&#22320;&#27745;&#26579;&#20351;&#29992;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#65288;&#22914;DQN&#12289;PPO&#12289;SAC&#31561;&#65289;&#26469;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#65292;&#22312;&#22810;&#20010;&#21463;&#27426;&#36814;&#30340;&#32463;&#20856;&#25511;&#21046;&#21644;MuJoCo&#29615;&#22659;&#20013;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#26368;&#20339;k&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#23548;&#21521;&#36873;&#25321;&#30340;&#31639;&#27861;&#65288;IDS&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;IDS&#38598;&#25104;&#30340;&#39030;&#37096;&#20004;&#20010;&#27748;&#22982;&#36874;&#37319;&#26679;&#22312;&#39640;&#26031;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#36798;&#21040;&#20102;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2205.12086</link><description>&lt;p&gt;
&#20449;&#24687;&#23548;&#21521;&#36873;&#25321;&#30340;&#21069;&#20004;&#20010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Information-Directed Selection for Top-Two Algorithms. (arXiv:2205.12086v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#26368;&#20339;k&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#23548;&#21521;&#36873;&#25321;&#30340;&#31639;&#27861;&#65288;IDS&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;IDS&#38598;&#25104;&#30340;&#39030;&#37096;&#20004;&#20010;&#27748;&#22982;&#36874;&#37319;&#26679;&#22312;&#39640;&#26031;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#36798;&#21040;&#20102;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;k&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#39034;&#24207;&#20998;&#37197;&#27979;&#37327;&#21162;&#21147;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#24179;&#22343;&#22870;&#21169;&#30340;k&#33218;&#20934;&#30830;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#20598;&#21464;&#37327;&#26469;&#34920;&#24449;&#26368;&#20248;&#20998;&#37197;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26368;&#20248;&#24615;&#26465;&#20214;&#23548;&#33268;&#20102;&#39030;&#37096;&#20004;&#20010;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65288;Russo, 2020&#65289;&#30340;&#25193;&#23637;&#65292;&#36825;&#26368;&#21021;&#26159;&#20026;&#20102;&#26368;&#20339;&#33218;&#35782;&#21035;&#32780;&#25552;&#20986;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#24341;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36873;&#25321;&#35268;&#21017;&#65292;&#31216;&#20026;&#20449;&#24687;&#23548;&#21521;&#36873;&#25321;&#65288;IDS&#65289;&#65292;&#23427;&#26681;&#25454;&#20449;&#24687;&#22686;&#30410;&#30340;&#24230;&#37327;&#36873;&#25321;&#21069;&#20004;&#20010;&#20505;&#36873;&#20013;&#30340;&#19968;&#20010;&#12290;&#20316;&#20026;&#29702;&#35770;&#20445;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;IDS&#38598;&#25104;&#30340;&#39030;&#37096;&#20004;&#20010;&#27748;&#22982;&#36874;&#37319;&#26679;&#22312;&#39640;&#26031;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#65288;&#28176;&#36817;&#22320;&#65289;&#36798;&#21040;&#20102;&#26368;&#20248;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#25991;&#29486;&#20013;&#31361;&#20986;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#65288;Russo, 2020&#65289;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;k &gt; 1&#65292;&#39030;&#37096;&#20004;&#20010;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the best-k-arm identification problem for multi-armed bandits, where the objective is to select the exact set of k arms with the highest mean rewards by sequentially allocating measurement effort. We characterize the necessary and sufficient conditions for the optimal allocation using dual variables. Remarkably these optimality conditions lead to the extension of top-two algorithm design principle (Russo, 2020), initially proposed for best-arm identification. Furthermore, our optimality conditions induce a simple and effective selection rule dubbed information-directed selection (IDS) that selects one of the top-two candidates based on a measure of information gain. As a theoretical guarantee, we prove that integrated with IDS, top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm identification, solving a glaring open problem in the pure exploration literature (Russo, 2020). As a by-product, we show that for k &gt; 1, top-two algorithms cannot achieve op
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23454;&#20363;&#30456;&#20851;&#30340;&#23545;&#25968;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#35774;&#35745;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.11168</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23545;&#25968;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23454;&#20363;&#30456;&#20851;&#30340;&#23545;&#25968;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#35774;&#35745;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#12289;&#24179;&#22343;&#22870;&#21169;&#35774;&#23450;&#19979;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#31163;&#25955;&#26102;&#38388;MDPs&#19981;&#21516;&#65292;&#36830;&#32493;&#26102;&#38388;&#36807;&#31243;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#20250;&#31227;&#21160;&#21040;&#19968;&#20010;&#29366;&#24577;&#24182;&#22312;&#27492;&#20572;&#30041;&#19968;&#20010;&#38543;&#26426;&#25345;&#32493;&#26102;&#38388;&#12290;&#22312;&#26410;&#30693;&#30340;&#36716;&#31227;&#27010;&#29575;&#21644;&#25351;&#25968;&#25345;&#32493;&#26102;&#38388;&#21464;&#21270;&#29575;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#26102;&#38388;&#36328;&#24230;&#23545;&#25968;&#30456;&#20851;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#19978;&#38480;&#32622;&#20449;&#22686;&#24378;&#23398;&#20064;&#12289;&#22343;&#20540;&#25345;&#32493;&#26102;&#38388;&#30340;&#31934;&#32454;&#20272;&#35745;&#20197;&#21450;&#28857;&#36807;&#31243;&#30340;&#38543;&#26426;&#27604;&#36739;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#34920;&#31034;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#26469;&#25552;&#39640;&#28210;&#26579;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.03923</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21457;&#29616;&#21644;&#21512;&#25104;&#29289;&#20307;&#20809;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#34920;&#31034;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#26469;&#25552;&#39640;&#28210;&#26579;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#65292;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#34920;&#31034;&#65292;&#24050;&#32463;&#25104;&#20026;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#30340;&#24378;&#22823;&#26032;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#26080;&#30417;&#30563;&#21457;&#29616;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#12290;&#28982;&#32780;&#65292;&#23556;&#32447;&#34892;&#36827;&#30340;&#39640;&#25104;&#26412;&#65292;&#21152;&#19978;&#27599;&#20010;&#29289;&#20307;&#34920;&#31034;&#27861;&#37117;&#24517;&#39035;&#21333;&#29420;&#36827;&#34892;&#23556;&#32447;&#34892;&#36827;&#30340;&#20107;&#23454;&#65292;&#23548;&#33268;&#37319;&#26679;&#19981;&#36275;&#30340;&#20142;&#24230;&#22330;&#65292;&#20174;&#32780;&#20135;&#29983;&#22122;&#28857;&#28210;&#26579;&#12289;&#20302;&#24103;&#29575;&#12289;&#39640;&#20869;&#23384;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#35757;&#32451;&#21644;&#28210;&#26579;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#29289;&#20307;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#22330;&#22797;&#21512;&#27169;&#22359;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20809;&#22330;&#37325;&#24314;&#20840;&#23616;&#20809;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#32452;&#21512;&#24615;&#29289;&#20307;&#20809;&#22330;&#65288;COLF&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#27861;&#65292;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#21644;&#26032;&#35270;&#35282;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38477;&#20302;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#35299;&#20915;&#20102;Thompson&#25277;&#26679;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2205.01970</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#30340;&#39044;&#27979;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38477;&#20302;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#35299;&#20915;&#20102;Thompson&#25277;&#26679;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a predictive sampling algorithm to solve the non-stationary bandit learning problem. By deprioritizing the acquisition of information that quickly loses usefulness, the algorithm outperforms Thompson sampling in all non-stationary environments examined.
&lt;/p&gt;
&lt;p&gt;
Thompson&#25277;&#26679;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#24179;&#31283;&#36172;&#21338;&#26426;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#24403;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#21487;&#33021;&#24456;&#24046;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#22833;&#36133;&#26159;&#30001;&#20110;&#22312;&#25506;&#32034;&#26102;&#65292;&#31639;&#27861;&#27809;&#26377;&#26681;&#25454;&#30001;&#20110;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#20449;&#24687;&#24555;&#36895;&#22833;&#21435;&#26377;&#29992;&#24615;&#30340;&#36895;&#24230;&#21306;&#20998;&#34892;&#21160;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#36825;&#20123;&#20449;&#24687;&#30001;&#20110;&#24555;&#36895;&#22833;&#21435;&#26377;&#29992;&#24615;&#32780;&#19981;&#20877;&#37325;&#35201;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39044;&#27979;&#25277;&#26679;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39044;&#27979;&#25277;&#26679;&#30340;&#29256;&#26412;&#65292;&#20854;&#35745;&#31639;&#21487;&#25193;&#23637;&#21040;&#23454;&#38469;&#24863;&#20852;&#36259;&#30340;&#22797;&#26434;&#36172;&#21338;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#27979;&#25277;&#26679;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#37117;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thompson sampling has proven effective across a wide range of stationary bandit environments. However, as we demonstrate in this paper, it can perform poorly when applied to non-stationary environments. We show that such failures are attributed to the fact that, when exploring, the algorithm does not differentiate actions based on how quickly the information acquired loses its usefulness due to non-stationarity. Building upon this insight, we propose predictive sampling, an algorithm that deprioritizes acquiring information that quickly loses usefulness. Theoretical guarantee on the performance of predictive sampling is established through a Bayesian regret bound. We provide versions of predictive sampling for which computations tractably scale to complex bandit environments of practical interest. Through numerical simulations, we demonstrate that predictive sampling outperforms Thompson sampling in all non-stationary environments examined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07028</link><description>&lt;p&gt;
&#25506;&#32034;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#22312;&#32852;&#37030;&#33976;&#39311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm, &#22312;&#27492;&#26381;&#21153;&#22120;&#21608;&#26399;&#24615;&#22320;&#25910;&#38598;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;, &#32780;&#19981;&#32452;&#35013;&#20854;&#31169;&#26377;&#25968;&#25454;. &#26377;&#38480;&#30340;&#36890;&#35759;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#23545;FL&#25552;&#20986;&#20102;&#20005;&#23803;&#25361;&#25112;. &#32852;&#37030;&#33976;&#39311; (FD) &#34987;&#25552;&#20986;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;, &#19982;&#27492;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;, &#25903;&#25345;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#35759;&#24320;&#38144;. &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FD&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;. &#19968;&#20123;&#26368;&#36817;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;FD&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#39069;&#22806;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#38656;&#27714;, &#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#32780;&#20135;&#29983;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;, &#23548;&#33268;&#26381;&#21153;&#22120;&#19978;&#30340;&#27169;&#22411;&#34920;&#31034;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;.
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20851;&#32852;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#65292;&#24471;&#21040;&#20102;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#26032;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.05767</link><description>&lt;p&gt;
&#22522;&#20110;PDE&#30340;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20851;&#32852;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#65292;&#24471;&#21040;&#20102;&#23545;&#31216;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#26032;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#29256;&#26412;&#30340;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#20004;&#20010;&#33218;&#30340;&#24179;&#22343;&#20540;&#20043;&#21644;&#20026;1&#65288;&#21363;&#23545;&#31216;&#30340;&#21452;&#33218;&#20271;&#21162;&#21033;&#36172;&#21338;&#26426;&#65289;&#12290;&#22312;&#33218;&#20043;&#38388;&#30340;&#24046;&#36317;&#36235;&#36817;&#20110;&#38646;&#19988;&#39044;&#27979;&#26399;&#25968;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#20010;&#35299;&#19982;&#32447;&#24615;&#28909;&#26041;&#31243;&#30340;&#35299;&#20851;&#32852;&#65292;&#24471;&#21040;&#20102;&#35813;&#38382;&#39064;&#30340;minmax&#26368;&#20248;&#36951;&#25022;&#21644;&#20266;&#36951;&#25022;&#30340;&#39046;&#20808;&#39033;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;&#20808;&#21069;&#24050;&#30693;&#30340;&#32467;&#26524;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#24046;&#36317;&#32553;&#25918;&#27169;&#24335;&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#36825;&#20123;&#39046;&#20808;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#33539;&#22260;&#30340;&#26032;&#30340;&#38750;&#28176;&#36817;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses a version of the two-armed Bernoulli bandit problem where the sum of the means of the arms is one (the symmetric two-armed Bernoulli bandit). In a regime where the gap between these means goes to zero and the number of prediction periods approaches infinity, we obtain the leading order terms of the minmax optimal regret and pseudoregret for this problem by associating each of them with a solution of a linear heat equation. Our results improve upon the previously known results; specifically, we explicitly compute these leading order terms in three different scaling regimes for the gap. Additionally, we obtain new non-asymptotic bounds for any given time horizon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#20445;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#31354;&#38388;&#20869;&#38543;&#26426;&#21270;&#27169;&#22411;&#36755;&#20837;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#28857;&#20043;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#29305;&#23450;&#30340;&#25200;&#21160;&#22823;&#23567;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#23545;&#22266;&#23450;&#22823;&#23567;&#25200;&#21160;&#30340;&#30830;&#20445;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2201.12440</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#39564;&#35777;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying Model Accuracy under Distribution Shifts. (arXiv:2201.12440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#20445;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#31354;&#38388;&#20869;&#38543;&#26426;&#21270;&#27169;&#22411;&#36755;&#20837;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#28857;&#20043;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#29305;&#23450;&#30340;&#25200;&#21160;&#22823;&#23567;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#23545;&#22266;&#23450;&#22823;&#23567;&#25200;&#21160;&#30340;&#30830;&#20445;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#20027;&#35201;&#20851;&#27880;&#25968;&#25454;&#20998;&#24067;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#22266;&#23450;&#25915;&#20987;&#39044;&#31639;&#19979;&#23545;&#36755;&#20837;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20302;&#38454;Wasserstein&#21464;&#25442;&#19979;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#36716;&#25442;&#31354;&#38388;&#20869;&#23545;&#27169;&#22411;&#36755;&#20837;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20174;&#32780;&#22312;&#36716;&#25442;&#19979;&#35777;&#26126;&#20854;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#25968;&#25454;&#29305;&#23450;&#30340;&#25200;&#21160;&#22823;&#23567;&#22312;&#36755;&#20837;&#20998;&#24067;&#30340;&#19981;&#21516;&#28857;&#20043;&#38388;&#21464;&#21270;&#65292;&#24182;&#19988;&#20063;&#21253;&#25324;&#22266;&#23450;&#22823;&#23567;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#20026;&#21407;&#22987;&#20998;&#24067;&#21608;&#22260;&#30340;&#20219;&#20309;&#65288;&#33258;&#28982;&#25110;&#23545;&#25239;&#24615;&#65289;&#36755;&#20837;&#20998;&#24067;&#30340;Wasserstein&#29699;&#20869;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#30830;&#20445;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#24212;&#29992;&#20110;&#20197;&#19979;&#39046;&#22495;&#65306;&#65288;i&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#33258;&#28982;&#65288;&#38750;&#23545;&#25239;&#24615;&#65289;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#20363;&#22914;&#39068;&#33394;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#29992;&#20110;&#38899;&#20048;&#33410;&#25293;&#36319;&#36394;&#21644;&#19979;&#25293;&#20272;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#33410;&#25293;&#30340;&#33258;&#21160;&#26631;&#27880;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#36215;&#22987;&#20989;&#25968;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33410;&#25293;&#36319;&#36394;&#35757;&#32451;&#38598;&#38750;&#24120;&#23567;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#35813;&#24037;&#20316;&#20026;&#38899;&#20048;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2201.01771</link><description>&lt;p&gt;
&#38899;&#20048;&#20449;&#21495;&#20013;&#30340;&#33258;&#30417;&#30563;&#33410;&#25293;&#36319;&#36394;&#19982;&#22810;&#22768;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#29992;&#20110;&#38899;&#20048;&#33410;&#25293;&#36319;&#36394;&#21644;&#19979;&#25293;&#20272;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#33410;&#25293;&#30340;&#33258;&#21160;&#26631;&#27880;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#36215;&#22987;&#20989;&#25968;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33410;&#25293;&#36319;&#36394;&#35757;&#32451;&#38598;&#38750;&#24120;&#23567;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#35813;&#24037;&#20316;&#20026;&#38899;&#20048;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#38899;&#20048;&#33410;&#25293;&#26159;&#19968;&#39033;&#38750;&#24120;&#28459;&#38271;&#20047;&#21619;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#29992;&#20110;&#33410;&#25293;&#36319;&#36394;&#21644;&#19979;&#25293;&#20272;&#35745;&#12290;&#35813;&#20219;&#21153;&#21033;&#29992;&#20102;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#27169;&#22411;Spleeter&#65292;&#23558;&#27468;&#26354;&#30340;&#40723;&#22768;&#19982;&#20854;&#20313;&#20449;&#21495;&#20998;&#31163;&#24320;&#26469;&#12290;&#31532;&#19968;&#32452;&#20449;&#21495;&#34987;&#29992;&#20316;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#32780;&#21435;&#40723;&#22768;&#30340;&#20449;&#21495;&#21017;&#20316;&#20026;&#38170;&#28857;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23545;&#23436;&#20840;&#21367;&#31215;&#21644;&#36882;&#24402;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#36215;&#22987;&#20989;&#25968;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21457;&#29616;&#36825;&#20010;&#20989;&#25968;&#34987;&#26144;&#23556;&#21040;&#27468;&#26354;&#20013;&#30340;&#21608;&#26399;&#24615;&#20803;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#33410;&#25293;&#36319;&#36394;&#35757;&#32451;&#38598;&#38750;&#24120;&#23567;&#30340;&#24773;&#20917;&#19979;&#65288;&#23567;&#20110;10&#20010;&#31034;&#20363;&#65289;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#24403;&#24773;&#20917;&#19981;&#26159;&#36825;&#26679;&#26102;&#65292;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#23398;&#20064;&#36895;&#24230;&#21152;&#24555;&#65292;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#35757;&#32451;&#38598;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#24037;&#20316;&#23450;&#20041;&#20102;&#26032;&#30340;&#33410;&#25293;&#36319;&#36394;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20026;&#38899;&#20048;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating musical beats is a very long and tedious process. In order to combat this problem, we present a new self-supervised learning pretext task for beat tracking and downbeat estimation. This task makes use of Spleeter, an audio source separation model, to separate a song's drums from the rest of its signal. The first set of signals are used as positives, and by extension negatives, for contrastive learning pre-training. The drum-less signals, on the other hand, are used as anchors. When pre-training a fully-convolutional and recurrent model using this pretext task, an onset function is learned. In some cases, this function is found to be mapped to periodic elements in a song. We find that pre-trained models outperform randomly initialized models when a beat tracking training set is extremely small (less than 10 examples). When this is not the case, pre-training leads to a learning speed-up that causes the model to overfit to the training set. More generally, this work defines new
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#22343;&#21248;&#20915;&#31574;&#26641;&#21644;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2108.12976</link><description>&lt;p&gt;
&#29992;&#30456;&#20851;&#24615;&#36924;&#36817;&#28504;&#22810;&#25289;&#39764;&#30418;
&lt;/p&gt;
&lt;p&gt;
Approximating Pandora's Box with Correlations. (arXiv:2108.12976v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12976
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#38382;&#39064;&#65292;&#23558;&#20854;&#19982;&#22343;&#21248;&#20915;&#31574;&#26641;&#21644;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#30456;&#20851;&#20998;&#24067;&#19979;&#30340;&#32463;&#20856;&#28504;&#22810;&#25289;&#39764;&#30418;&#65288;PB&#65289;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24471;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38382;&#39064;&#20013;&#25353;&#22266;&#23450;&#39034;&#24207;&#35775;&#38382;&#30418;&#23376;&#30340;&#19968;&#31867;&#38480;&#21046;&#31574;&#30053;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#30475;&#21040;&#30340;&#25968;&#20540;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#35775;&#38382;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#23558;PB&#19982;&#32463;&#20856;&#30340;&#22343;&#21248;&#20915;&#31574;&#26641;&#65288;UDT&#65289;&#38382;&#39064;&#20197;&#21450;&#21464;&#20307;&#30340;&#26368;&#23567;&#21644;&#38598;&#35206;&#30422;&#65288;MSSC_f&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#36817;&#20284;&#36716;&#21270;&#12290;&#23545;&#20110;&#20855;&#26377;&#25903;&#25345;&#24230;$m$&#30340;&#20998;&#24067;&#65292;UDT&#20855;&#26377;$\log m$&#30340;&#36817;&#20284;&#27604;&#65292;&#23613;&#31649;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20294;&#22312;&#20122;&#25351;&#25968;&#26102;&#38388;&#20869;&#23454;&#29616;&#24120;&#25968;&#36817;&#20284;&#27604;&#26159;&#21487;&#33021;&#30340;&#65288;arXiv:1906.11385&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;PB&#21644;MSSC_f&#20063;&#20855;&#26377;&#30456;&#21516;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;c
&lt;/p&gt;
&lt;p&gt;
We revisit the classic Pandora's Box (PB) problem under correlated distributions on the box values. Recent work of arXiv:1911.01632 obtained constant approximate algorithms for a restricted class of policies for the problem that visit boxes in a fixed order. In this work, we study the complexity of approximating the optimal policy which may adaptively choose which box to visit next based on the values seen so far.  Our main result establishes an approximation-preserving equivalence of PB to the well studied Uniform Decision Tree (UDT) problem from stochastic optimization and a variant of the Min-Sum Set Cover ($\text{MSSC}_f$) problem. For distributions of support $m$, UDT admits a $\log m$ approximation, and while a constant factor approximation in polynomial time is a long-standing open problem, constant factor approximations are achievable in subexponential time (arXiv:1906.11385). Our main result implies that the same properties hold for PB and $\text{MSSC}_f$.  We also study the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#20013;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#25193;&#23637;&#30340;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.04486</link><description>&lt;p&gt;
&#22312;&#27969;&#22270;&#20013;&#22522;&#20110;&#33609;&#22270;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sketch-Based Anomaly Detection in Streaming Graphs. (arXiv:2106.04486v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#21160;&#24577;&#22270;&#20013;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#25193;&#23637;&#30340;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#22270;&#30340;&#22270;&#36793;&#27969;&#20013;&#65292;&#22914;&#20309;&#20197;&#22312;&#32447;&#30340;&#26041;&#24335;&#20026;&#36793;&#21644;&#23376;&#22270;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#65292;&#20197;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#24120;&#25968;&#26102;&#38388;&#21644;&#20869;&#23384;&#19979;&#36827;&#34892;&#65311;&#26412;&#25991;&#39318;&#20808;&#23558;&#35745;&#25968;&#26368;&#23567;&#21270;&#33609;&#22270;&#25968;&#25454;&#32467;&#26500;&#25193;&#23637;&#20026;&#39640;&#38454;&#33609;&#22270;&#65292;&#35813;&#39640;&#38454;&#33609;&#22270;&#20855;&#26377;&#20445;&#30041;&#31264;&#23494;&#23376;&#22270;&#32467;&#26500;&#30340;&#26377;&#29992;&#23646;&#24615;&#65288;&#36755;&#20837;&#20013;&#30340;&#31264;&#23494;&#23376;&#22270;&#36716;&#21270;&#20026;&#25968;&#25454;&#32467;&#26500;&#20013;&#30340;&#23494;&#38598;&#23376;&#30697;&#38453;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;4&#20010;&#21033;&#29992;&#36825;&#20010;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#65288;a&#65289;&#21516;&#26102;&#26816;&#27979;&#36793;&#21644;&#22270;&#30340;&#24322;&#24120;&#65307;&#65288;b&#65289;&#20197;&#24120;&#25968;&#20869;&#23384;&#21644;&#24120;&#25968;&#26356;&#26032;&#26102;&#38388;&#22788;&#29702;&#27599;&#20010;&#26032;&#21040;&#36798;&#30340;&#36793;&#30340;&#36793;&#21644;&#22270;&#65307;&#65288;c&#65289;&#22312;4&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23558;&#31264;&#23494;&#23376;&#22270;&#25628;&#32034;&#32435;&#20837;&#27969;&#24335;&#26041;&#27861;&#20197;&#26816;&#27979;&#22270;&#24418;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose 4 online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on 4 real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect gra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2106.02797</link><description>&lt;p&gt;
&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;(DSC)&#26159;&#22312;&#27809;&#26377;&#30456;&#20114;&#20851;&#32852;&#30340;&#36793;&#38469;&#20449;&#24687;&#21487;&#20379;&#35299;&#30721;&#22120;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#23545;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#30340;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Slepian&#21644;Wolf&#22312;1973&#24180;&#35777;&#26126;&#65292;&#27809;&#26377;&#35775;&#38382;&#36793;&#38469;&#20449;&#24687;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#19982;&#36793;&#38469;&#20449;&#24687;&#21487;&#29992;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#21387;&#32553;&#29575;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#20294;&#23454;&#36341;&#20013;&#30340;DSC&#19968;&#30452;&#23616;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29305;&#23450;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#30456;&#20851;&#32467;&#26500;&#19981;&#21487;&#30693;&#19988;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#30340;&#26377;&#25439;DSC&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#28304;&#27169;&#22411;&#65292;&#32780;&#26159;&#21033;&#29992;&#26465;&#20214;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIV-GAN&#65292;&#19968;&#31181;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#20445;&#30041;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#36924;&#30495;&#12290;</title><link>http://arxiv.org/abs/2103.13389</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#39062;&#30340;&#22330;&#26223;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.13389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIV-GAN&#65292;&#19968;&#31181;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#20445;&#30041;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21487;&#20197;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#35757;&#32451;GAN&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#24448;&#24448;&#20250;&#21457;&#29983;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#35760;&#24518;&#25110;&#35757;&#32451;&#21457;&#25955;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SIV-GAN&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#35757;&#32451;&#22270;&#20687;&#25110;&#21333;&#20010;&#35270;&#39057;&#21098;&#36753;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#20004;&#25903;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#20998;&#21035;&#35774;&#35745;&#29992;&#20110;&#21028;&#26029;&#20869;&#37096;&#20869;&#23481;&#21644;&#22330;&#26223;&#24067;&#23616;&#30340;&#30495;&#23454;&#24615;&#12290;&#36825;&#31181;&#37492;&#21035;&#22120;&#35774;&#35745;&#21487;&#20197;&#21512;&#25104;&#35270;&#35273;&#19978;&#36924;&#30495;&#12289;&#26032;&#39062;&#30340;&#22330;&#26223;&#32452;&#21512;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20869;&#23481;&#21644;&#24067;&#23616;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#26679;&#26412;&#30340;&#19978;&#19979;&#25991;&#12290;&#19982;&#20197;&#21069;&#30340;&#21333;&#22270;&#20687;GAN&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#12289;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#19981;&#38480;&#20110;&#21333;&#20010;&#22270;&#20687;&#35774;&#32622;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Given a large dataset for training, generative adversarial networks (GANs) can achieve remarkable performance for the image synthesis task. However, training GANs in extremely low data regimes remains a challenge, as overfitting often occurs, leading to memorization or training divergence. In this work, we introduce SIV-GAN, an unconditional generative model that can generate new scene compositions from a single training image or a single video clip. We propose a two-branch discriminator architecture, with content and layout branches designed to judge internal content and scene layout realism separately from each other. This discriminator design enables synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single image GANs, our model generates more diverse, higher quality images, while not being restricted to a single image setting. We further introduce a new challengin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29468;&#24819;&#30340;&#25968;&#25454;&#27169;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#25968;&#20540;&#29305;&#24449;&#21644;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#24314;&#31435;&#20102;&#38750;&#32447;&#24615;&#21644;&#24067;&#23572;&#20851;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;COVID-19&#24739;&#32773;&#32423;&#21035;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#21487;&#33021;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2011.11576</link><description>&lt;p&gt;
&#22522;&#20110;&#29468;&#24819;&#30340;&#25968;&#25454;&#27169;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Conjecturing-Based Discovery of Patterns in Data. (arXiv:2011.11576v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.11576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29468;&#24819;&#30340;&#25968;&#25454;&#27169;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#25968;&#20540;&#29305;&#24449;&#21644;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#24314;&#31435;&#20102;&#38750;&#32447;&#24615;&#21644;&#24067;&#23572;&#20851;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;COVID-19&#24739;&#32773;&#32423;&#21035;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#21487;&#33021;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29468;&#24819;&#26426;&#22120;&#65292;&#23427;&#20197;&#38750;&#32447;&#24615;&#39033;&#30340;&#36793;&#30028;&#20197;&#21450;&#20998;&#31867;&#29305;&#24449;&#30340;&#24067;&#23572;&#34920;&#36798;&#24335;&#30340;&#24418;&#24335;&#24314;&#35758;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#29468;&#24819;&#26694;&#26550;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#24050;&#30693;&#30340;&#38750;&#32447;&#24615;&#21644;&#24067;&#23572;&#20851;&#31995;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#30495;&#23454;&#30340;&#22522;&#30784;&#20851;&#31995;&#34987;&#25581;&#31034;&#20986;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#31526;&#21495;&#22238;&#24402;&#26694;&#26550;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#22312;&#25968;&#25454;&#38598;&#20013;&#28385;&#36275;&#30340;&#26041;&#31243;&#24674;&#22797;&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;COVID-19&#32467;&#26524;&#30340;&#24739;&#32773;&#32423;&#21035;&#25968;&#25454;&#65292;&#20197;&#25552;&#20379;&#21487;&#33021;&#19982;&#21307;&#23398;&#25991;&#29486;&#20013;&#30830;&#35748;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the use of a conjecturing machine that suggests feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. The proposed Conjecturing framework recovers known nonlinear and boolean relationships among features from data. In both settings, true underlying relationships are revealed. We then compare the method to a previously-proposed framework for symbolic regression on the ability to recover equations that are satisfied among features in a dataset. The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in the medical literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#19982;&#26631;&#35760;&#25968;&#25454;&#22312;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#22312;&#23398;&#20064;&#36895;&#24230;&#19978;&#21516;&#26679;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2005.11018</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65306;&#24403;&#26410;&#26631;&#35760;&#25968;&#25454;&#21516;&#26679;&#26377;&#29992;&#26102;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful. (arXiv:2005.11018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.11018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#19982;&#26631;&#35760;&#25968;&#25454;&#22312;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#22312;&#23398;&#20064;&#36895;&#24230;&#19978;&#21516;&#26679;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#36739;&#20415;&#23452;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#25968;&#25454;&#20998;&#24067;&#21487;&#20197;&#30001;&#36830;&#32493;&#21442;&#25968;&#26469;&#25551;&#36848;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#22312;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#19982;&#26631;&#35760;&#25968;&#25454;&#21516;&#26679;&#26377;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;$n&#65292;m$&#20998;&#21035;&#20026;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;$m\sim n$&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#25353;$O(1/n)$&#32553;&#25918;&#65307;&#22914;&#26524;$m\sim n^{1+\gamma}$&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#25353;$O(1/n^{1+\gamma})$&#32553;&#25918;&#65292;&#20854;&#20013;$\gamma&gt;0$&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#25353;$O(1/n)$&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning algorithms attempt to take advantage of relatively inexpensive unlabeled data to improve learning performance. In this work, we consider statistical models where the data distributions can be characterized by continuous parameters. We show that under certain conditions on the distribution, unlabeled data is equally useful as labeled date in terms of learning rate. Specifically, let $n, m$ be the number of labeled and unlabeled data, respectively. It is shown that the learning rate of semi-supervised learning scales as $O(1/n)$ if $m\sim n$, and scales as $O(1/n^{1+\gamma})$ if $m\sim n^{1+\gamma}$ for some $\gamma&gt;0$, whereas the learning rate of supervised learning scales as $O(1/n)$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#27425;&#37319;&#26679;&#24352;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#24352;&#37327;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21305;&#37197;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19979;&#25214;&#21040;&#20809;&#28369;&#19988;&#28508;&#22312;&#38750;&#20984;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/1911.10367</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#27425;&#37319;&#26679;&#24352;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sub-sampled Tensor Method for Non-convex Optimization. (arXiv:1911.10367v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.10367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#27425;&#37319;&#26679;&#24352;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#24352;&#37327;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21305;&#37197;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19979;&#25214;&#21040;&#20809;&#28369;&#19988;&#28508;&#22312;&#38750;&#20984;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22235;&#38454;&#27491;&#21017;&#21270;&#27169;&#22411;&#26469;&#23547;&#25214;&#20809;&#28369;&#19988;&#28508;&#22312;&#38750;&#20984;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#31934;&#30830;&#30340;&#23548;&#25968;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#27425;&#37319;&#26679;&#23548;&#25968;&#32780;&#19981;&#26159;&#31934;&#30830;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#24352;&#37327;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33267;&#22810; $\bigO\left(\max\left(\epsilon_1^{-4/3}, \epsilon_2^{-2}, \epsilon_3^{-4}\right)\right)$ &#27425;&#36845;&#20195;&#20013;&#25214;&#21040;&#19968;&#20010; $(\epsilon_1,\epsilon_2,\epsilon_3)$ &#19977;&#38454;&#20020;&#30028;&#28857;&#65292;&#20174;&#32780;&#19982;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a stochastic optimization method that uses a fourth-order regularized model to find local minima of smooth and potentially non-convex objective functions with a finite-sum structure. This algorithm uses sub-sampled derivatives instead of exact quantities. The proposed approach is shown to find an $(\epsilon_1,\epsilon_2,\epsilon_3)$-third-order critical point in at most $\bigO\left(\max\left(\epsilon_1^{-4/3}, \epsilon_2^{-2}, \epsilon_3^{-4}\right)\right)$ iterations, thereby matching the rate of deterministic approaches. In order to prove this result, we derive a novel tensor concentration inequality for sums of tensors of any order that makes explicit use of the finite-sum structure of the objective function.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33041;&#37096;MRI&#25195;&#25551;&#23558;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Resnet-50&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26368;&#20302;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;</title><link>http://arxiv.org/abs/1911.02265</link><description>&lt;p&gt;
&#39044;&#27979;&#33041;&#32959;&#30244;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predictive modeling of brain tumor: A Deep learning approach. (arXiv:1911.02265v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.02265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33041;&#37096;MRI&#25195;&#25551;&#23558;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Resnet-50&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26368;&#20302;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21487;&#20197;&#21487;&#35270;&#21270;&#20154;&#20307;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#37096;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#20013;&#26816;&#27979;&#30284;&#32452;&#32455;&#29983;&#38271;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#33041;MRI&#25195;&#25551;&#20998;&#31867;&#20026;&#20004;&#31867;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Resnet-50&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26368;&#39640;&#65292;&#20551;&#38452;&#24615;&#29575;&#26368;&#20302;&#65292;&#20998;&#21035;&#20026;95&#65285;&#21644;&#38646;&#12290;&#20854;&#27425;&#26159;VGG-16&#27169;&#22411;&#21644;Inception-V3&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;90&#65285;&#21644;55&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image processing concepts can visualize the different anatomy structure of the human body. Recent advancements in the field of deep learning have made it possible to detect the growth of cancerous tissue just by a patient's brain Magnetic Resonance Imaging (MRI) scans. These methods require very high accuracy and meager false negative rates to be of any practical use. This paper presents a Convolutional Neural Network (CNN) based transfer learning approach to classify the brain MRI scans into two classes using three pre-trained models. The performances of these models are compared with each other. Experimental results show that the Resnet-50 model achieves the highest accuracy and least false negative rates as 95% and zero respectively. It is followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55% respectively.
&lt;/p&gt;</description></item><item><title>MDP Playground&#26159;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#32500;&#24230;&#30340;&#38590;&#24230;&#25511;&#21046;&#26041;&#24335;&#65292;&#25361;&#25112;&#20195;&#29702;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#23545;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/1909.07750</link><description>&lt;p&gt;
MDP Playground: &#19968;&#31181;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;&#21644;&#35843;&#35797;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.07750
&lt;/p&gt;
&lt;p&gt;
MDP Playground&#26159;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#32500;&#24230;&#30340;&#38590;&#24230;&#25511;&#21046;&#26041;&#24335;&#65292;&#25361;&#25112;&#20195;&#29702;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#21442;&#25968;&#21270;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#23545;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MDP Playground&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#26681;&#25454;&#38590;&#24230;&#30340;&#19981;&#21516;&#32500;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#20197;&#25361;&#25112;&#20195;&#29702;&#24182;&#22312;&#29609;&#20855;&#21644;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#33719;&#24471;&#19981;&#21516;&#31243;&#24230;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#24182;&#20801;&#35768;&#23545;&#21508;&#31181;&#32500;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#24310;&#36831;&#22870;&#21169;&#12289;&#24207;&#21015;&#38271;&#24230;&#12289;&#22870;&#21169;&#23494;&#24230;&#12289;&#38543;&#26426;&#24615;&#12289;&#22270;&#20687;&#34920;&#31034;&#12289;&#26080;&#20851;&#29305;&#24449;&#12289;&#26102;&#38388;&#21333;&#20301;&#12289;&#21160;&#20316;&#33539;&#22260;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;OpenAI Gym&#20013;&#21464;&#21270;&#36825;&#20123;&#32500;&#24230;&#26469;&#23450;&#20041;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24555;&#36895;&#36816;&#34892;&#30340;&#29609;&#20855;&#29615;&#22659;&#38598;&#21512;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#20195;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;MDP Playground&#35774;&#35745;&#23454;&#39564;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#29609;&#20855;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#20197;&#23558;&#35768;&#22810;&#36825;&#20123;&#32500;&#24230;&#27880;&#20837;&#21040;&#20219;&#20309;Gym&#29615;&#22659;&#20013;&#30340;&#21253;&#35013;&#22120;&#12290;&#25105;&#20204;&#22312;Atari&#21644;Mujoco&#19978;&#20351;&#29992;&#36825;&#20123;&#21253;&#35013;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#32500;&#24230;&#23545;&#27604;&#29609;&#20855;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MDP Playground, a testbed for Reinforcement Learning (RL) agents with dimensions of hardness that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in toy and complex RL environments. We consider and allow control over a wide variety of dimensions, including delayed rewards, sequence lengths, reward density, stochasticity, image representations, irrelevant features, time unit, action range and more. We define a parameterised collection of fast-to-run toy environments in OpenAI Gym by varying these dimensions and propose to use these to understand agents better. We then show how to design experiments using MDP Playground to gain insights on the toy environments. We also provide wrappers that can inject many of these dimensions into any Gym environment. We experiment with these wrappers on Atari and Mujoco to allow for understanding the effects of these dimensions on environments that are more complex than the toy envi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;MCMC&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#26041;&#27861;&#65292;&#21033;&#29992;&#31890;&#23376;&#32531;&#20914;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#37327;&#35299;&#20915;&#20102;&#38271;&#26102;&#38388;&#24207;&#21015;&#19979;&#35745;&#31639;&#21644;&#31890;&#23376;&#36864;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/1901.10568</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;MCMC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient MCMC for Nonlinear State Space Models. (arXiv:1901.10568v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.10568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;MCMC&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#26041;&#27861;&#65292;&#21033;&#29992;&#31890;&#23376;&#32531;&#20914;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#37327;&#35299;&#20915;&#20102;&#38271;&#26102;&#38388;&#24207;&#21015;&#19979;&#35745;&#31639;&#21644;&#31890;&#23376;&#36864;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#36890;&#36807;&#28508;&#22312;&#30340;&#38543;&#26426;&#36807;&#31243;&#25552;&#20379;&#20102;&#24314;&#27169;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#28789;&#27963;&#26694;&#26550;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#12289;&#38750;&#39640;&#26031;&#30340;SSM&#25512;&#26029;&#36890;&#24120;&#20351;&#29992;&#31890;&#23376;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#24207;&#21015;&#26102;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25361;&#25112;&#26377;&#20004;&#26041;&#38754;&#65306;&#35745;&#31639;&#19982;&#26102;&#38388;&#32447;&#24615;&#25193;&#23637;&#65292;&#31890;&#23376;&#28388;&#27874;&#22120;&#22312;&#38271;&#24207;&#21015;&#20013;&#36824;&#20250;&#20986;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20351;&#29992;&#32531;&#20914;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#37327;&#26469;&#24212;&#23545;&#26102;&#24207;&#20381;&#36182;&#24615;&#30340;&#26377;&#38480;&#29366;&#24577;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#32447;&#24615;SSM&#30340;&#38543;&#26426;&#26799;&#24230;MCMC&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#38750;&#32447;&#24615;SSM&#65292;&#24182;&#25552;&#20986;&#20102;&#35823;&#24046;&#30028;&#38480;&#65292;&#32771;&#34385;&#20102;&#32531;&#20914;&#35823;&#24046;&#21644;&#31890;&#23376;&#35823;&#24046;&#65292;&#36866;&#29992;&#20110;&#22312;&#28508;&#22312;&#36807;&#31243;&#20013;&#20855;&#26377;&#23545;&#25968;&#20985;&#24615;&#30340;&#38750;&#32447;&#24615;SSM&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;MCMC&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31890;&#23376;&#32531;&#20914;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) provide a flexible framework for modeling complex time series via a latent stochastic process. Inference for nonlinear, non-Gaussian SSMs is often tackled with particle methods that do not scale well to long time series. The challenge is two-fold: not only do computations scale linearly with time, as in the linear case, but particle filters additionally suffer from increasing particle degeneracy with longer series. Stochastic gradient MCMC methods have been developed to scale Bayesian inference for finite-state hidden Markov models and linear SSMs using buffered stochastic gradient estimates to account for temporal dependencies. We extend these stochastic gradient estimators to nonlinear SSMs using particle methods. We present error bounds that account for both buffering error and particle error in the case of nonlinear SSMs that are log-concave in the latent process. We evaluate our proposed particle buffered stochastic gradient using stochastic gradient MCMC
&lt;/p&gt;</description></item></channel></rss>