<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04214</link><description>&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#65306;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04214
&lt;/p&gt;
&lt;p&gt;
PiML&#24037;&#20855;&#31665;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#35786;&#26029;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36824;&#25903;&#25345;&#19982;MLOps&#24179;&#21488;&#30340;&#38598;&#25104;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PiML&#26159;&#19968;&#20010;&#32508;&#21512;&#19988;&#24320;&#25918;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#27169;&#22411;&#35786;&#26029;&#12290;&#23427;&#35774;&#35745;&#20102;&#20302;&#20195;&#30721;&#21644;&#39640;&#20195;&#30721;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#65292;&#21253;&#25324;&#25968;&#25454;&#31649;&#36947;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27169;&#22411;&#35299;&#37322;&#21644;&#35828;&#26126;&#20197;&#21450;&#27169;&#22411;&#35786;&#26029;&#21644;&#27604;&#36739;&#12290;&#35813;&#24037;&#20855;&#31665;&#25903;&#25345;&#26085;&#30410;&#22686;&#38271;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;&#20363;&#22914;GAM&#12289;GAMI-Net&#12289;XGB2&#65289;&#65292;&#20855;&#26377;&#26412;&#22320;&#21644;/&#25110;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;&#23427;&#36824;&#25903;&#25345;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;&#20363;&#22914;PFI&#12289;PDP&#12289;LIME&#12289;SHAP&#65289;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#26080;&#20851;&#35786;&#26029;&#22871;&#20214;&#65288;&#20363;&#22914;&#24369;&#28857;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#65289;&#12290;&#36890;&#36807;&#28789;&#27963;&#30340;&#39640;&#20195;&#30721; API&#65292;&#23558; PiML &#27169;&#22411;&#21644;&#27979;&#35797;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340; MLOps &#24179;&#21488;&#20197;&#23454;&#29616;&#36136;&#37327;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;PiML &#24037;&#20855;&#31665;&#36824;&#24102;&#26377;&#32508;&#21512;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#23454;&#36341;&#20363;&#23376;&#65292;&#21253;&#25324;&#38134;&#34892;&#19994;&#20013;&#30340;&#27169;&#22411;&#24320;&#21457;&#21644;&#39564;&#35777;&#24212;&#29992;&#12290;&#35813;&#39033;&#30446;&#21487;&#36890;&#36807;arXiv:2305.04214v1[cs.LG]&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
PiML (read $\pi$-ML, /`pai.`em.`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04203</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;&#21512;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65306;&#35299;&#38145;&#24320;&#25918;&#38598;&#21512;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22024;&#26434;&#30340;&#25968;&#25454;&#19968;&#30452;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#22312;&#23553;&#38381;&#38598;&#30340;&#26631;&#31614;&#22122;&#22768;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#21516;&#26102;&#23384;&#22312;&#24320;&#25918;&#38598;&#21512;&#21644;&#23553;&#38381;&#38598;&#21512;&#30340;&#22122;&#22768;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20026;&#27599;&#31181;&#31867;&#22411;&#35774;&#35745;&#29305;&#23450;&#30340;&#31574;&#30053;&#26469;&#21306;&#20998;&#21644;&#22788;&#29702;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#38598;&#24050;&#32463;&#20005;&#37325;&#25439;&#22351;&#26102;&#12290;&#26412;&#25991;&#23545;&#27169;&#22411;&#38754;&#23545;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#26102;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#24182;&#21457;&#29616;&#37096;&#20998;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#36880;&#28176;&#34701;&#20837;&#26576;&#20123;&#24050;&#30693;&#31867;&#21035;&#65292;&#36825;&#26377;&#21033;&#20110;&#24050;&#30693;&#31867;&#21035;&#30340;&#20998;&#31163;&#12290;&#22312;&#36825;&#31181;&#29616;&#35937;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;CECL&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#30340;&#26377;&#29992;&#20449;&#24687;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#23558;&#19968;&#20123;&#24320;&#25918;&#38598;&#21512;&#31034;&#20363;&#20316;&#20026;&#36127;&#20363;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#27169;&#22411;&#23545;&#24320;&#25918;&#38598;&#21512;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22788;&#29702;&#24320;&#25918;&#38598;&#21512;&#21644;&#23553;&#38381;&#38598;&#21512;&#26631;&#31614;&#22122;&#22768;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20256;&#36755;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#23398;&#20064;&#33539;&#20363;&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#24322;&#26500;&#21644;&#35201;&#25512;&#23548;&#25968;&#25454;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#37319;&#29992;&#31283;&#23450;&#30340;&#25945;&#24072;&#12289;&#20462;&#27491;&#30340;&#33976;&#39311;&#21644;&#32858;&#31867;&#26631;&#31614;&#31934;&#28860;&#31561;&#25216;&#26415;&#26469;&#20419;&#36827;&#27169;&#22411;&#31934;&#28860;&#36807;&#31243;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04201</link><description>&lt;p&gt;
MrTF&#65306;&#38754;&#21521;&#20256;&#36755;&#32852;&#37030;&#23398;&#20064;&#30340;&#27169;&#22411;&#31934;&#28860;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MrTF: Model Refinery for Transductive Federated Learning. (arXiv:2305.04201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20256;&#36755;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#23398;&#20064;&#33539;&#20363;&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#24322;&#26500;&#21644;&#35201;&#25512;&#23548;&#25968;&#25454;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#37319;&#29992;&#31283;&#23450;&#30340;&#25945;&#24072;&#12289;&#20462;&#27491;&#30340;&#33976;&#39311;&#21644;&#32858;&#31867;&#26631;&#31614;&#31934;&#28860;&#31561;&#25216;&#26415;&#26469;&#20419;&#36827;&#27169;&#22411;&#31934;&#28860;&#36807;&#31243;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#21363;&#22312;&#38544;&#31169;&#20445;&#25252;&#25919;&#31574;&#19979;&#65292;&#19968;&#20010;&#26032;&#25104;&#31435;&#30340;&#35797;&#28857;&#39033;&#30446;&#38656;&#35201;&#20511;&#21161;&#20854;&#20182;&#26041;&#30340;&#24110;&#21161;&#23545;&#26032;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539;&#20363;&#33268;&#21147;&#20110;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#35201;&#25512;&#23548;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#31216;&#20026;&#20256;&#36755;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;&#35201;&#25512;&#23548;&#25968;&#25454;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#19968;&#26041;&#38754;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#20351;&#29992;&#39044;&#20808;&#21487;&#29992;&#30340;&#27979;&#35797;&#26679;&#26412;&#26469;&#20248;&#21270;&#21512;&#24182;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;FL&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31934;&#28860;&#36807;&#31243;&#23558;&#27979;&#35797;&#26679;&#26412;&#32435;&#20837;&#35757;&#32451;&#20013;&#65292;&#24182;&#21487;&#20197;&#20197;&#20256;&#36755;&#26041;&#24335;&#29983;&#25104;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#25945;&#24072;&#12289;&#20462;&#27491;&#30340;&#33976;&#39311;&#21644;&#32858;&#31867;&#26631;&#31614;&#31934;&#28860;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#31934;&#28860;&#36807;&#31243;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a real-world scenario in which a newly-established pilot project needs to make inferences for newly-collected data with the help of other parties under privacy protection policies. Current federated learning (FL) paradigms are devoted to solving the data heterogeneity problem without considering the to-be-inferred data. We propose a novel learning paradigm named transductive federated learning (TFL) to simultaneously consider the structural information of the to-be-inferred data. On the one hand, the server could use the pre-available test samples to refine the aggregated models for robust model fusion, which tackles the data heterogeneity problem in FL. On the other hand, the refinery process incorporates test samples into training and could generate better predictions in a transductive manner. We propose several techniques including stabilized teachers, rectified distillation, and clustered label refinery to facilitate the model refinery process. Abundant experimental stu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.04152</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#32447;&#36890;&#20449;&#30340;&#36890;&#36947;&#39537;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#36125;&#21494;&#26031;&#32852;&#37030;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#30340;&#36817;&#26399;&#21457;&#23637;&#24050;&#32463;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#37319;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#39057;&#29575;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#20852;&#36259;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#32852;&#37030;&#24179;&#22343; Langevin &#21160;&#21147;&#23398;(FALD)&#20316;&#20026;&#32852;&#37030;&#24179;&#22343;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#36890;&#20449;&#23384;&#22312;&#19979;&#26377;&#25928;&#22320;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#32447; FALD(WFALD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#21644;&#22522;&#20110;&#36890;&#36947;&#39537;&#21160;&#30340; Monte Carlo &#26356;&#26032;&#26469;&#23454;&#29616;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340; FALD&#12290;&#19982;&#20808;&#21069;&#30340;&#26080;&#32447;&#36125;&#21494;&#26031;&#23398;&#20064;&#30456;&#27604;&#65292;WFALD &#21487;&#20197;&#23454;&#29616;(i) &#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65307;&#24182;&#19988;(ii) &#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#20197; 2-Wasserstein &#36317;&#31163;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32473;&#20986;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#65292;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#65292;&#24182;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04148</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#20856;shadow&#39640;&#25928;&#24674;&#22797;Pauli&#22122;&#22768;&#20013;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Efficient information recovery from Pauli noise via classical shadow. (arXiv:2305.04148v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#65292;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#65292;&#24182;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#36805;&#29467;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#20174;&#37327;&#23376;&#31995;&#32479;&#20013;&#25552;&#21462;&#32463;&#20856;&#20449;&#24687;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24191;&#27867;&#38656;&#27714;&#65292;&#23588;&#20854;&#26159;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#21270;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#31995;&#32479;&#26412;&#36136;&#19978;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25439;&#22351;&#37327;&#23376;&#31995;&#32479;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#26469;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#12290;&#23545;&#20110;&#23616;&#37096;&#21644;&#26377;&#30028;&#24230;&#25968;&#30340;&#21487;&#35266;&#23519;&#37327;&#65292;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#36825;&#19982;&#24120;&#35268;&#26041;&#27861;&#65288;&#22914;&#27010;&#29575;&#35823;&#24046;&#28040;&#38500;&#65289;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#38656;&#35201;&#36890;&#36947;&#30340;&#23436;&#25972;&#20449;&#24687;&#24182;&#23637;&#31034;&#25351;&#25968;&#32423;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of quantum computing has led to an extensive demand for effective techniques to extract classical information from quantum systems, particularly in fields like quantum machine learning and quantum chemistry. However, quantum systems are inherently susceptible to noises, which adversely corrupt the information encoded in quantum systems. In this work, we introduce an efficient algorithm that can recover information from quantum states under Pauli noise. The core idea is to learn the necessary information of the unknown Pauli channel by post-processing the classical shadows of the channel. For a local and bounded-degree observable, only partial knowledge of the channel is required rather than its complete classical description to recover the ideal information, resulting in a polynomial-time algorithm. This contrasts with conventional methods such as probabilistic error cancellation, which requires the full information of the channel and exhibits exponential scaling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#29702;&#35770;&#65292;&#21487;&#20197;&#30028;&#23450;&#23454;&#20363;&#32534;&#30721;&#38544;&#31169;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#30028;&#23450;&#32534;&#30721;&#21487;&#36870;&#24615;&#65292;&#23454;&#29616;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.04146</link><description>&lt;p&gt;
&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30028;&#23450;&#38544;&#31169;&#20445;&#25252;&#23454;&#20363;&#32534;&#30721;&#30340;&#21487;&#36870;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information. (arXiv:2305.04146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#29702;&#35770;&#65292;&#21487;&#20197;&#30028;&#23450;&#23454;&#20363;&#32534;&#30721;&#38544;&#31169;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#30028;&#23450;&#32534;&#30721;&#21487;&#36870;&#24615;&#65292;&#23454;&#29616;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#23454;&#20363;&#32534;&#30721;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#20026;&#29305;&#24449;&#21521;&#37327;&#65292;&#32780;&#19981;&#27844;&#38706;&#20854;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#12290;&#24403;&#35774;&#35745;&#24471;&#24403;&#26102;&#65292;&#36825;&#20123;&#32534;&#30721;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20363;&#22914;&#26377;&#38480;&#38544;&#31169;&#39118;&#38505;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23454;&#20363;&#32534;&#30721;&#26041;&#26696;&#37117;&#26159;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#19988;&#21482;&#36890;&#36807;&#24212;&#23545;&#26377;&#38480;&#25968;&#37327;&#30340;&#25915;&#20987;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20854;&#38544;&#31169;&#20445;&#25252;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#29702;&#35770;&#21407;&#29702;&#24230;&#37327;&#23454;&#20363;&#32534;&#30721;&#38544;&#31169;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38544;&#31169;&#27979;&#37327;&#26159;&#30452;&#35266;&#30340;&#12289;&#26131;&#20110;&#24212;&#29992;&#30340;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#30028;&#23450;&#32534;&#30721;&#30340;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving instance encoding aims to encode raw data as feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing instance encoding schemes are based on heuristics and their privacy-preserving properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the privacy of instance encoding based on Fisher information. We show that our privacy measure is intuitive, easily applicable, and can be used to bound the invertibility of encodings both theoretically and empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#33041;&#32593;&#32476;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#36816;&#34892;&#26102;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#33041;&#21306;&#22495;&#21151;&#33021;&#32452;&#32455;&#30340;&#21512;&#29702;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.04142</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33041;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#23618;&#27425;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Hierarchical Clustering for Brain Network Analysis. (arXiv:2305.04142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#23618;&#27425;&#32858;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#33041;&#32593;&#32476;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#36816;&#34892;&#26102;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#33041;&#21306;&#22495;&#21151;&#33021;&#32452;&#32455;&#30340;&#21512;&#29702;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#65292;&#21363;MRI&#26500;&#24314;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#22312;&#30149;&#29702;&#39044;&#27979;&#21644;&#33041;&#21151;&#33021;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#22797;&#26434;&#30340;&#33041;&#31995;&#32479;&#20869;&#65292;&#31070;&#32463;&#20803;&#38388;&#36830;&#25509;&#24378;&#24230;&#30340;&#24046;&#24322;&#23558;&#33041;&#21010;&#20998;&#20026;&#21508;&#31181;&#21151;&#33021;&#27169;&#22359;&#65288;&#32593;&#32476;&#31038;&#21306;&#65289;&#65292;&#36825;&#23545;&#20110;&#33041;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#33041;&#20013;&#30340;&#36825;&#20123;&#31038;&#21306;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23618;&#27425;&#32858;&#31867;&#35782;&#21035;&#21644;&#33041;&#32593;&#32476;&#20998;&#31867;&#12290;&#22312;&#30495;&#23454;&#30340;&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#30340;&#24110;&#21161;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#20851;&#20110;&#33041;&#21306;&#22495;&#21151;&#33021;&#32452;&#32455;&#21512;&#29702;&#35265;&#35299;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#36816;&#34892;&#26102;&#22797;&#26434;&#24230;&#12290;&#23454;&#29616;&#21487;&#22312;https://github.com/DDVD233/THC&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain networks, graphical models such as those constructed from MRI, have been widely used in pathological prediction and analysis of brain functions. Within the complex brain system, differences in neuronal connection strengths parcellate the brain into various functional modules (network communities), which are critical for brain analysis. However, identifying such communities within the brain has been a nontrivial issue due to the complexity of neuronal interactions. In this work, we propose a novel interpretable transformer-based model for joint hierarchical cluster identification and brain network classification. Extensive experimental results on real-world brain network datasets show that with the help of hierarchical clustering, the model achieves increased accuracy and reduced runtime complexity while providing plausible insight into the functional organization of brain regions. The implementation is available at https://github.com/DDVD233/THC.
&lt;/p&gt;</description></item><item><title>AMC&#26159;&#19968;&#31181;&#22522;&#20110;&#20445;&#30041;&#20043;&#21069;&#21644;&#24403;&#21069;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#29983;&#25104;&#20803;&#36755;&#20986;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#21516;&#26102;&#21363;&#20351;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20173;&#28982;&#20445;&#25345;&#25110;&#25552;&#39640;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04135</link><description>&lt;p&gt;
&#32500;&#25252;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20197;&#39044;&#27979;&#27969;&#22833;&#29575;&#30340;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Maintaining Stability and Plasticity for Predictive Churn Reduction. (arXiv:2305.04135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04135
&lt;/p&gt;
&lt;p&gt;
AMC&#26159;&#19968;&#31181;&#22522;&#20110;&#20445;&#30041;&#20043;&#21069;&#21644;&#24403;&#21069;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#29983;&#25104;&#20803;&#36755;&#20986;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#21516;&#26102;&#21363;&#20351;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20173;&#28982;&#20445;&#25345;&#25110;&#25552;&#39640;&#24635;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#35813;&#26356;&#26032;&#20197;&#21033;&#29992;&#26356;&#22810;&#30340;&#26679;&#26412;&#25552;&#39640;&#24615;&#33021;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#25552;&#39640;&#20102;&#35832;&#22914;&#20934;&#30830;&#24615;&#20043;&#31867;&#30340;&#32858;&#21512;&#25351;&#26631;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#22312;&#20808;&#21069;&#27169;&#22411;&#27491;&#30830;&#39044;&#27979;&#30340;&#26679;&#26412;&#20013;&#20986;&#29616;&#38169;&#35823;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#36880;&#20010;&#38477;&#20302;&#65292;&#36825;&#31216;&#20026;&#39044;&#27979;&#24615;&#27969;&#22833;&#12290;&#36825;&#31181;&#39044;&#27979;&#19978;&#30340;&#24046;&#38169;&#20250;&#21066;&#24369;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#20174;&#32780;&#38477;&#20302;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25972;&#20307;&#25928;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;&#31215;&#32047;&#27169;&#22411;&#32452;&#21512;(AMC)&#65292;&#22522;&#20110;&#20445;&#30041;&#20043;&#21069;&#21644;&#24403;&#21069;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#29983;&#25104;&#19968;&#20010;&#20803;&#36755;&#20986;&#12290;AMC&#26159;&#19968;&#31181;&#36890;&#29992;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#26377;&#23427;&#33258;&#24049;&#30340;&#20248;&#28857;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#23646;&#24615;&#12290;AMC&#38656;&#35201;&#26368;&#23569;&#30340;&#39069;&#22806;&#35745;&#31639;&#21644;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#21333;&#20010;&#27169;&#22411;&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#30340;&#22256;&#38590;&#26469;&#35828;&#26126;AMC&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AMC&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#24635;&#20307;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#29978;&#33267;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed machine learning models should be updated to take advantage of a larger sample size to improve performance, as more data is gathered over time. Unfortunately, even when model updates improve aggregate metrics such as accuracy, they can lead to errors on samples that were correctly predicted by the previous model causing per-sample regression in performance known as predictive churn. Such prediction flips erode user trust thereby reducing the effectiveness of the human-AI team as a whole. We propose a solution called Accumulated Model Combination (AMC) based keeping the previous and current model version, and generating a meta-output using the prediction of the two models. AMC is a general technique and we propose several instances of it, each having their own advantages depending on the model and data properties. AMC requires minimal additional computation and changes to training procedures. We motivate the need for AMC by showing the difficulty of making a single model consis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#24456;&#23569;&#30340;&#26679;&#26412;&#19988;&#33021;&#22815;&#23545;&#26435;&#37325;&#21644;&#22343;&#20540;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.04127</link><description>&lt;p&gt;
&#20351;&#29992;&#25130;&#26029;&#25968;&#25454;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Mixtures of Gaussians with Censored Data. (arXiv:2305.04127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#38656;&#35201;&#24456;&#23569;&#30340;&#26679;&#26412;&#19988;&#33021;&#22815;&#23545;&#26435;&#37325;&#21644;&#22343;&#20540;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#25130;&#26029;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#21363;&#20174;&#19968;&#20010;&#28151;&#21512;&#21333;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2)$&#20013;&#35266;&#27979;&#21040;&#30340;&#26679;&#26412;&#21482;&#26377;&#24403;&#20854;&#20301;&#20110;$S$&#38598;&#21512;&#20869;&#26102;&#25165;&#20250;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20165;&#38656;&#35201;$\frac{1}{\varepsilon^{O(k)}}$&#20010;&#26679;&#26412;&#21363;&#21487;&#22312;$\varepsilon$&#35823;&#24046;&#20869;&#20272;&#35745;&#26435;&#37325;$w_i$&#21644;&#22343;&#20540;$\mu_i$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning mixtures of Gaussians with censored data. Statistical learning with censored data is a classical problem, with numerous practical applications, however, finite-sample guarantees for even simple latent variable models such as Gaussian mixtures are missing. Formally, we are given censored data from a mixture of univariate Gaussians $$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2),$$ i.e. the sample is observed only if it lies inside a set $S$. The goal is to learn the weights $w_i$ and the means $\mu_i$. We propose an algorithm that takes only $\frac{1}{\varepsilon^{O(k)}}$ samples to estimate the weights $w_i$ and the means $\mu_i$ within $\varepsilon$ error.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.04120</link><description>&lt;p&gt;
&#19968;&#31181;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Latent Diffusion Model for Protein Structure Generation. (arXiv:2305.04120v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#22797;&#26434;&#30340;&#29983;&#29289;&#20998;&#23376;&#65292;&#33021;&#22312;&#29983;&#29289;&#20307;&#20869;&#25191;&#34892;&#22810;&#31181;&#20851;&#38190;&#21151;&#33021;&#12290;&#35774;&#35745;&#21644;&#29983;&#25104;&#26032;&#22411;&#34507;&#30333;&#36136;&#21487;&#20026;&#26410;&#26469;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#24212;&#29992;&#65288;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#20294;&#30001;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#24314;&#27169;&#31354;&#38388;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20943;&#23569;&#34507;&#30333;&#36136;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#28789;&#27963;&#22320;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#21464;&#34507;&#30333;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#34507;&#30333;&#36136;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#28508;&#22312;&#34507;&#30333;&#36136;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#29992;&#20110;&#27714;&#35299;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#32593;&#26684;&#21270;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#21518;&#22788;&#29702;&#36719;&#20214;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.04107</link><description>&lt;p&gt;
DMF-TONN: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30452;&#25509;&#26080;&#32593;&#26684;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks. (arXiv:2305.04107v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#65292;&#38598;&#25104;&#20102;&#31070;&#32463;&#32593;&#32476;&#24182;&#29992;&#20110;&#27714;&#35299;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20256;&#32479;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#21644;&#32593;&#26684;&#21270;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#21518;&#22788;&#29702;&#36719;&#20214;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26080;&#32593;&#26684;&#27861;&#36827;&#34892;&#25299;&#25169;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23494;&#24230;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#19982;&#20301;&#31227;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#30452;&#25509;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#19982;&#20256;&#32479;&#25299;&#25169;&#20248;&#21270;&#25216;&#26415;&#21487;&#27604;&#25311;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#21518;&#22788;&#29702;&#36719;&#20214;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#28857;&#65292;&#36824;&#26377;&#21487;&#33021;&#29992;&#20110;&#32593;&#26684;&#21644;&#26377;&#38480;&#20803;&#20998;&#26512;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21512;&#36866;&#30340;&#25299;&#25169;&#20248;&#21270;&#30446;&#26631;&#12290;DMF-TONN&#25509;&#25910;&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;&#22352;&#26631;&#20316;&#20026;&#36755;&#20837;&#65292;&#25214;&#21040;&#26368;&#20248;&#23494;&#24230;&#22330;&#20197;&#26368;&#23567;&#21270;&#36981;&#20174;&#24230;&#21644;&#20307;&#31215;&#20998;&#25968;&#32422;&#26463;&#36829;&#35268;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26080;&#32593;&#26684;&#30340;&#24615;&#36136;&#26159;&#30001;&#19968;&#20010;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#20301;&#31227;&#22330;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#24377;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#24182;&#26367;&#20195;&#20256;&#32479;&#29992;&#20110;&#35745;&#31639;&#36981;&#20174;&#24230;&#30340;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a direct mesh-free method for performing topology optimization by integrating a density field approximation neural network with a displacement field approximation neural network. We show that this direct integration approach can give comparable results to conventional topology optimization techniques, with an added advantage of enabling seamless integration with post-processing software, and a potential of topology optimization with objectives where meshing and Finite Element Analysis (FEA) may be expensive or not suitable. Our approach (DMF-TONN) takes in as inputs the boundary conditions and domain coordinates and finds the optimum density field for minimizing the loss function of compliance and volume fraction constraint violation. The mesh-free nature is enabled by a physics-informed displacement field approximation neural network to solve the linear elasticity partial differential equation and replace the FEA conventionally used for calculating the compliance. We show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04106</link><description>&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. (arXiv:2305.04106v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#23545;&#20195;&#30721;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20195;&#30721;&#20013;&#30340;&#26222;&#36941;&#25216;&#26415;&#65292;&#21033;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#33719;&#21462;&#20851;&#20110;&#20195;&#30721;&#30340;&#36890;&#29992;&#30693;&#35782;&#24182;&#19987;&#38376;&#20174;&#20107;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#20195;&#30721;&#24211;&#30340;&#21160;&#24577;&#24615;&#23545;PLMs&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#24378;&#35843;&#38656;&#35201;&#35843;&#25972;&#36866;&#24212;&#20195;&#30721;&#30340;PLMs&#65292;&#36866;&#24212;&#20998;&#24067;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#65292;&#36825;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#25152;&#24573;&#35270;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#21160;&#26426;&#26159;&#23558;PLM&#35270;&#20026;&#19968;&#20010;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#24212;&#23545;&#28436;&#21270;&#30340;&#36719;&#20214;&#25968;&#25454;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#20840;&#27969;&#31243;&#65292;&#21487;&#22312;FPGA&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#24179;&#34913;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04099</link><description>&lt;p&gt;
FPGAs &#19978;&#30340;&#31526;&#21495;&#22238;&#24402;&#29992;&#20110;&#24555;&#36895;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression on FPGAs for Fast Machine Learning Inference. (arXiv:2305.04099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#20840;&#27969;&#31243;&#65292;&#21487;&#22312;FPGA&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#24179;&#34913;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#33021;&#29289;&#29702;&#30028;&#27491;&#22312;&#30740;&#31350;&#22312;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#19978;&#37096;&#32626;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#25913;&#21892;&#29289;&#29702;&#28789;&#25935;&#24230;&#24182;&#28385;&#36275;&#25968;&#25454;&#22788;&#29702;&#26102;&#24310;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#26032;&#31471;&#21040;&#31471;&#27969;&#31243;&#12290;&#23427;&#22312;&#26041;&#31243;&#31354;&#38388;&#20013;&#25628;&#32034;&#36817;&#20284;&#34920;&#31034;&#25968;&#25454;&#38598;&#30340;&#20195;&#25968;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992; PySR&#65288;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#21457;&#29616;&#36825;&#20123;&#34920;&#36798;&#24335;&#30340;&#36719;&#20214;&#65289;&#24182;&#25193;&#23637;&#20102; hls4ml &#30340;&#21151;&#33021;&#65288;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;FPGAs&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#30340;&#36719;&#20214;&#21253;&#65289;&#65292;&#20197;&#25903;&#25345;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992; PySR &#29983;&#25104;&#30340;&#34920;&#36798;&#24335;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22266;&#23450;&#32593;&#32476;&#22823;&#23567;&#26469;&#20248;&#21270;&#39030;&#32423;&#25351;&#26631;&#65292;&#22240;&#20026;&#24040;&#22823;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#20250;&#38450;&#27490;&#24191;&#27867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;&#30456;&#21453;&#65292;SR &#36873;&#25321;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#19968;&#32452;&#27169;&#22411;&#65292;&#36825;&#20801;&#35768;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high-energy physics community is investigating the feasibility of deploying machine-learning-based solutions on Field-Programmable Gate Arrays (FPGAs) to improve physics sensitivity while meeting data processing latency limitations. In this contribution, we introduce a novel end-to-end procedure that utilizes a machine learning technique called symbolic regression (SR). It searches equation space to discover algebraic relations approximating a dataset. We use PySR (software for uncovering these expressions based on evolutionary algorithm) and extend the functionality of hls4ml (a package for machine learning inference in FPGAs) to support PySR-generated expressions for resource-constrained production environments. Deep learning models often optimise the top metric by pinning the network size because vast hyperparameter space prevents extensive neural architecture search. Conversely, SR selects a set of models on the Pareto front, which allows for optimising the performance-resource
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.04095</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#31169;&#26377;&#25968;&#25454;&#20445;&#25345;&#26412;&#22320;&#65292;&#20801;&#35768;&#23433;&#20840;&#35745;&#31639;&#21644;&#26412;&#22320;&#27169;&#22411;&#26799;&#24230;&#19982;&#31532;&#19977;&#26041;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21361;&#21450;&#38544;&#31169;&#24182;&#24674;&#22797;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#35270;&#35282;&#12290;&#36825;&#20123;&#29702;&#35770;&#24037;&#20316;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#21482;&#26377;&#38145;&#23450;&#30340;&#26799;&#24230;&#34987;&#20256;&#36755;&#21040;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#25152;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#23494;&#38053;&#38145;&#27169;&#22359;&#21487;&#20197;&#30830;&#20445;&#65292;&#27809;&#26377;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#31169;&#26377;&#20449;&#24687;&#65306;a) &#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;UCB-N&#21644;TS-N&#31639;&#27861;&#30340;&#25913;&#36827;&#21518;&#30340;&#20266;&#21518;&#24724;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#23558;$\log(T)$&#22240;&#23376;&#26367;&#25442;&#20026;$\log_2(\alpha)+3$&#22240;&#23376;&#65292;&#20854;&#20013;$\alpha$&#26159;&#21453;&#39304;&#22270;&#30340;&#29420;&#31435;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.04093</link><description>&lt;p&gt;
UCB-N&#21644;TS-N&#30340;&#25913;&#36827;&#21518;&#30340;&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An improved regret analysis for UCB-N and TS-N. (arXiv:2305.04093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;UCB-N&#21644;TS-N&#31639;&#27861;&#30340;&#25913;&#36827;&#21518;&#30340;&#20266;&#21518;&#24724;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#23558;$\log(T)$&#22240;&#23376;&#26367;&#25442;&#20026;$\log_2(\alpha)+3$&#22240;&#23376;&#65292;&#20854;&#20013;$\alpha$&#26159;&#21453;&#39304;&#22270;&#30340;&#29420;&#31435;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26080;&#21521;&#21453;&#39304;&#22270;&#30340;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;Lykouris&#31561;&#20154;&#65288;2020&#65289;&#20808;&#21069;&#20998;&#26512;&#20102;&#22522;&#20110;&#19978;&#32622;&#20449;&#30028;&#31639;&#27861;UCB-N&#21644;&#22522;&#20110;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;TS-N&#30340;&#20266;&#21518;&#24724;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25913;&#36827;&#20182;&#20204;&#30340;&#20266;&#21518;&#24724;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#28041;&#21450;&#21040;&#19978;&#19968;&#20010;&#20998;&#26512;&#30340;&#19968;&#20010;&#20851;&#38190;&#24341;&#29702;&#30340;&#32454;&#21270;&#65292;&#20801;&#35768;&#23558;$\log(T)$&#22240;&#23376;&#26367;&#25442;&#20026;$\log_2(\alpha)+3$&#22240;&#23376;&#65292;&#20854;&#20013;$\alpha$&#26159;&#21453;&#39304;&#22270;&#30340;&#29420;&#31435;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the setting of stochastic online learning with undirected feedback graphs, Lykouris et al. (2020) previously analyzed the pseudo-regret of the upper confidence bound-based algorithm UCB-N and the Thompson Sampling-based algorithm TS-N. In this note, we show how to improve their pseudo-regret analysis. Our improvement involves refining a key lemma of the previous analysis, allowing a $\log(T)$ factor to be replaced by a factor $\log_2(\alpha) + 3$ for $\alpha$ the independence number of the feedback graph.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#33258;&#28982;&#35821;&#35328;&#21160;&#20316;&#31354;&#38388;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#28216;&#25103;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#20132;&#20114;&#29615;&#22659;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#34987;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#22823;&#37327;&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#30830;&#23450;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#24517;&#35201;&#25110;&#34987;&#36807;&#24230;&#20351;&#29992;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#25506;&#32034;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102; &#949;-&#21487;&#25509;&#21463;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21487;&#25509;&#21463;&#30340;&#21160;&#20316;&#30340;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#35282;&#33394;-&#35780;&#35770;&#65288;TAC&#65289;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#20165;&#20174;&#28216;&#25103;&#35266;&#23519;&#20013;&#29983;&#25104;&#25991;&#26412;&#21629;&#20196;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#25110;&#30693;&#35782;&#22270;&#35889;&#12290;&#22312; Jericho &#30340; 10 &#31181;&#28216;&#25103;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#20248;&#20110;&#24378;&#22522;&#32447;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20984;&#26174;&#20986;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#26356;&#36731;&#37327;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21033;&#29992;&#29615;&#22659;&#20449;&#24687;&#30340;&#26032;&#35270;&#35282;&#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $ \epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#21644;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#24555;&#36895;&#23454;&#29616;&#23545;&#31232;&#30095;&#25439;&#22351;&#30340;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.04080</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65306;&#23545;&#31232;&#30095;&#25439;&#22351;&#36827;&#34892;&#24555;&#36895;&#20302;Tucker&#31209;&#24352;&#37327;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption. (arXiv:2305.04080v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#21644;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#24555;&#36895;&#23454;&#29616;&#23545;&#31232;&#30095;&#25439;&#22351;&#30340;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;TRPCA&#65289;&#38382;&#39064;&#65292;&#23427;&#26159;&#30697;&#38453;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;RPCA&#65289;&#30340;&#24352;&#37327;&#25193;&#23637;&#65292;&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#24352;&#37327;&#20998;&#35299;&#20026;&#22522;&#30784;&#20302;&#31209;&#20998;&#37327;&#21644;&#31232;&#30095;&#24322;&#24120;&#20998;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#24615;&#24352;&#37327;CUR&#20998;&#35299;&#65288;RTCUR&#65289;&#65292;&#29992;&#20110;Tucker&#31209;&#35774;&#32622;&#19979;&#30340;&#22823;&#35268;&#27169;&#38750;&#20984;TRPCA&#38382;&#39064;&#12290;RTCUR&#26159;&#22312;&#20132;&#26367;&#25237;&#24433;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65292;&#23427;&#22312;&#20302;&#31209;&#24352;&#37327;&#38598;&#21644;&#31232;&#30095;&#24352;&#37327;&#38598;&#20043;&#38388;&#36827;&#34892;&#25237;&#24433;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#24352;&#37327;CUR&#20998;&#35299;&#65292;&#22312;&#27599;&#20010;&#25237;&#24433;&#20013;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#24320;&#21457;&#20102;&#22235;&#20010;RTCUR&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;RTCUR&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#65292;&#20197;&#23545;&#25239;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm, called Robust Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems under the Tucker rank setting. RTCUR is developed within a framework of alternating projections that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently developed tensor CUR decomposition to substantially reduce the computational complexity in each projection. In addition, we develop four variants of RTCUR for different application settings. We demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art methods on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04073</link><description>&lt;p&gt;
&#29992;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26159;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#20013;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#36825;&#20123;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#29992;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#30340;&#21322;&#24322;&#27493;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;PAOTA&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#36890;&#36807;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#31934;&#24230;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#35745;&#31639;&#26159;&#25552;&#39640;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#25928;&#29575;&#30340;&#26377;&#25928;&#20256;&#36755;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;FEEL&#31995;&#32479;&#36890;&#24120;&#22312;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#20013;&#37319;&#29992;&#20256;&#32479;&#30340;&#21516;&#27493;&#32858;&#21512;&#26426;&#21046;&#65292;&#32780;&#36825;&#20123;&#26426;&#21046;&#23481;&#26131;&#21463;&#21040;&#28382;&#21518;&#32773;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#21322;&#24322;&#27493;&#32858;&#21512;FEEL&#26426;&#21046;&#65288;PAOTA&#65289;&#65292;&#20197;&#25913;&#21892;&#25968;&#25454;&#21644;&#35774;&#22791;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;FEEL&#31995;&#32479;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#36793;&#32536;&#35774;&#22791;&#27169;&#22411;&#26356;&#26032;&#30340;&#38472;&#26087;&#24615;&#21644;&#21457;&#25955;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#32858;&#21512;&#26399;&#35843;&#25972;&#36793;&#32536;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#21151;&#29575;&#26469;&#26368;&#23567;&#21270;FEEL&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#19978;&#30028;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#29702;&#24819;&#30340;&#23616;&#37096;SGD&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#30456;&#21516;&#30340;&#30446;&#26631;&#20934;&#30830;&#24230;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#30340;&#35757;&#32451;&#36895;&#24230;&#26174;&#30528;&#24555;&#20110;&#20855;&#26377;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#30340;&#20256;&#32479;&#21516;&#27493;FEEL&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-the-air Computation (AirComp) has been demonstrated as an effective transmission scheme to boost the efficiency of federated edge learning (FEEL). However, existing FEEL systems with AirComp scheme often employ traditional synchronous aggregation mechanisms for local model aggregation in each global round, which suffer from the stragglers issues. In this paper, we propose a semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to improve the training efficiency of the FEEL system in the case of significant heterogeneity in data and devices. Taking the staleness and divergence of model updates from edge devices into consideration, we minimize the convergence upper bound of the FEEL global model by adjusting the uplink transmit power of edge devices at each aggregation period. The simulation results demonstrate that our proposed algorithm achieves convergence performance close to that of the ideal Local SGD. Furthermore, with the same target accuracy, the training
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26368;&#26032;&#21355;&#26143;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#32771;&#34385;&#21355;&#26143;&#25805;&#20316;&#32422;&#26463;&#65292;&#36798;&#21040;&#20102;&#39640;&#31934;&#24230;&#65292;&#20026;&#33322;&#22825;&#36890;&#20449;&#21644;&#36816;&#33829;&#25104;&#26412;&#33410;&#32422;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04059</link><description>&lt;p&gt;
&#20302;&#36712;&#22330;&#26223;&#20998;&#31867;&#30340;&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit. (arXiv:2305.04059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26368;&#26032;&#21355;&#26143;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#32771;&#34385;&#21355;&#26143;&#25805;&#20316;&#32422;&#26463;&#65292;&#36798;&#21040;&#20102;&#39640;&#31934;&#24230;&#65292;&#20026;&#33322;&#22825;&#36890;&#20449;&#21644;&#36816;&#33829;&#25104;&#26412;&#33410;&#32422;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#21355;&#26143;&#30828;&#20214;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20026;&#36890;&#20449;&#21644;&#36816;&#33829;&#25104;&#26412;&#30340;&#33410;&#32422;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21355;&#26143;&#26143;&#24231;&#19978;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#32771;&#34385;&#21040;&#21355;&#26143;&#22788;&#29702;&#22120;&#22522;&#20934;&#30340;&#28201;&#24230;&#21644;&#26377;&#38480;&#21151;&#29575;&#39044;&#31639;&#31561;&#25805;&#20316;&#32422;&#26463;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#37319;&#29992;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#20219;&#21153;&#26041;&#26696;&#12290;&#25152;&#26377;&#26041;&#26696;&#22312;&#19968;&#22825;&#30340;&#20219;&#21153;&#26102;&#38388;&#20869;&#37117;&#33021;&#36798;&#21040;&#39640;&#31934;&#24230;&#65288;&#27431;&#27954;&#21355;&#26143;RGB&#25968;&#25454;&#38598;&#32422;91%&#65289;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Onboard machine learning on the latest satellite hardware offers the potential for significant savings in communication and operational costs. We showcase the training of a machine learning model on a satellite constellation for scene classification using semi-supervised learning while accounting for operational constraints such as temperature and limited power budgets based on satellite processor benchmarks of the neural network. We evaluate mission scenarios employing both decentralised and federated learning approaches. All scenarios achieve convergence to high accuracy (around 91% on EuroSAT RGB dataset) within a one-day mission timeframe.
&lt;/p&gt;</description></item><item><title>Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04043</link><description>&lt;p&gt;
Echoes: &#22522;&#20110;&#20266;&#20559;&#24046;&#26631;&#35760;&#30340;&#27169;&#20223;&#24335;&#22238;&#22768;&#23460;&#26080;&#30417;&#30563;&#21435;&#20559;
&lt;/p&gt;
&lt;p&gt;
Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04043
&lt;/p&gt;
&lt;p&gt;
Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#26292;&#38706;&#20110;&#26377;&#20559;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36890;&#24120;&#20250;&#23398;&#20064;&#21040;&#19981;&#27491;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#25299;&#23637;&#39046;&#22495;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;Echoes&#8221;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#20197;&#24378;&#21046;&#20351;&#20266;&#26631;&#31614;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#29992;&#20110;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Echoes&#23454;&#29616;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein-Fisher-Rao&#24230;&#37327;&#26469;&#21152;&#26435;&#32771;&#34385;&#23884;&#20837;&#20043;&#38388;&#30340;&#26412;&#22320;&#65288;&#23616;&#37096;&#65289;&#19982;&#20840;&#23616;&#65288;&#25972;&#20307;&#65289;&#29305;&#24449;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31639;&#27861;&#26469;&#36827;&#34892;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#22359;&#23545;&#35282;&#26680;&#26469;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04034</link><description>&lt;p&gt;
Wasserstein-Fisher-Rao&#23884;&#20837;&#65306;&#20855;&#26377;&#26412;&#22320;&#27604;&#36739;&#21644;&#20840;&#23616;&#20256;&#36755;&#30340;&#36923;&#36753;&#26597;&#35810;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport. (arXiv:2305.04034v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein-Fisher-Rao&#24230;&#37327;&#26469;&#21152;&#26435;&#32771;&#34385;&#23884;&#20837;&#20043;&#38388;&#30340;&#26412;&#22320;&#65288;&#23616;&#37096;&#65289;&#19982;&#20840;&#23616;&#65288;&#25972;&#20307;&#65289;&#29305;&#24449;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31639;&#27861;&#26469;&#36827;&#34892;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#22359;&#23545;&#35282;&#26680;&#26469;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#24456;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#65292;&#23427;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21644;&#20351;&#29992;&#38598;&#21512;&#36816;&#31639;&#31526;&#27169;&#25311;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#29305;&#23450;&#24418;&#24335;&#30340;&#23884;&#20837;&#65292;&#20294;&#23884;&#20837;&#20043;&#38388;&#30340;&#35780;&#20998;&#20989;&#25968;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26412;&#22320;&#27604;&#36739;&#25110;&#20840;&#23616;&#20256;&#36755;&#30340;&#35780;&#20998;&#20989;&#25968;&#19981;&#21516;&#65292;&#26412;&#25991;&#20351;&#29992;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#26469;&#30740;&#31350;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38598;&#21512;&#23884;&#20837;&#21040;&#24102;&#26377;Wasserstein-Fisher-Rao&#24230;&#37327;&#30340;&#26377;&#30028;&#27979;&#24230;&#31354;&#38388;&#19978;&#65292;&#24182;&#20351;&#29992;&#36825;&#26679;&#30340;&#24230;&#37327;&#26469;&#35774;&#35745;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#31181;&#35774;&#35745;&#36824;&#20419;&#36827;&#20102;&#23884;&#20837;&#31354;&#38388;&#20869;&#30340;&#23553;&#38381;&#24418;&#24335;&#38598;&#21512;&#36816;&#31639;&#31526;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#32447;&#24615;&#35745;&#31639;&#31639;&#27861;&#21644;&#19968;&#20010;&#22359;&#23545;&#35282;&#26680;&#20197;&#23454;&#29616;&#26435;&#34913;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;WFRE&#21487;&#20197;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learning-based models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scoring functions motivated by local comparison or global transport, this work investigates the local and global trade-off with unbalanced optimal transport theory. Specifically, we embed sets as bounded measures in $\real$ endowed with a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a design also facilitates closed-form set operators in the embedding space. Moreover, we introduce a convolution-based algorithm for linear time computation and a block-diagonal kernel to enforce the trade-off. Results show that WFRE can outperform existing query embedding methods on standard datasets, eval
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#32908;&#30005;&#22270;&#20449;&#21495;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#35757;&#32451;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#23545;&#29031;&#32452;&#12289;&#32908;&#30149;&#21644;ALS&#24739;&#32773;&#20449;&#21495;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#20934;&#30830;&#29575;99%&#30340;&#25104;&#26524;&#65292;&#36825;&#20123;&#25216;&#26415;&#23558;&#26377;&#21161;&#20110;&#20020;&#24202;&#35786;&#26029;&#31070;&#32463;&#32908;&#32905;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2305.04006</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32908;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Electromyography Signal Classification Using Deep Learning. (arXiv:2305.04006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#32908;&#30005;&#22270;&#20449;&#21495;&#30340;&#20998;&#31867;&#65292;&#36890;&#36807;&#35757;&#32451;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#23545;&#29031;&#32452;&#12289;&#32908;&#30149;&#21644;ALS&#24739;&#32773;&#20449;&#21495;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#20934;&#30830;&#29575;99%&#30340;&#25104;&#26524;&#65292;&#36825;&#20123;&#25216;&#26415;&#23558;&#26377;&#21161;&#20110;&#20020;&#24202;&#35786;&#26029;&#31070;&#32463;&#32908;&#32905;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;L2&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23545;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#25968;&#25454;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#35813;&#25968;&#25454;&#21253;&#25324;&#26469;&#33258;&#23545;&#29031;&#32452;&#12289;&#32908;&#30149;&#21644;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#65288;ALS&#65289;&#24739;&#32773;&#30340;EMG&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20843;&#23618;&#32452;&#25104;&#65292;&#21253;&#25324;&#20116;&#20010;&#20840;&#36830;&#25509;&#23618;&#12289;&#20004;&#20010;&#25209;&#26631;&#20934;&#21270;&#23618;&#21644;&#19968;&#20010;&#38543;&#26426;&#22833;&#27963;&#23618;&#12290;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#20998;&#20026;&#23376;&#35757;&#32451;&#21644;&#39564;&#35777;&#37096;&#20998;&#65292;&#36827;&#19968;&#27493;&#23558;&#25968;&#25454;&#20998;&#25104;&#35757;&#32451;&#21644;&#27979;&#35797;&#37096;&#20998;&#12290;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;99%&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;100%&#20934;&#30830;&#22320;&#21306;&#20998;&#27491;&#24120;&#30149;&#20363;&#65288;&#23545;&#29031;&#32452;&#65289;&#21644;&#20854;&#20182;&#30149;&#20363;&#65292;&#24182;&#20197;97.4%&#21644;98.2%&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#32908;&#30149;&#21644;ALS&#30149;&#20363;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#39640;&#24230;&#25913;&#36827;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#23558;&#26377;&#21161;&#20110;&#20020;&#24202;&#35786;&#26029;&#31070;&#32463;&#32908;&#32905;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have implemented a deep learning model with L2 regularization and trained it on Electromyography (EMG) data. The data comprises of EMG signals collected from control group, myopathy and ALS patients. Our proposed deep neural network consists of eight layers; five fully connected, two batch normalization and one dropout layers. The data is divided into training and testing sections by subsequently dividing the training data into sub-training and validation sections. Having implemented this model, an accuracy of 99 percent is achieved on the test data set. The model was able to distinguishes the normal cases (control group) from the others at a precision of 100 percent and classify the myopathy and ALS with high accuracy of 97.4 and 98.2 percents, respectively. Thus we believe that, this highly improved classification accuracies will be beneficial for their use in the clinical diagnosis of neuromuscular disorders.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03960</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#27969;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#35268;&#21017;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;(arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#33258;&#21160;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#24418;&#24335;&#21270;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#27969;&#31243;&#23454;&#20307;&#65288;&#22914;&#21442;&#19982;&#32773;&#12289;&#27963;&#21160;&#12289;&#23545;&#35937;&#31561;&#65289;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24102;&#26377;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;(PET)&#24050;&#32463;&#20986;&#29256;&#65292;&#20854;&#20276;&#38543;&#30528;&#19968;&#31181;&#22522;&#26412;&#30340;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#65292;PET&#32570;&#20047;&#26377;&#20851;&#20004;&#20010;&#25552;&#21450;&#26159;&#21542;&#25351;&#20195;&#20102;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;&#27969;&#31243;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#26159;&#21542;&#22312;&#30446;&#26631;&#27169;&#22411;&#20013;&#21019;&#24314;&#19968;&#20010;&#25110;&#20004;&#20010;&#24314;&#27169;&#20803;&#32032;&#30340;&#37325;&#35201;&#20915;&#31574;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#20363;&#22914;&#65292;&#20004;&#20010;&#25968;&#25454;&#22788;&#29702;&#30340;&#25552;&#21450;&#26159;&#21542;&#24847;&#21619;&#30528;&#22788;&#29702;&#19981;&#21516;&#25110;&#30456;&#21516;&#30340;&#25968;&#25454;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#26469;&#25193;&#23637;PET&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21452;&#26497;&#21270;&#22825;&#32447;&#20998;&#31867;GPS&#20449;&#21495;&#25509;&#25910;&#29366;&#20917;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22478;&#24066;&#22320;&#21306;&#30340;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03956</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21452;&#26497;&#21270;&#22825;&#32447;&#22312;&#22478;&#24066;&#22320;&#21306;&#20998;&#31867;GPS&#20449;&#21495;&#25509;&#25910;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Machine-Learning-Based Classification of GPS Signal Reception Conditions Using a Dual-Polarized Antenna in Urban Areas. (arXiv:2305.03956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21452;&#26497;&#21270;&#22825;&#32447;&#20998;&#31867;GPS&#20449;&#21495;&#25509;&#25910;&#29366;&#20917;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22478;&#24066;&#22320;&#21306;&#30340;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#22320;&#21306;&#65292;&#23494;&#38598;&#30340;&#24314;&#31569;&#29289;&#32463;&#24120;&#20250;&#38459;&#25377;&#21644;&#21453;&#23556;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479;&#65288;GPS&#65289;&#20449;&#21495;&#65292;&#23548;&#33268;&#25509;&#25910;&#21040;&#24456;&#22810;&#22810;&#24452;&#20449;&#21495;&#30340;&#23569;&#25968;&#21487;&#35265;&#21355;&#26143;&#12290;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#22312;&#22478;&#24066;&#22320;&#21306;&#20301;&#32622;&#19981;&#21487;&#38752;&#12290;&#22914;&#26524;&#33021;&#22815;&#26816;&#27979;&#21040;&#26469;&#33258;&#26576;&#20010;&#21355;&#26143;&#30340;&#20449;&#21495;&#25509;&#25910;&#29366;&#20917;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#25490;&#38500;&#25110;&#20943;&#36731;&#22810;&#24452;&#27745;&#26579;&#30340;&#21355;&#26143;&#20449;&#21495;&#26469;&#25913;&#21892;&#23450;&#20301;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#26497;&#21270;&#22825;&#32447;&#20998;&#31867;GPS&#20449;&#21495;&#25509;&#25910;&#29366;&#20917;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#26641;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#20854;&#20013;&#19977;&#20010;&#29305;&#24449;&#20043;&#19968;&#21482;&#33021;&#20174;&#21452;&#26497;&#21270;&#22825;&#32447;&#20013;&#33719;&#24471;&#12290;&#20351;&#29992;&#20174;&#21508;&#31181;&#20301;&#32622;&#25910;&#38598;&#30340;GPS&#20449;&#21495;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#24403;&#36755;&#20837;&#20174;GPS&#21407;&#22987;&#20449;&#21495;&#25552;&#21462;&#30340;&#29305;&#24449;&#26102;&#65292;&#29983;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20250;&#36755;&#20986;&#19977;&#31181;&#20449;&#21495;&#25509;&#25910;&#29366;&#20917;&#20043;&#19968;&#65306;&#38750;&#35270;&#36317;&#65288;NLOS&#65289;&#12289;&#35270;&#36317;&#65288;LOS&#65289;&#21644;&#37096;&#20998;&#21487;&#35270;&#65288;PLOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In urban areas, dense buildings frequently block and reflect global positioning system (GPS) signals, resulting in the reception of a few visible satellites with many multipath signals. This is a significant problem that results in unreliable positioning in urban areas. If a signal reception condition from a certain satellite can be detected, the positioning performance can be improved by excluding or de-weighting the multipath contaminated satellite signal. Thus, we developed a machine-learning-based method of classifying GPS signal reception conditions using a dual-polarized antenna. We employed a decision tree algorithm for classification using three features, one of which can be obtained only from a dual-polarized antenna. A machine-learning model was trained using GPS signals collected from various locations. When the features extracted from the GPS raw signal are input, the generated machine-learning model outputs one of the three signal reception conditions: non-line-of-sight (N
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03954</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#20197;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#35780;&#20272;&#65288;OPE&#65289;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30001;&#19981;&#21516;&#31574;&#30053;&#25910;&#38598;&#30340;&#35760;&#24405;&#25968;&#25454;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#39044;&#26399;&#22870;&#21169;&#12290; OPE&#26159;&#36816;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;A / B&#27979;&#35797;&#30340;&#21487;&#34892;&#36873;&#25321;&#65306;&#23427;&#21487;&#20197;&#21152;&#24555;&#26032;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#24182;&#38477;&#20302;&#21521;&#23458;&#25143;&#26292;&#38706;&#27425;&#20248;&#27835;&#30103;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#24403;&#21160;&#20316;&#25968;&#37327;&#24456;&#22823;&#25110;&#35760;&#24405;&#31574;&#30053;&#26410;&#20805;&#20998;&#25506;&#32034;&#26576;&#20123;&#25805;&#20316;&#26102;&#65292;&#22522;&#20110;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#30340;&#29616;&#26377;&#20272;&#35745;&#22120;&#21487;&#33021;&#20855;&#26377;&#39640;&#29978;&#33267;&#26080;&#38480;&#26041;&#24046;&#12290;Saito&#21644;Joachims&#25552;&#20986;&#20351;&#29992;&#21160;&#20316;&#23884;&#20837;&#30340;&#36793;&#38469;IPS&#65288;MIPS&#65289;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#38477;&#20302;IPS&#30340;&#26041;&#24046;&#12290; MIPS&#20551;&#35774;&#20174;&#19994;&#32773;&#21487;&#20197;&#23450;&#20041;&#33391;&#22909;&#30340;&#21160;&#20316;&#23884;&#20837;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#26469;&#23450;&#20041;&#21160;&#20316;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;MIPS&#20272;&#35745;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25910;&#25947;&#24615;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#21644;&#24212;&#23545;&#37325;&#23614;&#22122;&#22768;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03938</link><description>&lt;p&gt;
Adam&#23478;&#26063;&#31639;&#27861;&#22312;&#26080;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25910;&#25947;&#24615;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#21644;&#24212;&#23545;&#37325;&#23614;&#22122;&#22768;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#23478;&#26063;&#31639;&#27861;&#22312;&#26080;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#32467;&#21512;&#26799;&#24230;&#35009;&#21098;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#20165;&#20551;&#23450;&#35780;&#20272;&#22122;&#22768;&#21487;&#31215;&#30340;&#24773;&#20917;&#19979;&#20063;&#20250;&#25910;&#25947;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.03935</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65288;&#21363;&#25193;&#25955;ODE&#65289;&#26159;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#23427;&#20351;&#24471;&#30830;&#23450;&#24615;&#25512;&#26029;&#21644;&#31934;&#30830;&#20284;&#28982;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#25955;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#32467;&#26524;&#20173;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#21644;&#35780;&#20272;&#20004;&#20010;&#26041;&#38754;&#65292;&#29992;&#20110;&#25193;&#25955;ODE&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36895;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#25506;&#32034;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#24182;&#24179;&#28369;&#20854;&#36712;&#36857;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#39035;&#35757;&#32451;&#30340;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#26469;&#22635;&#34917;&#35757;&#32451;-&#35780;&#20272;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#20351;&#29992;&#27973;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#24674;&#22797;&#32534;&#35793;&#22120;&#37197;&#32622;&#23646;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#24050;&#32463;&#22312;x86-64&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26377;&#25928;&#30340;&#25805;&#20316;&#30721;&#21644;&#23492;&#23384;&#22120;&#27966;&#29983;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#39033;&#24037;&#20316;&#19982;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#36731;&#37327;&#32423;&#29305;&#24449;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.03934</link><description>&lt;p&gt;
&#37325;&#35775;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#30340;&#36731;&#37327;&#32423;&#32534;&#35793;&#22120;&#28335;&#28304;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Revisiting Lightweight Compiler Provenance Recovery on ARM Binaries. (arXiv:2305.03934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#20351;&#29992;&#27973;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#39640;&#25928;&#21644;&#20934;&#30830;&#22320;&#24674;&#22797;&#32534;&#35793;&#22120;&#37197;&#32622;&#23646;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#24050;&#32463;&#22312;x86-64&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26377;&#25928;&#30340;&#25805;&#20316;&#30721;&#21644;&#23492;&#23384;&#22120;&#27966;&#29983;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#39033;&#24037;&#20316;&#19982;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#36731;&#37327;&#32423;&#29305;&#24449;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34892;&#20026;&#21463;&#32534;&#35793;&#22120;&#22914;&#20309;&#26500;&#24314;&#20854;&#28304;&#20195;&#30721;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#32534;&#35793;&#36807;&#31243;&#20013;&#22823;&#22810;&#25968;&#32534;&#35793;&#22120;&#37197;&#32622;&#32454;&#33410;&#37117;&#34987;&#25277;&#35937;&#21270;&#20102;&#65292;&#20294;&#24674;&#22797;&#23427;&#20204;&#23545;&#20110;&#26410;&#30693;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#36870;&#21521;&#24037;&#31243;&#21644;&#31243;&#24207;&#29702;&#35299;&#20219;&#21153;&#65288;&#20363;&#22914;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#65289;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#22312;x86-64&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#32780;&#36825;&#31181;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#27973;&#23618;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#39640;&#25928;&#32780;&#20934;&#30830;&#22320;&#24674;&#22797;&#32534;&#35793;&#22120;&#37197;&#32622;&#23646;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#24050;&#32463;&#22312;x86-64&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26377;&#25928;&#30340;&#25805;&#20316;&#30721;&#21644;&#23492;&#23384;&#22120;&#27966;&#29983;&#29305;&#24449;&#21040;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#39033;&#24037;&#20316;&#19982;Pizzolotto&#31561;&#20154;&#30340;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36731;&#37327;&#32423;&#29305;&#24449;&#22312;ARM&#20108;&#36827;&#21046;&#20195;&#30721;&#19978;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A binary's behavior is greatly influenced by how the compiler builds its source code. Although most compiler configuration details are abstracted away during compilation, recovering them is useful for reverse engineering and program comprehension tasks on unknown binaries, such as code similarity detection. We observe that previous work has thoroughly explored this on x86-64 binaries. However, there has been limited investigation of ARM binaries, which are increasingly prevalent.  In this paper, we extend previous work with a shallow-learning model that efficiently and accurately recovers compiler configuration properties for ARM binaries. We apply opcode and register-derived features, that have previously been effective on x86-64 binaries, to ARM binaries. Furthermore, we compare this work with Pizzolotto et al., a recent architecture-agnostic model that uses deep learning, whose dataset and code are available.  We observe that the lightweight features are reproducible on ARM binaries
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03923</link><description>&lt;p&gt;
&#20027;&#21160;&#30340;&#36830;&#32493;&#23398;&#20064;&#65306;&#22312;&#20219;&#21153;&#24207;&#21015;&#20013;&#26631;&#35760;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20013;&#65292;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#19981;&#24536;&#35760;&#24050;&#23398;&#20869;&#23481;&#26159;&#20854;&#26680;&#24515;&#12290;&#32780;&#20219;&#21153;&#26159;&#25353;&#39034;&#24207;&#20986;&#29616;&#30340;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20934;&#22791;&#21644;&#27880;&#37322;&#21017;&#36890;&#24120;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#21644;&#19968;&#20010;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;AL&#21644;CL&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#65292;&#31867;&#21035;&#21644;&#20219;&#21153;&#22686;&#37327;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#22312;CL&#21644;AL&#20013;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#22312;&#20197;&#21069;&#20219;&#21153;&#30340;&#27880;&#37322;&#25910;&#38598;&#19978;&#26465;&#20214;&#26597;&#35810;&#31574;&#30053;&#20250;&#25552;&#39640;&#39046;&#22495;&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#21017;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#12289;&#32570;&#22833;&#21644;&#20998;&#24067;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.03920</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Spatio-Temporal Graph Contrastive Learning. (arXiv:2305.03920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#12289;&#32570;&#22833;&#21644;&#20998;&#24067;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#21306;&#22495;&#23884;&#20837;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#30001;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#32467;&#26500;&#34920;&#31034;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23578;&#26410;&#35299;&#20915;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;i&#65289;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#25968;&#25454;&#22122;&#38899;&#21644;&#32570;&#22833;&#22312;&#35768;&#22810;&#26102;&#31354;&#22330;&#26223;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;ii&#65289;&#36755;&#20837;&#30340;&#26102;&#31354;&#25968;&#25454;&#65288;&#20363;&#22914;&#31227;&#21160;&#36712;&#36857;&#65289;&#36890;&#24120;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#34920;&#29616;&#20986;&#20998;&#24067;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#29983;&#25104;&#21306;&#22495;&#22270;&#30340;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#20174;&#22810;&#35270;&#22270;&#25968;&#25454;&#28304;&#29983;&#25104;&#30340;&#24322;&#26500;&#21306;&#22495;&#22270;&#19978;&#30340;&#33258;&#21160;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65288;AutoST&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;"AutoST"&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#24322;&#26500;&#22270;&#31070;&#32463;&#26550;&#26500;&#20043;&#19978;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#22270;&#21306;&#22495;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#26410;&#30693;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#38382;&#39064;&#30340;&#21464;&#20998;&#38750;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#21464;&#37327;&#21644;&#25512;&#23548;&#20986;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#31616;&#21270;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#21040;&#22788;&#29702;&#20219;&#24847;&#38750;&#32447;&#24615;&#35266;&#27979;&#26041;&#31243;&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03914</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#30340;&#21464;&#20998;&#38750;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Variational Nonlinear Kalman Filtering with Unknown Process Noise Covariance. (arXiv:2305.03914v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#26410;&#30693;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#38382;&#39064;&#30340;&#21464;&#20998;&#38750;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#21464;&#37327;&#21644;&#25512;&#23548;&#20986;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#31616;&#21270;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#21040;&#22788;&#29702;&#20219;&#24847;&#38750;&#32447;&#24615;&#35266;&#27979;&#26041;&#31243;&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#32852;&#21512;&#21644;&#36882;&#24402;&#20272;&#35745;&#21160;&#24577;&#29366;&#24577;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#65292;&#26469;&#35299;&#20915;&#38647;&#36798;&#21644;&#22768;&#32435;&#31561;&#20256;&#24863;&#22120;&#36319;&#36394;&#26426;&#21160;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#30340;&#38750;&#20849;&#36717;&#24615;&#65292;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#21407;&#29702;&#30340;&#36882;&#24402;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#38543;&#26426;&#25628;&#32034;&#21464;&#20998;&#25512;&#29702;&#27861;&#26469;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#22122;&#22768;&#33258;&#36866;&#24212;&#28388;&#27874;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#21464;&#37327;&#65292;&#25512;&#23548;&#20986;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#23558;&#38750;&#20849;&#36717;&#20808;&#39564;&#36716;&#21270;&#20026;&#20849;&#36717;&#24418;&#24335;&#65292;&#31616;&#21270;&#20102;&#20272;&#35745;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#22788;&#29702;&#20219;&#24847;&#38750;&#32447;&#24615;&#35266;&#27979;&#26041;&#31243;&#65292;&#20351;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#26356;&#20855;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the maneuvering target tracking with sensors such as radar and sonar, this paper considers the joint and recursive estimation of the dynamic state and the time-varying process noise covariance in nonlinear state space models. Due to the nonlinearity of the models and the non-conjugate prior, the state estimation problem is generally intractable as it involves integrals of general nonlinear functions and unknown process noise covariance, resulting in the posterior probability distribution functions lacking closed-form solutions. This paper presents a recursive solution for joint nonlinear state estimation and model parameters identification based on the approximate Bayesian inference principle. The stochastic search variational inference is adopted to offer a flexible, accurate, and effective approximation of the posterior distributions. We make two contributions compared to existing variational inference-based noise adaptive filtering methods. First, we introduce an auxili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21512;&#25104;PET&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;PET&#25104;&#20687;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#30340;&#32570;&#38519;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#25935;&#24863;&#24230;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.03901</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#23558;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#22270;&#20687;&#21512;&#25104;PET&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model. (arXiv:2305.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21512;&#25104;PET&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;PET&#25104;&#20687;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#30340;&#32570;&#38519;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#25935;&#24863;&#24230;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#21644;PET&#26159;&#35786;&#26029;&#33041;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;PET&#25195;&#25551;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#65292;&#23548;&#33268;PET&#32570;&#20047;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#36827;&#34892;&#36229;&#39640;&#22330;MRI&#21644;PET&#30446;&#21069;&#20960;&#20046;&#19981;&#21487;&#34892;&#12290;&#36229;&#39640;&#22330;&#25104;&#20687;&#22312;&#20020;&#24202;&#21644;&#23398;&#26415;&#29615;&#22659;&#20013;&#37117;&#34987;&#35777;&#26126;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35748;&#30693;&#31070;&#32463;&#24433;&#20687;&#23398;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#21512;&#25104;PET&#30340;&#26041;&#27861;&#12290;&#20174;&#32479;&#35745;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65288;JPD&#65289;&#26159;&#25551;&#32472;PET&#21644;MRI&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#26368;&#30452;&#25509;&#21644;&#22522;&#26412;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#27880;&#24847;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;JDAM&#12290;JDAM&#20855;&#26377;&#25193;&#25955;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#25193;&#25955;&#36807;&#31243;&#28041;&#21450;&#23558;PET&#36880;&#28176;&#25193;&#25955;&#21040;&#39640;&#26031;PET&#65292;&#37319;&#26679;&#36807;&#31243;&#20174;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JDAM&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PET&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768; lesss &#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI and PET are crucial diagnostic tools for brain diseases, as they provide complementary information on brain structure and function. However, PET scanning is costly and involves radioactive exposure, resulting in a lack of PET. Moreover, simultaneous PET and MRI at ultra-high-field are currently hardly infeasible. Ultra-high-field imaging has unquestionably proven valuable in both clinical and academic settings, especially in the field of cognitive neuroimaging. These motivate us to propose a method for synthetic PET from high-filed MRI and ultra-high-field MRI. From a statistical perspective, the joint probability distribution (JPD) is the most direct and fundamental means of portraying the correlation between PET and MRI. This paper proposes a novel joint diffusion attention model which has the joint probability distribution and attention strategy, named JDAM. JDAM has a diffusion process and a sampling process. The diffusion process involves the gradual diffusion of PET to Gaussi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;&#65292;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03900</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#22343;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class Imbalance in Machine Learning. (arXiv:2305.03900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;&#65292;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#23427;&#20851;&#27880;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#23558;&#31867;&#21035;&#19981;&#24179;&#34913;&#23450;&#20041;&#20026;&#27604;&#20363;&#19981;&#24179;&#34913;&#65292;&#21363;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#20363;&#19981;&#24179;&#34913;&#65292;&#32780;&#24573;&#35270;&#27604;&#20363;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#31867;&#21035;&#20043;&#38388;/&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#26041;&#24046;&#12289;&#36317;&#31163;&#12289;&#37051;&#36817;&#21644;&#36136;&#37327;&#31561;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;:&#20840;&#23616;&#21644;&#23616;&#37096;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#26469;&#35828;&#26126;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. 
&lt;/p&gt;</description></item><item><title>NL-CS Net&#26159;&#19968;&#31181;&#32467;&#21512;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22522;&#20110;&#32593;&#32476;&#26041;&#27861;&#30340;&#36895;&#24230;&#30340;&#26032;&#22411;CS&#26041;&#27861;&#65292;&#20351;&#29992;&#38750;&#23616;&#37096;&#20808;&#39564;&#65292;&#22312;&#37325;&#26500;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03899</link><description>&lt;p&gt;
NL-CS Net:&#22522;&#20110;&#38750;&#23616;&#37096;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#29255;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive Sensing. (arXiv:2305.03899v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03899
&lt;/p&gt;
&lt;p&gt;
NL-CS Net&#26159;&#19968;&#31181;&#32467;&#21512;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22522;&#20110;&#32593;&#32476;&#26041;&#27861;&#30340;&#36895;&#24230;&#30340;&#26032;&#22411;CS&#26041;&#27861;&#65292;&#20351;&#29992;&#38750;&#23616;&#37096;&#20808;&#39564;&#65292;&#22312;&#37325;&#26500;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22270;&#20687;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#24448;&#24448;&#20316;&#20026;&#40657;&#21283;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#20808;&#39564;&#30693;&#35782;&#26159;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340; bottleneck&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22522;&#20110;&#32593;&#32476;&#26041;&#27861;&#30340;&#36895;&#24230;&#30340;&#26032;&#22411;CS&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38750;&#23616;&#37096;&#20808;&#39564;&#30340;NL-CS Net&#12290;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#23637;&#24320;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#20197;&#35299;&#20915;&#38750;&#23616;&#37096;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#12290;NL-CS Net&#30001;&#19978;&#37319;&#26679;&#27169;&#22359;&#21644;&#24674;&#22797;&#27169;&#22359;&#32452;&#25104;&#65292;&#22312;&#19978;&#37319;&#26679;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#19978;&#37319;&#26679;&#30697;&#38453;&#20195;&#26367;&#39044;&#23450;&#20041;&#30340;&#19978;&#37319;&#26679;&#30697;&#38453;&#65307;&#22312;&#24674;&#22797;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#22270;&#22359;&#30340;&#38750;&#23616;&#37096;&#32593;&#32476;&#26469;&#25429;&#33719;&#38271;&#36317;&#31163;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;NL-CS Net&#20013;&#28041;&#21450;&#30340;&#37325;&#35201;&#21442;&#25968;&#65288;&#20363;&#22914;&#37319;&#26679;&#30697;&#38453;&#65292;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#25910;&#32553;&#37327;&#65292;&#27491;&#21017;&#21270;&#65289;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;NL-CS Net&#22312;&#37325;&#26500;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been applied to compressive sensing (CS) of images successfully in recent years. However, existing network-based methods are often trained as the black box, in which the lack of prior knowledge is often the bottleneck for further performance improvement. To overcome this drawback, this paper proposes a novel CS method using non-local prior which combines the interpretability of the traditional optimization methods with the speed of network-based methods, called NL-CS Net. We unroll each phase from iteration of the augmented Lagrangian method solving non-local and sparse regularized optimization problem by a network. NL-CS Net is composed of the up-sampling module and the recovery module. In the up-sampling module, we use learnable up-sampling matrix instead of a predefined one. In the recovery module, patch-wise non-local network is employed to capture long-range feature correspondences. Important parameters involved (e.g. sampling matrix, nonlinear transforms, shrink
&lt;/p&gt;</description></item><item><title>TSVQR&#33021;&#22815;&#25429;&#25417;&#29616;&#20195;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#21644;&#19981;&#23545;&#31216;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#24322;&#36136;&#20998;&#24067;&#20449;&#24687;&#12290;&#36890;&#36807;&#26500;&#36896;&#20004;&#20010;&#36739;&#23567;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;TSVQR&#29983;&#25104;&#20004;&#20010;&#38750;&#24179;&#34892;&#24179;&#38754;&#65292;&#27979;&#37327;&#27599;&#20010;&#20998;&#20301;&#25968;&#27700;&#24179;&#19979;&#38480;&#21644;&#19978;&#38480;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#65292;TSVQR&#20248;&#20110;&#20197;&#21069;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03894</link><description>&lt;p&gt;
&#21452;&#25903;&#25345;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Twin support vector quantile regression. (arXiv:2305.03894v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03894
&lt;/p&gt;
&lt;p&gt;
TSVQR&#33021;&#22815;&#25429;&#25417;&#29616;&#20195;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#21644;&#19981;&#23545;&#31216;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#24322;&#36136;&#20998;&#24067;&#20449;&#24687;&#12290;&#36890;&#36807;&#26500;&#36896;&#20004;&#20010;&#36739;&#23567;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;TSVQR&#29983;&#25104;&#20004;&#20010;&#38750;&#24179;&#34892;&#24179;&#38754;&#65292;&#27979;&#37327;&#27599;&#20010;&#20998;&#20301;&#25968;&#27700;&#24179;&#19979;&#38480;&#21644;&#19978;&#38480;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#65292;TSVQR&#20248;&#20110;&#20197;&#21069;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25903;&#25345;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;(TSVQR)&#65292;&#29992;&#20110;&#25429;&#25417;&#29616;&#20195;&#25968;&#25454;&#20013;&#24322;&#36136;&#21644;&#19981;&#23545;&#31216;&#20449;&#24687;&#12290;TSVQR&#21033;&#29992;&#20998;&#20301;&#25968;&#21442;&#25968;&#26377;&#25928;&#22320;&#25551;&#36848;&#20102;&#25152;&#26377;&#25968;&#25454;&#28857;&#30340;&#24322;&#36136;&#20998;&#24067;&#20449;&#24687;&#12290;&#30456;&#24212;&#22320;&#65292;TSVQR&#26500;&#36896;&#20102;&#20004;&#20010;&#36739;&#23567;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;(QPPs)&#65292;&#29983;&#25104;&#20004;&#20010;&#38750;&#24179;&#34892;&#24179;&#38754;&#65292;&#20197;&#27979;&#37327;&#27599;&#20010;&#20998;&#20301;&#25968;&#27700;&#24179;&#19979;&#38480;&#21644;&#19978;&#38480;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#12290;TSVQR&#20013;&#30340;QPP&#27604;&#20197;&#21069;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#26356;&#23567;&#19988;&#26356;&#26131;&#20110;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;TSVQR&#30340;&#21452;&#37325;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#20063;&#21152;&#36895;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;&#20845;&#20010;&#20154;&#36896;&#25968;&#25454;&#38598;&#12289;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12289;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TSVQR&#22312;&#23436;&#20840;&#25429;&#33719;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20197;&#21069;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a twin support vector quantile regression (TSVQR) to capture the heterogeneous and asymmetric information in modern data. Using a quantile parameter, TSVQR effectively depicts the heterogeneous distribution information with respect to all portions of data points. Correspondingly, TSVQR constructs two smaller sized quadratic programming problems (QPPs) to generate two nonparallel planes to measure the distributional asymmetry between the lower and upper bounds at each quantile level. The QPPs in TSVQR are smaller and easier to solve than those in previous quantile regression methods. Moreover, the dual coordinate descent algorithm for TSVQR also accelerates the training speed. Experimental results on six artiffcial data sets, ffve benchmark data sets, two large scale data sets, two time-series data sets, and two imbalanced data sets indicate that the TSVQR outperforms previous quantile regression methods in terms of the effectiveness of completely capturing the heterogeneous 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03890</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#32593;&#32476;&#36924;&#36817;&#29992;&#20110;&#36328;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20247;&#22810;&#36807;&#31243;&#65288;&#22914;&#65306;&#27973;&#23618;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#21644;&#21508;&#31181;&#20869;&#26680;&#26041;&#27861;&#65289;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#34920;&#36798;&#33021;&#21147;&#65289;&#30740;&#31350;&#20013;&#20419;&#36827;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21464;&#23398;&#20064;&#12289;&#20256;&#36882;&#23398;&#20064;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#31561;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#26469;&#30740;&#31350;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#32452;&#20869;&#26680;&#30340;&#26356;&#19968;&#33324;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#24179;&#31227;&#32593;&#32476;&#65288;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#31227;&#19981;&#21464;&#26680;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65289;&#21644;&#26059;&#36716;&#21306;&#20989;&#25968;&#26680;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#35201;&#27714;&#20869;&#26680;&#26159;&#27491;&#23450;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#21487;&#33021;&#22312;&#20998;&#24067;&#19978;&#19981;&#21516;&#30340;&#36328;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24352;&#37327;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#34892;&#21160;&#21644;&#31995;&#32479;&#21442;&#25968;&#30001;&#24352;&#37327;&#34920;&#31034;&#65292;&#30528;&#37325;&#20110;&#26410;&#30693;&#31995;&#32479;&#24352;&#37327;&#20026;&#20302;&#31209;&#30340;&#24773;&#20917;&#12290;&#25152;&#24320;&#21457;&#30340; TOFU &#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#28789;&#27963;&#30340;&#24352;&#37327;&#22238;&#24402;&#25216;&#26415;&#20272;&#35745;&#19982;&#31995;&#32479;&#24352;&#37327;&#30456;&#20851;&#32852;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#25442;&#25104;&#19968;&#20010;&#20855;&#26377;&#31995;&#32479;&#21442;&#25968;&#33539;&#25968;&#32422;&#26463;&#30340;&#26032;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#33539;&#25968;&#32422;&#26463;&#36172;&#21338;&#23376;&#20363;&#31243;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.03884</link><description>&lt;p&gt;
&#39640;&#32500;&#20302;&#31209;&#24352;&#37327;&#36172;&#21338;&#26426;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On High-dimensional and Low-rank Tensor Bandits. (arXiv:2305.03884v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24352;&#37327;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#34892;&#21160;&#21644;&#31995;&#32479;&#21442;&#25968;&#30001;&#24352;&#37327;&#34920;&#31034;&#65292;&#30528;&#37325;&#20110;&#26410;&#30693;&#31995;&#32479;&#24352;&#37327;&#20026;&#20302;&#31209;&#30340;&#24773;&#20917;&#12290;&#25152;&#24320;&#21457;&#30340; TOFU &#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#28789;&#27963;&#30340;&#24352;&#37327;&#22238;&#24402;&#25216;&#26415;&#20272;&#35745;&#19982;&#31995;&#32479;&#24352;&#37327;&#30456;&#20851;&#32852;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#25442;&#25104;&#19968;&#20010;&#20855;&#26377;&#31995;&#32479;&#21442;&#25968;&#33539;&#25968;&#32422;&#26463;&#30340;&#26032;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#33539;&#25968;&#32422;&#26463;&#36172;&#21338;&#23376;&#20363;&#31243;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#32500;&#29305;&#24449;&#12290;&#34429;&#28982;&#20195;&#34920;&#24615;&#24456;&#24378;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#33021;&#27169;&#25311;&#39640;&#32500;&#20294;&#26377;&#20248;&#21183;&#32467;&#26500;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20302;&#31209;&#24352;&#37327;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24352;&#37327;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#34892;&#21160;&#21644;&#31995;&#32479;&#21442;&#25968;&#30001;&#24352;&#37327;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#26410;&#30693;&#31995;&#32479;&#24352;&#37327;&#20026;&#20302;&#31209;&#30340;&#24773;&#20917;&#12290;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#36172;&#21338;&#26426;&#31639;&#27861;TOFU&#65288;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#24352;&#37327;&#20048;&#35266;&#65289;&#65292;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#28789;&#27963;&#30340;&#24352;&#37327;&#22238;&#24402;&#25216;&#26415;&#20272;&#35745;&#19982;&#31995;&#32479;&#24352;&#37327;&#30456;&#20851;&#32852;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20272;&#35745;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#25442;&#20026;&#19968;&#20010;&#20855;&#26377;&#20854;&#31995;&#32479;&#21442;&#25968;&#33539;&#25968;&#32422;&#26463;&#30340;&#26032;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;TOFU&#37319;&#29992;&#33539;&#25968;&#32422;&#26463;&#36172;&#21338;&#23376;&#20363;&#31243;&#65292;&#21033;&#29992;&#36825;&#20123;&#32422;&#26463;&#26469;&#23454;&#29616;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing studies on linear bandits focus on the one-dimensional characterization of the overall system. While being representative, this formulation may fail to model applications with high-dimensional but favorable structures, such as the low-rank tensor representation for recommender systems. To address this limitation, this work studies a general tensor bandits model, where actions and system parameters are represented by tensors as opposed to vectors, and we particularly focus on the case that the unknown system tensor is low-rank. A novel bandit algorithm, coined TOFU (Tensor Optimism in the Face of Uncertainty), is developed. TOFU first leverages flexible tensor regression techniques to estimate low-dimensional subspaces associated with the system tensor. These estimates are then utilized to convert the original problem to a new one with norm constraints on its system parameters. Lastly, a norm-constrained bandit subroutine is adopted by TOFU, which utilizes these constraint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102; sFML &#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03874</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#26144;&#23556;&#31639;&#23376;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Stochastic Dynamical System via Flow Map Operator. (arXiv:2305.03874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102; sFML &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#12290;&#31216;&#20026;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#36825;&#20010;&#26032;&#26694;&#26550;&#26159;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;FML&#65289;&#30340;&#25193;&#23637;&#65292;&#21518;&#32773;&#26159;&#20026;&#20102;&#23398;&#20064;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#32780;&#24320;&#21457;&#30340;&#12290;&#23545;&#20110;&#23398;&#20064;&#38543;&#26426;&#31995;&#32479;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38543;&#26426;&#27969;&#26144;&#23556;&#65292;&#23427;&#26159;&#20004;&#20010;&#23376;&#27969;&#26144;&#23556;&#30340;&#21472;&#21152;&#65306;&#19968;&#20010;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#21644;&#19968;&#20010;&#38543;&#26426;&#23376;&#26144;&#23556;&#12290;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#39318;&#20808;&#29992;&#20110;&#26500;&#24314;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#65292;&#28982;&#21518;&#26159;&#38543;&#26426;&#23376;&#26144;&#23556;&#12290;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#37319;&#29992;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#24418;&#24335;&#65292;&#31867;&#20284;&#20110;FML&#23545;&#20110;&#30830;&#23450;&#24615;&#31995;&#32479;&#30340;&#24037;&#20316;&#12290;&#23545;&#20110;&#38543;&#26426;&#23376;&#26144;&#23556;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26412;&#25991;&#20013;&#24212;&#29992;&#12290;&#26368;&#32456;&#26500;&#24314;&#30340;&#38543;&#26426;&#27969;&#26144;&#23556;&#23450;&#20041;&#20102;&#19968;&#20010;&#38543;&#26426;&#28436;&#21270;&#27169;&#22411;&#65292;&#23427;&#22312;&#20998;&#24067;&#26041;&#38754;&#26159;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#24369;&#36817;&#20284;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;sFML&#25581;&#31034;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#21508;&#31181;&#31867;&#22411;&#30340;&#38750;&#32447;&#24615;&#12289;&#22122;&#22768;&#21327;&#26041;&#24046;&#32467;&#26500;&#21644;&#26102;&#38388;&#30456;&#20851;&#29305;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a numerical framework for learning unknown stochastic dynamical systems using measurement data. Termed stochastic flow map learning (sFML), the new framework is an extension of flow map learning (FML) that was developed for learning deterministic dynamical systems. For learning stochastic systems, we define a stochastic flow map that is a superposition of two sub-flow maps: a deterministic sub-map and a stochastic sub-map. The stochastic training data are used to construct the deterministic sub-map first, followed by the stochastic sub-map. The deterministic sub-map takes the form of residual network (ResNet), similar to the work of FML for deterministic systems. For the stochastic sub-map, we employ a generative model, particularly generative adversarial networks (GANs) in this paper. The final constructed stochastic flow map then defines a stochastic evolution model that is a weak approximation, in term of distribution, of the unknown stochastic system. A comprehensive set
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23454;&#38469;&#39046;&#22495;&#20013;&#31163;&#32447;&#35757;&#32451;&#25110;&#22686;&#38271;&#25209;&#37327;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03870</link><description>&lt;p&gt;
&#22312;&#22686;&#38271;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning. (arXiv:2305.03870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23454;&#38469;&#39046;&#22495;&#20013;&#31163;&#32447;&#35757;&#32451;&#25110;&#22686;&#38271;&#25209;&#37327;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#21521;&#23398;&#20064;&#32773;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#26631;&#20934;&#26041;&#27861;&#21033;&#29992;&#20102;&#26234;&#33021;&#20307;&#19981;&#26029;&#19982;&#20854;&#29615;&#22659;&#20132;&#20114;&#21644;&#25913;&#21892;&#20854;&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#12289;&#20262;&#29702;&#21644;&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#35797;&#38169;&#23454;&#39564;&#26041;&#27861;&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65289;&#20013;&#24448;&#24448;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#25511;&#21046;&#31574;&#30053;&#36890;&#24120;&#26159;&#36890;&#36807;&#20197;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20026;&#22522;&#30784;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#25110;&#36880;&#27493;&#25193;&#23637;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#22266;&#23450;&#30340;&#31574;&#30053;&#34987;&#37096;&#32626;&#21040;&#29615;&#22659;&#20013;&#65292;&#24182;&#29992;&#20110;&#25910;&#38598;&#19968;&#25972;&#20010;&#25209;&#27425;&#30340;&#26032;&#25968;&#25454;&#65292;&#28982;&#21518;&#19982;&#36807;&#21435;&#30340;&#25209;&#27425;&#27719;&#24635;&#24182;&#29992;&#20110;&#26356;&#26032;&#31574;&#30053;&#12290;&#21487;&#20197;&#22810;&#27425;&#37325;&#22797;&#36825;&#20010;&#25913;&#36827;&#21608;&#26399;&#12290;&#34429;&#28982;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#26377;&#38480;&#25968;&#37327;&#30340;&#36825;&#26679;&#30340;&#21608;&#26399;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#20135;&#29983;&#30340;&#25968;&#25454;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#36828;&#36828;&#20302;&#20110;&#26631;&#20934;&#30340;&#19981;&#26029;&#20132;&#20114;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#19982;&#20154;&#31867;&#19987;&#23478;&#21327;&#20316;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches to sequential decision-making exploit an agent's ability to continually interact with its environment and improve its control policy. However, due to safety, ethical, and practicality constraints, this type of trial-and-error experimentation is often infeasible in many real-world domains such as healthcare and robotics. Instead, control policies in these domains are typically trained offline from previously logged data or in a growing-batch manner. In this setting a fixed policy is deployed to the environment and used to gather an entire batch of new data before being aggregated with past batches and used to update the policy. This improvement cycle can then be repeated multiple times. While a limited number of such cycles is feasible in real-world domains, the quantity and diversity of the resulting data are much lower than in the standard continually-interacting approach. However, data collection in these domains is often performed in conjunction with human expert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22312;&#32447;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#21644;&#20256;&#32479;&#30340;&#38750;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03866</link><description>&lt;p&gt;
&#24102;&#26377;Hebbian&#21487;&#22609;&#24615;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks with Hebbian plasticity for unsupervised representation learning. (arXiv:2305.03866v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22312;&#32447;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#21644;&#20256;&#32479;&#30340;&#38750;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26080;&#30417;&#30563;&#36807;&#31243;&#20013;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#24067;&#24335;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#38750;&#33033;&#20914;&#21069;&#39304;&#36125;&#21494;&#26031;&#32622;&#20449;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#65288;BCPNN&#65289;&#27169;&#22411;&#36716;&#21270;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;Hebbian-Bayesian&#23398;&#20064;&#21644;&#37325;&#36830;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20808;&#21069;&#24050;&#34920;&#29616;&#20986;&#25191;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#33033;&#20914;&#27169;&#22411;&#37319;&#29992;&#27850;&#26494;&#32479;&#35745;&#21644;&#20302;&#21457;&#28779;&#29575;&#19982;&#22312;&#20307;&#26143;&#24418;&#31070;&#32463;&#20803;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#33033;&#20914;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#38750;&#33033;&#20914;BCPNN&#25509;&#36817;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#22522;&#20110;Hebbian&#30340;&#33033;&#20914;&#32593;&#32476;&#22312;MNIST&#21644;F-MNIST&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel spiking neural network model for learning distributed internal representations from data in an unsupervised procedure. We achieved this by transforming the non-spiking feedforward Bayesian Confidence Propagation Neural Network (BCPNN) model, employing an online correlation-based Hebbian-Bayesian learning and rewiring mechanism, shown previously to perform representation learning, into a spiking neural network with Poisson statistics and low firing rate comparable to in vivo cortical pyramidal neurons. We evaluated the representations learned by our spiking model using a linear classifier and show performance close to the non-spiking BCPNN, and competitive with other Hebbian-based spiking networks when trained on MNIST and F-MNIST machine learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#36719;&#20214;&#23454;&#29616;&#30340;&#33258;&#21160;&#24494;&#20998;&#23384;&#22312;&#19981;&#21487;&#25511;&#30340;&#35745;&#31639;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.03863</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#20214;&#30340;&#33258;&#21160;&#24494;&#20998;&#23384;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Software-based Automatic Differentiation is Flawed. (arXiv:2305.03863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03863
&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23454;&#29616;&#30340;&#33258;&#21160;&#24494;&#20998;&#23384;&#22312;&#19981;&#21487;&#25511;&#30340;&#35745;&#31639;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#36719;&#20214;&#23581;&#35797;&#37319;&#29992;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#30340;&#24605;&#24819;&#65292;&#23454;&#29616;&#38142;&#24335;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26469;&#23454;&#29616;&#25152;&#35859;&#30340;&#33258;&#21160;&#24494;&#20998;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#22312;&#27714;&#20540;&#20043;&#21069;&#26080;&#27861;&#31616;&#21270;&#34920;&#36798;&#24335;&#65288;&#36890;&#36807;&#38142;&#24335;&#35268;&#21017;&#33719;&#24471;&#30340;&#65289;&#65292;&#23548;&#33268;&#35745;&#31639;&#32467;&#26524;&#30340;&#35823;&#24046;&#24448;&#24448;&#26080;&#38480;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various software efforts embrace the idea that object oriented programming enables a convenient implementation of the chain rule, facilitating so-called automatic differentiation via backpropagation. Such frameworks have no mechanism for simplifying the expressions (obtained via the chain rule) before evaluating them. As we illustrate below, the resulting errors tend to be unbounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.03859</link><description>&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#65306;&#20197;&#33521;&#22269;COVID-19&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#22270;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#31639;&#27861;&#25552;&#20379;&#30340;&#22240;&#26524;&#34920;&#31034;&#20351;&#24471;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20851;&#32852;&#24615;&#26426;&#22120;&#23398;&#20064;&#30456;&#27604;&#65292;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#21508;&#31181;&#20844;&#20849;&#26469;&#28304;&#25972;&#21512;&#25968;&#25454;&#65292;&#24182;&#30740;&#31350;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21450;&#31639;&#27861;&#32452;&#20135;&#29983;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22270;&#24418;&#32467;&#26500;&#12289;&#27169;&#22411;&#32500;&#24230;&#12289;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#28151;&#28102;&#21464;&#37327;&#12289;&#39044;&#27979;&#21644;&#24178;&#39044;&#25512;&#26029;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#31361;&#20986;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#23398;&#20064;&#38477;&#38454;&#36816;&#21160;&#23398;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.03846</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#23398;&#20064;&#38477;&#38454;&#36816;&#21160;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data-Free Learning of Reduced-Order Kinematics. (arXiv:2305.03846v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#23398;&#20064;&#38477;&#38454;&#36816;&#21160;&#23398;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#29289;&#29702;&#31995;&#32479;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#31995;&#32479;&#20174;&#24377;&#24615;&#20307;&#21040;&#36816;&#21160;&#36830;&#26438;&#37117;&#23450;&#20041;&#22312;&#39640;&#32500;&#37197;&#32622;&#31354;&#38388;&#19978;&#65292;&#20294;&#23427;&#20204;&#30340;&#20856;&#22411;&#20302;&#33021;&#37197;&#32622;&#38598;&#20013;&#22312;&#26356;&#20302;&#32500;&#30340;&#23376;&#31354;&#38388;&#20013;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35782;&#21035;&#27492;&#31867;&#23376;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical systems ranging from elastic bodies to kinematic linkages are defined on high-dimensional configuration spaces, yet their typical low-energy configurations are concentrated on much lower-dimensional subspaces. This work addresses the challenge of identifying such subspaces automatically: given as input an energy function for a high-dimensional system, we produce a low-dimensional map whose image parameterizes a diverse yet low-energy submanifold of configurations. The only additional input needed is a single seed configuration for the system to initialize our procedure; no dataset of trajectories is required. We represent subspaces as neural networks that map a low-dimensional latent vector to the full configuration space, and propose a training scheme to fit network parameters to any system of interest. This formulation is effective across a very general range of physical systems; our experiments demonstrate not only nonlinear and very low-dimensional elastic body and cloth s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#27169;&#25311;&#20013;&#26174;&#31034;&#20986;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03835</link><description>&lt;p&gt;
&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Transformer for Stock Movement Prediction. (arXiv:2305.03835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03835
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#27169;&#25311;&#20013;&#26174;&#31034;&#20986;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24066;&#22330;&#26159;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#22320;&#26041;&#65292;&#22914;&#26524;&#26102;&#38388;&#25484;&#25569;&#24471;&#24403;&#65292;&#25237;&#36164;&#32773;&#21487;&#20197;&#33719;&#24471;&#24040;&#22823;&#30340;&#21033;&#28070;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#24577;&#12289;&#38750;&#32447;&#24615;&#29305;&#24615;&#20351;&#24471;&#26410;&#26469;&#20215;&#26684;&#36208;&#21183;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STST&#65292;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ACL18&#21644;KDD17&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;63.707%&#21644;56.879%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27169;&#25311;&#20013;&#29992;&#20110;&#30830;&#23450;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#30340;&#26368;&#23567;&#24180;&#21270;&#25910;&#30410;&#29575;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#39640;&#20986;10.41%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41% higher profit than the S&amp;P500 stock index, with a minimum annualized 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#25552;&#39640;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;
&lt;/p&gt;
&lt;p&gt;
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#22122;&#22768;&#22810;&#30340;&#21307;&#30103;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31934;&#20934;&#21307;&#30103;&#26088;&#22312;&#26681;&#25454;&#20010;&#20307;&#30340;&#29420;&#29305;&#25104;&#20687;&#29305;&#24449;&#20010;&#24615;&#21270;&#35786;&#30103;&#20915;&#31574;&#65292;&#20197;&#25913;&#21892;&#20854;&#20020;&#24202;&#32467;&#26524;&#12290;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20316;&#20026;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23558;&#26356;&#21152;&#23433;&#20840;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#20934;&#21307;&#30103;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#39564;&#35777;&#25351;&#26631;&#12290;&#26412;&#25991;&#37319;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#22810;&#31181;&#27835;&#30103;&#30340;&#22240;&#26524;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#20272;&#35745;&#27599;&#31181;&#27835;&#30103;&#36873;&#39033;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20219;&#24847;&#20004;&#31181;&#27835;&#30103;&#20043;&#38388;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#23545;&#24739;&#26377;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#30340;&#24739;&#32773;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20197;&#39044;&#27979;&#26032;&#30340;&#21644;&#25193;&#22823;&#30340;T2&#30149;&#21464;&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;</title><link>http://arxiv.org/abs/2305.03827</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#29992;&#20110;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#32852;&#21512;&#25277;&#21462;&#12290;&#36890;&#36807;&#25506;&#32034;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#25105;&#38598;&#25104;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#24471;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#24182;&#19988;&#32531;&#35299;&#20102;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#24102;&#26377;&#27169;&#31946;&#25110;&#22122;&#22768;&#26631;&#31614;&#30340;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#26102;&#65292;&#32852;&#21512;&#25277;&#21462;&#23454;&#20307;&#23545;&#21450;&#20854;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;Bootstrap&#23398;&#20064;&#65292;&#20854;&#21160;&#26426;&#26159;&#26681;&#25454;&#30452;&#35273;&#65292;&#19968;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#39640;&#65292;&#27169;&#22411;&#32622;&#20449;&#24230;&#19982;&#30495;&#23454;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#21487;&#33021;&#24615;&#23601;&#36234;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#23454;&#20363;&#32423;&#21035;&#30340;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#32622;&#20449;&#30340;&#21021;&#22987;&#26679;&#20363;&#38598;&#12290;&#36825;&#26679;&#30340;&#23376;&#38598;&#29992;&#20110;&#36807;&#28388;&#22122;&#22768;&#23454;&#20363;&#65292;&#24182;&#26377;&#21161;&#20110;&#27169;&#22411;&#22312;&#26089;&#26399;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;Bootstrap&#23398;&#20064;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20943;&#36731;&#22122;&#22768;&#26631;&#31614;&#20135;&#29983;&#30340;&#27169;&#22411;&#38388;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#32852;&#21512;&#26631;&#35760;&#27010;&#29575;&#30340;&#27010;&#29575;&#26041;&#24046;&#65292;&#20197;&#20272;&#35745;&#20869;&#37096;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#36873;&#25321;&#21644;&#24314;&#31435;&#26032;&#30340;&#21487;&#38752;&#35757;&#32451;&#23454;&#20363;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#23454;&#20307;&#23545;&#25277;&#21462;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;F1&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;4.43%&#21644;4.92%&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;CUQB&#65292;&#26469;&#35299;&#20915;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#36827;&#34892;&#20102;&#26368;&#20248;&#25511;&#21046;&#30340;&#27969;&#20307;&#27969;&#37327;&#21644;&#25299;&#25169;&#32467;&#26500;&#20248;&#21270;&#65292;&#21518;&#32773;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.03824</link><description>&lt;p&gt;
&#26080;&#36951;&#25022;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#21644;&#26114;&#36149;&#28151;&#21512;&#27169;&#22411;&#30340;&#24046;&#20998;&#20998;&#20301;&#25968;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations. (arXiv:2305.03824v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;CUQB&#65292;&#26469;&#35299;&#20915;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#22343;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#36827;&#34892;&#20102;&#26368;&#20248;&#25511;&#21046;&#30340;&#27969;&#20307;&#27969;&#37327;&#21644;&#25299;&#25169;&#32467;&#26500;&#20248;&#21270;&#65292;&#21518;&#32773;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#20989;&#25968;&#65288;&#28151;&#21512;&#27169;&#22411;&#65289;&#30340;&#39640;&#25928;&#32422;&#26463;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#30340;&#36755;&#20837;&#26159;&#20855;&#26377;&#30690;&#37327;&#20540;&#36755;&#20986;&#21644;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#65292;&#36825;&#22312;&#23454;&#38469;&#30340;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#21046;&#36896;&#21644;&#25511;&#21046;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Constrained Upper Quantile Bound&#65288;CUQB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#31181;&#38382;&#39064;&#65292;&#30452;&#25509;&#21033;&#29992;&#20102;&#25105;&#20204;&#23637;&#31034;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#30340;&#22797;&#21512;&#32467;&#26500;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;CUQB&#30340;&#27010;&#24565;&#31616;&#21333;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#26041;&#27861;&#25152;&#20351;&#29992;&#30340;&#32422;&#26463;&#36924;&#36817;&#12290;&#34429;&#28982;CUQB&#30340;&#25910;&#36141;&#20989;&#25968;&#19981;&#22312;&#23553;&#38381;&#24418;&#24335;&#20013;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#38543;&#26426;&#36924;&#36817;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#20986;&#20102;&#23545;&#20110;&#32047;&#31215;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#30028;&#38480;&#12290;&#30001;&#20110;&#22312;&#26576;&#20123;&#35268;&#21017;&#20551;&#35774;&#19979;&#36825;&#20123;&#30028;&#38480;&#23545;&#36845;&#20195;&#27425;&#25968;&#20855;&#26377;&#27425;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26080;&#36951;&#25022;&#24182;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#23637;&#31034;&#20102;CUQB&#30340;&#21151;&#25928;&#65292;&#21253;&#25324;&#26725;&#26550;&#25299;&#25169; - &#22312;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#32467;&#26500;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#24378;2&#20493; - &#20197;&#21450;&#27969;&#20307;&#27969;&#37327;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#20102;3&#20493;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of efficient constrained global optimization of composite functions (hybrid models) whose input is an expensive black-box function with vector-valued outputs and noisy observations, which often arises in real-world science, engineering, manufacturing, and control applications. We propose a novel algorithm, Constrained Upper Quantile Bound (CUQB), to solve such problems that directly exploits the composite structure of the objective and constraint functions that we show leads substantially improved sampling efficiency. CUQB is conceptually simple and avoids the constraint approximations used by previous methods. Although the CUQB acquisition function is not available in closed form, we propose a novel differentiable stochastic approximation that enables it to be efficiently maximized. We further derive bounds on the cumulative regret and constraint violation. Since these bounds depend sublinearly on the number of iterations under some regularity assum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#24555;&#36895;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26631;&#35760;&#22823;&#35268;&#27169;&#21151;&#33021;&#36830;&#25509;&#27169;&#24335;(&#38745;&#24687;&#29366;&#24577;&#32593;&#32476;)&#65292;&#20854;&#21487;&#29992;&#20110;&#26415;&#21069;&#35268;&#21010;&#25351;&#23548;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#24615;&#21644;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03814</link><description>&lt;p&gt;
fMRI&#33041;&#32593;&#32476;&#30340;&#28145;&#24230;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Deep Labeling of fMRI Brain Networks. (arXiv:2305.03814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#12289;&#24555;&#36895;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26631;&#35760;&#22823;&#35268;&#27169;&#21151;&#33021;&#36830;&#25509;&#27169;&#24335;(&#38745;&#24687;&#29366;&#24577;&#32593;&#32476;)&#65292;&#20854;&#21487;&#29992;&#20110;&#26415;&#21069;&#35268;&#21010;&#25351;&#23548;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#24615;&#21644;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38745;&#24687;&#29366;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;(RS-fMRI)&#20013;&#25552;&#21462;&#20986;&#30340;&#33041;&#38745;&#24687;&#29366;&#24577;&#32593;&#32476;(RSNs)&#65292;&#34987;&#29992;&#20110;&#26415;&#21069;&#35268;&#21010;&#25351;&#23548;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#65292;&#28982;&#32780;&#65292;&#19987;&#19994;&#30693;&#35782;&#38656;&#35201;&#29992;&#20110;&#26631;&#35760;&#27599;&#20010;RSN&#65292;&#24182;&#32570;&#20047;&#26377;&#25928;&#21644;&#26631;&#20934;&#21270;&#30340;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26631;&#35760;&#38745;&#24687;&#29366;&#24577;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#32676;&#32452;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;(IC/A)&#25552;&#21462;&#22823;&#35268;&#27169;&#30340;&#21151;&#33021;&#36830;&#25509;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;&#22238;&#24402;&#23558;&#23427;&#20204;&#25237;&#23556;&#21040;&#20010;&#20307;&#20027;&#20307;RSN&#19978;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#19982;2D&#21644;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;MLP&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12290;&#23613;&#31649;&#23427;&#27604;&#20854;&#20182;&#20316;&#21697;&#34920;&#29616;&#24471;&#22909;&#25110;&#26356;&#22909;&#65292;&#20294;&#25105;&#20204;&#30340;MLP&#26041;&#27861;&#26159;&#21487;&#25512;&#24191;&#30340;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#19981;&#21516;&#30340;&#37319;&#38598;&#25216;&#26415;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resting State Networks (RSNs) of the brain extracted from Resting State functional Magnetic Resonance Imaging (RS-fMRI) are used in the pre-surgical planning to guide the neurosurgeon. This is difficult, though, as expert knowledge is required to label each of the RSNs. There is a lack of efficient and standardized methods to be used in clinical workflows. Additionally, these methods need to be generalizable since the method needs to work well regardless of the acquisition technique. We propose an accurate, fast, and lightweight deep learning approach to label RSNs. Group Independent Component Analysis (ICA) was used to extract large scale functional connectivity patterns in the cohort and dual regression was used to back project them on individual subject RSNs. We compare a Multi-Layer Perceptron (MLP) based method with 2D and 3D Convolutional Neural Networks (CNNs) and find that the MLP is faster and more accurate. The MLP method performs as good or better than other works despite it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#27169;&#25311;Kondo&#26230;&#26684;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#26059;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#37325;&#29616;&#19977;&#35282;&#26230;&#26684;&#20013;&#22825;&#31354;&#23376;&#26230;&#20307;&#30456;&#21464;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03804</link><description>&lt;p&gt;
&#22522;&#20110;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#28418;&#27850;&#30913;&#20307;&#33258;&#26059;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Equivariant Neural Networks for Spin Dynamics Simulations of Itinerant Magnets. (arXiv:2305.03804v1 [cond-mat.str-el])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#27169;&#25311;Kondo&#26230;&#26684;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#26059;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#37325;&#29616;&#19977;&#35282;&#26230;&#26684;&#20013;&#22825;&#31354;&#23376;&#26230;&#20307;&#30456;&#21464;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#27169;&#25311;Kondo&#26230;&#26684;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#26059;&#21160;&#21147;&#23398;&#12290;&#35813;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#30001;&#22522;&#20110;&#24352;&#37327;&#31215;&#30340;&#21367;&#31215;&#23618;&#26500;&#25104;&#65292;&#24182;&#30830;&#20445;&#20004;&#20010;&#31561;&#21464;&#24615;&#65306;&#26230;&#26684;&#30340;&#24179;&#31227;&#21644;&#33258;&#26059;&#30340;&#26059;&#36716;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#23545;&#20108;&#32500;&#27491;&#26041;&#24418;&#21644;&#19977;&#35282;&#24418;&#26230;&#26684;&#19978;&#20004;&#31181;Kondo&#26230;&#26684;&#27169;&#22411;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#22312;&#27491;&#26041;&#24418;&#26230;&#26684;&#30340;&#31561;&#21464;&#27169;&#22411;&#20013;&#65292;&#39564;&#35777;&#35823;&#24046;&#65288;&#22522;&#20110;&#22343;&#26041;&#26681;&#35823;&#24046;&#65289;&#30456;&#23545;&#20110;&#20351;&#29992;&#19981;&#21464;&#25551;&#36848;&#31526;&#20316;&#20026;&#36755;&#20837;&#30340;&#27169;&#22411;&#20943;&#23569;&#20102;&#19977;&#20998;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#37325;&#29616;&#19977;&#35282;&#26230;&#26684;&#20013;&#22825;&#31354;&#23376;&#26230;&#20307;&#30456;&#21464;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
I present a novel equivariant neural network architecture for the large-scale spin dynamics simulation of the Kondo lattice model. This neural network mainly consists of tensor-product-based convolution layers and ensures two equivariances: translations of the lattice and rotations of the spins. I implement equivariant neural networks for two Kondo lattice models on two-dimensional square and triangular lattices, and perform training and validation. In the equivariant model for the square lattice, the validation error (based on root mean squared error) is reduced to less than one-third compared to a model using invariant descriptors as inputs. Furthermore, I demonstrate the ability to reproduce phase transitions of skyrmion crystals in the triangular lattice, by performing dynamics simulations using the trained model.
&lt;/p&gt;</description></item><item><title>&#26448;&#26009;&#20449;&#24687;&#23398;&#26159;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#31532;&#22235;&#20010;&#33539;&#24335;&#65292;&#36890;&#36807;&#26448;&#26009;&#29305;&#24615;&#25351;&#32441;&#21644;&#32479;&#35745;&#25512;&#26029;&#23398;&#20064;&#29702;&#35770;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12289;&#36923;&#36753;&#20844;&#29702;&#21644;&#25512;&#29702;&#20449;&#24687;&#31185;&#23398;&#31561;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#26426;&#21322;&#23548;&#20307;&#30340;&#26032;&#22411;&#21457;&#29616;&#21644;&#30693;&#35782;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.03797</link><description>&lt;p&gt;
&#26448;&#26009;&#20449;&#24687;&#23398;&#65306;&#31639;&#27861;&#35774;&#35745;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials Informatics: An Algorithmic Design Rule. (arXiv:2305.03797v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03797
&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#20449;&#24687;&#23398;&#26159;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#31532;&#22235;&#20010;&#33539;&#24335;&#65292;&#36890;&#36807;&#26448;&#26009;&#29305;&#24615;&#25351;&#32441;&#21644;&#32479;&#35745;&#25512;&#26029;&#23398;&#20064;&#29702;&#35770;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12289;&#36923;&#36753;&#20844;&#29702;&#21644;&#25512;&#29702;&#20449;&#24687;&#31185;&#23398;&#31561;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#26426;&#21322;&#23548;&#20307;&#30340;&#26032;&#22411;&#21457;&#29616;&#21644;&#30693;&#35782;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#20449;&#24687;&#23398;&#26159;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#31532;&#22235;&#20010;&#33539;&#24335;&#65292;&#26159;&#20256;&#32479;&#32463;&#39564;&#26041;&#27861;&#12289;&#29702;&#35770;&#31185;&#23398;&#21644;&#35745;&#31639;&#30740;&#31350;&#20043;&#21518;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#26041;&#27861;&#12290;&#26448;&#26009;&#20449;&#24687;&#23398;&#30340;&#22522;&#26412;&#35201;&#32032;&#26377;&#20004;&#20010;&#65306;&#26448;&#26009;&#29305;&#24615;&#25351;&#32441;&#21644;&#32479;&#35745;&#25512;&#26029;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12289;&#36923;&#36753;&#20844;&#29702;&#21644;&#25512;&#29702;&#20449;&#24687;&#31185;&#23398;&#31561;&#26041;&#27861;&#65292;&#36890;&#36807;&#26448;&#26009;&#20449;&#24687;&#23398;&#26041;&#27861;&#30740;&#31350;&#20102;&#26377;&#26426;&#21322;&#23548;&#20307;&#30340;&#35868;&#22242;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#21322;&#23548;&#20307;&#24037;&#19994;&#30340;&#26032;&#22411;&#26377;&#26426;&#21322;&#23548;&#20307;&#21457;&#29616;&#21644;&#26448;&#26009;&#31185;&#23398;&#30028;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#25105;&#20204;&#36824;&#23545;&#26448;&#26009;&#20449;&#24687;&#23398;&#25968;&#25454;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#25299;&#25169;&#26041;&#24335;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials informatics, data-enabled investigation, is a "fourth paradigm" in materials science research after the conventional empirical approach, theoretical science, and computational research. Materials informatics has two essential ingredients: fingerprinting materials proprieties and the theory of statistical inference and learning. We have researched the organic semiconductor's enigmas through the materials informatics approach. By applying diverse neural network topologies, logical axiom, and inferencing information science, we have developed data-driven procedures for novel organic semiconductor discovery for the semiconductor industry and knowledge extraction for the materials science community. We have reviewed and corresponded with various algorithms for the neural network design topology for the materials informatics dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFSP&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#31616;&#21333;&#30340;&#26631;&#31614;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#65292;&#24182;&#22312;&#32473;&#23450;&#27880;&#37322;&#21518;&#65292;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03793</link><description>&lt;p&gt;
&#37319;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#26412;&#20307;&#21644;&#31616;&#21333;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#24103;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels. (arXiv:2305.03793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFSP&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#31616;&#21333;&#30340;&#26631;&#31614;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#65292;&#24182;&#22312;&#32473;&#23450;&#27880;&#37322;&#21518;&#65292;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24103;&#35821;&#20041;&#35299;&#26512;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24403;&#21069;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25104;&#21151;&#22320;&#35782;&#21035;&#29992;&#25143;&#36755;&#20837;&#35805;&#35821;&#20013;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#12290;&#36825;&#22312;&#23558;&#26032;&#39046;&#22495;&#28155;&#21152;&#21040;&#34394;&#25311;&#21161;&#25163;&#21151;&#33021;&#26102;&#20250;&#20135;&#29983;&#37325;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFSP&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;NLP&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#31616;&#21333;&#26631;&#31614;&#20013;&#26041;&#20415;&#22320;&#21019;&#24314;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21019;&#24314;&#19968;&#20010;&#23567;&#32780;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#12289;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#25554;&#27133;&#31867;&#22411;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26032;&#39046;&#22495;&#30340;&#31616;&#21333;&#27880;&#37322;&#12290;&#22312;&#32473;&#23450;&#36825;&#26679;&#30340;&#27880;&#37322;&#21518;&#65292;&#20381;&#36182;&#20110;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#31639;&#27861;&#39044;&#27979;&#30001;&#32456;&#31471;&#29992;&#25143;&#23450;&#20041;&#30340;&#39046;&#22495;&#30340;&#24847;&#22270;&#21644;&#25554;&#27133;&#12290;&#22312;TopV2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36825;&#31181;&#31616;&#21333;&#26631;&#31614;&#35774;&#32622;&#19979;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frame semantic parsing is an important component of task-oriented dialogue systems. Current models rely on a significant amount training data to successfully identify the intent and slots in the user's input utterance. This creates a significant barrier for adding new domains to virtual assistant capabilities, as creation of this data requires highly specialized NLP expertise. In this work we propose OpenFSP, a framework that allows for easy creation of new domains from a handful of simple labels that can be generated without specific NLP knowledge. Our approach relies on creating a small, but expressive, set of domain agnostic slot types that enables easy annotation of new domains. Given such annotation, a matching algorithm relying on sentence encoders predicts the intent and slots for domains defined by end-users. Extensive experiments on the TopV2 dataset shows that our model outperforms strong baselines in this simple labels setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;"EE-Net"&#65292;&#23427;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#21033;&#29992;&#21644;&#25506;&#32034;&#65292;&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#21516;&#26102;&#20063;&#36866;&#24212;&#24615;&#22320;&#23398;&#20064;&#28508;&#22312;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.03784</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#19982;&#25506;&#32034;&#30340;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Exploitation and Exploration of Contextual Bandits. (arXiv:2305.03784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;"EE-Net"&#65292;&#23427;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#21033;&#29992;&#21644;&#25506;&#32034;&#65292;&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#21516;&#26102;&#20063;&#36866;&#24212;&#24615;&#22320;&#23398;&#20064;&#28508;&#22312;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#21033;&#29992;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"EE-Net"&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#21644;&#25506;&#32034;&#31574;&#30053;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#21033;&#29992;&#32593;&#32476;&#65289;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#25506;&#32034;&#32593;&#32476;&#65289;&#26469;&#36866;&#24212;&#24615;&#22320;&#23398;&#20064;&#30456;&#23545;&#20110;&#24403;&#21069;&#20272;&#35745;&#22870;&#21169;&#30340;&#28508;&#22312;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#30693;&#35782;&#30340;&#12289;&#36793;&#30028;&#26465;&#20214;&#24863;&#30693;&#30340;&#12289;&#23616;&#37096;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;E2C&#21644;E2CO&#27169;&#22411;&#25193;&#23637;&#21040;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.03774</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#30340;&#23616;&#37096;&#21270;&#23398;&#20064;&#29992;&#20110;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#30693;&#35782;&#30340;&#12289;&#36793;&#30028;&#26465;&#20214;&#24863;&#30693;&#30340;&#12289;&#23616;&#37096;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;E2C&#21644;E2CO&#27169;&#22411;&#25193;&#23637;&#21040;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#20197;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#26032;&#33021;&#28304;&#35299;&#20915;&#26041;&#26696;&#30340;&#36861;&#27714;&#65292;&#22914;&#22320;&#28909;&#21644;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#35745;&#21010;&#65292;&#23545;&#24403;&#21069;&#20808;&#36827;&#30340;&#22320;&#19979;&#27969;&#20307;&#27169;&#25311;&#22120;&#20135;&#29983;&#20102;&#26032;&#30340;&#38656;&#27714;&#12290;&#35201;&#27714;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#21516;&#26102;&#27169;&#25311;&#22823;&#37327;&#20648;&#23618;&#29366;&#24577;&#65292;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#24320;&#36767;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23616;&#37096;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#21644;&#36793;&#30028;&#26465;&#20214;&#24847;&#35782;&#65292;&#23558;&#23884;&#20837;&#24335;&#25511;&#21046;&#65288;E2C&#65289;&#21644;&#23884;&#20837;&#24335;&#25511;&#21046;&#21644;&#35266;&#23519;&#65288;E2CO&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#23398;&#20064;&#23545;&#27969;&#25193;&#25955;&#21453;&#24212;&#31995;&#32479;&#20013;&#20840;&#23616;&#29366;&#24577;&#21464;&#37327;&#30340;&#23616;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20648;&#23618;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#20197;&#21482;&#20351;&#29992;&#23569;&#37327;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#24230;&#20934;&#30830;&#22320;&#39044;&#27979;&#31995;&#32479;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#21516;&#26102;&#19982;&#21407;&#22987;&#30340;E2C&#21644;E2C
&lt;/p&gt;
&lt;p&gt;
The global push for new energy solutions, such as Geothermal, and Carbon Capture and Sequestration initiatives has thrust new demands upon the current state-of the-art subsurface fluid simulators. The requirement to be able to simulate a large order of reservoir states simultaneously in a short period of time has opened the door of opportunity for the application of machine learning techniques for surrogate modelling. We propose a novel physics-informed and boundary conditions-aware Localized Learning method which extends the Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn local representations of global state variables in an Advection-Diffusion Reaction system. We show that our model trained on reservoir simulation data is able to predict future states of the system, given a set of controls, to a great deal of accuracy with only a fraction of the available information, while also reducing training times significantly compared to the original E2C and E2C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;CWoLa&#26469;&#35782;&#21035;&#38134;&#27827;&#31995;&#20013;&#30340;&#20919;&#20957;&#32858;&#27969;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#27969;&#25110;&#20102;&#35299;&#22825;&#20307;&#29289;&#29702;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35782;&#21035;&#23616;&#37096;&#24322;&#24120;&#30340;&#39046;&#22495;&#65292;&#24182;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03761</link><description>&lt;p&gt;
&#38134;&#27827;&#31995;&#20013;&#30340;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Anomaly Detection in the Milky Way. (arXiv:2305.03761v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;CWoLa&#26469;&#35782;&#21035;&#38134;&#27827;&#31995;&#20013;&#30340;&#20919;&#20957;&#32858;&#27969;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#27969;&#25110;&#20102;&#35299;&#22825;&#20307;&#29289;&#29702;&#21407;&#29702;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35782;&#21035;&#23616;&#37096;&#24322;&#24120;&#30340;&#39046;&#22495;&#65292;&#24182;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#22825;&#20307;&#29289;&#29702;&#25968;&#25454;&#38598;&#20026;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#20256;&#32479;&#25628;&#32034;&#21487;&#33021;&#24573;&#30053;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;CWoLa&#30340;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#26469;&#30830;&#23450;&#30001;Gaia&#21355;&#26143;&#35266;&#27979;&#21040;&#30340;&#21313;&#20159;&#22810;&#39063;&#38134;&#27827;&#31995;&#24658;&#26143;&#20013;&#30340;&#20919;&#20957;&#32858;&#27969;&#12290;CWoLa&#25805;&#20316;&#19981;&#38656;&#35201;&#20351;&#29992;&#26631;&#35760;&#27969;&#25110;&#20102;&#35299;&#22825;&#20307;&#29289;&#29702;&#21407;&#29702;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#27604;&#20363;&#26410;&#30693;&#30340;&#28151;&#21512;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#35745;&#31639;&#36731;&#37327;&#32423;&#30340;&#31574;&#30053;&#33021;&#22815;&#26816;&#27979;&#20986;&#27169;&#25311;&#30340;&#27969;&#21644;&#24050;&#30693;&#30340;&#27969;GD-1&#12290;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#39640;&#33021;&#23545;&#25758;&#26426;&#29289;&#29702;&#65292;&#35813;&#25216;&#26415;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#20197;&#21450;&#20854;&#20182;&#26377;&#20852;&#36259;&#35782;&#21035;&#23616;&#37096;&#24322;&#24120;&#30340;&#39046;&#22495;&#21487;&#33021;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale astrophysics datasets present an opportunity for new machine learning techniques to identify regions of interest that might otherwise be overlooked by traditional searches. To this end, we use Classification Without Labels (CWoLa), a weakly-supervised anomaly detection method, to identify cold stellar streams within the more than one billion Milky Way stars observed by the Gaia satellite. CWoLa operates without the use of labeled streams or knowledge of astrophysical principles. Instead, we train a classifier to distinguish between mixed samples for which the proportions of signal and background samples are unknown. This computationally lightweight strategy is able to detect both simulated streams and the known stream GD-1 in data. Originally designed for high-energy collider physics, this technique may have broad applicability within astrophysics as well as other domains interested in identifying localized anomalies.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#38271;&#26399;&#21453;&#23556;&#21160;&#21147;&#23398;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#21516;&#21270;&#30340;&#20808;&#39564;&#65292;&#25968;&#25454;&#38598;&#20844;&#24320;&#21457;&#24067;&#65292;&#36866;&#29992;&#20110;Sentinel-2&#22810;&#20809;&#35889;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.03743</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#21516;&#21270;&#21644;&#39044;&#27979;&#30340;Sentinel-2&#21453;&#23556;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Sentinel-2 reflectance dynamics for data-driven assimilation and forecasting. (arXiv:2305.03743v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#38271;&#26399;&#21453;&#23556;&#21160;&#21147;&#23398;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#21516;&#21270;&#30340;&#20808;&#39564;&#65292;&#25968;&#25454;&#38598;&#20844;&#24320;&#21457;&#24067;&#65292;&#36866;&#29992;&#20110;Sentinel-2&#22810;&#20809;&#35889;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35206;&#30422;&#22320;&#29699;&#34920;&#38754;&#30340;&#22823;&#35268;&#27169;&#21355;&#26143;&#22810;&#20809;&#35889;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#24050;&#34987;&#20844;&#24320;&#29992;&#20110;&#31185;&#23398;&#30740;&#31350;&#65292;&#20363;&#22914;&#36890;&#36807;&#27431;&#27954;Copernicus&#39033;&#30446;&#12290;&#21516;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#36965;&#24863;&#30028;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#20351;&#24471;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#25554;&#20540;&#12289;&#39044;&#27979;&#25110;&#35299;&#28151;&#12290;&#27839;&#30528;&#36825;&#26465;&#32447;&#36335;&#65292;&#25105;&#20204;&#20197;Koopman&#31639;&#23376;&#29702;&#35770;&#20026;&#28789;&#24863;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#27169;&#25311;&#38271;&#26399;&#21453;&#23556;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#25968;&#25454;&#21516;&#21270;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;Sentinel-2&#22810;&#20809;&#35889;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#32452;&#25104;&#65292;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#22810;&#20010;&#32423;&#21035;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, massive amounts of satellite multispectral and hyperspectral images covering the Earth's surface have been made publicly available for scientific purpose, for example through the European Copernicus project. Simultaneously, the development of self-supervised learning (SSL) methods has sparked great interest in the remote sensing community, enabling to learn latent representations from unlabeled data to help treating downstream tasks for which there is few annotated examples, such as interpolation, forecasting or unmixing. Following this line, we train a deep learning model inspired from the Koopman operator theory to model long-term reflectance dynamics in an unsupervised way. We show that this trained model, being differentiable, can be used as a prior for data assimilation in a straightforward way. Our datasets, which are composed of Sentinel-2 multispectral image time series, are publicly released with several levels of treatment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03742</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31526;&#21495;&#32534;&#31243;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming. (arXiv:2305.03742v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#21487;&#38752;&#22320;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;&#26412;&#25991;&#22522;&#20110;&#31526;&#21495;&#32534;&#31243;&#30340;&#35270;&#35282;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DSR-LM&#65292;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340;LMs&#31649;&#29702;&#20107;&#23454;&#30693;&#35782;&#30340;&#24863;&#30693;&#65292;&#31526;&#21495;&#27169;&#22359;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#12290;&#19982;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#12290;DSR-LM&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20801;&#35768;&#36731;&#26494;&#38598;&#25104;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25903;&#25345;&#24191;&#27867;&#30340;&#31526;&#21495;&#32534;&#31243;&#65292;&#20197;&#31283;&#20581;&#22320;&#25512;&#20986;&#36923;&#36753;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSR-LM&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;20%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;DSR-LM&#36824;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#38382;&#39064;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR
&lt;/p&gt;</description></item><item><title>AmGCL&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#21644;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#23646;&#24615;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03741</link><description>&lt;p&gt;
AmGCL: &#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32570;&#22833;&#22270;&#29305;&#24449;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AmGCL: Feature Imputation of Attribute Missing Graph via Self-supervised Contrastive Learning. (arXiv:2305.03741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03741
&lt;/p&gt;
&lt;p&gt;
AmGCL&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#21644;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#23646;&#24615;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#22312;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#20351;&#29992;&#24191;&#27867;&#65292;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#23646;&#24615;&#22270;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#19981;&#23436;&#25972;&#30340;&#22270;&#25968;&#25454;&#21644;&#32570;&#22833;&#30340;&#33410;&#28857;&#23646;&#24615;&#21487;&#33021;&#20250;&#23545;&#23186;&#20307;&#30693;&#35782;&#21457;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#30340;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#20551;&#35774;&#25110;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#23646;&#24615;-&#22270;&#24418;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32570;&#22833;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;AmGCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#22270;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#33410;&#28857;&#23646;&#24615;&#12290;AmGCL&#21033;&#29992;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#29305;&#24449;&#39044;&#32534;&#30721;&#26469;&#23545;&#32570;&#22833;&#23646;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#22270;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#32467;&#26500;&#65288;GACLS&#65289;&#20174;&#32534;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AmGCL&#21033;&#29992;&#32467;&#26500;-&#23646;&#24615;&#33021;&#37327;&#26368;&#23567;&#21270;&#36827;&#34892;&#29305;&#24449;&#37325;&#26500;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#19979;&#38480;&#20197;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribute graphs are ubiquitous in multimedia applications, and graph representation learning (GRL) has been successful in analyzing attribute graph data. However, incomplete graph data and missing node attributes can have a negative impact on media knowledge discovery. Existing methods for handling attribute missing graph have limited assumptions or fail to capture complex attribute-graph dependencies. To address these challenges, we propose Attribute missing Graph Contrastive Learning (AmGCL), a framework for handling missing node attributes in attribute graph data. AmGCL leverages Dirichlet energy minimization-based feature precoding to encode in missing attributes and a self-supervised Graph Augmentation Contrastive Learning Structure (GACLS) to learn latent variables from the encoded-in data. Specifically, AmGCL utilizies feature reconstruction based on structure-attribute energy minimization while maximizes the lower bound of evidence for latent representation mutual information.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36965;&#24863;&#25968;&#25454;&#30340;&#39550;&#39542;&#39118;&#38505;&#39044;&#27979;&#26694;&#26550;&#65292;&#26368;&#22823;&#21270;&#21033;&#29992;&#24773;&#22659;&#20449;&#24687;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#22686;&#24378;&#24369;&#30340;&#39118;&#38505;&#26631;&#31614;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03740</link><description>&lt;p&gt;
&#22312;&#32972;&#26223;&#19979;&#36827;&#34892;&#39550;&#39542;&#39118;&#38505;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#36965;&#24863;&#25968;&#25454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Judge Me in Context: A Telematics-Based Driving Risk Prediction Framework in Presence of Weak Risk Labels. (arXiv:2305.03740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36965;&#24863;&#25968;&#25454;&#30340;&#39550;&#39542;&#39118;&#38505;&#39044;&#27979;&#26694;&#26550;&#65292;&#26368;&#22823;&#21270;&#21033;&#29992;&#24773;&#22659;&#20449;&#24687;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#22686;&#24378;&#24369;&#30340;&#39118;&#38505;&#26631;&#31614;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#39550;&#39542;&#39118;&#38505;&#39044;&#27979;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28909;&#28857;&#20043;&#19968;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39550;&#39542;&#39118;&#38505;&#24182;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#22312;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#21033;&#29992;&#26159;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20445;&#38505;&#35268;&#21010;&#20013;&#26377;&#24212;&#29992;&#65292;&#20294;&#26159;&#36890;&#36807;&#36825;&#26679;&#30340;&#31895;&#31890;&#24230;&#22240;&#32032;&#24456;&#38590;&#25429;&#25417;&#21040;&#30495;&#23454;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#36965;&#24863;&#25968;&#25454;&#30340;&#20351;&#29992;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#20197;&#21450;&#36965;&#24863;&#25968;&#25454;&#65292;&#32780;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#20197;&#21450;&#24773;&#22659;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#31867;&#22411;&#65289;&#26469;&#26500;&#24314;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#39118;&#38505;&#39044;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#36965;&#24863;&#25968;&#25454;&#24773;&#22659;&#21270;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#24320;&#21457;&#19968;&#20010;&#39118;&#38505;&#20998;&#31867;&#22120;&#65292;&#20551;&#35774;&#26377;&#19968;&#20123;&#24369;&#30340;&#39118;&#38505;&#26631;&#31614;&#21487;&#29992;&#65288;&#20363;&#22914;&#65292;&#36807;&#21435;&#30340;&#20132;&#36890;&#36829;&#31456;&#35760;&#24405;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26500;&#24314;&#39118;&#38505;&#20998;&#31867;&#22120;&#20043;&#21069;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#26469;&#22686;&#24378;&#24369;&#30340;&#39118;&#38505;&#26631;&#31614;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving risk prediction has been a topic of much research over the past few decades to minimize driving risk and increase safety. The use of demographic information in risk prediction is a traditional solution with applications in insurance planning, however, it is difficult to capture true driving behavior via such coarse-grained factors. Therefor, the use of telematics data has gained a widespread popularity over the past decade. While most of the existing studies leverage demographic information in addition to telematics data, our objective is to maximize the use of telematics as well as contextual information (e.g., road-type) to build a risk prediction framework with real-world applications. We contextualize telematics data in a variety of forms, and then use it to develop a risk classifier, assuming that there are some weak risk labels available (e.g., past traffic citation records). Before building a risk classifier though, we employ a novel data-driven process to augment weak r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#33521;&#29305;&#23572;Movidius VPU&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30828;&#20214;&#25104;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;VPU&#19978;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#21644;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#30340;&#21152;&#36895;&#21644;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03739</link><description>&lt;p&gt;
&#38754;&#21521;&#33521;&#29305;&#23572;Movidius VPU&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Intel Movidius VPU. (arXiv:2305.03739v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#33521;&#29305;&#23572;Movidius VPU&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30828;&#20214;&#25104;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;VPU&#19978;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#21644;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#30340;&#21152;&#36895;&#21644;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#27169;&#22411;&#35774;&#35745;&#65292;&#20197;&#28385;&#36275;&#32473;&#23450;&#30828;&#20214;&#19978;&#30340;&#36136;&#37327;&#21644;&#25512;&#29702;&#25928;&#29575;&#35201;&#27714;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;NAS&#22312;&#30828;&#20214;&#29305;&#23450;&#32593;&#32476;&#35774;&#35745;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;NAS&#24212;&#29992;&#20110;&#33521;&#29305;&#23572;Movidius VPU&#65288;&#35270;&#35273;&#22788;&#29702;&#22120;&#21333;&#20803;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#35201;&#24182;&#20837;NAS&#36807;&#31243;&#20013;&#30340;&#30828;&#20214;&#25104;&#26412;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#22312;&#35774;&#22791;&#19978;&#39044;&#20808;&#25910;&#38598;&#30340;&#30828;&#20214;&#25104;&#26412;&#21644;&#35774;&#22791;&#29305;&#23450;&#30340;&#30828;&#20214;&#25104;&#26412;&#27169;&#22411;VPUNN&#12290;&#36890;&#36807;NAS&#30340;&#24110;&#21161;&#65292;&#22312;VPU&#19978;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#27604;Mobilenet-v2-1.4&#26356;&#24555;1.3&#20493;&#30340;fps&#21152;&#36895;&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#31934;&#24230;&#19979;&#27604;Resnet50&#26356;&#24555;2.2&#20493;&#12290;&#23545;&#20110;VPU&#19978;&#30340;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#65292;&#19982;EDSR3&#30456;&#27604;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;1.08&#20493;&#30340;PSNR&#21644;6&#20493;&#30340;&#26356;&#39640;fps&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardware-aware Neural Architecture Search (NAS) technologies have been proposed to automate and speed up model design to meet both quality and inference efficiency requirements on a given hardware. Prior arts have shown the capability of NAS on hardware specific network design. In this whitepaper, we further extend the use of NAS to Intel Movidius VPU (Vision Processor Units). To determine the hardware-cost to be incorporated into the NAS process, we introduced two methods: pre-collected hardware-cost on device and device-specific hardware-cost model VPUNN. With the help of NAS, for classification task on VPU, we can achieve 1.3x fps acceleration over Mobilenet-v2-1.4 and 2.2x acceleration over Resnet50 with the same accuracy score. For super resolution task on VPU, we can achieve 1.08x PSNR and 6x higher fps compared with EDSR3.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03737</link><description>&lt;p&gt;
&#35843;&#25972;&#20256;&#32479;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20197;&#36866;&#29992;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Tuning Traditional Language Processing Approaches for Pashto Text Classification. (arXiv:2305.03737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22312;&#24456;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#26469;&#24320;&#21457;&#22269;&#38469;&#21644;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#20026;&#26412;&#22320;&#35821;&#35328;&#24314;&#31435;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;, &#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#30001;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#25991;&#26723;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19981;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;K&#36817;&#37051;(KNN)&#12289;&#20915;&#31574;&#26641;, &#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;, &#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;, &#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#22312;&#20869;&#30340;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#21253;&#25324;&#35789;&#34955;&#21644;tf-idf&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;97.19%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today text classification becomes critical task for concerned individuals for numerous purposes. Hence, several researches have been conducted to develop automatic text classification for national and international languages. However, the need for an automatic text categorization system for local languages is felt. The main aim of this study is to establish a Pashto automatic text classification system. In order to pursue this work, we built a Pashto corpus which is a collection of Pashto documents due to the unavailability of public datasets of Pashto text documents. Besides, this study compares several models containing both statistical and neural network machine learning techniques including Multilayer Perceptron (MLP), Support Vector Machine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression to discover the most effective approach. Moreover, this investigation evaluates two different feature extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#36755;&#36816;&#24314;&#27169;&#26041;&#27861;&#65288;MSBTM&#65289;&#22312;&#27714;&#35299;&#24179;&#22343;&#22330;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24314;&#31435;Kullback-Leibler&#25955;&#24230;&#26102;&#38388;&#23548;&#25968;&#30340;&#19978;&#30028;&#65292;&#39564;&#35777;&#20102;MSBTM&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;MSBTM&#35299;&#12289;&#30456;&#24212;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31215;&#20998;&#32467;&#26524;&#21644;&#35299;&#26512;&#35299;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;MSBTM&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03729</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#36755;&#36816;&#24314;&#27169;&#22312;&#24179;&#22343;&#22330;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#27714;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Score-based Transport Modeling for Mean-Field Fokker-Planck Equations. (arXiv:2305.03729v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#36755;&#36816;&#24314;&#27169;&#26041;&#27861;&#65288;MSBTM&#65289;&#22312;&#27714;&#35299;&#24179;&#22343;&#22330;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24314;&#31435;Kullback-Leibler&#25955;&#24230;&#26102;&#38388;&#23548;&#25968;&#30340;&#19978;&#30028;&#65292;&#39564;&#35777;&#20102;MSBTM&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23545;MSBTM&#35299;&#12289;&#30456;&#24212;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31215;&#20998;&#32467;&#26524;&#21644;&#35299;&#26512;&#35299;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;MSBTM&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#36755;&#36816;&#24314;&#27169;&#26041;&#27861;&#65288;MSBTM&#65289;&#35299;&#20915;&#24179;&#22343;&#22330;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;&#25105;&#20204;&#20174;&#31934;&#30830;&#35299;&#21040;MSBTM&#25968;&#20540;&#20272;&#31639;&#24314;&#31435;&#20102;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#26102;&#38388;&#23548;&#25968;&#30340;&#19978;&#30028;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;MSBTM&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#35823;&#24046;&#20998;&#26512;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#24179;&#22343;&#22330;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20197;&#21450;&#23427;&#20204;&#23545;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#20013;&#31890;&#23376;&#30340;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;MSBTM&#35299;&#12289;&#30456;&#24212;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31215;&#20998;&#32467;&#26524;&#21644;&#35299;&#26512;&#35299;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;MSBTM&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the score-based transport modeling method to solve the mean-field Fokker-Planck equations, which we call MSBTM. We establish an upper bound on the time derivative of the Kullback-Leibler (KL) divergence to MSBTM numerical estimation from the exact solution, thus validates the MSBTM approach. Besides, we provide an error analysis for the algorithm. In numerical experiments, we study two types of mean-field Fokker-Planck equation and their corresponding dynamics of particles in interacting systems. The MSBTM algorithm is numerically validated through qualitative and quantitative comparison between the MSBTM solutions, the results of integrating the associated stochastic differential equation and the analytical solutions if available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03686</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#36817;&#20284;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20027;&#35201;&#20851;&#27880;&#23616;&#37096;&#40065;&#26834;&#24615;&#65292;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#32473;&#23450;&#23646;&#24615;&#26159;&#21542;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#20869;&#20840;&#23616;&#25104;&#31435;&#65292;&#22914;&#26524;&#19981;&#25104;&#31435;&#65292;&#21017;&#38656;&#35201;&#30693;&#36947;&#23646;&#24615;&#25104;&#31435;&#30340;&#36755;&#20837;&#27604;&#20363;&#26159;&#22810;&#23569;&#12290;&#23613;&#31649;&#31934;&#30830;&#30340;&#21407;&#20687;&#29983;&#25104;&#21487;&#20197;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#20215;&#34920;&#31034;&#65292;&#20294;&#22312;&#35268;&#27169;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26494;&#24347;&#30340;&#39640;&#25928;&#23454;&#29992;&#30340;&#20219;&#24847;&#26102;&#21051;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21407;&#20687;&#30340;&#31526;&#21495;&#19979;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21306;&#22495;&#21010;&#20998;&#20026;&#23376;&#21306;&#22495;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#26494;&#24347;&#36793;&#30028;&#21464;&#24471;&#26356;&#32039;&#65292;&#36845;&#20195;&#22320;&#26368;&#23567;&#21270;&#20307;&#31215;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#37319;&#26679;&#21644;&#21487;&#24494;&#20307;&#31215;&#36924;&#36817;&#26469;&#20248;&#20808;&#21010;&#20998;&#21306;&#22495;&#65292;&#24182;&#20248;&#21270;&#26494;&#24347;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#25913;&#36827;&#21644;&#26356;&#39640;&#30340;&#21387;&#32553;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03655</link><description>&lt;p&gt;
&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#23545;&#35805;&#29983;&#25104;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;DGSlow&#12290;&#36890;&#36807;&#24179;&#34913;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#38271;&#24230;&#20004;&#20010;&#30446;&#26631;&#65292;DGSlow&#21033;&#29992;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#22312;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#29983;&#25104;&#31995;&#32479;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#22312;&#20256;&#32479;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;DG&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22909;&#22855;&#24515;&#12290; &#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25915;&#20987;DG&#27169;&#22411;&#30340;&#26102;&#20505;&#65292;&#23545;&#24403;&#21069;&#21477;&#23376;&#30340;&#25200;&#21160;&#20960;&#20046;&#19981;&#20250;&#38477;&#20302;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#26410;&#25913;&#21464;&#30340;&#32842;&#22825;&#35760;&#24405;&#20063;&#20250;&#34987;&#32771;&#34385;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#26469;&#36843;&#20351;&#29983;&#25104;&#26356;&#38271;&#30340;&#36755;&#20986;&#65292;&#26377;&#21033;&#20110;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615; - &#29983;&#25104;&#30340;&#21709;&#24212;&#36890;&#24120;&#26159;&#19981;&#30456;&#20851;&#12289;&#20887;&#38271;&#21644;&#37325;&#22797;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGSlow&#30340;&#30333;&#30418;&#22810;&#30446;&#26631;&#25915;&#20987;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DGSlow&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#22120;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631; - &#29983;&#25104;&#20934;&#30830;&#24230;&#21644;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03136</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#30340;&#24191;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03136
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#20989;&#25968;&#23558;&#29983;&#29289;&#24207;&#21015;&#30340;&#22823;&#32452;&#21512;&#31354;&#38388;&#26144;&#23556;&#21040;&#25152;&#20851;&#27880;&#30340;&#29305;&#24615;&#19978;&#12290;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22810;&#27169;&#24577;&#20989;&#25968;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#25928;&#19988;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#31232;&#30095;&#30340;&#28508;&#22312;&#20989;&#25968;&#36890;&#36807;&#21333;&#35843;&#38750;&#32447;&#24615;&#21464;&#25442;&#20197;&#21457;&#23556;&#21487;&#27979;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914; Bradley-Terry &#25439;&#22833;&#65289;&#26159;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#25152;&#38544;&#31034;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#30340;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#36866;&#24212;&#24615;-&#19978;&#20301;&#32852;&#31995;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20105;&#36777;&#65292;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#21487;&#20197;&#20135;&#29983;&#19981;&#20855;&#22791;&#31232;&#30095;&#34920;&#31034;&#30340;&#35266;&#23519;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#36866;&#21512;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#65289;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#25439;&#22833;&#21487;&#29992;&#20110;&#25512;&#26029;&#19981;&#36866;&#21512; MSE &#25439;&#22833;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.02931</link><description>&lt;p&gt;
&#36229;&#36234;&#21516;&#36136;&#24615;&#65306;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering. (arXiv:2305.02931v1 [cs.SI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32858;&#31867;&#26041;&#27861;&#22312;&#33410;&#28857;&#32858;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#21516;&#36136;&#22270;&#30340;&#20551;&#35774;&#35774;&#35745;&#30340;&#65292;&#32780;&#22312;&#24322;&#36136;&#22270;&#19978;&#36827;&#34892;&#32858;&#31867;&#34987;&#24573;&#35270;&#20102;&#12290;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#65292;&#19981;&#21487;&#33021;&#22312;&#25214;&#21040;&#36866;&#21512;&#30340;GNN&#27169;&#22411;&#20043;&#21069;&#39318;&#20808;&#23558;&#22270;&#24418;&#35782;&#21035;&#20026;&#21516;&#36136;&#25110;&#24322;&#36136;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21508;&#31181;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#24418;&#36827;&#34892;&#32858;&#31867;&#23558;&#20026;&#22270;&#24418;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20351;&#20854;&#23545;&#22270;&#24418;&#26080;&#20851;&#65292;&#25105;&#20204;&#26681;&#25454;&#25968;&#25454;&#26500;&#24314;&#20102;&#39640;&#24230;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20004;&#20010;&#22270;&#24418;&#12290;&#22522;&#20110;&#26032;&#22270;&#26500;&#24314;&#30340;&#28151;&#21512;&#28388;&#27874;&#22120;&#25552;&#21462;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) based methods have achieved impressive performance on node clustering task. However, they are designed on the homophilic assumption of graph and clustering on heterophilic graph is overlooked. Due to the lack of labels, it is impossible to first identify a graph as homophilic or heterophilic before a suitable GNN model can be found. Hence, clustering on real-world graph with various levels of homophily poses a new challenge to the graph research community. To fill this gap, we propose a novel graph clustering method, which contains three key components: graph reconstruction, a mixed filter, and dual graph clustering network. To be graph-agnostic, we empirically construct two graphs which are high homophily and heterophily from each data. The mixed filter based on the new graphs extracts both low-frequency and high-frequency information. To reduce the adverse coupling between node attribute and topological structure, we separately map them into two subspaces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2305.02350</link><description>&lt;p&gt;
&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;CNN&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#32452;&#25104;&#20102;&#21253;&#25324;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#35805;&#39064;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;8&#32452;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#24635;&#32467;&#25104;&#19968;&#20010;&#26435;&#34913;&#21015;&#34920;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DeepIM&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02200</link><description>&lt;p&gt;
&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#30340;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Graph Representation Learning and Optimization for Influence Maximization. (arXiv:2305.02200v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DeepIM&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#34987;&#23450;&#20041;&#20026;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#36873;&#25321;&#19968;&#32452;&#21021;&#22987;&#29992;&#25143;&#20197;&#26368;&#22823;&#21270;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20854;&#29702;&#35770;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#21319;&#25509;&#36817;&#20110;&#26497;&#38480;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;IM&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#26410;&#30693;&#22270;&#24418;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;IM&#26041;&#27861;&#30340;&#21457;&#23637;&#20173;&#28982;&#21463;&#21040;&#22522;&#26412;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#65306;1&#65289;&#26377;&#25928;&#35299;&#20915;&#30446;&#26631;&#20989;&#25968;&#30340;&#22256;&#38590;&#24615;&#65307;2&#65289;&#34920;&#24449;&#22810;&#26679;&#21270;&#30340;&#22522;&#30784;&#25193;&#25955;&#27169;&#24335;&#30340;&#22256;&#38590;&#24615;&#65307;&#20197;&#21450;3&#65289;&#22312;&#21508;&#31181;&#33410;&#28857;&#20013;&#24515;&#24615;&#32422;&#26463;&#30340;IM&#21464;&#20307;&#19979;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DeepIM&#26469;&#29983;&#25104;&#22320;&#34920;&#24449;&#31181;&#23376;&#38598;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#22810;&#26679;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information di
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01997</link><description>&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#20307;&#31215;&#25351;&#25968;&#30340;&#25552;&#21462;&#65306;&#21738;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20020;&#24202;&#65311;
&lt;/p&gt;
&lt;p&gt;
Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#33258;&#21160;&#20998;&#26512;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#21033;&#29992;&#22810;&#20010;&#30001;&#19987;&#23478;&#27880;&#37322;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;CAMUS&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#20043;&#19968;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#24191;&#33021;&#21147;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34987;&#20020;&#24202;&#21307;&#29983;&#35748;&#20026;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CARDINAL&#30340;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#25324;&#24515;&#23574;&#20004;&#33108;&#21644;&#24515;&#23574;&#22235;&#33108;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#23436;&#25972;&#24515;&#33039;&#21608;&#26399;&#30340;&#21442;&#32771;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;3D nnU-Net&#20248;&#20110;&#26367;&#20195;&#30340;2D&#21644;&#24490;&#29615;&#20998;&#21106;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#25253;&#21578;&#20102;&#22312;CARDINAL&#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#12289;&#23500;&#26377;&#34920;&#29616;&#21147;&#12289;&#24182;&#36991;&#20813;&#20102;&#20266;&#20687;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.01241</link><description>&lt;p&gt;
AQ-GT:&#19968;&#31181;&#26102;&#38388;&#23545;&#40784;&#24182;&#37327;&#21270;&#30340;GRU-Transformer&#65292;&#29992;&#20110;&#20849;&#35821;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#12289;&#23500;&#26377;&#34920;&#29616;&#21147;&#12289;&#24182;&#36991;&#20813;&#20102;&#20266;&#20687;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24314;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#29983;&#25104;&#36924;&#30495;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#20849;&#35821;&#25163;&#21183;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#20849;&#35821;&#25163;&#21183;&#34920;&#31034;&#21644;&#23454;&#38469;&#21160;&#20316;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#20135;&#29983;&#20284;&#20046;&#33258;&#28982;&#20294;&#24120;&#24120;&#19981;&#20196;&#20154;&#20449;&#26381;&#30340;&#25163;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#30721;&#26412;&#21521;&#37327;&#26082;&#26159;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#36755;&#20837;&#65292;&#20063;&#26159;&#36755;&#20986;&#65292;&#26500;&#25104;&#25163;&#21183;&#30340;&#29983;&#25104;&#21644;&#37325;&#26500;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#30452;&#25509;&#26144;&#23556;&#21040;&#30690;&#37327;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#39640;&#24230;&#36924;&#30495;&#19988;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#25163;&#21183;&#29983;&#25104;&#65292;&#36825;&#20123;&#25163;&#21183;&#19982;&#20154;&#31867;&#30340;&#36816;&#21160;&#21644;&#34892;&#20026;&#38750;&#24120;&#30456;&#20284;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20266;&#20687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#23427;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#36924;&#30495;&#21644;&#20016;&#23500;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01094</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30001;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#30340;&#24773;&#24418;&#19979;&#39044;&#27979;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#23454;&#29616;&#24335;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#20854;&#20551;&#35774;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23454;&#29616;&#24335;&#39044;&#27979;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#20984;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20984;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00769</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Transformer-based Network for Emotion Recognition from Multi Physiological Signals. (arXiv:2305.00769v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#29616;&#20195;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#20174;&#36825;&#20123;&#20449;&#21495;&#20013;&#25552;&#21462;&#22823;&#37327;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#19968;&#20219;&#21153;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#20197;&#24314;&#31435;&#20869;&#37096;&#36523;&#20307;&#20449;&#21495;&#19982;&#20154;&#31867;&#24773;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#65292;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;RMSE&#24471;&#20998;&#20026;1.45&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient Multi-scale Transformer-based approach for the task of Emotion recognition from Physiological data, which has gained widespread attention in the research community due to the vast amount of information that can be extracted from these signals using modern sensors and machine learning techniques. Our approach involves applying a Multi-modal technique combined with scaling data to establish the relationship between internal body signals and human emotions. Additionally, we utilize Transformer and Gaussian Transformation techniques to improve signal encoding effectiveness and overall performance. Our model achieves decent results on the CASE dataset of the EPiC competition, with an RMSE score of 1.45.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00312</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#36890;&#24120;&#26159;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20540;&#24471;&#20449;&#36182;&#65292;&#23427;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;/&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12289;&#26368;&#23567;&#21270;&#38544;&#31169;&#27844;&#38706;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#23545;&#24694;&#24847;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26088;&#22312;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#38750;&#24120;&#36866;&#21512;&#35299;&#20915;&#20540;&#24471;&#20449;&#36182;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;TFL&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;MOO&#21644;TFL&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21046;&#23450;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#32852;&#21512;&#23398;&#20064;&#65288;CMOFL&#65289;&#38382;&#39064;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#21046;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#36866;&#29992;&#20110;TFL&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;CMOFL&#20316;&#21697;&#19987;&#27880;&#20110;&#25928;&#29992;&#12289;&#25928;&#29575;&#12289;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36825;&#26159;TFL&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;CMOFL&#31639;&#27861;&#65292;&#23427;&#20204;&#36820;&#22238;&#19968;&#32452;&#24179;&#34913;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#28385;&#36275;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#22522;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#32500;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13833</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussian process experts based on kernel stick-breaking processes. (arXiv:2304.13833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13833
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#32500;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31867;&#33021;&#21516;&#26102;&#35299;&#20915;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#21487;&#25193;&#23637;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#20316;&#20026;&#38376;&#20989;&#25968;&#30340;&#27169;&#22411;&#33021;&#22815;&#30452;&#35266;&#22320;&#35299;&#37322;&#21644;&#33258;&#21160;&#36873;&#25321;&#28151;&#21512;&#29289;&#20013;&#19987;&#23478;&#30340;&#25968;&#37327;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#22312;&#24863;&#30693;&#38750;&#24179;&#31283;&#24615;&#12289;&#22810;&#27169;&#24615;&#21644;&#24322;&#26041;&#24046;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#38376;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#22312;&#24212;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#30456;&#20851;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#25991;&#29486;&#20013;&#30340;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26829;&#26834;&#36807;&#31243;&#30340;&#26032;&#22411;&#39640;&#26031;&#36807;&#31243;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20351;&#20854;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21518;&#39564;&#35745;&#31639;&#30340;&#20999;&#29255;&#25277;&#26679;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussian process experts is a class of models that can simultaneously address two of the key limitations inherent in standard Gaussian processes: scalability and predictive performance. In particular, models that use Dirichlet processes as gating functions permit straightforward interpretation and automatic selection of the number of experts in a mixture. While the existing models are intuitive and capable of capturing non-stationarity, multi-modality and heteroskedasticity, the simplicity of their gating functions may limit the predictive performance when applied to complex data-generating processes. Capitalising on the recent advancement in the dependent Dirichlet processes literature, we propose a new mixture model of Gaussian process experts based on kernel stick-breaking processes. Our model maintains the intuitive appeal yet improve the performance of the existing models. To make it practical, we design a sampler for posterior computation based on the slice sampling. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13534</link><description>&lt;p&gt;
&#29992;&#22343;&#22330;&#21338;&#24328;&#20026;&#29983;&#25104;&#27169;&#22411;&#25645;&#24314;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22343;&#22330;&#21338;&#24328; (MFGs) &#20316;&#20026;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#12289;&#22686;&#24378;&#21644;&#35774;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102; MFGs &#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#20195;&#20215;&#20989;&#25968;&#25512;&#23548;&#20102;&#36825;&#19977;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#8212;&#8212;&#19968;&#32452;&#32806;&#21512;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#29305;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#21512;&#25104;&#26679;&#26412;&#65292;&#21478;&#19968;&#20010;&#20195;&#29702;&#23545;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#19988;&#36924;&#30495;&#65292;&#21516;&#26102;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#31361;&#26174;&#20102; MFGs &#20316;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#39564;&#23460;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13273</link><description>&lt;p&gt;
&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#65306;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#30340;&#32431;&#25991;&#26412;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#65306;&#36890;&#36807;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#26377;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20197;CLIP&#21644;ALIGN&#20026;&#20195;&#34920;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;CLIP&#30340;&#38646;-shot&#33021;&#21147;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#31561;&#22522;&#20110;&#20851;&#32852;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#20294;&#26159;&#65292;CLIP&#38590;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;K&#26368;&#36817;&#37051;&#36328;&#27169;&#24577;&#26144;&#23556;&#65288;Knight&#65289;&#65292;&#19968;&#31181;&#20174;&#20851;&#32852;&#21040;&#29983;&#25104;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#36890;&#36807;&#31364;&#23383;&#24149;&#20219;&#21153;&#30340;&#32431;&#25991;&#26412;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;/&#35270;&#39057;&#25237;&#24433;&#21040;&#35821;&#35328;&#27169;&#24577;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#23383;&#24149;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Knight&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#26144;&#23556;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#19988;&#23558;&#22312;&#35270;&#39057;&#23383;&#24149;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.12875</link><description>&lt;p&gt;
&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;(TnALE): &#29992;&#36739;&#23569;&#30340;&#35780;&#20272;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;(TN)&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20294;&#36873;&#25321;&#19968;&#20010;&#22909;&#30340;TN&#27169;&#22411;&#65292;&#21363;TN&#32467;&#26500;&#25628;&#32034;(TN-SS)&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;TNLS ~ \cite {li2022permutation} &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#30340;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#26159;&#26080;&#27861;&#25215;&#21463;&#30340;&#65292;&#38656;&#35201;&#22826;&#22810;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#30340;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnALE&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#26522;&#20030;&#20132;&#26367;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#19982;TNLS&#30456;&#27604;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;TNLS&#21644;TnALE&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#35777;&#26126;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;&#37027;&#20040;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;TNLS&#21644;TnALE&#30340;&#35780;&#20272;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#22312;TNLS&#20013;&#36890;&#24120;&#38656;&#35201;&#937;(2 ^ N)&#20010;&#35780;&#20272;&#25165;&#33021;&#22312;&#37051;&#22495;&#20869;&#36798;&#21040;&#30446;&#26631;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12519</link><description>&lt;p&gt;
RenderDiffusion: &#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#29983;&#25104;&#33539;&#24335;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#29305;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#30446;&#26631;&#25991;&#26412;&#21576;&#29616;&#20026;&#21253;&#21547;&#35270;&#35273;&#35821;&#35328;&#20869;&#23481;&#30340;"&#23383;&#24418;&#22270;&#20687;"&#12290;&#36825;&#26679;&#65292;&#26465;&#20214;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#28982;&#21518;&#33258;&#28982;&#22320;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.12322</link><description>&lt;p&gt;
&#20351;&#29992;Pylogik&#36827;&#34892;&#21307;&#23398;&#24433;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#20449;&#24687;&#26041;&#38754;&#39035;&#27880;&#24847;&#65292;&#24517;&#39035;&#28165;&#27927;&#21644;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#12290;&#24403;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#23884;&#20837;&#22312;&#24433;&#20687;&#20803;&#25968;&#25454;&#20013;&#26102;&#65292;&#20419;&#36827;&#22810;&#20013;&#24515;&#21512;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#35843;&#21464;&#24471;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#26694;&#26550;&#19979;&#30340;&#24211;&#65292;&#31216;&#20026;PyLogik&#65292;&#24110;&#21161;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#22270;&#20687;&#30452;&#25509;&#21253;&#21547;&#24456;&#22810;PHI&#12290;PyLogik&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25991;&#26412;&#26816;&#27979;/&#25552;&#21462;&#12289;&#36807;&#28388;&#12289;&#38408;&#20540;&#21270;&#12289;&#24418;&#24577;&#23398;&#21644;&#36718;&#24275;&#27604;&#36739;&#22788;&#29702;&#22270;&#20687;&#20307;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#21435;&#26631;&#35782;&#21270;&#22270;&#20687;&#65292;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#20934;&#22791;&#22909;&#20102;&#22270;&#20687;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;PyLogik&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#35782;&#21035;&#26377;&#25928;&#24615;&#65292;&#38543;&#26426;&#25277;&#21462;&#20102;50&#24352;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#25511;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21487;&#20449;&#12289;&#22810;&#26679;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#35774;&#22791;&#30340;&#20351;&#29992;&#23551;&#21629;&#12290;&#20854;&#20013;&#20351;&#29992;&#20102;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11702</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#25511;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#26410;&#30693;&#25805;&#20316;&#26465;&#20214;&#19979;&#28145;&#24230;&#23398;&#20064;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Controlled physics-informed data generation for deep learning-based remaining useful life prediction under unseen operation conditions. (arXiv:2304.11702v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#25511;&#29289;&#29702;&#20449;&#24687;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21487;&#20449;&#12289;&#22810;&#26679;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#35774;&#22791;&#30340;&#20351;&#29992;&#23551;&#21629;&#12290;&#20854;&#20013;&#20351;&#29992;&#20102;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26377;&#38480;&#30340;&#20195;&#34920;&#24615;&#25925;&#38556;&#26102;&#38388;&#36712;&#36857;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#29983;&#25104;&#21512;&#29702;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;&#21487;&#25511;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#29289;&#29702;&#20449;&#24687;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CPI-GAN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#29289;&#29702;&#21487;&#35299;&#37322;&#19988;&#22810;&#26679;&#30340;&#21512;&#25104;&#23551;&#21629;&#34928;&#20943;&#36712;&#36857;&#12290;&#22312;&#29983;&#25104;&#22120;&#20013;&#25552;&#20986;&#20102;&#20116;&#20010;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#20316;&#20026;&#21487;&#25511;&#35774;&#32622;&#12290;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#29992;&#20316;&#27491;&#21017;&#21270;&#39033;&#65292;&#30830;&#20445;&#35760;&#24405;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#31995;&#32479;&#20581;&#24247;&#29366;&#24577;&#21464;&#21270;&#36235;&#21183;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited availability of representative time-to-failure (TTF) trajectories either limits the performance of deep learning (DL)-based approaches on remaining useful life (RUL) prediction in practice or even precludes their application. Generating synthetic data that is physically plausible is a promising way to tackle this challenge. In this study, a novel hybrid framework combining the controlled physics-informed data generation approach with a deep learning-based prediction model for prognostics is proposed. In the proposed framework, a new controlled physics-informed generative adversarial network (CPI-GAN) is developed to generate synthetic degradation trajectories that are physically interpretable and diverse. Five basic physics constraints are proposed as the controllable settings in the generator. A physics-informed loss function with penalty is designed as the regularization term, which ensures that the changing trend of system health state recorded in the synthetic data is consi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10226</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10226
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#21378;&#21830;&#30340;&#22270;&#20687;&#39118;&#26684;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26679;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#21378;&#21830;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.07407</link><description>&lt;p&gt;
&#26410;&#35266;&#27979;&#21040;&#20195;&#29702;&#22870;&#21169;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. (arXiv:2304.07407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#22330;&#26223;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#19968;&#31181;&#32769;&#34382;&#26426;&#21518;&#20250;&#33719;&#24471;&#22870;&#21169;&#21644;&#28608;&#21169;&#65292;&#20294;&#36127;&#36131;&#20154;&#21482;&#33021;&#35266;&#23519;&#21040;&#20195;&#29702;&#36873;&#25321;&#20102;&#21738;&#20010;&#32769;&#34382;&#26426;&#20197;&#21450;&#20195;&#29702;&#30456;&#24212;&#30340;&#28608;&#21169;&#65292;&#32780;&#24819;&#35201;&#35774;&#35745;&#19968;&#31181;&#21512;&#36866;&#30340;&#31574;&#30053;&#21364;&#20805;&#28385;&#20102;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.05294</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#36873;&#25321;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#21019;&#24314;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#12289;&#28508;&#22312;&#20132;&#20114;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#38598;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#65288;M&#65289;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Tigramite Python&#21253;&#20013;&#23454;&#29616;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;PC1&#25110;PCMCI&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#25512;&#26029;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#23558;&#21097;&#20313;&#22240;&#26524;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;ML&#27169;&#22411;&#65288;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65289;&#39044;&#27979;&#30446;&#26631;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#35199;&#22826;&#24179;&#27915;&#28909;&#24102;&#22320;&#21306;&#30340;&#22320;&#38663;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.06311</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;EXO-200&#38378;&#28865;&#20449;&#21495;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#25110;&#23454;&#38469;&#20107;&#20214;&#26679;&#26412;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#29983;&#25104;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#23545;&#32473;&#23450;&#23545;&#35937;&#38598;&#30340;&#24635;&#20307;&#20998;&#24067;&#36827;&#34892;&#38544;&#24335;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#21407;&#22987;&#38378;&#28865;&#27874;&#24418;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26657;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#32593;&#32476;&#27491;&#30830;&#25512;&#26029;&#20986;&#25506;&#27979;&#22120;&#20013;&#38378;&#28865;&#20809;&#21709;&#24212;&#30340;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#24182;&#27604;&#36739;&#20102;&#20013;&#22830;&#22788;&#29702;&#22120;&#37319;&#29992;&#31639;&#26415;&#24179;&#22343;&#21644;&#20960;&#20309;&#24179;&#22343;&#30340;&#32858;&#21512;&#31574;&#30053;&#12290;&#32467;&#26524;&#30830;&#35748;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2303.06109</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#30340;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#24182;&#27604;&#36739;&#20102;&#20013;&#22830;&#22788;&#29702;&#22120;&#37319;&#29992;&#31639;&#26415;&#24179;&#22343;&#21644;&#20960;&#20309;&#24179;&#22343;&#30340;&#32858;&#21512;&#31574;&#30053;&#12290;&#32467;&#26524;&#30830;&#35748;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32452;&#20195;&#29702;&#21512;&#20316;&#25512;&#26029;&#33258;&#28982;&#30028;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#32780;&#19981;&#19982;&#20013;&#22830;&#22788;&#29702;&#22120;&#25110;&#24444;&#27492;&#20849;&#20139;&#20854;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20195;&#29702;&#23558;&#20854;&#20010;&#20154;&#35266;&#23519;&#32467;&#26524;&#20351;&#29992;&#36125;&#21494;&#26031;&#35268;&#21017;&#21512;&#24182;&#20026;&#24847;&#35265;&#65288;&#21363;&#36719;&#20915;&#31574;&#65289;&#65292;&#28982;&#21518;&#20013;&#22830;&#22788;&#29702;&#22120;&#36890;&#36807;&#31639;&#26415;&#25110;&#20960;&#20309;&#24179;&#22343;&#25968;&#32858;&#21512;&#36825;&#20123;&#24847;&#35265;&#12290;&#24314;&#31435;&#22312;&#25105;&#20204;&#20197;&#21069;&#30340;&#24037;&#20316;&#20043;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25512;&#23548;&#20986;&#38169;&#35823;&#27010;&#29575;&#30340;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#39564;&#35777;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized to derive approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
&lt;/p&gt;</description></item><item><title>DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.04201</link><description>&lt;p&gt;
DR-VIDAL--&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#39044;&#27979;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04201
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#24615;&#65288;&#38750;&#38543;&#26426;&#21270;&#65289;&#25968;&#25454;&#20013;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#20363;&#22914;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#27835;&#30103;&#37325;&#29992;&#65292;&#30001;&#20110;&#28508;&#22312;&#20559;&#24046;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#20256;&#32479;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#20010;&#24615;&#21270;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#65288;DR-VIDAL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#27835;&#30103;&#21644;&#32467;&#26524;&#20004;&#20010;&#32852;&#21512;&#27169;&#22411;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#30830;&#20445;&#26080;&#20559;&#30340;ITE&#20272;&#35745;&#65292;&#21363;&#20351;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#35774;&#23450;&#19981;&#27491;&#30830;&#12290;DR-VIDAL&#25972;&#21512;&#20102;&#65306; &#65288;i&#65289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26681;&#25454;&#22240;&#26524;&#20551;&#35774;&#23558;&#28151;&#28102;&#21464;&#37327;&#20998;&#35299;&#20026;&#28508;&#22312;&#21464;&#37327;; &#65288;ii&#65289;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;Info-GAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#20917;; &#65288;iii&#65289;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#27835;&#30103;&#20542;&#21521;&#20110;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65288;Infant Health&#21644;Development Program&#65292;Transforming Clinical Practice Initiative [TCPI]&#65289;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DR-VIDAL&#22312;&#20272;&#35745;ITE&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#27491;&#30830;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#22266;&#23450;&#25511;&#21046;&#26469;&#24341;&#23548;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#31867;&#20013;&#24515;&#26469;&#30417;&#30563;&#33410;&#28857;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#31867;&#26631;&#31614;&#30340;&#33410;&#28857;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01265</link><description>&lt;p&gt;
&#29992;&#22266;&#23450;&#25511;&#21046;&#24341;&#23548;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Graph Neural Networks with Pinning Control. (arXiv:2303.01265v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#22266;&#23450;&#25511;&#21046;&#26469;&#24341;&#23548;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#31867;&#20013;&#24515;&#26469;&#30417;&#30563;&#33410;&#28857;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#31867;&#26631;&#31614;&#30340;&#33410;&#28857;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#26631;&#31614;&#25968;&#25454;&#21463;&#38480;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#31867;&#26631;&#31614;&#30340;&#33410;&#28857;&#30340;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#20173;&#28982;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#21407;&#29702;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#21407;&#22411;&#65288;&#21363;&#31867;&#20013;&#24515;&#65289;&#26469;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#23558;&#22270;&#23398;&#20064;&#35270;&#20026;&#31163;&#25955;&#21160;&#24577;&#36807;&#31243;&#65292;&#23558;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#21407;&#22411;&#35270;&#20026;&#8220;&#26399;&#26395;&#8221;&#30340;&#31867;&#21035;&#34920;&#31034;&#65292;&#25105;&#20204;&#20174;&#33258;&#21160;&#25511;&#21046;&#29702;&#35770;&#20013;&#20511;&#37492;&#22266;&#23450;&#25511;&#21046;&#24605;&#24819;&#65292;&#35774;&#35745;&#20102;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#26469;&#25351;&#23548;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#65292;&#35797;&#22270;&#22312;&#27599;&#19968;&#36718;&#20013;&#26368;&#23567;&#21270;&#28040;&#24687;&#20256;&#36882;&#20135;&#29983;&#30340;&#29305;&#24449;&#21644;&#31867;&#21035;&#21407;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20197;&#29983;&#25104;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#20026;&#27599;&#20010;&#33410;&#28857;&#22312;&#27599;&#19968;&#36718;&#20013;&#37197;&#22791;&#26368;&#20339;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the semi-supervised setting where labeled data are largely limited, it remains to be a big challenge for message passing based graph neural networks (GNNs) to learn feature representations for the nodes with the same class label that is distributed discontinuously over the graph. To resolve the discontinuous information transmission problem, we propose a control principle to supervise representation learning by leveraging the prototypes (i.e., class centers) of labeled data. Treating graph learning as a discrete dynamic process and the prototypes of labeled data as "desired" class representations, we borrow the pinning control idea from automatic control theory to design learning feedback controllers for the feature learning process, attempting to minimize the differences between message passing derived features and the class prototypes in every round so as to generate class-relevant features. Specifically, we equip every node with an optimal controller in each round through learnin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;PaGE-Link&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#65292;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12465</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65288;PaGE-Link&#65289;
&lt;/p&gt;
&lt;p&gt;
PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction. (arXiv:2302.12465v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;PaGE-Link&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#65292;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#24050;&#25104;&#20026;&#40657;&#31665;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#27169;&#22411;&#34892;&#20026;&#30340;&#36866;&#24403;&#35299;&#37322;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#36127;&#36131;&#20219;&#30340;&#27169;&#22411;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#22270;&#24418;ML&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#35299;&#37322;&#23427;&#20204;&#24050;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#26041;&#38754;&#65292;GNN&#30340;&#35299;&#37322;&#23578;&#32570;&#23569;&#25991;&#29486;&#25903;&#25345;&#12290; LP&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;GNN&#20219;&#21153;&#65292;&#23545;&#24212;&#20110;Web&#19978;&#30340;&#25512;&#33616;&#21644;&#36190;&#21161;&#25628;&#32034;&#31561;&#24212;&#29992;&#31243;&#24207;&#12290;&#37492;&#20110;&#29616;&#26377;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#20165;&#35299;&#20915;&#33410;&#28857;/&#22270;&#32423;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65288;PaGE-Link&#65289;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;&#23450;&#24615;&#22320;&#65292;PaGE-Link&#21487;&#20197;&#29983;&#25104;&#23558;&#33410;&#28857;&#23545;&#36830;&#25509;&#36215;&#26469;&#30340;&#36335;&#24452;&#20316;&#20026;&#35299;&#37322;&#65292;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#36830;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2302.11313</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Time-varying Signals Recovery via Graph Neural Networks. (arXiv:2302.11313v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#21464;&#22270;&#20449;&#21495;&#30340;&#24674;&#22797;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#36825;&#20123;&#22270;&#20449;&#21495;&#26102;&#38388;&#24046;&#20998;&#30340;&#24179;&#28369;&#24615;&#20316;&#20026;&#19968;&#20010;&#21021;&#22987;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#35813;&#20808;&#39564;&#19981;&#25104;&#31435;&#26102;&#65292;&#36825;&#31181;&#24179;&#28369;&#24615;&#20551;&#35774;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TimeGNN)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#30001;&#22343;&#26041;&#35823;&#24046;&#20989;&#25968;&#21644;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#12290;TimeGNN&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#20197;&#21069;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recovery of time-varying graph signals is a fundamental problem with numerous applications in sensor networks and forecasting in time series. Effectively capturing the spatio-temporal information in these signals is essential for the downstream tasks. Previous studies have used the smoothness of the temporal differences of such graph signals as an initial assumption. Nevertheless, this smoothness assumption could result in a degradation of performance in the corresponding application when the prior does not hold. In this work, we relax the requirement of this hypothesis by including a learning module. We propose a Time Graph Neural Network (TimeGNN) for the recovery of time-varying graph signals. Our algorithm uses an encoder-decoder architecture with a specialized loss composed of a mean squared error function and a Sobolev smoothness operator.TimeGNN shows competitive performance against previous methods in real datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;Auto.gov&#8221;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#29702;&#30001;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.09551</link><description>&lt;p&gt;
Auto.gov&#65306;&#38754;&#21521;DeFi&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#38142;&#19978;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Auto.gov: Learning-based On-chain Governance for Decentralized Finance (DeFi). (arXiv:2302.09551v2 [q-fin.RM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;Auto.gov&#8221;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#29702;&#30001;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#28044;&#29616;&#20986;&#20102;&#21508;&#31181;&#21327;&#35758;&#65292;&#20363;&#22914;&#20511;&#36151;&#21327;&#35758;&#21644;&#33258;&#21160;&#21270;&#20570;&#24066;&#21830;&#65288;AMM&#65289;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#21327;&#35758;&#37319;&#29992;&#38142;&#19979;&#27835;&#29702;&#65292;&#20854;&#20013;&#20195;&#24065;&#25345;&#26377;&#32773;&#25237;&#31080;&#20462;&#25913;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#21327;&#35758;&#26680;&#24515;&#22242;&#38431;&#36827;&#34892;&#30340;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#23481;&#26131;&#36973;&#21463;&#21246;&#32467;&#25915;&#20987;&#65292;&#21361;&#21450;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#32431;&#31929;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#26041;&#27861;&#21487;&#33021;&#20250;&#20351;&#21327;&#35758;&#21463;&#21040;&#26032;&#30340;&#21033;&#29992;&#21644;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Auto.gov&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;DeFi&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#38142;&#19978;&#27835;&#29702;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#23433;&#20840;&#24615;&#24182;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#21270;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#19982;&#37327;&#21270;&#30340;&#29702;&#30001;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#21644;&#32531;&#35299;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, decentralized finance (DeFi) has experienced remarkable growth, with various protocols such as lending protocols and automated market makers (AMMs) emerging. Traditionally, these protocols employ off-chain governance, where token holders vote to modify parameters. However, manual parameter adjustment, often conducted by the protocol's core team, is vulnerable to collusion, compromising the integrity and security of the system. Furthermore, purely deterministic, algorithm-based approaches may expose the protocol to novel exploits and attacks.  In this paper, we present "Auto.gov", a learning-based on-chain governance framework for DeFi that enhances security and reduces susceptibility to attacks. Our model leverages a deep Q- network (DQN) reinforcement learning approach to propose semi-automated, intuitive governance proposals with quantitative justifications. This methodology enables the system to efficiently adapt to and mitigate the negative impact of malicious beha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#36848;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#32467;&#21512;&#65306;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#65292;&#25506;&#35752;&#20102;TNN&#22312;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#38754;&#30340;&#20248;&#32570;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09019</link><description>&lt;p&gt;
&#35780;&#36848;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks Meet Neural Networks: A Survey and Future Perspectives. (arXiv:2302.09019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#36848;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#32467;&#21512;&#65306;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#65292;&#25506;&#35752;&#20102;TNN&#22312;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#38754;&#30340;&#20248;&#32570;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;(TN)&#21644;&#31070;&#32463;&#32593;&#32476;(NN)&#26159;&#20004;&#31181;&#22522;&#26412;&#30340;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#12290;TN&#26088;&#22312;&#36890;&#36807;&#23558;&#25351;&#25968;&#32500;&#24230;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#26469;&#35299;&#20915;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#21560;&#24341;&#20102;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;NN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#20154;&#30740;&#31350;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#36825;&#20004;&#31181;&#32593;&#32476;&#28304;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;TNN (&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;)&#27010;&#24565;&#30340;&#32467;&#21512;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#65292;&#20849;&#21516;&#25903;&#25745;&#30528;&#22810;&#32447;&#24615;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TNN&#30340;&#19977;&#20010;&#26041;&#38754;&#65306;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#24182;&#38416;&#36848;&#20102;TNN&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24046;&#24322;&#20197;&#21450;TNN&#30340;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21450;&#20854;&#23545;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;TNN&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the common multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of intellectual developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks (TNNs), and present an introduction to TNNs in three primary aspects: network compression, information fusion, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.08888</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31227;&#21160;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#22810;&#23186;&#20307;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#21033;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#32780;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;&#24847;&#35782;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;FL&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#32423;&#21035;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#38480;&#21046;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#27169;&#24577;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#23481;&#37327;&#65292;&#26356;&#19981;&#29992;&#35828;&#20219;&#21153;&#22810;&#26679;&#24615;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;&#21644;&#22810;&#27169;&#24577;FL&#32858;&#21512;&#65288;CreamFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#34701;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;-
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>FOSI&#26159;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#25913;&#21892;&#19968;&#31867;&#20248;&#21270;&#22120;&#30340;&#26465;&#20214;&#25968;</title><link>http://arxiv.org/abs/2302.08484</link><description>&lt;p&gt;
FOSI&#65306;&#28151;&#21512;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08484
&lt;/p&gt;
&lt;p&gt;
FOSI&#26159;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#25913;&#21892;&#19968;&#31867;&#20248;&#21270;&#22120;&#30340;&#26465;&#20214;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#35745;&#31639;&#26354;&#29575;&#30340;&#22256;&#38590;&#23548;&#33268;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#26041;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#20165;&#20351;&#29992;&#19968;&#38454;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FOSI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#31639;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;FOSI&#38544;&#21547;&#22320;&#23558;&#20989;&#25968;&#20998;&#20026;&#20004;&#20010;&#23450;&#20041;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#19978;&#30340;&#20108;&#27425;&#20989;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#20108;&#38454;&#26041;&#27861;&#26368;&#23567;&#21270;&#19968;&#20010;&#20989;&#25968;&#65292;&#20351;&#29992;&#22522;&#26412;&#20248;&#21270;&#22120;&#26368;&#23567;&#21270;&#21478;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;FOSI&#25910;&#25947;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#23427;&#22312;&#19968;&#31867;&#20248;&#21270;&#22120;&#20013;&#25913;&#21892;&#20102;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#23545;&#20110;&#38899;&#39057;&#20998;&#31867;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#29289;&#20307;&#20998;&#31867;&#31561;&#20960;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;FOSI&#24212;&#29992;&#20110;GD&#65292;Heavy-Ball&#21644;Adam&#31561;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.05313</link><description>&lt;p&gt;
&#26234;&#33021;&#26448;&#26009;&#20013;&#31232;&#30095;&#28382;&#21518;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#65288;&#23588;&#20854;&#26159;&#21387;&#30005;&#26448;&#26009;&#65289;&#28382;&#21518;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#39034;&#24207;&#38408;&#20540;&#26041;&#27861;&#23545;&#36127;&#36131;&#28382;&#21518;&#30340;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#20102;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#21644;&#23454;&#39564;&#21387;&#30005;&#26448;&#26009;&#25968;&#25454;&#30340;&#28382;&#21518;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#23398;&#20064;&#34676;&#34678;&#24418;&#28382;&#21518;&#12289;&#23545;&#21387;&#30005;&#33268;&#21160;&#22120;&#30340;&#30495;&#23454;&#28382;&#21518;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#20197;&#38750;&#21462;&#21521;&#30005;&#24037;&#38050;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#23545;&#30913;&#24615;&#26448;&#26009;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24178;&#25200;&#22120;&#25239;&#24178;&#25200;&#39057;&#29575;&#21644;&#21151;&#29575;&#20998;&#37197;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26080;&#32447;&#32593;&#32476;&#25805;&#20316;&#22330;&#26223;&#19979;&#20351;&#29992;&#12290;&#22312;&#23454;&#29616;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#39640;&#25928;&#39057;&#35889;&#20351;&#29992;&#30340;&#21516;&#26102;&#23545;&#24178;&#25200;&#20855;&#26377;&#24456;&#24378;&#30340;&#25239;&#24178;&#25200;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02250</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24178;&#25200;&#22120;&#25239;&#24178;&#25200;&#39057;&#29575;&#21644;&#21151;&#29575;&#20998;&#37197;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of Deep Reinforcement Learning for Jammer-Resilient Frequency and Power Allocation. (arXiv:2302.02250v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24178;&#25200;&#22120;&#25239;&#24178;&#25200;&#39057;&#29575;&#21644;&#21151;&#29575;&#20998;&#37197;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26080;&#32447;&#32593;&#32476;&#25805;&#20316;&#22330;&#26223;&#19979;&#20351;&#29992;&#12290;&#22312;&#23454;&#29616;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#39640;&#25928;&#39057;&#35889;&#20351;&#29992;&#30340;&#21516;&#26102;&#23545;&#24178;&#25200;&#20855;&#26377;&#24456;&#24378;&#30340;&#25239;&#24178;&#25200;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#22312;&#39057;&#29575;&#21644;&#21151;&#29575;&#20998;&#37197;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26080;&#32447;&#38382;&#39064;&#22312;&#29305;&#23450;&#30340;&#39044;&#35774;&#30340;&#26080;&#32447;&#32593;&#32476;&#22330;&#26223;&#20013;&#12290;&#35757;&#32451;&#20195;&#29702;&#30340;&#24615;&#33021;&#36890;&#24120;&#38750;&#24120;&#29305;&#23450;&#20110;&#32593;&#32476;&#65292;&#24182;&#22312;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#25805;&#20316;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#22823;&#23567;&#12289;&#37051;&#22495;&#21644;&#31227;&#21160;&#24615;&#31561;&#65289;&#26102;&#21464;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;&#35757;&#32451;&#65292;&#20197;&#22312;&#24694;&#24847;&#24178;&#25200;&#29615;&#22659;&#19979;&#36890;&#36807;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#26041;&#24335;&#37096;&#32626;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26102;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#22823;&#23567;&#21644;&#32467;&#26500;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20223;&#30495;&#26080;&#32447;&#32593;&#32476;&#19978;&#27979;&#35797;&#26102;&#30340;&#25913;&#36827;&#35757;&#32451;&#21644;&#25512;&#29702;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20026;&#20102;&#35777;&#26126;&#23454;&#38469;&#24433;&#21709;&#65292;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#24050;&#23454;&#29616;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#35777;&#39640;&#25928;&#39057;&#35889;&#20351;&#29992;&#30340;&#21516;&#26102;&#23545;&#24178;&#25200;&#20855;&#26377;&#24456;&#24378;&#30340;&#25239;&#24178;&#25200;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of joint frequency and power allocation while emphasizing the generalization capability of a deep reinforcement learning model. Most of the existing methods solve reinforcement learning-based wireless problems for a specific pre-determined wireless network scenario. The performance of a trained agent tends to be very specific to the network and deteriorates when used in a different network operating scenario (e.g., different in size, neighborhood, and mobility, among others). We demonstrate our approach to enhance training to enable a higher generalization capability during inference of the deployed model in a distributed multi-agent setting in a hostile jamming environment. With all these, we show the improved training and inference performance of the proposed methods when tested on previously unseen simulated wireless networks of different sizes and architectures. More importantly, to prove practical impact, the end-to-end solution was implemented on the embedde
&lt;/p&gt;</description></item><item><title>&#21452;&#37325; PatchNorm &#22312; Vision Transformers &#20013;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01327</link><description>&lt;p&gt;
&#21452;&#37325; PatchNorm
&lt;/p&gt;
&lt;p&gt;
Dual PatchNorm. (arXiv:2302.01327v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01327
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325; PatchNorm &#22312; Vision Transformers &#20013;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325; PatchNorm&#65306;&#22312; Vision Transformers &#30340; patch &#23884;&#20837;&#23618;&#20043;&#21069;&#21644;&#20043;&#21518;&#37319;&#29992;&#20102;&#20004;&#20010; Layer Normalization &#23618;&#65288;LayerNorms&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#21452;&#37325; PatchNorm &#34920;&#29616;&#27604;&#22312; Transformer &#22359;&#26412;&#36523;&#20013;&#36827;&#34892;&#26367;&#20195; LayerNorm &#25918;&#32622;&#31574;&#30053;&#30340;&#35814;&#23613;&#25628;&#32034;&#30340;&#32467;&#26524;&#26356;&#20248;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#36825;&#31181;&#24494;&#23567;&#30340;&#25913;&#36827;&#36890;&#24120;&#20250;&#23548;&#33268;&#27604;&#31934;&#35843; Vision Transformers &#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20174;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms), before and after the patch embedding layer in Vision Transformers. We demonstrate that Dual PatchNorm outperforms the result of exhaustive search for alternative LayerNorm placement strategies in the Transformer block itself. In our experiments, incorporating this trivial modification, often leads to improved accuracy over well-tuned Vision Transformers and never hurts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2302.01018</link><description>&lt;p&gt;
&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#65288;&#38745;&#24577;&#65289;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#22270;&#21644;&#33410;&#28857;/&#36793;&#23646;&#24615;&#38543;&#30528;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25193;&#23637;GNN&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#26102;&#24577;GNN&#30340;&#29616;&#29366;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24341;&#20837;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#34920;&#31034;&#21644;&#22788;&#29702;&#26102;&#24577;&#26041;&#38754;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#32467;&#26463;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.00422</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#24182;&#19981;&#31616;&#21333;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#24178;&#39044;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#27979;&#35797;&#35774;&#22791;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#25311;&#21512;&#27169;&#22411;&#26102;&#26368;&#20449;&#24687;&#25968;&#25454;&#28857;&#30340;&#24314;&#35758;&#12290;&#20943;&#23569;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#21487;&#20197;&#20943;&#36731;&#35757;&#32451;&#25152;&#38656;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26631;&#35760;&#30456;&#20851;&#30340;&#25805;&#20316;&#25903;&#20986;&#12290;&#29305;&#21035;&#26159;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#22312;&#38656;&#35201;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#20915;&#23450;&#26159;&#21542;&#33719;&#21462;&#25968;&#25454;&#28857;&#26631;&#35760;&#30340;&#39640;&#23481;&#37327;&#29983;&#20135;&#36807;&#31243;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#32447;&#20027;&#21160;&#32447;&#24615;&#22238;&#24402;&#22312;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#35777;&#31283;&#23450;&#24615;&#24182;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;</title><link>http://arxiv.org/abs/2301.13392</link><description>&lt;p&gt;
&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Causal Bandits without Graph Skeleton. (arXiv:2301.13392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#19968;&#36718;&#36873;&#25321;&#19968;&#32452;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#25910;&#38598;&#35266;&#27979;&#21464;&#37327;&#30340;&#21453;&#39304;&#20197;&#26368;&#23567;&#21270;&#26399;&#26395;&#36951;&#25022;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;&#20108;&#20540;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;BGLMs&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#37117;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#26500;&#24314;&#22240;&#26524;&#20851;&#31995;&#22270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#25351;&#25968;&#19979;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#22270;&#39592;&#26550;&#26469;&#23454;&#29616;BGLMs&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#34920;&#26126;&#23427;&#20173;&#28982;&#36798;&#21040;$O(\sqrt{T}\ln T)$&#30340;&#26399;&#26395;&#36951;&#25022;&#12290;&#36825;&#20010;&#28176;&#36827;&#30340;&#36951;&#25022;&#29575;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In combinatorial causal bandits (CCB), the learning agent chooses a subset of variables in each round to intervene and collects feedback from the observed variables to minimize expected regret or sample complexity. Previous works study this problem in both general causal models and binary generalized linear models (BGLMs). However, all of them require prior knowledge of causal graph structure. This paper studies the CCB problem without the graph structure on binary general causal models and BGLMs. We first provide an exponential lower bound of cumulative regrets for the CCB problem on general causal models. To overcome the exponentially large space of parameters, we then consider the CCB problem on BGLMs. We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$ expected regret. This asymptotic regret is the same as the state-of-art algorithms relying on the graph structure. Moreover, we sacrifice the regret t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36880;&#19968;&#22788;&#29702;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#23588;&#20854;&#26159;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2301.07863</link><description>&lt;p&gt;
&#20174;&#27969;&#25968;&#25454;&#20013;&#22312;&#32447;&#21457;&#29616;&#28436;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online discovering governing differential equations of evolving systems from streaming data. (arXiv:2301.07863v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36880;&#19968;&#22788;&#29702;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#23588;&#20854;&#26159;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29616;&#26377;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#28436;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#26041;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65306;&#20174;&#27969;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#32508;&#21512;&#32771;&#34385;&#26679;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#26080;&#27861;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#27969;&#25968;&#25454;&#32780;&#38750;&#22788;&#29702;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#36880;&#19968;&#22788;&#29702;&#26679;&#26412;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22312;&#21457;&#29616;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#28436;&#21270;&#31995;&#32479;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#20854;&#29366;&#24577;&#20063;&#38543;&#20043;&#25913;&#21464;&#12290;&#22240;&#27492;&#25214;&#21040;&#31934;&#30830;&#30340;&#21464;&#21270;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#21464;&#21270;&#21518;&#30340;&#31995;&#32479;&#20135;&#29983;&#30340;&#27979;&#37327;&#20998;&#24067;&#19982;&#20197;&#21069;&#19981;&#21516;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35782;&#21035;&#20986;&#24046;&#24322;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#28436;&#21270;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#20013;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering the governing equations of evolving systems from available observations is essential and challenging. In this paper, we consider a new scenario: discovering governing equations from streaming data. Current methods struggle to discover governing differential equations with considering measurements as a whole, leading to failure to handle this task. We propose an online modeling method capable of handling samples one by one sequentially by modeling streaming data instead of processing the entire dataset. The proposed method performs well in discovering ordinary differential equations (ODEs) and partial differential equations (PDEs) from streaming data. Evolving systems are changing over time, which invariably changes with system status. Thus, finding the exact change points is critical. The measurement generated from a changed system is distributed dissimilarly to before; hence, the difference can be identified by the proposed method. Our proposal is competitive in identifyin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RedMule&#30340;&#20302;&#21151;&#32791;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#21644;&#26684;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#30340;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.03904</link><description>&lt;p&gt;
RedMule: &#29992;&#20110;&#33455;&#29255;&#32447;&#24615;&#20195;&#25968;&#21644;TinyML&#35757;&#32451;&#21152;&#36895;&#30340;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration. (arXiv:2301.03904v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RedMule&#30340;&#20302;&#21151;&#32791;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#21644;&#26684;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#30340;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20110;&#21151;&#32791;&#21482;&#26377;&#20960;&#21313;&#27627;&#29926;&#30340;&#36817;&#20256;&#24863;&#22120;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#38271;&#65292;&#32780;&#24403;&#21069;TinyML&#35757;&#32451;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#24418;&#24335;&#30340;&#35823;&#24046;&#21644;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#38656;&#35201;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#26469;&#28385;&#36275;&#31934;&#24230;&#21644;&#21160;&#24577;&#33539;&#22260;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#36816;&#31639;&#30340;&#33021;&#37327;&#21644;&#21151;&#32791;&#25104;&#26412;&#34987;&#35748;&#20026;&#22826;&#39640;&#65292;&#26080;&#27861;&#36866;&#24212;TinyML&#22330;&#26223;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#22312;&#23569;&#37327;&#27627;&#29926;&#21151;&#32791;&#39044;&#31639;&#19979;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;RedMulE-&#31616;&#21270;&#31934;&#24230;&#30697;&#38453;&#20056;&#27861;&#24341;&#25806;&#65288;Reduced-Precision Matrix Multiplication Engine&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22810;&#31934;&#24230;&#28014;&#28857;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM-Ops&#65289;&#21152;&#36895;&#32780;&#35774;&#35745;&#30340;&#20302;&#21151;&#32791;&#21152;&#36895;&#22120;&#65292;&#25903;&#25345;FP16&#21644;&#28151;&#21512;FP8&#26684;&#24335;&#65292;&#37319;&#29992;&#31526;&#21495;&#12289;&#25351;&#25968;&#12289;&#23614;&#25968;&#20026;&#65288;{1,4,3}&#65292;{1,5,2}&#65289;&#12290;&#25105;&#20204;&#23558;RedMule&#38598;&#25104;&#21040;&#19968;&#20010;&#21253;&#21547;&#20843;&#20010;&#33410;&#33021;RISC-V&#26680;&#24515;&#30340;&#24182;&#34892;&#36229;&#20302;&#21151;&#32791;&#65288;PULP&#65289;&#38598;&#32676;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V core
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MEOW&#65292;&#20351;&#29992;&#20803;&#36335;&#24452;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21152;&#26435;&#36127;&#26679;&#26412;&#65292;&#36890;&#36807;&#31895;&#30053;&#21644;&#32454;&#31890;&#24230;&#35270;&#22270;&#23545;&#24322;&#26500;&#22270;&#36827;&#34892;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2212.13847</link><description>&lt;p&gt;
&#24102;&#20803;&#36335;&#24452;&#32972;&#26223;&#21644;&#21152;&#26435;&#36127;&#26679;&#26412;&#30340;&#24322;&#26500;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Contrastive Learning with Meta-path Contexts and Weighted Negative Samples. (arXiv:2212.13847v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MEOW&#65292;&#20351;&#29992;&#20803;&#36335;&#24452;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21152;&#26435;&#36127;&#26679;&#26412;&#65292;&#36890;&#36807;&#31895;&#30053;&#21644;&#32454;&#31890;&#24230;&#35270;&#22270;&#23545;&#24322;&#26500;&#22270;&#36827;&#34892;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23545;&#27604;&#23398;&#20064;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#20803;&#36335;&#24452;&#26500;&#24314;&#23545;&#27604;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#26041;&#27861;&#24573;&#30053;&#20102;&#25551;&#36848;&#20803;&#36335;&#24452;&#36830;&#25509;&#23545;&#35937;&#30340;&#20016;&#23500;&#20803;&#36335;&#24452;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEOW&#65292;&#23427;&#32771;&#34385;&#20102;&#20803;&#36335;&#24452;&#19978;&#19979;&#25991;&#21644;&#21152;&#26435;&#36127;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MEOW&#20026;&#23545;&#27604;&#26500;&#36896;&#20102;&#19968;&#20010;&#31895;&#30053;&#35270;&#22270;&#21644;&#19968;&#20010;&#32454;&#31890;&#24230;&#35270;&#22270;&#12290;&#21069;&#32773;&#21453;&#26144;&#21738;&#20123;&#23545;&#35937;&#36890;&#36807;&#20803;&#36335;&#24452;&#30456;&#20114;&#36830;&#25509;&#65292;&#32780;&#21518;&#32773;&#20351;&#29992;&#20803;&#36335;&#24452;&#19978;&#19979;&#25991;&#24182;&#25551;&#36848;&#23545;&#35937;&#26159;&#22914;&#20309;&#36830;&#25509;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;InfoNCE&#25439;&#22833;&#24182;&#35748;&#35782;&#21040;&#23427;&#35745;&#31639;&#36127;&#26679;&#26412;&#26799;&#24230;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21306;&#20998;&#36127;&#26679;&#26412;&#65292;&#25105;&#20204;&#23398;&#20064;&#21152;&#26435;&#36127;&#26679;&#26412;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph contrastive learning has received wide attention recently. Some existing methods use meta-paths, which are sequences of object types that capture semantic relationships between objects, to construct contrastive views. However, most of them ignore the rich meta-path context information that describes how two objects are connected by meta-paths. Further, they fail to distinguish negative samples, which could adversely affect the model performance. To address the problems, we propose MEOW, which considers both meta-path contexts and weighted negative samples. Specifically, MEOW constructs a coarse view and a fine-grained view for contrast. The former reflects which objects are connected by meta-paths, while the latter uses meta-path contexts and characterizes details on how the objects are connected. Then, we theoretically analyze the InfoNCE loss and recognize its limitations for computing gradients of negative samples. To better distinguish negative samples, we learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#23384;&#22312;&#30340;&#33219;&#32959;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#21644;&#37327;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33219;&#32959;&#20250;&#21344;&#25454;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80&#65285;&#65292; &#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#19988;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#26368;&#39640;&#36798;99&#65285;&#12290;</title><link>http://arxiv.org/abs/2212.09437</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33219;&#32959;&#19988;&#23384;&#22312;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Systems are Bloated and Vulnerable. (arXiv:2212.09437v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#23384;&#22312;&#30340;&#33219;&#32959;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#21644;&#37327;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33219;&#32959;&#20250;&#21344;&#25454;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80&#65285;&#65292; &#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#19988;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#26368;&#39640;&#36798;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#36719;&#20214;&#20195;&#30721;&#21644;&#21151;&#33021;&#20887;&#20313;&#65292;&#24182;&#22823;&#22810;&#25968;&#29992;&#25143;&#24182;&#19981;&#20351;&#29992;&#12290;&#36825;&#31181;&#33219;&#32959;&#21344;&#25454;&#25972;&#20010;&#36719;&#20214;&#26632;&#65292;&#20174;&#25805;&#20316;&#31995;&#32479;&#21040;&#21518;&#31471;&#12289;&#21069;&#31471;&#21644;&#32593;&#39029;&#37117;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#21644;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#30340;&#33219;&#32959;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#30340;&#33219;&#32959;&#65292;&#27979;&#37327;&#23481;&#22120;&#21644;&#36719;&#20214;&#21253;&#30340;&#33219;&#32959;&#31243;&#24230;&#12290;&#35813;&#24037;&#20855;&#37327;&#21270;&#20102;&#33219;&#32959;&#30340;&#26469;&#28304;&#65292;&#24182;&#19982;&#28431;&#27934;&#20998;&#26512;&#24037;&#20855;&#38598;&#25104;&#65292;&#20197;&#35780;&#20272;&#33219;&#32959;&#23545;&#23481;&#22120;&#28431;&#27934;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;Tensorflow&#12289;Pytorch&#21644;NVIDIA&#30340;15&#20010;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#33219;&#32959;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21344;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33219;&#32959;&#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#24182;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#65292;&#26368;&#39640;&#36798;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from the operating system, all the way to software backends, frontends, and web-pages. In this paper, we focus on analyzing and quantifying bloat in machine learning containers. We develop MMLB, a framework to analyze bloat in machine learning containers, measuring the amount of bloat that exists on the container and package levels. Our tool quantifies the sources of bloat and integrates with vulnerability analysis tools to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from Tensorflow, Pytorch, and NVIDIA, we show that bloat is a significant issue, accounting for up to 80% of the container size in some cases. Our results demonstrate that bloat significantly increases the container provisioning time by up to 370% and exacerbates vulnerabilities by up to 99%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20195;&#25968;&#26354;&#38754;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#20043;&#38388;&#20132;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#25214;&#21040;&#25152;&#26377;&#23646;&#20110;&#35813;&#20132;&#38598;&#30340;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2212.03851</link><description>&lt;p&gt;
&#35745;&#31639;&#20195;&#25968;&#26354;&#38754;&#30340;&#32447;&#24615;&#37096;&#20998;&#65306;&#37327;&#23376;&#32416;&#32544;&#12289;&#24352;&#37327;&#20998;&#35299;&#21450;&#20854;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Computing linear sections of varieties: quantum entanglement, tensor decompositions and beyond. (arXiv:2212.03851v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#20195;&#25968;&#26354;&#38754;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#20043;&#38388;&#20132;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#25214;&#21040;&#25152;&#26377;&#23646;&#20110;&#35813;&#20132;&#38598;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;$\mathbb{F}^n$&#20013;&#65292;&#23547;&#25214;&#20219;&#24847;&#23545;&#21512;&#20195;&#25968;&#26354;&#38754;&#19982;&#32473;&#23450;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#20132;&#28857;&#20803;&#32032;&#65288;&#20854;&#20013;$\mathbb{F}$&#21487;&#20197;&#26159;&#23454;&#25968;&#25110;&#22797;&#25968;&#22495;&#65289;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#28085;&#30422;&#20102;&#19981;&#21516;&#36873;&#25321;&#30340;&#20195;&#25968;&#26354;&#38754;&#19979;&#30340;&#20016;&#23500;&#30340;&#31639;&#27861;&#38382;&#39064;&#26063;&#12290;&#29305;&#21035;&#22320;&#65292;&#26354;&#38754;&#30001;&#31209;&#20026;1&#30340;&#30697;&#38453;&#32452;&#25104;&#30340;&#24773;&#20917;&#24050;&#32463;&#19982;&#37327;&#23376;&#20449;&#24687;&#29702;&#35770;&#21644;&#24352;&#37327;&#20998;&#35299;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#26680;&#24515;&#38382;&#39064;&#23384;&#22312;&#32039;&#23494;&#32852;&#31995;&#12290;&#21363;&#20351;&#22312;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#38382;&#39064;&#24050;&#30693;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;NP-&#38590;&#30340;&#12290;&#20294;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#8220;&#20856;&#22411;&#8221;&#30340;&#23376;&#31354;&#38388;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#23376;&#31354;&#38388;$U \subseteq \mathbb{F}^n$&#34987;&#36873;&#25321;&#20026;&#29305;&#23450;&#32500;&#25968;&#30340;&#26222;&#36890;&#23376;&#31354;&#38388;&#65292;&#21487;&#33021;&#21253;&#21547;&#19968;&#20123;&#36890;&#29992;&#20803;&#32032;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#25214;&#21040;&#25152;&#26377;&#23646;&#20110;$U$&#21644;&#26354;&#38754;&#30340;&#20132;&#38598;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of finding elements in the intersection of an arbitrary conic variety in $\mathbb{F}^n$ with a given linear subspace (where $\mathbb{F}$ can be the real or complex field). This problem captures a rich family of algorithmic problems under different choices of the variety. The special case of the variety consisting of rank-1 matrices already has strong connections to central problems in different areas like quantum information theory and tensor decompositions. This problem is known to be NP-hard in the worst case, even for the variety of rank-1 matrices.  Surprisingly, despite these hardness results we develop an algorithm that solves this problem efficiently for "typical" subspaces. Here, the subspace $U \subseteq \mathbb{F}^n$ is chosen generically of a certain dimension, potentially with some generic elements of the variety contained in it. Our main result is a guarantee that our algorithm recovers all the elements of $U$ that lie in the variety, under some mild n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;Diffusion-SDF&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#65292;&#37319;&#29992;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2212.03293</link><description>&lt;p&gt;
&#36890;&#36807;&#20307;&#32032;&#25193;&#25955;&#23454;&#29616;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-SDF: Text-to-Shape via Voxelized Diffusion. (arXiv:2212.03293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;Diffusion-SDF&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#65292;&#37319;&#29992;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#19977;&#32500;&#34394;&#25311;&#24314;&#27169;&#25216;&#26415;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#22522;&#20110;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#65289;&#29983;&#25104;&#26032;&#39062;&#30340;&#19977;&#32500;&#20869;&#23481;&#24050;&#25104;&#20026;&#28909;&#38376;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-SDF&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#19977;&#32500;&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;UinU-Net&#32467;&#26500;&#65292;&#22312;&#26631;&#20934;&#30340;U-Net&#32467;&#26500;&#20013;&#23884;&#20837;&#19968;&#20010;&#23616;&#37096;&#32858;&#28966;&#30340;&#20869;&#37096;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29420;&#31435;&#20110;&#34917;&#19969;&#30340;SDF&#34920;&#31034;&#26041;&#27861;&#26356;&#22909;&#30340;&#37325;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#36827;&#19968;&#27493;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#24418;&#29366;&#23436;&#25104;&#21644;&#25552;&#39640;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#30340;&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#38382;&#39064;&#65292;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.11691</link><description>&lt;p&gt;
&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#30340;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Signature Algorithm for Multi-dimensional Path-Dependent Options. (arXiv:2211.11691v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#30340;&#22810;&#32500;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#38382;&#39064;&#65292;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36335;&#24452;&#20381;&#36182;&#26399;&#26435;&#30340;&#28145;&#24230;&#31614;&#21517;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;Hur\'e-Pham-Warin&#22312;2020&#24180;&#38024;&#23545;&#20855;&#26377;&#21453;&#23556;&#30340;&#29366;&#24577;&#20381;&#36182;&#22411;FBSDE&#30340;&#21453;&#21521;&#26041;&#26696;&#25193;&#23637;&#21040;&#20855;&#26377;&#21453;&#23556;&#30340;&#36335;&#24452;&#20381;&#36182;&#22411;FBSDE&#65292;&#36890;&#36807;&#21521;&#21453;&#21521;&#26041;&#26696;&#28155;&#21152;&#31614;&#21517;&#23618;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#27431;&#24335;&#22411;&#21644;&#32654;&#24335;&#22411;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#65292;&#32780;&#25903;&#20184;&#20989;&#25968;&#21462;&#20915;&#20110;&#22522;&#30784;&#27491;&#21521;&#32929;&#31080;&#36807;&#31243;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#20540;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#26126;&#30830;&#20381;&#36182;&#20110;&#31614;&#21517;&#30340;&#25130;&#26029;&#38454;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#35823;&#24046;&#12290;&#31639;&#27861;&#30340;&#25968;&#20540;&#31034;&#20363;&#21253;&#25324;&#65306;Black-Scholes&#27169;&#22411;&#19979;&#30340;Amerasian&#26399;&#26435;&#12289;&#20855;&#26377;&#36335;&#24452;&#20381;&#36182;&#20960;&#20309;&#24179;&#22343;&#25910;&#30410;&#20989;&#25968;&#30340;&#32654;&#24335;&#26399;&#26435;&#20197;&#21450;Shiryaev&#30340;&#26368;&#20248;&#20572;&#27490;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the deep signature algorithms for path-dependent options. We extend the backward scheme in [Hur\'e-Pham-Warin. Mathematics of Computation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to path-dependent FBSDEs with reflections, by adding the signature layer to the backward scheme. Our algorithm applies to both European and American type option pricing problems while the payoff function depends on the whole paths of the underlying forward stock process. We prove the convergence analysis of our numerical algorithm with explicit dependence on the truncation order of the signature and the neural network approximation errors. Numerical examples for the algorithm are provided including: Amerasian option under the Black-Scholes model, American option with a path-dependent geometric mean payoff function, and the Shiryaev's optimal stopping problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;SDA&#21644;&#36890;&#20449;(ISDAC)&#31995;&#32479;&#65292;&#29992;&#20110;&#25915;&#20987;&#32773;&#26816;&#27979;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36319;&#36394;&#38543;&#26426;&#27169;&#24335;&#24182;&#28385;&#36275;SDA&#35201;&#27714;&#65292;&#31995;&#32479;&#22312;12&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;&#37197;&#32622;&#19979;&#36798;&#21040;&#20102;97.8%&#20197;&#19978;&#30340;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.10260</link><description>&lt;p&gt;
&#38598;&#25104;&#31354;&#38388;&#22495;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Integrated Space Domain Awareness and Communication System. (arXiv:2211.10260v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;SDA&#21644;&#36890;&#20449;(ISDAC)&#31995;&#32479;&#65292;&#29992;&#20110;&#25915;&#20987;&#32773;&#26816;&#27979;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36319;&#36394;&#38543;&#26426;&#27169;&#24335;&#24182;&#28385;&#36275;SDA&#35201;&#27714;&#65292;&#31995;&#32479;&#22312;12&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;&#37197;&#32622;&#19979;&#36798;&#21040;&#20102;97.8%&#20197;&#19978;&#30340;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#27491;&#22312;&#21457;&#29983;&#21464;&#38761;&#65292;&#36825;&#31181;&#28436;&#21464;&#24102;&#26469;&#20102;&#26032;&#30340;&#23041;&#32961;&#65292;&#21152;&#19978;&#25216;&#26415;&#21457;&#23637;&#21644;&#24694;&#24847;&#24847;&#22270;&#65292;&#21487;&#33021;&#26500;&#25104;&#24040;&#22823;&#25361;&#25112;&#12290;&#31354;&#38388;&#22495;&#24863;&#30693;(SDA)&#26159;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#24615;&#24819;&#27861;&#65292;&#20854;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#33258;&#20027;&#24615;&#12289;&#26234;&#33021;&#24615;&#21644;&#28789;&#27963;&#24615;&#26469;&#23454;&#29616;&#31354;&#38388;&#28508;&#22312;&#23041;&#32961;&#30340;&#24863;&#30693;&#12289;&#26816;&#27979;&#12289;&#35782;&#21035;&#21644;&#23545;&#31574;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#28165;&#26224;&#30340;&#26032;&#31354;&#38388;&#35270;&#35282;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;SDA&#21644;&#36890;&#20449;(ISDAC)&#31995;&#32479;&#65292;&#29992;&#20110;&#25915;&#20987;&#32773;&#26816;&#27979;&#12290;&#25105;&#20204;&#20551;&#35774;&#25915;&#20987;&#32773;&#26377;&#27874;&#26463;&#25104;&#24418;&#22825;&#32447;&#65292;&#24182;&#33021;&#22815;&#21464;&#21270;&#25915;&#20987;&#24773;&#26223;&#65292;&#20363;&#22914;&#23545;&#26576;&#20123;&#25509;&#25910;&#22825;&#32447;&#36827;&#34892;&#38543;&#26426;&#25915;&#20987;&#12290;&#20026;&#20102;&#36319;&#36394;&#38543;&#26426;&#27169;&#24335;&#21644;&#28385;&#36275;SDA&#35201;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;ISDAC&#31995;&#32479;&#22312;12&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;&#37197;&#32622;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#21644;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26816;&#27979;&#31934;&#24230;&#36229;&#36807;97.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Space has been reforming and this evolution brings new threats that, together with technological developments and malicious intent, can pose a major challenge. Space domain awareness (SDA), a new conceptual idea, has come to the forefront. It aims sensing, detection, identification and countermeasures by providing autonomy, intelligence and flexibility against potential threats in space. In this study, we first present an insightful and clear view of the new space. Secondly, we propose an integrated SDA and communication (ISDAC) system for attacker detection. We assume that the attacker has beam-steering antennas and is capable to vary attack scenarios, such as random attacks on some receiver antennas. To track random patterns and meet SDA requirements, a lightweight convolutional neural network architecture is developed. The proposed ISDAC system shows superior and robust performance under 12 different attacker configurations with a detection accuracy of over 97.8%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#26412;&#22320;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#21644;&#20840;&#23616;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#65292;&#36890;&#36807;&#23454;&#29616;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.08914</link><description>&lt;p&gt;
&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Class-Aware Contrastive Federated Semi-Supervised Learning. (arXiv:2211.08914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#30340;&#26412;&#22320;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#21644;&#20840;&#23616;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#65292;&#36890;&#36807;&#23454;&#29616;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#65292;&#23427;&#21033;&#29992;&#20266;&#26631;&#31614;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#30693;&#35782;&#65292;&#22312;&#26410;&#32463;&#22788;&#29702;&#30340;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35757;&#32451;&#36807;&#31243;&#21463;&#21040;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#22823;&#20559;&#24046;&#20197;&#21450;&#20266;&#26631;&#31614;&#24341;&#20837;&#30340;&#30830;&#35748;&#20559;&#24046;&#30340;&#38459;&#30861;&#65292;&#36825;&#20004;&#32773;&#37117;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;DCCFSSL&#65289;&#30340;&#26032;&#22411;FSSL&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#26412;&#22320;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#21644;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20840;&#23616;&#31867;&#21035;&#24863;&#30693;&#20998;&#24067;&#12290;&#36890;&#36807;&#23454;&#29616;&#21452;&#31867;&#21035;&#24863;&#30693;&#23545;&#27604;&#27169;&#22359;&#65292;DCCFSSL&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, these training processes are hindered by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method accounts for both the local class-aware distribution of each client's data and the global class-aware distribution of all clients' data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08262</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
A mixed-categorical correlation kernel for Gaussian process. (arXiv:2211.08262v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#28151;&#21512;&#31867;&#21035;&#20803;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26680;&#65288;&#20363;&#22914;&#65292;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65289;&#25110;&#36890;&#36807;&#30452;&#25509;&#20272;&#35745;&#30456;&#20851;&#30697;&#38453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#25351;&#25968;&#26680;&#25193;&#23637;&#20026;&#22788;&#29702;&#28151;&#21512;&#31867;&#21035;&#21464;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26680;&#24341;&#23548;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#20195;&#29702;&#65292;&#23427;&#27010;&#25324;&#20102;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#27604;&#20854;&#20182;&#22522;&#20110;&#26680;&#30340;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#21644;&#26356;&#23567;&#30340;&#27531;&#24046;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;&#21040;Agent-State&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07805</link><description>&lt;p&gt;
&#24102;&#36741;&#21161;&#36755;&#20837;&#30340;Agent-State&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Agent-State Construction with Auxiliary Inputs. (arXiv:2211.07805v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;&#21040;Agent-State&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#30340;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#20915;&#31574;&#20195;&#29702;&#24448;&#24448;&#26080;&#27861;&#27169;&#25311;&#19990;&#30028;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#29615;&#22659;&#24448;&#24448;&#27604;&#20195;&#29702;&#26356;&#22823;&#26356;&#22797;&#26434;&#65292;&#36825;&#20063;&#31216;&#20026;&#37096;&#20998;&#35266;&#27979;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#24517;&#39035;&#21033;&#29992;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#30340;&#24863;&#23448;&#36755;&#20837;; &#23427;&#24517;&#39035;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#20197;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#12290;&#30446;&#21069;&#65292;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#24490;&#29615;&#32593;&#32476;&#20174;&#20195;&#29702;&#30340;&#24863;&#23448;&#27969;&#20316;&#20026;&#36755;&#20837;&#26469;&#23398;&#20064;Agent-State&#20989;&#25968;&#12290;&#35768;&#22810;&#24378;&#22823;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#20989;&#25968;&#26469;&#24110;&#21161;&#20195;&#29702;&#36755;&#20837;&#21382;&#21490;&#25688;&#35201;&#12290;&#36825;&#20123;&#22686;&#24378;&#26377;&#22810;&#31181;&#26041;&#24335;&#65292;&#20174;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#36830;&#25509;&#35266;&#23519;&#65292;&#21040;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#25105;&#20204;&#31216;&#20043;&#20026;&#36741;&#21161;&#36755;&#20837;&#30340;&#36825;&#20123;&#38468;&#21152;&#36755;&#20837;&#36890;&#24120;&#20197;&#19968;&#31181;&#29305;&#27530;&#30340;&#26041;&#24335;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;Agent-State&#26500;&#24314;&#36807;&#31243;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#20854;&#20013;&#36741;&#21161;&#36755;&#20837;&#29992;&#20110;&#35843;&#25972;&#20195;&#29702;&#29366;&#24577;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#20445;&#30041;&#19982;&#20915;&#31574;&#21046;&#23450;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.07098</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#23436;&#25104;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#24314;&#31435;&#26469;&#23384;&#20648;&#22823;&#37327;&#30693;&#35782;&#65292;&#28982;&#32780;&#36825;&#20123;&#30693;&#35782;&#24211;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#23436;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#26469;&#25552;&#21462;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20165;&#20165;&#36890;&#36807;&#38750;&#24120;&#23569;&#30340;&#38382;&#39064;&#23601;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#38382;&#31572;&#31995;&#32479;&#36824;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#27604;&#22914;&#23454;&#20307;&#31867;&#22411;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#36712;&#36857;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;TO&#24341;&#23548;&#30340;RL&#31574;&#30053;&#25628;&#32034;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#26469;&#36991;&#20813;&#22312;&#25511;&#21046;&#36807;&#31243;&#20013;&#38519;&#20837;&#36139;&#20047;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.06625</link><description>&lt;p&gt;
CACTO&#65306;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#36712;&#36857;&#20248;&#21270;Actor-Critic&#31639;&#27861;&#8212;&#8212;&#36208;&#21521;&#20840;&#23616;&#26368;&#20248;&#12290;&#65288;arXiv:2211.06625v3 [cs.RO] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality. (arXiv:2211.06625v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#36712;&#36857;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36890;&#36807;TO&#24341;&#23548;&#30340;RL&#31574;&#30053;&#25628;&#32034;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#26469;&#36991;&#20813;&#22312;&#25511;&#21046;&#36807;&#31243;&#20013;&#38519;&#20837;&#36139;&#20047;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32467;&#21512;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#35813;&#31639;&#27861;&#30340;&#21160;&#26426;&#26159;TO&#21644;RL&#22312;&#24212;&#29992;&#20110;&#38750;&#20984;&#20195;&#20215;&#20989;&#25968;&#30340;&#36830;&#32493;&#38750;&#32447;&#24615;&#31995;&#32479;&#26102;&#23384;&#22312;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#26412;&#25991;&#26041;&#27861;&#21033;&#29992;TO&#24341;&#23548;&#30340;RL&#31574;&#30053;&#25628;&#32034;&#26469;&#23398;&#20064;&#8220;&#22909;&#8221;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#29992;&#20316;TO&#30340;&#21021;&#22987;&#29468;&#27979;&#25552;&#20379;&#32773;&#65292;&#20351;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#19981;&#23481;&#26131;&#25910;&#25947;&#21040;&#36139;&#20047;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#19981;&#21516;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#38750;&#20984;&#38556;&#30861;&#29289;&#36991;&#20813;&#30340;&#21040;&#36798;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a "good" minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a "good" control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20809;&#23398;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.02578</link><description>&lt;p&gt;
&#20809;&#23398;&#22270;&#20687;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#28418;&#31227;&#25511;&#21046;&#30340;&#25968;&#25454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data Models for Dataset Drift Controls in Machine Learning With Optical Images. (arXiv:2211.02578v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20809;&#23398;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#22270;&#20687;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#22312;&#21307;&#23398;&#21644;&#29615;&#22659;&#35843;&#26597;&#31561;&#37325;&#35201;&#39046;&#22495;&#21457;&#25381;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24378;&#24230;&#38382;&#39064;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20027;&#35201;&#30340;&#25925;&#38556;&#27169;&#24335;&#26159;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#21644;&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#26377;&#26041;&#27861;&#21487;&#20197;&#21069;&#30651;&#24615;&#22320;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#36825;&#31181;&#25968;&#25454;&#38598;&#28418;&#31227;&#30340;&#24378;&#24230;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#20027;&#35201;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937; - &#25968;&#25454;&#30340;&#26174;&#24335;&#27169;&#22411;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#22312;&#29289;&#29702;&#19978;&#20934;&#30830;&#22320;&#30740;&#31350;&#21644;&#29702;&#35299;&#25968;&#25454;&#29983;&#25104;&#19982;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#29289;&#29702;&#20809;&#23398;&#37197;&#23545;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can 
&lt;/p&gt;</description></item><item><title>DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01095</link><description>&lt;p&gt;
DPM-Solver++&#65306;&#29992;&#20110;&#24341;&#23548;&#37319;&#26679;&#30340;&#24555;&#36895;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. (arXiv:2211.01095v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01095
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411; (DPMs) &#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780; DPM &#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#25152;&#38656;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#24341;&#23548;&#37319;&#26679;&#12290;&#29616;&#26377;&#30340;&#24555;&#36895;&#37319;&#26679;&#22120; DDIM &#22312;&#24341;&#23548;&#37319;&#26679;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201; 100 &#33267; 250 &#27493;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; DPM-Solver++&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#24341;&#23548;&#37319;&#26679;&#30340;&#39640;&#38454;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#25506;&#31350;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#36825;&#20123;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447;&#27169;&#22411; MOSNet&#65292;&#22312; MOS &#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#24179;&#22343;12%&#26080;&#20844;&#23475;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.00342</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#20110;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#30340;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features. (arXiv:2211.00342v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#25506;&#31350;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#25991;&#26412;&#26391;&#35835;&#33258;&#28982;&#24230;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#36825;&#20123;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447;&#27169;&#22411; MOSNet&#65292;&#22312; MOS &#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#24179;&#22343;12%&#26080;&#20844;&#23475;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21512;&#25104;&#38899;&#22768;&#30340;&#26368;&#26032;&#35780;&#20272;&#26041;&#27861;&#26159;&#22522;&#20110; MOS &#39044;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#20854;&#20013; MOSNet &#21644; LDNet &#20351;&#29992;&#39057;&#35889;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780; SSL-MOS &#21017;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#30452;&#25509;&#20351;&#29992;&#35821;&#38899;&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#12290;&#22312;&#29616;&#20195;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463; TTS &#31995;&#32479;&#20013;&#65292;&#23601;&#21457;&#38899;&#20869;&#23481;&#32780;&#35328;&#65292;&#22768;&#35843;&#30340;&#36866;&#24403;&#24615;&#26159;&#20915;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312; MOS &#39044;&#27979;&#31995;&#32479;&#20013;&#21253;&#25324;&#22768;&#35843;&#21644;&#35821;&#35328;&#29305;&#24449;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38899;&#32032;&#32423;&#30340; F0 &#21644;&#25345;&#32493;&#26102;&#38388;&#29305;&#24449;&#20316;&#20026;&#22768;&#35843;&#36755;&#20837;&#65292;&#20197;&#21450; Tacotron &#32534;&#30721;&#22120;&#36755;&#20986;&#12289;POS &#26631;&#35760;&#21644; BERT &#23884;&#20837;&#20316;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#36755;&#20837;&#12290;&#25152;&#26377; MOS &#39044;&#27979;&#31995;&#32479;&#22343;&#22312; SOMOS &#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#20165;&#26377;&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#21644;&#20247;&#21253;&#33258;&#28982;&#24230; MOS &#35780;&#20272;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#39069;&#22806;&#29305;&#24449;&#33021;&#22815;&#26377;&#21033;&#20110;&#25552;&#39640; MOS &#39044;&#27979;&#32467;&#26524;&#65292;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;&#39057;&#35889;&#29305;&#24449;&#30340;&#22522;&#32447; MOSNet&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;12%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#26694;&#26550;&#65292;&#20026;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#39044;&#27979;&#36136;&#37327;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.11222</link><description>&lt;p&gt;
&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#30340;&#23398;&#20064;&#22686;&#24378;&#31169;&#26377;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Private Algorithms for Multiple Quantile Release. (arXiv:2210.11222v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#26694;&#26550;&#65292;&#20026;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#39044;&#27979;&#36136;&#37327;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20110;&#25935;&#24863;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#24120;&#24120;&#21487;&#20197;&#21033;&#29992;&#39069;&#22806;&#30340;&#20449;&#24687;&#20363;&#22914;&#20854;&#20182;&#25935;&#24863;&#25968;&#25454;&#12289;&#20844;&#20247;&#25968;&#25454;&#25110;&#20154;&#31867;&#20449;&#24687;&#20808;&#39564;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65288;&#25110;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#31639;&#27861;&#65289;&#26694;&#26550;&#65292;&#36825;&#20010;&#26694;&#26550;&#36890;&#24120;&#20351;&#29992;&#20110;&#20248;&#21270;&#26102;&#38388;&#22797;&#26434;&#24230;&#25110;&#31454;&#20105;&#27604;&#29575;&#12290;&#35813;&#26694;&#26550;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#20197;&#25552;&#39640;&#25928;&#29992;&#12290;&#35813;&#24819;&#27861;&#20307;&#29616;&#22312;&#37325;&#35201;&#30340;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#20013;&#65292;&#22312;&#27492;&#25105;&#20204;&#24471;&#20986;&#20102;&#38543;&#30528;&#33258;&#28982;&#36136;&#37327;&#39044;&#27979;&#30340;&#38169;&#35823;&#20445;&#35777;&#65292;&#21516;&#26102;&#65288;&#20960;&#20046;&#65289;&#24674;&#22797;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#29420;&#31435;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#23545;&#25968;&#25454;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#20026;&#20004;&#20010;&#20174;&#20854;&#20182;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26032;&#39062;&#8220;&#20803;&#8221;&#31639;&#27861;&#25552;&#20379;&#26377;&#29992;&#30340;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework -- previously applied largely to improve time complexity or competitive ratios -- as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. This idea is instantiated on the important task of multiple quantile release, for which we derive error guarantees that scale with a natural measure of prediction quality while (almost) recovering state-of-the-art prediction-independent guarantees. Our analysis enjoys several advantages, including minimal assumptions about the data, a natural way of adding robustness, and the provision of useful surrogate losses for two novel ``meta" algorithms that learn predictions from other (potentially sensitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#20887;&#20313;&#30340;&#21487;&#38752;&#30340;&#25235;&#21462;&#21830;&#21697;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31995;&#32479;MMRNet&#65292;&#33021;&#22815;&#22312;&#20256;&#24863;&#22120;&#22833;&#25928;&#31561;&#24322;&#24120;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;YCB-Video&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10842</link><description>&lt;p&gt;
MMRNet&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#20887;&#20313;&#25552;&#39640;&#22810;&#27169;&#24577;&#29289;&#20307;&#26816;&#27979;&#19982;&#20998;&#21106;&#30340;&#21487;&#38752;&#24615;&#65292;&#24212;&#29992;&#20110;&#25235;&#21462;&#21830;&#21697;
&lt;/p&gt;
&lt;p&gt;
MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy. (arXiv:2210.10842v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#20887;&#20313;&#30340;&#21487;&#38752;&#30340;&#25235;&#21462;&#21830;&#21697;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31995;&#32479;MMRNet&#65292;&#33021;&#22815;&#22312;&#20256;&#24863;&#22120;&#22833;&#25928;&#31561;&#24322;&#24120;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;YCB-Video&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24037;&#19994;4.0&#22522;&#30784;&#35774;&#26045;&#30340;&#20852;&#36215;&#35299;&#20915;&#20102;&#20840;&#29699;&#20379;&#24212;&#38142;&#30340;&#21171;&#21160;&#21147;&#30701;&#32570;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#37096;&#32626;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#36135;&#29289;&#25441;&#25342;&#31995;&#32479;&#26469;&#20943;&#36731;&#24037;&#20154;&#30340;&#21387;&#21147;&#21644;&#20307;&#21147;&#38656;&#27714;&#65292;&#25552;&#39640;&#20179;&#24211;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#24050;&#32463;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#22312;&#20256;&#24863;&#22120;&#22833;&#28789;&#31561;&#24322;&#24120;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#25439;&#22351;&#39118;&#38505;&#65292;&#22240;&#27492;&#21487;&#38752;&#24615;&#25104;&#20026;&#23558;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#36716;&#21270;&#20026;&#23454;&#38469;&#24212;&#29992;&#21644;&#20135;&#21697;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31995;&#32479;MMRNet&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36827;&#34892;&#25235;&#21462;&#21830;&#21697;&#30340;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#32780;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#20887;&#20313;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#25552;&#39640;&#21487;&#38752;&#24615;&#30340;&#25235;&#21462;&#21830;&#21697;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;YCB-Video&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20256;&#24863;&#22120;&#22833;&#25928;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#35843;&#26597;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;&#31038;&#20250;&#21644;&#32593;&#32476;&#23433;&#20840;&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25552;&#20379;&#20102;&#26368;&#23436;&#25972;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#35780;&#20272;&#65292;&#20026;&#24212;&#23545;&#23041;&#32961;&#27169;&#22411;&#21644;&#35299;&#20915;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.07321</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65306;&#23041;&#32961;&#27169;&#22411;&#21644;&#26816;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#35843;&#26597;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;&#31038;&#20250;&#21644;&#32593;&#32476;&#23433;&#20840;&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25552;&#20379;&#20102;&#26368;&#23436;&#25972;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#35780;&#20272;&#65292;&#20026;&#24212;&#23545;&#23041;&#32961;&#27169;&#22411;&#21644;&#35299;&#20915;&#26816;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#38590;&#20197;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#12290;&#21151;&#33021;&#24378;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#65292;&#21487;&#27665;&#20027;&#21270;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24037;&#20855;&#27491;&#22312;&#36805;&#36895;&#22686;&#22810;&#12290;&#26412;&#27425;&#35843;&#26597;&#30340;&#31532;&#19968;&#29256;&#38754;&#19990;&#21518;&#19981;&#20037;&#65292;&#21457;&#24067;&#20102;ChatGPT&#65292;&#36825;&#19968;&#36235;&#21183;&#34987;&#24432;&#26174;&#20986;&#26469;&#12290;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#34987;&#21508;&#31181;&#28389;&#29992;&#36884;&#24452;&#25152;&#25233;&#21046;&#12290;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#20943;&#23569;NLG&#27169;&#22411;&#28389;&#29992;&#30340;&#20027;&#35201;&#23545;&#31574;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#37325;&#22823;&#25216;&#26415;&#25361;&#25112;&#21644;&#20247;&#22810;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#65292;&#21253;&#25324;1&#65289;&#23545;&#24403;&#20195;NLG&#31995;&#32479;&#36896;&#25104;&#23041;&#32961;&#27169;&#22411;&#30340;&#24191;&#27867;&#20998;&#26512;&#21644;2&#65289;&#25130;&#33267;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#30340;&#26368;&#23436;&#25972;&#30340;&#32508;&#36848;&#12290;&#36825;&#20221;&#35843;&#26597;&#23558;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#32622;&#20110;&#20854;&#32593;&#32476;&#23433;&#20840;&#21644;&#31038;&#20250;&#32972;&#26223;&#20043;&#20013;&#65292;&#24182;&#20026;&#26410;&#26469;&#35299;&#20915;&#26368;&#37325;&#35201;&#23041;&#32961;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25351;&#23548;&#65292;&#24182;&#27010;&#36848;&#20102;&#26368;&#26377;&#21069;&#36884;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine generated text is increasingly difficult to distinguish from human authored text. Powerful open-source models are freely available, and user-friendly tools that democratize access to generative models are proliferating. ChatGPT, which was released shortly after the first edition of this survey, epitomizes these trends. The great potential of state-of-the-art natural language generation (NLG) systems is tempered by the multitude of avenues for abuse. Detection of machine generated text is a key countermeasure for reducing abuse of NLG models, with significant technical challenges and numerous open problems. We provide a survey that includes both 1) an extensive analysis of threat models posed by contemporary NLG systems, and 2) the most complete review of machine generated text detection methods to date. This survey places machine generated text within its cybersecurity and social context, and provides strong guidance for future work addressing the most critical threat models, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.03919</link><description>&lt;p&gt;
CLIP-PAE&#65306;&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#20197;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#21487;&#20998;&#31163;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#25511;&#30340;&#25991;&#26412;&#25351;&#23548;&#33080;&#37096;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#22823;&#38376;&#65292;&#21363;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25991;&#23383;&#35828;&#26126;&#26469;&#25805;&#20316;&#36755;&#20837;&#22270;&#20687;&#30340;&#20016;&#23500;&#25991;&#23398;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#22270;&#20687;&#20013;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#12290;&#23545;&#20110;&#25805;&#32437;&#26469;&#35828;&#65292;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#20063;&#24456;&#38590;&#20445;&#35777;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23450;&#20041;&#30001;&#30456;&#20851;&#25552;&#31034;&#23637;&#24320;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#25429;&#33719;&#29305;&#23450;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#35745;&#31639;&#21644;&#36866;&#24212;&#65292;&#24182;&#24179;&#31283;&#22320;&#34701;&#20837;&#21040;&#20219;&#20309;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#25805;&#20316;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;PINN&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#39640;&#38454;PDE&#27745;&#26579;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#38382;&#39064;&#65292;&#20943;&#23567;&#20102;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#65292;&#20351;&#38750;&#24179;&#28369;&#35299;&#30340;PDE&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.09988</link><description>&lt;p&gt;
&#25506;&#31350;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25925;&#38556;&#27169;&#24335;&#21450;&#20854;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating and Mitigating Failure Modes in Physics-informed Neural Networks (PINNs). (arXiv:2209.09988v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;PINN&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#39640;&#38454;PDE&#27745;&#26579;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#38382;&#39064;&#65292;&#20943;&#23567;&#20102;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#65292;&#20351;&#38750;&#24179;&#28369;&#35299;&#30340;PDE&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;PINNs&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#25163;&#21160;&#35843;&#33410;&#36229;&#21442;&#25968;&#65292;&#22914;&#26524;&#27809;&#26377;&#39564;&#35777;&#25968;&#25454;&#25110;&#20808;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#30693;&#35782;&#65292;&#21017;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#29289;&#29702;&#23398;&#30340;&#25439;&#22833;&#26223;&#35266;&#21644;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#22256;&#38590;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#20250;&#20135;&#29983;&#38590;&#20197;&#23548;&#33322;&#30340;&#38750;&#20984;&#25439;&#22833;&#26223;&#35266;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#39640;&#38454;PDEs&#20250;&#27745;&#26579;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#24182;&#19988;&#38459;&#30861;&#25910;&#25947;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32469;&#36807;&#39640;&#38454;&#23548;&#25968;&#31639;&#23376;&#30340;&#35745;&#31639;&#65292;&#24182;&#20943;&#36731;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#30340;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#24182;&#20351;&#38750;&#24179;&#28369;&#35299;&#30340;PDE&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35843;&#33410;&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35843;&#33410;&#36229;&#21442;&#25968;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the difficulties in solving partial differential equations (PDEs) using physics-informed neural networks (PINNs). PINNs use physics as a regularization term in the objective function. However, a drawback of this approach is the requirement for manual hyperparameter tuning, making it impractical in the absence of validation data or prior knowledge of the solution. Our investigations of the loss landscapes and backpropagated gradients in the presence of physics reveal that existing methods produce non-convex loss landscapes that are hard to navigate. Our findings demonstrate that high-order PDEs contaminate backpropagated gradients and hinder convergence. To address these challenges, we introduce a novel method that bypasses the calculation of high-order derivative operators and mitigates the contamination of backpropagated gradients. Consequently, we reduce the dimension of the search space and make learning PDEs with non-smooth solutions feasible. Our method also pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Magicube&#65292;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#26680;&#30340;&#20302;&#31934;&#24230;&#25972;&#25968;&#30340;&#39640;&#24615;&#33021;&#31232;&#30095;&#30697;&#38453;&#24211;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#36816;&#31639;&#12290;&#22312;NVIDIA A100 GPU&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;Magicube&#30456;&#23545;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#24179;&#22343;&#33719;&#24471;&#20102;1.44&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29366;&#24577;-of-the-art&#30340;GPU&#24211;&#65292;&#36895;&#24230;&#25552;&#21319;&#20102;1.43&#20493;&#12290;</title><link>http://arxiv.org/abs/2209.06979</link><description>&lt;p&gt;
&#24352;&#37327;&#26680;&#19978;&#39640;&#25928;&#30340;&#37327;&#21270;&#31232;&#30095;&#30697;&#38453;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantized Sparse Matrix Operations on Tensor Cores. (arXiv:2209.06979v4 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Magicube&#65292;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#26680;&#30340;&#20302;&#31934;&#24230;&#25972;&#25968;&#30340;&#39640;&#24615;&#33021;&#31232;&#30095;&#30697;&#38453;&#24211;&#65292;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#36816;&#31639;&#12290;&#22312;NVIDIA A100 GPU&#19978;&#23454;&#39564;&#34920;&#26126;&#65292;Magicube&#30456;&#23545;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#24179;&#22343;&#33719;&#24471;&#20102;1.44&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29366;&#24577;-of-the-art&#30340;GPU&#24211;&#65292;&#36895;&#24230;&#25552;&#21319;&#20102;1.43&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19981;&#26029;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#24102;&#26469;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#24050;&#32463;&#34987;&#30740;&#31350;&#12290;&#20174;&#30828;&#20214;&#35282;&#24230;&#26469;&#30475;&#65292;&#30828;&#20214;&#20379;&#24212;&#21830;&#20026;&#21152;&#36895;&#25552;&#20379;&#20102;&#24352;&#37327;&#26680;&#12290;&#28982;&#32780;&#65292;&#20174;&#20302;&#31934;&#24230;&#30697;&#38453;&#36816;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22312;&#24352;&#37327;&#26680;&#19978;&#23454;&#29616;&#31232;&#30095;&#65292;&#20302;&#31934;&#24230;&#30697;&#38453;&#25805;&#20316;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#28385;&#36275;&#19981;&#21516;&#30340;&#25968;&#25454;&#24067;&#23616;&#38656;&#27714;&#21644;&#26377;&#25928;&#22320;&#25805;&#20316;&#20302;&#31934;&#24230;&#25972;&#25968;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Magicube&#65292;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#26680;&#30340;&#20302;&#31934;&#24230;&#25972;&#25968;&#30340;&#39640;&#24615;&#33021;&#31232;&#30095;&#30697;&#38453;&#24211;&#12290;Magicube&#25903;&#25345;&#28151;&#21512;&#31934;&#24230;&#19979;&#30340;SpMM&#21644;SDDMM&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#20004;&#20010;&#20027;&#35201;&#30340;&#31232;&#30095;&#25805;&#20316;&#12290;&#22312;NVIDIA A100 GPU&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Magicube&#30456;&#23545;&#20110;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#65292;&#24179;&#22343;&#33719;&#24471;1.44&#20493;&#65288;&#26368;&#39640;2.37&#20493;&#65289;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29366;&#24577;-of-the-art&#30340;GPU&#24211;&#65292;&#36895;&#24230;&#25552;&#21319;&#20102;1.43&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#36857;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#29992;&#20110;&#39044;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#27531;&#20313;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#24314;&#27169;&#36825;&#20123;&#27531;&#20313;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#21160;&#24577;&#27169;&#22411;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.03210</link><description>&lt;p&gt;
Real-to-Sim: &#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#36857;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39044;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#27531;&#20313;&#35823;&#24046;&#30340;&#31232;&#30095;&#25968;&#25454;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Real-to-Sim: Predicting Residual Errors of Robotic Systems with Sparse Data using a Learning-based Unscented Kalman Filter. (arXiv:2209.03210v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#36857;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#29992;&#20110;&#39044;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#27531;&#20313;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#24314;&#27169;&#36825;&#20123;&#27531;&#20313;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#21160;&#24577;&#27169;&#22411;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#19982;&#23454;&#38469;&#26426;&#22120;&#20154;&#25509;&#36817;&#30340;&#39640;&#31934;&#24230;&#21160;&#24577;&#27169;&#22411;&#25110;&#20223;&#30495;&#27169;&#22411;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65288;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#25110;&#32447;&#24615;&#20108;&#27425;&#22411;&#25511;&#21046;&#22120;&#65289;&#12289;&#22522;&#20110;&#27169;&#22411;&#30340;&#36712;&#36857;&#35268;&#21010;&#65288;&#22914;&#36712;&#36857;&#20248;&#21270;&#65289;&#20197;&#21450;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25152;&#38656;&#30340;&#23398;&#20064;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#21644;/&#25110;&#20223;&#30495;&#27169;&#22411;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#27531;&#20313;&#35823;&#24046;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#36890;&#36807;&#26080;&#36857;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;UKF&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#23601;&#21487;&#20197;&#23545;&#36825;&#20123;&#27531;&#20313;&#35823;&#24046;&#36827;&#34892;&#24314;&#27169;&#8212;&#8212;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#20174;&#23454;&#38469;&#25805;&#20316;&#20013;&#23398;&#20064;&#26469;&#25913;&#36827;&#27169;&#25311;&#22120;/&#21160;&#24577;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#30828;&#20214;&#19978;&#65288;&#20363;&#22914;&#25805;&#20316;&#33218;&#21644;&#36718;&#24335;&#26426;&#22120;&#20154;&#65289;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#27531;&#20313;&#35823;&#24046;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#32553;&#23567;&#21160;&#24577;&#27169;&#22411;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving highly accurate dynamic or simulator models that are close to the real robot can facilitate model-based controls (e.g., model predictive control or linear-quadradic regulators), model-based trajectory planning (e.g., trajectory optimization), and decrease the amount of learning time necessary for reinforcement learning methods. Thus, the objective of this work is to learn the residual errors between a dynamic and/or simulator model and the real robot. This is achieved using a neural network, where the parameters of a neural network are updated through an Unscented Kalman Filter (UKF) formulation. Using this method, we model these residual errors with only small amounts of data -- a necessity as we improve the simulator/dynamic model by learning directly from real-world operation. We demonstrate our method on robotic hardware (e.g., manipulator arm, and a wheeled robot), and show that with the learned residual errors, we can further close the reality gap between dynamic models
&lt;/p&gt;</description></item><item><title>&#20010;&#20307;&#27010;&#29575;&#39044;&#27979;&#30340;&#21327;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#20110;&#20010;&#20307;&#27010;&#29575;&#30340;&#23454;&#35777;&#39564;&#35777;&#21644;&#25913;&#36827;&#65292;&#20351;&#24471;&#27169;&#22411;&#24471;&#21040;&#25552;&#21319;&#24182;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#36798;&#25104;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2209.01687</link><description>&lt;p&gt;
&#20010;&#20307;&#27010;&#29575;&#39044;&#27979;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Reconciling Individual Probability Forecasts. (arXiv:2209.01687v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01687
&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#27010;&#29575;&#39044;&#27979;&#30340;&#21327;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#20110;&#20010;&#20307;&#27010;&#29575;&#30340;&#23454;&#35777;&#39564;&#35777;&#21644;&#25913;&#36827;&#65292;&#20351;&#24471;&#27169;&#22411;&#24471;&#21040;&#25552;&#21319;&#24182;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#36798;&#25104;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#27010;&#29575;&#25351;&#30340;&#26159;&#20165;&#22312;&#19968;&#27425;&#32467;&#26524;&#20013;&#23454;&#29616;&#30340;&#32467;&#26524;&#27010;&#29575;&#65292;&#22914;&#26126;&#22825;&#19979;&#38632;&#27010;&#29575;&#12289;Alice&#22312;&#25509;&#19979;&#26469;&#30340;12&#20010;&#26376;&#20869;&#27515;&#20129;&#27010;&#29575;&#12289;Bob&#22312;&#25509;&#19979;&#26469;&#30340;18&#20010;&#26376;&#20869;&#22240;&#26292;&#21147;&#29359;&#32618;&#34987;&#25429;&#30340;&#27010;&#29575;&#31561;&#12290;&#20010;&#20307;&#27010;&#29575;&#26412;&#36136;&#19978;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21516;&#24847;&#25968;&#25454;&#25110;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#20004;&#20010;&#24403;&#20107;&#26041;&#19981;&#33021;&#22312;&#22914;&#20309;&#24314;&#27169;&#20010;&#20307;&#27010;&#29575;&#19978;&#36798;&#25104;&#19968;&#33268;&#65292;&#22240;&#20026;&#20219;&#20309;&#20004;&#31181;&#26377;&#30528;&#23454;&#36136;&#24615;&#20998;&#27495;&#30340;&#20010;&#20307;&#27010;&#29575;&#27169;&#22411;&#37117;&#21487;&#20197;&#19968;&#36215;&#29992;&#20110;&#23454;&#35777;&#22320;&#35777;&#26126;&#24182;&#25913;&#21892;&#20854;&#20013;&#33267;&#23569;&#19968;&#20010;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#39640;&#25928;&#22320;&#36845;&#20195;&#65292;&#22312;"&#21327;&#35843;"&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#21452;&#26041;&#37117;&#35748;&#21516;&#30340;&#20248;&#20110;&#21021;&#22987;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#20917;&#19979;&#65288;&#20960;&#20046;&#65289;&#36798;&#25104;&#20102;&#20010;&#20307;&#27010;&#29575;&#39044;&#27979;&#30340;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#26159;&#65292;&#23613;&#31649;&#20010;&#20307;&#27010;&#29575;&#39044;&#27979;&#26412;&#36136;&#19978;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#38388;&#25509;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data -- or on how to sample from a data distribution -- cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of "reconciliation" that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#30456;&#20851;&#30740;&#31350;&#65292;&#21253;&#25324;&#35782;&#21035;&#20854;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#12289;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20102;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2208.11857</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#25463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shortcut Learning of Large Language Models in Natural Language Understanding. (arXiv:2208.11857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#30456;&#20851;&#30740;&#31350;&#65292;&#21253;&#25324;&#35782;&#21035;&#20854;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#12289;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#35752;&#20102;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#21487;&#33021;&#20250;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#32570;&#38519;&#20316;&#20026;&#39044;&#27979;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#36825;&#26174;&#33879;&#22320;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#35299;&#20915;LLMs&#24555;&#25463;&#23398;&#20064;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#25463;&#23398;&#20064;&#27010;&#24565;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#35782;&#21035;&#35821;&#35328;&#27169;&#22411;&#24555;&#25463;&#23398;&#20064;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#34920;&#24449;&#24555;&#25463;&#23398;&#20064;&#30340;&#21407;&#22240;&#65292;&#24182;&#20171;&#32461;&#20102;&#32531;&#35299;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness. In this paper, we provide a review of recent developments that address the shortcut learning and robustness challenge of LLMs. We first introduce the concepts of shortcut learning of language models. We then introduce methods to identify shortcut learning behavior in language models, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we discuss key research challenges and potential research directions in order to advance the field of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#26694;&#26550;(SSSD)&#26469;&#25554;&#34917;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#21644;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;SSSD&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#40657;&#23631;&#32570;&#22833;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2208.09399</link><description>&lt;p&gt;
&#24102;&#32422;&#26463;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#25193;&#25955;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models. (arXiv:2208.09399v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#26694;&#26550;(SSSD)&#26469;&#25554;&#34917;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#21644;&#36827;&#34892;&#39044;&#27979;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;SSSD&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#40657;&#23631;&#32570;&#22833;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#20540;&#30340;&#25554;&#34917;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#26512;&#31649;&#36947;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;SSSD&#65292;&#36825;&#26159;&#19968;&#31181;&#20381;&#36182;&#20110;&#20004;&#20010;&#26032;&#20852;&#25216;&#26415;&#30340;&#25554;&#34917;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;&#65288;&#26465;&#20214;&#65289;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#24102;&#32422;&#26463;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#20869;&#37096;&#27169;&#22411;&#26550;&#26500;&#65292;&#20854;&#29305;&#21035;&#36866;&#29992;&#20110;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;SSSD&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#25361;&#25112;&#24615;&#30340;&#40657;&#23631;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#22343;&#21487;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#25554;&#34917;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#20808;&#21069;&#30340;&#26041;&#27861;&#21017;&#26080;&#27861;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#22312;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#26469;&#23454;&#29616;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2208.07282</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;WORLD&#21512;&#25104;&#22120;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#21450;&#20854;&#22312;&#31471;&#21040;&#31471;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer. (arXiv:2208.07282v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07282
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#22312;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#26469;&#23454;&#29616;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;WORLD&#21512;&#25104;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31471;&#21040;&#31471;&#38899;&#39057;&#39118;&#26684;&#36716;&#25442;&#20219;&#21153;&#65288;&#22914;&#65288;&#21809;&#65289;&#22768;&#38899;&#36716;&#25442;&#21644;DDSP&#38899;&#33394;&#36716;&#25442;&#20219;&#21153;&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#24494;&#20998;&#21512;&#25104;&#22120;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#23427;&#20135;&#29983;&#20102;&#36275;&#22815;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#36731;&#37327;&#32423;&#30340;&#40657;&#31665;&#21518;&#32593;&#32476;&#26469;&#25193;&#23637;&#22522;&#32447;&#21512;&#25104;&#22120;&#65292;&#20197;&#36827;&#19968;&#27493;&#22788;&#29702;&#22522;&#32447;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;&#21478;&#19968;&#31181;&#21487;&#24494;&#20998;&#26041;&#27861;&#26159;&#30452;&#25509;&#25552;&#21462;&#28304;&#28608;&#21457;&#35889;&#65292;&#36825;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#24230;&#65292;&#20294;&#20165;&#36866;&#29992;&#20110;&#36739;&#31364;&#30340;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#31867;&#21035;&#12290;&#25105;&#20204;&#26041;&#27861;&#20351;&#29992;&#30340;&#22768;&#23398;&#29305;&#24449;&#21442;&#25968;&#21270;&#20855;&#26377;&#38468;&#21152;&#30340;&#22909;&#22788;&#65292;&#23427;&#33258;&#28982;&#22320;&#23558;&#38899;&#39640;&#21644;&#38899;&#33394;&#20449;&#24687;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#20998;&#21035;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26377;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#22768;&#36947;&#38899;&#39057;&#28304;&#20272;&#35745;&#36825;&#20123;&#22768;&#23398;&#29305;&#24449;&#65292;&#23427;&#20801;&#35768;&#39118;&#26684;&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#21442;&#25968;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks such as (singing) voice conversion and the DDSP timbre transfer task. Accordingly, our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We can extend the baseline synthesizer by appending lightweight black-box postnets which apply further processing to the baseline output in order to improve fidelity. An alternative differentiable approach considers extraction of the source excitation spectrum directly, which can improve naturalness albeit for a narrower class of style transfer applications. The acoustic feature parameterization used by our approaches has the added benefit that it naturally disentangles pitch and timbral information so that they can be modeled separately. Moreover, as there exists a robust means of estimating these acoustic features from monophonic audio sources, it allows for parameter loss 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#20960;&#20309;&#40065;&#26834;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GPU&#20248;&#21270;&#39564;&#35777;&#22120;&#65292;&#33021;&#22815;&#24555;&#36895;&#22320;&#35757;&#32451;&#40065;&#26834;&#30340;DNNs&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978; consistently achieve state-of-the-art deterministic certified geometric robustness &#21644;&#24178;&#20928;&#20934;&#30830;&#24230;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11177</link><description>&lt;p&gt;
&#20960;&#20309;&#21464;&#25442;&#30340;&#21487;&#35777;&#26126;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Provable Defense Against Geometric Transformations. (arXiv:2207.11177v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#20960;&#20309;&#40065;&#26834;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GPU&#20248;&#21270;&#39564;&#35777;&#22120;&#65292;&#33021;&#22815;&#24555;&#36895;&#22320;&#35757;&#32451;&#40065;&#26834;&#30340;DNNs&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978; consistently achieve state-of-the-art deterministic certified geometric robustness &#21644;&#24178;&#20928;&#20934;&#30830;&#24230;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20986;&#29616;&#30340;&#20960;&#20309;&#22270;&#20687;&#21464;&#25442;&#65288;&#22914;&#32553;&#25918;&#21644;&#26059;&#36716;&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#36731;&#26494;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;DNN&#20197;&#22312;&#36825;&#20123;&#25200;&#21160;&#19979;&#26377;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#39564;&#35777;&#22120;&#36895;&#24230;&#36807;&#24930;&#65292;&#26410;&#33021;&#23558;&#30830;&#23450;&#24615;&#35748;&#35777;&#40065;&#26834;&#24615;&#23545;&#25239;&#20960;&#20309;&#21464;&#25442;&#30340;&#30446;&#26631;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#20960;&#20309;&#40065;&#26834;&#24615;&#30340;&#38450;&#24481;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GPU&#20248;&#21270;&#39564;&#35777;&#22120;&#65292;&#20854;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20960;&#20309;&#40065;&#26834;&#24615;&#39564;&#35777;&#22120;&#24555;60&#20493;&#21040;42,600&#20493;&#65292;&#22240;&#27492;&#19982;&#29616;&#26377;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#26694;&#26550;&#36275;&#22815;&#24555;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#32593;&#32476;&#22987;&#32456;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#20960;&#20309;&#40065;&#26834;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22522;&#20110;&#25200;&#21160;&#35757;&#32451;&#30340;DNNs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric image transformations that arise in the real world, such as scaling and rotation, have been shown to easily deceive deep neural networks (DNNs). Hence, training DNNs to be certifiably robust to these perturbations is critical. However, no prior work has been able to incorporate the objective of deterministic certified robustness against geometric transformations into the training procedure, as existing verifiers are exceedingly slow. To address these challenges, we propose the first provable defense for deterministic certified geometric robustness. Our framework leverages a novel GPU-optimized verifier that can certify images between 60$\times$ to 42,600$\times$ faster than existing geometric robustness verifiers, and thus unlike existing works, is fast enough for use in training. Across multiple datasets, our results show that networks trained via our framework consistently achieve state-of-the-art deterministic certified geometric robustness and clean accuracy. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.05442</link><description>&lt;p&gt;
Wasserstein&#22810;&#20803;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#21450;&#20854;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#21253;&#25324;&#19968;&#32452;&#22312;&#23454;&#32447;&#26377;&#30028;&#38388;&#38548;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#22810;&#20010;&#31995;&#21015;&#65292;&#24182;&#19988;&#34987;&#19981;&#21516;&#26102;&#38388;&#30636;&#38388;&#25152;&#32034;&#24341;&#12290;&#27010;&#29575;&#27979;&#24230;&#34987;&#24314;&#27169;&#20026;Wasserstein&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Lebesgue&#27979;&#24230;&#30340;&#20999;&#31354;&#38388;&#20013;&#24314;&#31435;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#39318;&#20808;&#23545;&#25152;&#26377;&#21407;&#22987;&#27979;&#24230;&#36827;&#34892;&#23621;&#20013;&#22788;&#29702;&#65292;&#20197;&#20415;&#23427;&#20204;&#30340;Fr&#233;chet&#24179;&#22343;&#20540;&#25104;&#20026;Lebesgue&#27979;&#24230;&#12290;&#21033;&#29992;&#36845;&#20195;&#38543;&#26426;&#20989;&#25968;&#31995;&#32479;&#30340;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#24179;&#31283;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#22411;&#31995;&#25968;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#38500;&#20102;&#23545;&#27169;&#25311;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27169;&#22411;&#28436;&#31034;&#65306;&#19968;&#20010;&#26159;&#19981;&#21516;&#22269;&#23478;&#24180;&#40836;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#24052;&#40654;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
&lt;/p&gt;</description></item><item><title>TabPFN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;Transformer&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#23427;&#20351;&#29992;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#36924;&#36817;&#22522;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#12290;</title><link>http://arxiv.org/abs/2207.01848</link><description>&lt;p&gt;
TabPFN&#65306;&#22312;&#19968;&#31186;&#20869;&#35299;&#20915;&#23567;&#22411;&#34920;&#26684;&#20998;&#31867;&#38382;&#39064;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01848
&lt;/p&gt;
&lt;p&gt;
TabPFN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;Transformer&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#23427;&#20351;&#29992;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#36924;&#36817;&#22522;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TabPFN&#65292;&#19968;&#31181;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#30340;&#26102;&#38388;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#26041;&#27861;&#30340;&#26368;&#26032;&#29366;&#24577;&#19979;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;TabPFN&#23436;&#20840;&#21253;&#21547;&#22312;&#25105;&#20204;&#32593;&#32476;&#30340;&#26435;&#37325;&#20013;&#65292;&#25509;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20316;&#20026;&#35774;&#32622;&#20540;&#36755;&#20837;&#65292;&#24182;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#20026;&#25972;&#20010;&#27979;&#35797;&#38598;&#25552;&#20379;&#39044;&#27979;&#12290;TabPFN&#26159;&#19968;&#31181;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#65292;&#21482;&#38656;&#35201;&#32447;&#19979;&#35757;&#32451;&#19968;&#27425;&#65292;&#21363;&#21487;&#36924;&#36817;&#22522;&#20110;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#36825;&#20010;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#65306;&#23427;&#21253;&#25324;&#19968;&#20010;&#22823;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#31354;&#38388;&#65292;&#20559;&#22909;&#20110;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;OpenML-CC18&#22871;&#20214;&#30340;18&#20010;&#21253;&#21547;&#26368;&#22810;1000&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#12289;&#26368;&#22810;100&#20010;&#32431;&#25968;&#20540;&#29305;&#24449;&#19988;&#26080;&#32570;&#22833;&#20540;&#12289;&#26368;&#22810;10&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#25552;&#21319;&#26641;&#65292;&#19982;&#22797;&#26434;&#30340;&#26368;&#26032;AutoM&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoM
&lt;/p&gt;</description></item><item><title>ChordMixer &#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#32452;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#38271;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.05852</link><description>&lt;p&gt;
ChordMixer&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#38271;&#24230;&#19981;&#21516;&#30340;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths. (arXiv:2206.05852v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05852
&lt;/p&gt;
&lt;p&gt;
ChordMixer &#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#32452;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#38271;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#39034;&#24207;&#25968;&#25454;&#33258;&#28982;&#20855;&#26377;&#19981;&#21516;&#30340;&#38271;&#24230;&#65292;&#26377;&#20123;&#24207;&#21015;&#38750;&#24120;&#38271;&#12290;&#20316;&#20026;&#37325;&#35201;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#31070;&#32463;&#27880;&#24847;&#21147;&#24212;&#35813;&#25429;&#25417;&#36825;&#31181;&#24207;&#21015;&#20013;&#30340;&#36828;&#36317;&#31163;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#27880;&#24847;&#21147;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#30701;&#24207;&#21015;&#65292;&#25110;&#32773;&#23427;&#20204;&#19981;&#24471;&#19981;&#20351;&#29992;&#20998;&#22359;&#25110;&#22635;&#20805;&#26469;&#24378;&#21046;&#36755;&#20837;&#38271;&#24230;&#20445;&#25345;&#24658;&#23450;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;ChordMixer&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#38271;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#12290;&#27599;&#20010;ChordMixer&#22359;&#30001;&#19968;&#20010;&#27809;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#20301;&#32622;&#26059;&#36716;&#23618;&#21644;&#19968;&#20010;&#36880;&#20803;&#32032;MLP&#23618;&#32452;&#25104;&#12290;&#37325;&#22797;&#24212;&#29992;&#36825;&#20123;&#22359;&#24418;&#25104;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32593;&#32476;&#39592;&#24178;&#65292;&#23558;&#36755;&#20837;&#20449;&#21495;&#28151;&#21512;&#21040;&#23398;&#20064;&#30446;&#26631;&#20013;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21152;&#38382;&#39064;&#12289;&#38271;&#25991;&#26723;&#20998;&#31867;&#21644;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;ChordMixer&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31070;&#32463;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#65292;&#24182;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;CLIP&#34920;&#31034;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.01986</link><description>&lt;p&gt;
&#25506;&#31350;CLIP&#30340;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
Delving into the Openness of CLIP. (arXiv:2206.01986v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#65292;&#24182;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;CLIP&#34920;&#31034;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#20998;&#31867;&#20316;&#20026;&#19968;&#39033;&#22270;&#20687;&#21040;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#21363;&#23558;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#32780;&#19981;&#26159;&#31163;&#25955;&#30340;&#31867;&#21035;ID&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#20174;&#24320;&#25918;&#31867;&#38598;&#65288;&#20063;&#31216;&#20026;&#24320;&#25918;&#35789;&#27719;&#34920;&#65289;&#20013;&#35782;&#21035;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#30340;&#24320;&#25918;&#24615;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#29702;&#35770;&#19978;&#23545;&#20219;&#24847;&#35789;&#27719;&#24320;&#25918;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#20854;&#31934;&#24230;&#26377;&#25152;&#21464;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36882;&#22686;&#30340;&#35270;&#35282;&#36890;&#36807;&#35789;&#27719;&#25193;&#23637;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#21487;&#25193;&#23637;&#24615;&#26469;&#34913;&#37327;&#27169;&#22411;&#22788;&#29702;&#26032;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#24182;&#19981;&#30495;&#27491;&#24320;&#25918;&#65292;&#24182;&#19988;&#38543;&#30528;&#35789;&#27719;&#34920;&#30340;&#25193;&#23637;&#20854;&#24615;&#33021;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#34920;&#31034;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#30340;&#35282;&#24230;&#21078;&#26512;&#20102;CLIP&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;CLIP&#34920;&#31034;&#22312;&#19981;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#20013;&#22312;&#19981;&#21464;&#24615;&#21644;&#29305;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#29305;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary vocabulary in theory, but their accuracy varies in practice. To address this, we resort to an incremental perspective to assess the openness through vocabulary expansions, and define extensibility to measure a model's ability to handle novel classes. Our evaluation shows that CLIP-like models are not truly open, and their performance deteriorates as the vocabulary expands. We further dissect the feature space of CLIP from the perspectives of representation alignment and uniformity. Our investigation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.00738</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#29305;&#24449;&#30340;&#26500;&#25104;&#21450;&#20854;&#22312;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Composition of Relational Features with an Application to Explaining Black-Box Predictors. (arXiv:2206.00738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#20851;&#31995;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;&#65288;1&#65289;&#33021;&#22815;&#23545;&#25968;&#25454;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24314;&#27169;&#65307;&#65288;2&#65289;&#22312;&#27169;&#22411;&#26500;&#24314;&#26399;&#38388;&#20351;&#29992;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#20851;&#31995;&#65307;&#65288;3&#65289;&#26500;&#24314;&#30340;&#27169;&#22411;&#26159;&#20154;&#31867;&#21487;&#35835;&#30340;&#65292;&#36825;&#36890;&#24120;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#22312;&#27169;&#25454;&#35821;&#35328;M&#20013;&#30340; $\text{M}$-&#31616;&#21333;&#29305;&#24449;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#32452;&#21512;&#31639;&#23376;&#65288;$\rho_1$&#21644;$\rho_2$&#65289;&#65292;&#25152;&#26377;&#21487;&#33021;&#30340;&#22797;&#26434;&#29305;&#24449;&#37117;&#21487;&#20197;&#20174;&#20013;&#27966;&#29983;&#20986;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#23454;&#29616;&#20102;&#19968;&#31181;&#8220;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#21644;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational machine learning programs like those developed in Inductive Logic Programming (ILP) offer several advantages: (1) The ability to model complex relationships amongst data instances; (2) The use of domain-specific relations during model construction; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. However, these ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. We formulate the notion of a set of $\text{M}$-simple features in a mode language $\text{M}$ and identify two composition operators ($\rho_1$ and $\rho_2$) from which all possible complex features can be derived. We use these results to implement a form of "explainable neural 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24102;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#38382;&#39064;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#19988;&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#36825;&#31181;&#35745;&#31639;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.13832</link><description>&lt;p&gt;
&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Analysis in Dynamic Latent State Models. (arXiv:2205.13832v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24102;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#38382;&#39064;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#65292;&#24182;&#19988;&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#36825;&#31181;&#35745;&#31639;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#26469;&#25191;&#34892;&#20855;&#26377;&#38544;&#34255;&#29366;&#24577;&#30340;&#21160;&#24577;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#8220;&#29468;&#27979;&#12289;&#34892;&#21160;&#21644;&#39044;&#27979;&#8221;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#20197;&#22238;&#31572;&#21453;&#20107;&#23454;&#26597;&#35810;&#65292;&#24182;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;(1)&#29366;&#24577;&#26159;&#38544;&#34255;&#30340;&#65292;(2)&#27169;&#22411;&#26159;&#21160;&#24577;&#30340;&#12290;&#32771;&#34385;&#21040;&#23545;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#30340;&#32570;&#20047;&#20102;&#35299;&#20197;&#21450;&#21487;&#33021;&#23384;&#22312;&#26080;&#38480;&#22810;&#20010;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;&#35813;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#35745;&#31639;&#25152;&#20851;&#24515;&#30340;&#21453;&#20107;&#23454;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27719;&#38598;&#20102;&#22240;&#26524;&#20851;&#31995;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#24605;&#24819;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#26696;&#20363;&#30740;&#31350;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#21160;&#24577;&#28508;&#22312;&#29366;&#24577;&#27169;&#22411;&#20013;&#35745;&#31639;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an optimization-based framework to perform counterfactual analysis in a dynamic model with hidden states. Our framework is grounded in the ``abduction, action, and prediction'' approach to answer counterfactual queries and handles two key challenges where (1) the states are hidden and (2) the model is dynamic. Recognizing the lack of knowledge on the underlying causal mechanism and the possibility of infinitely many such mechanisms, we optimize over this space and compute upper and lower bounds on the counterfactual quantity of interest. Our work brings together ideas from causality, state-space models, simulation, and optimization, and we apply it on a breast cancer case study. To the best of our knowledge, we are the first to compute lower and upper bounds on a counterfactual query in a dynamic latent-state model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21457;&#29616;&#32452;&#20869;&#23376;&#32676;&#32467;&#26500;&#65292;&#25552;&#39640;&#21516;&#31561;&#23545;&#24453;&#19981;&#21516;&#32452;&#30340;&#20844;&#27491;&#24615;&#12289;&#20943;&#23569;&#22403;&#22334;&#20449;&#24687;&#26816;&#27979;&#35823;&#24046;&#65292;&#23545;&#20110;&#25552;&#39640;&#35780;&#35770;&#32773;&#21442;&#19982;&#24230;&#21644;&#29992;&#25143;&#23545;&#35780;&#35770;&#32593;&#31449;&#30340;&#20449;&#20219;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2204.11164</link><description>&lt;p&gt;
&#26816;&#27979;&#32593;&#32476;&#35780;&#20215;&#30340;&#20844;&#24179;&#24615;&#65306;&#21457;&#29616;&#23376;&#32676;&#24046;&#24322;&#20197;&#25552;&#39640;&#22403;&#22334;&#20449;&#24687;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Are Your Reviewers Being Treated Equally? Discovering Subgroup Structures to Improve Fairness in Spam Detection. (arXiv:2204.11164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21457;&#29616;&#32452;&#20869;&#23376;&#32676;&#32467;&#26500;&#65292;&#25552;&#39640;&#21516;&#31561;&#23545;&#24453;&#19981;&#21516;&#32452;&#30340;&#20844;&#27491;&#24615;&#12289;&#20943;&#23569;&#22403;&#22334;&#20449;&#24687;&#26816;&#27979;&#35823;&#24046;&#65292;&#23545;&#20110;&#25552;&#39640;&#35780;&#35770;&#32773;&#21442;&#19982;&#24230;&#21644;&#29992;&#25143;&#23545;&#35780;&#35770;&#32593;&#31449;&#30340;&#20449;&#20219;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#29983;&#25104;&#30340;&#20135;&#21697;&#35780;&#20215;&#26159;&#20122;&#39532;&#36874;&#21644;Yelp&#31561;&#22312;&#32447;&#21830;&#19994;&#30340;&#37325;&#35201;&#36164;&#20135;&#65292;&#32780;&#34394;&#20551;&#35780;&#35770;&#21017;&#26222;&#36941;&#23384;&#22312;&#20197;&#27450;&#39575;&#28040;&#36153;&#32773;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#22270;&#24418;&#25299;&#25169;&#30340;GNN&#26159;&#26816;&#27979;&#21487;&#30097;&#35780;&#35770;&#32773;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#32452;&#30340;&#35780;&#35770;&#32773;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#38477;&#20302;&#35780;&#35770;&#32773;&#30340;&#21442;&#19982;&#24230;&#24182;&#25439;&#23475;&#29992;&#25143;&#23545;&#35780;&#35770;&#32593;&#31449;&#30340;&#20449;&#20219;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32452;&#20869;&#23376;&#32676;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#20063;&#21487;&#33021;&#23548;&#33268;&#22788;&#29702;&#19981;&#21516;&#32452;&#26102;&#20986;&#29616;&#24046;&#24322;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23450;&#20041;&#12289;&#36817;&#20284;&#21644;&#21033;&#29992;&#26032;&#23376;&#32676;&#32467;&#26500;&#36827;&#34892;&#20844;&#24179;&#22403;&#22334;&#20449;&#24687;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20102;&#35780;&#23457;&#22270;&#20013;&#23548;&#33268;&#32452;&#20869;&#20934;&#30830;&#24615;&#24046;&#24322;&#30340;&#23376;&#32676;&#32467;&#26500;&#12290;&#35780;&#35770;&#22270;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#20250;&#23548;&#33268;&#25214;&#20986;&#23376;&#32676;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-generated reviews of products are vital assets of online commerce, such as Amazon and Yelp, while fake reviews are prevalent to mislead customers. GNN is the state-of-the-art method that detects suspicious reviewers by exploiting the topologies of the graph connecting reviewers, reviews, and target products. However, the discrepancy in the detection accuracy over different groups of reviewers can degrade reviewer engagement and customer trust in the review websites. Unlike the previous belief that the difference between the groups causes unfairness, we study the subgroup structures within the groups that can also cause discrepancies in treating different groups. This paper addresses the challenges of defining, approximating, and utilizing a new subgroup structure for fair spam detection. We first identify subgroup structures in the review graph that lead to discrepant accuracy in the groups. The complex dependencies over the review graph create difficulties in teasing out subgroup
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#21306;&#38388;&#30028;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2204.03511</link><description>&lt;p&gt;
&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#21306;&#38388;&#30028;&#25554;&#20540;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03511
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#21306;&#38388;&#30028;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#25152;&#33719;&#21462;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#30456;&#21516;&#20219;&#21153;&#20998;&#24067;&#20013;&#30340;&#26032;&#20219;&#21153;&#12290;&#23454;&#29616;&#26377;&#25928;&#30340;&#23569;&#27425;&#23398;&#20064;&#27867;&#21270;&#30340;&#22522;&#26412;&#21069;&#25552;&#26159;&#23398;&#20064;&#20219;&#21153;&#27969;&#24418;&#30340;&#22909;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#22312;&#20165;&#26377;&#21463;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#23569;&#20219;&#21153;&#23569;&#23398;&#20064;&#24773;&#20917;&#19979;&#65292;&#26174;&#24335;&#22320;&#20445;&#30041;&#20219;&#21153;&#27969;&#24418;&#20013;&#30340;&#26412;&#22320;&#37051;&#22495;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#35757;&#32451;&#20154;&#24037;&#20219;&#21153;&#21487;&#20197;&#21327;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#23436;&#20840;&#24378;&#38887;&#24615;&#35757;&#32451;&#25991;&#29486;&#20013;&#30340;&#21306;&#38388;&#30028;&#27010;&#24565;&#24341;&#20837;&#21040;&#20102;&#24494;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#12290;&#21306;&#38388;&#30028;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#20219;&#21153;&#21608;&#22260;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#37051;&#22495;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#20219;&#21153;&#21450;&#20854;&#30456;&#24212;&#36793;&#30028;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#30041;&#12290;&#28982;&#21518;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#36890;&#36807;&#25554;&#20540;&#26469;&#20154;&#20026;&#24418;&#25104;&#26032;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#36866;&#24212;&#30340;&#31890;&#23376;&#26469;&#35299;&#20915;&#27748;&#26222;&#26862;&#25277;&#26679;&#20013;&#31890;&#23376;&#26435;&#37325;&#25910;&#25947;&#20110;&#38646;&#30340;&#38382;&#39064;&#12290;RPTS&#22312;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#26524;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.08082</link><description>&lt;p&gt;
&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Regenerative Particle Thompson Sampling. (arXiv:2203.08082v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#36866;&#24212;&#30340;&#31890;&#23376;&#26469;&#35299;&#20915;&#27748;&#26222;&#26862;&#25277;&#26679;&#20013;&#31890;&#23376;&#26435;&#37325;&#25910;&#25947;&#20110;&#38646;&#30340;&#38382;&#39064;&#12290;RPTS&#22312;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#21644;&#25928;&#26524;&#30340;&#25552;&#21319;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20877;&#29983;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;RPTS&#65289;&#65292;&#36825;&#26159;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19968;&#31181;&#28789;&#27963;&#21464;&#20307;&#12290;&#27748;&#26222;&#26862;&#25277;&#26679;&#26412;&#36523;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#32500;&#25252;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#24456;&#38590;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#12290;&#31890;&#23376;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;PTS&#65289;&#26159;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19968;&#31181;&#36817;&#20284;&#26041;&#24335;&#65292;&#36890;&#36807;&#29992;&#31163;&#25955;&#20998;&#24067;&#26367;&#25442;&#22312;&#19968;&#32452;&#21152;&#26435;&#38745;&#24577;&#31890;&#23376;&#19978;&#25903;&#25345;&#30340;&#36830;&#32493;&#20998;&#24067;&#26469;&#33719;&#21462;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;PTS&#20013;&#65292;&#38500;&#20102;&#23569;&#25968;&#36866;&#24212;&#30340;&#31890;&#23376;&#20043;&#22806;&#65292;&#25152;&#26377;&#20854;&#20182;&#31890;&#23376;&#30340;&#26435;&#37325;&#37117;&#25910;&#25947;&#20110;&#38646;&#12290;RPTS&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65306;&#21024;&#38500;&#34928;&#20943;&#30340;&#19981;&#36866;&#24212;&#31890;&#23376;&#65292;&#24182;&#22312;&#36866;&#24212;&#30340;&#24184;&#23384;&#31890;&#23376;&#38468;&#36817;&#20877;&#29983;&#26032;&#31890;&#23376;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;PTS&#21040;RPTS&#30340;&#26222;&#36941;&#25913;&#36827;&#21644;RPTS&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#21151;&#25928;&#65292;&#21253;&#25324;&#23545;5G&#32593;&#32476;&#20999;&#29255;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes regenerative particle Thompson sampling (RPTS), a flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian heuristic for solving stochastic bandit problems, but it is hard to implement in practice due to the intractability of maintaining a continuous posterior distribution. Particle Thompson sampling (PTS) is an approximation of Thompson sampling obtained by simply replacing the continuous distribution by a discrete distribution supported at a set of weighted static particles. We observe that in PTS, the weights of all but a few fit particles converge to zero. RPTS is based on the heuristic: delete the decaying unfit particles and regenerate new particles in the vicinity of fit surviving particles. Empirical evidence shows uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS across a set of representative bandit problems, including an application to 5G network slicing.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31561;&#36317;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#31561;&#36317;&#27969;&#24418;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#31616;&#21270;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#25972;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2203.03934</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31561;&#36317;&#27969;&#24418;&#23398;&#20064;&#29992;&#20110;&#21333;&#23556;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Isometric Manifold Learning for Injective Normalizing Flows. (arXiv:2203.03934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03934
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31561;&#36317;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#31561;&#36317;&#27969;&#24418;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#31616;&#21270;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#25972;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#24314;&#27169;&#27969;&#24418;&#25968;&#25454;&#65292;&#25105;&#20204;&#37319;&#29992;&#31561;&#36317;&#33258;&#32534;&#30721;&#22120;&#26469;&#35774;&#35745;&#20855;&#26377;&#26174;&#24335;&#36870;&#30340;&#23884;&#20837;&#65292;&#20197;&#19981;&#25913;&#21464;&#27010;&#29575;&#20998;&#24067;&#12290;&#20351;&#29992;&#31561;&#36317;&#26144;&#23556;&#23558;&#27969;&#24418;&#23398;&#20064;&#21644;&#23494;&#24230;&#20272;&#35745;&#20998;&#31163;&#65292;&#24182;&#20351;&#20004;&#37096;&#20998;&#30340;&#35757;&#32451;&#36798;&#21040;&#39640;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;&#19982;&#29616;&#26377;&#21333;&#23556;&#24402;&#19968;&#21270;&#27969;&#30456;&#27604;&#65292;&#27169;&#22411;&#36873;&#25321;&#21644;&#35843;&#25972;&#26356;&#20026;&#31616;&#21270;&#12290;&#24212;&#29992;&#20110;&#65288;&#36817;&#20284;&#65289;&#24179;&#22374;&#27969;&#24418;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#32452;&#21512;&#26041;&#27861;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
To model manifold data using normalizing flows, we employ isometric autoencoders to design embeddings with explicit inverses that do not distort the probability distribution. Using isometries separates manifold learning and density estimation and enables training of both parts to high accuracy. Thus, model selection and tuning are simplified compared to existing injective normalizing flows. Applied to data sets on (approximately) flat manifolds, the combined approach generates high-quality data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;iable hypergeometric distribution&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#23558;&#19968;&#32452;&#20803;&#32032;&#21010;&#20998;&#20026;&#20808;&#39564;&#26410;&#30693;&#22823;&#23567;&#30340;&#23376;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#32858;&#31867;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.01629</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#36229;&#20960;&#20309;&#20998;&#24067;&#36827;&#34892;&#23567;&#32452;&#37325;&#35201;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Group Importance using the Differentiable Hypergeometric Distribution. (arXiv:2203.01629v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;iable hypergeometric distribution&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#23558;&#19968;&#32452;&#20803;&#32032;&#21010;&#20998;&#20026;&#20808;&#39564;&#26410;&#30693;&#22823;&#23567;&#30340;&#23376;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#32858;&#31867;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23558;&#19968;&#32452;&#20803;&#32032;&#21010;&#20998;&#20026;&#20808;&#39564;&#26410;&#30693;&#22823;&#23567;&#30340;&#23376;&#38598;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20123;&#23376;&#38598;&#22823;&#23567;&#24456;&#23569;&#26126;&#30830;&#23398;&#20064; - &#26080;&#35770;&#26159;&#32858;&#31867;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#31751;&#22823;&#23567;&#36824;&#26159;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#19982;&#29420;&#31435;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#30340;&#25968;&#37327;&#12290;&#30001;&#20110;&#30828;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#27491;&#30830;&#23376;&#38598;&#22823;&#23567;&#30340;&#27010;&#29575;&#20998;&#24067;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#31105;&#27490;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#24494;&#20998;&#36229;&#20960;&#20309;&#20998;&#24067;&#12290;&#36229;&#20960;&#20309;&#20998;&#24067;&#22522;&#20110;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#27169;&#25311;&#19981;&#21516;&#32452;&#22823;&#23567;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#21487;&#37325;&#21442;&#25968;&#21270;&#26799;&#24230;&#26469;&#23398;&#20064;&#23567;&#32452;&#20043;&#38388;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#22312;&#20004;&#20010;&#20856;&#22411;&#24212;&#29992;&#31243;&#24207;&#20013;&#26174;&#24335;&#23398;&#20064;&#23376;&#38598;&#22823;&#23567;&#30340;&#20248;&#28857;&#65306;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#32858;&#31867;&#12290;&#22312;&#36825;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25105;&#20204;&#20248;&#20110;&#20381;&#36182;&#20110;&#27425;&#20248;&#21551;&#21457;&#24335;&#27169;&#25311;&#26410;&#30693;&#22823;&#23567;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#65292;&#20197;&#33410;&#28857;&#30340;&#26041;&#24335;&#37096;&#32626;&#19981;&#21516;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#23454;&#29616;&#21069;&#21521;&#20256;&#36882;&#21644;&#36866;&#24212;&#20197;&#21069;&#23398;&#36807;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#28176;&#36827;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#25972;&#20010;&#32593;&#32476;&#20998;&#37197;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.10821</link><description>&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Increasing Depth of Neural Networks for Life-long Learning. (arXiv:2202.10821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#65292;&#20197;&#33410;&#28857;&#30340;&#26041;&#24335;&#37096;&#32626;&#19981;&#21516;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#23454;&#29616;&#21069;&#21521;&#20256;&#36882;&#21644;&#36866;&#24212;&#20197;&#21069;&#23398;&#36807;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#28176;&#36827;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#25972;&#20010;&#32593;&#32476;&#20998;&#37197;&#22823;&#37327;&#20869;&#23384;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32456;&#36523;&#23398;&#20064;&#29615;&#22659;&#19979;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#26159;&#21542;&#26377;&#30410;&#22788;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#32593;&#32476;&#19978;&#26041;&#28155;&#21152;&#26032;&#23618;&#26469;&#23454;&#29616;&#30693;&#35782;&#30340;&#21069;&#21521;&#20256;&#36882;&#65292;&#24182;&#36866;&#24212;&#20197;&#21069;&#23398;&#36807;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#30830;&#23450;&#26368;&#30456;&#20284;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20197;&#36873;&#25321;&#32593;&#32476;&#20013;&#28155;&#21152;&#26377;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#26032;&#33410;&#28857;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#21019;&#24314;&#19968;&#31181;&#26641;&#24418;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#26159;&#19987;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#38598;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21463;&#21040;&#28176;&#36827;&#24335;&#31070;&#32463;&#32593;&#32476;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#21160;&#24577;&#21464;&#21270;&#20013;&#33719;&#30410;&#12290;&#28982;&#32780;&#65292;&#28176;&#36827;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20026;&#25972;&#20010;&#32593;&#32476;&#32467;&#26500;&#20998;&#37197;&#22823;&#37327;&#20869;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20165;&#28155;&#21152;&#37096;&#20998;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: We propose a novel method for continual learning based on the increasing depth of neural networks. This work explores whether extending neural network depth may be beneficial in a life-long learning setting.  Methods: We propose a novel approach based on adding new layers on top of existing ones to enable the forward transfer of knowledge and adapting previously learned representations. We employ a method of determining the most similar tasks for selecting the best location in our network to add new nodes with trainable parameters. This approach allows for creating a tree-like model, where each node is a set of neural network parameters dedicated to a specific task. The Progressive Neural Network concept inspires the proposed method. Therefore, it benefits from dynamic changes in network structure. However, Progressive Neural Network allocates a lot of memory for the whole network structure during the learning process. The proposed method alleviates this by adding only part of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#28145;&#24230;&#22270;&#23398;&#20064;&#65288;DGL&#65289;&#30340;&#26368;&#26032;&#21487;&#38752;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#28085;&#30422;&#20102;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#65292;&#21516;&#26102;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.07114</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#36817;&#26399;&#36827;&#23637;: &#20869;&#22312;&#22122;&#22768;&#12289;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack. (arXiv:2202.07114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#28145;&#24230;&#22270;&#23398;&#20064;&#65288;DGL&#65289;&#30340;&#26368;&#26032;&#21487;&#38752;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#28085;&#30422;&#20102;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#65292;&#21516;&#26102;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#23398;&#20064; (DGL) &#22312;&#37329;&#34701;&#12289;&#30005;&#23376;&#21830;&#21153;&#12289;&#33647;&#29289;&#21644;&#20808;&#36827;&#26448;&#26009;&#21457;&#29616;&#31561;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23558; DGL &#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#19968;&#31995;&#21015;&#21487;&#38752;&#24615;&#23041;&#32961;&#65292;&#21253;&#25324;&#20869;&#22312;&#22122;&#22768;&#12289;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#38024;&#23545;&#19978;&#36848;&#23041;&#32961;&#25913;&#36827; DGL &#31639;&#27861;&#21487;&#38752;&#24615;&#30340;&#20840;&#38754;&#36827;&#23637;&#12290;&#19982;&#20043;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#30456;&#20851;&#32508;&#36848;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#28085;&#30422;&#20102;&#26356;&#22810;&#20851;&#20110; DGL &#21487;&#38752;&#24615;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#21363;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#19978;&#26041;&#38754;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#38656;&#35201;&#25506;&#32034;&#30340;&#19968;&#20123;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph learning (DGL) has achieved remarkable progress in both business and scientific areas ranging from finance and e-commerce to drug and advanced material discovery. Despite the progress, applying DGL to real-world applications faces a series of reliability threats including inherent noise, distribution shift, and adversarial attacks. This survey aims to provide a comprehensive review of recent advances for improving the reliability of DGL algorithms against the above threats. In contrast to prior related surveys which mainly focus on adversarial attacks and defense, our survey covers more reliability-related aspects of DGL, i.e., inherent noise and distribution shift. Additionally, we discuss the relationships among above aspects and highlight some important issues to be explored in future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TPC&#30340;&#36716;&#25442;&#29305;&#23450;&#24179;&#28369;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28857;&#20113;&#27169;&#22411;&#23545;&#35821;&#20041;&#36716;&#25442;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#38450;&#24481;&#33021;&#21147;&#65292;&#24182;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2201.12733</link><description>&lt;p&gt;
TPC&#65306;&#38024;&#23545;&#28857;&#20113;&#27169;&#22411;&#30340;&#29305;&#23450;&#36716;&#25442;&#24179;&#28369;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TPC&#30340;&#36716;&#25442;&#29305;&#23450;&#24179;&#28369;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28857;&#20113;&#27169;&#22411;&#23545;&#35821;&#20041;&#36716;&#25442;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#38450;&#24481;&#33021;&#21147;&#65292;&#24182;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#28857;&#20113;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#35782;&#21035;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#26088;&#22312;&#26045;&#21152;&#26059;&#36716;&#21644;&#38181;&#24418;&#31561;&#38544;&#31192;&#30340;&#35821;&#20041;&#21464;&#25442;&#26469;&#35823;&#23548;&#27169;&#22411;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#25442;&#29305;&#23450;&#24179;&#28369;&#30340;&#26694;&#26550;TPC&#65292;&#20026;&#28857;&#20113;&#27169;&#22411;&#25552;&#20379;&#32039;&#23494;&#19988;&#21487;&#25193;&#23637;&#30340;&#38450;&#24481;&#20445;&#35777;&#65292;&#25269;&#24481;&#35821;&#20041;&#36716;&#25442;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#24120;&#35265;&#30340;3D&#36716;&#25442;&#20998;&#20026;&#19977;&#31867;&#65306;&#21487;&#21472;&#21152;&#30340;&#65288;&#20363;&#22914;&#21098;&#20999;&#65289;&#12289;&#21487;&#32452;&#21512;&#30340;&#65288;&#20363;&#22914;&#26059;&#36716;&#65289;&#21644;&#38388;&#25509;&#21487;&#32452;&#21512;&#30340;&#65288;&#22914;&#38181;&#24418;&#65289;&#65292;&#28982;&#21518;&#20998;&#21035;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#31574;&#30053;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#35821;&#20041;&#36716;&#25442;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#25351;&#23450;&#20102;&#21807;&#19968;&#30340;&#35748;&#35777;&#21327;&#35758;&#12290;&#22312;&#20960;&#20010;&#24120;&#35265;&#30340;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;TPC&#21487;&#20197;&#22312;&#25552;&#20379;&#26356;&#24378;&#30340;&#38450;&#24481;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#28857;&#20113;&#27169;&#22411;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable to adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#36731;&#37327;&#32423;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#35759;&#24102;&#23485;&#38480;&#21046;&#19979;&#23436;&#25104;&#20102;&#36890;&#29992;&#22270;&#19978;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#22270;&#19981;&#23436;&#20840;&#21644;&#38750;&#33391;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.12482</link><description>&lt;p&gt;
&#26377;&#38480;&#35760;&#24518;&#19979;&#30340;&#36890;&#29992;&#22270;&#21327;&#20316;&#23398;&#20064;&#65306;&#22797;&#26434;&#24230;&#12289;&#21487;&#23398;&#20064;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability. (arXiv:2201.12482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#36731;&#37327;&#32423;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#35759;&#24102;&#23485;&#38480;&#21046;&#19979;&#23436;&#25104;&#20102;&#36890;&#29992;&#22270;&#19978;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#22270;&#19981;&#23436;&#20840;&#21644;&#38750;&#33391;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#20219;&#24847;&#36830;&#25509;&#22270;&#20013;&#26377;&#38480;&#35760;&#24518;&#21644;&#36890;&#20449;&#24102;&#23485;&#30340;&#26234;&#33021;&#20307;&#23436;&#25104; K-&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#36890;&#20449;&#22270;&#24517;&#39035;&#26159;&#23436;&#20840;&#25110;&#33391;&#26500;&#30340;&#65292;&#28982;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#32780;&#26377;&#38480;&#30340;&#35760;&#24518;&#21644;&#36890;&#20449;&#24102;&#23485;&#20063;&#20250;&#38480;&#21046;&#24050;&#26377;&#32463;&#39564;&#30340;&#20849;&#20139;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#21521;&#20854;&#20182;&#26234;&#33021;&#20307;&#20256;&#36882;&#34394;&#20551;&#30340;&#32463;&#39564;&#20449;&#24687;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;&#38024;&#23545;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#30340;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#27599;&#20010;&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#38543;&#26426;&#28216;&#36208;&#23454;&#29616;&#26368;&#26032;&#32463;&#39564;&#30340;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a K-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorizing capabilities and communication bandwidth. The goal is to let each of the agents eventually learn the best arm. It is assumed in these studies that the communication graph should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since the agents memorize and communicate very few experiences. Additionally, an agent may be corrupted to share falsified experiences to its peers, while the resource limit in terms of memorization and communication may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their latest experiences with each other through light-weight random walks in a ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2201.11989</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#30340;&#23384;&#22312;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#65292;&#22914;&#19981;&#21516;&#30340;&#24658;&#23450;&#29575;&#25110;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#31561;&#65292;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#26377;&#21161;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#27492;&#22806;&#65292;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#20063;&#24456;&#37325;&#35201;&#65292;&#20004;&#32773;&#37117;&#24433;&#21709;&#20102;&#35757;&#32451;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#37327;&#12290;&#26412;&#25991;&#22522;&#20110;&#24658;&#23450;&#23398;&#20064;&#29575;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#19982;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;TTUR&#65292;&#20026;&#20102;&#25214;&#21040;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;&#25152;&#38656;&#27493;&#39588;&#25968;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Fr'echet Inception Distance&#65288;FID&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#32534;&#30721;&#26041;&#27861;-&#8220;one-hot&#32534;&#30721;&#8221;&#21644;&#8220;target&#32534;&#30721;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.11358</link><description>&lt;p&gt;
&#32534;&#30721;&#20445;&#25252;&#20998;&#31867;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness Implications of Encoding Protected Categorical Attributes. (arXiv:2201.11358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#32534;&#30721;&#26041;&#27861;-&#8220;one-hot&#32534;&#30721;&#8221;&#21644;&#8220;target&#32534;&#30721;&#8221;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26126;&#30830;&#20351;&#29992;&#20445;&#25252;&#23646;&#24615;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#20294;&#26159;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#20998;&#31867;&#23646;&#24615;&#65292;&#20363;&#22914;&#20986;&#29983;&#22269;&#23478;&#25110;&#31181;&#26063;&#12290;&#30001;&#20110;&#20445;&#25252;&#23646;&#24615;&#32463;&#24120;&#26159;&#20998;&#31867;&#30340;&#65292;&#22240;&#27492;&#24517;&#39035;&#23558;&#20854;&#32534;&#30721;&#20026;&#21487;&#20197;&#36755;&#20837;&#25152;&#36873;&#25321;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#25110;&#32447;&#24615;&#27169;&#22411;&#12290;&#32534;&#30721;&#26041;&#27861;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#23398;&#20064;&#22914;&#20309;&#21644;&#20160;&#20040;&#65292;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#33879;&#21517;&#30340;&#32534;&#30721;&#26041;&#27861;&#8212;&#8212;&#8220;one-hot&#32534;&#30721;&#8221;&#21644;&#8220;target&#32534;&#30721;&#8221;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#36825;&#20123;&#32534;&#30721;&#26041;&#27861;&#21487;&#33021;&#20135;&#29983;&#30340;&#20004;&#31181;&#35825;&#23548;&#20559;&#24046;&#31867;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#31867;&#22411;&#26159;&#26080;&#27861;&#28040;&#38500;&#30340;&#20559;&#24046;&#65292;&#30001;&#20110;&#30452;&#25509;&#32452;&#21035;&#31867;&#21035;&#27495;&#35270;&#32780;&#23548;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\ support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: \emph{one-hot encoding} and \emph{target encoding}. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, \textit{irreducible bias}, is due to direct group category discriminatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36816;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#39318;&#27425;&#21457;&#29616;&#20108;&#32500;&#24067;&#24335;&#20869;&#26031;&#20811;&#26041;&#31243;&#21644;&#19977;&#32500;&#27431;&#25289;&#26041;&#31243;&#30340;&#20809;&#28369;&#33258;&#30456;&#20284;&#29190;&#30772;&#36718;&#24275;&#65292;&#36825;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;&#36827;&#34892;&#20004;&#20010;&#26041;&#31243;&#29190;&#30772;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35777;&#26126;&#30340;&#22522;&#30784;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#26694;&#26550;&#36824;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27714;&#35299;&#27969;&#20307;&#26041;&#31243;&#30340;&#19981;&#31283;&#23450;&#33258;&#30456;&#20284;&#35299;&#12290;</title><link>http://arxiv.org/abs/2201.06780</link><description>&lt;p&gt;
&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#19977;&#32500;&#36724;&#23545;&#31216;&#27431;&#25289;&#26041;&#31243;&#30340;&#28176;&#36817;&#33258;&#30456;&#20284;&#30636;&#26102;&#29190;&#30772;&#36718;&#24275;
&lt;/p&gt;
&lt;p&gt;
Asymptotic self-similar blow-up profile for three-dimensional axisymmetric Euler equations using neural networks. (arXiv:2201.06780v3 [math.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36816;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#39318;&#27425;&#21457;&#29616;&#20108;&#32500;&#24067;&#24335;&#20869;&#26031;&#20811;&#26041;&#31243;&#21644;&#19977;&#32500;&#27431;&#25289;&#26041;&#31243;&#30340;&#20809;&#28369;&#33258;&#30456;&#20284;&#29190;&#30772;&#36718;&#24275;&#65292;&#36825;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;&#36827;&#34892;&#20004;&#20010;&#26041;&#31243;&#29190;&#30772;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35777;&#26126;&#30340;&#22522;&#30784;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#26694;&#26550;&#36824;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27714;&#35299;&#27969;&#20307;&#26041;&#31243;&#30340;&#19981;&#31283;&#23450;&#33258;&#30456;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20108;&#32500;&#24067;&#24335;&#20869;&#26031;&#20811;&#26041;&#31243;&#21644;&#19977;&#32500;&#27431;&#25289;&#26041;&#31243;&#26159;&#21542;&#23384;&#22312;&#26377;&#38480;&#26102;&#38388;&#29190;&#30772;&#35299;&#26159;&#27969;&#20307;&#21147;&#23398;&#39046;&#22495;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#36816;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#20540;&#26694;&#26550;&#65292;&#39318;&#27425;&#21457;&#29616;&#20102;&#20004;&#20010;&#26041;&#31243;&#30340;&#20809;&#28369;&#33258;&#30456;&#20284;&#29190;&#30772;&#36718;&#24275;&#12290;&#36825;&#20123;&#35299;&#26412;&#36523;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;&#36827;&#34892;&#20004;&#20010;&#26041;&#31243;&#29190;&#30772;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35777;&#26126;&#30340;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#19981;&#31283;&#23450;&#30340;&#33258;&#30456;&#20284;&#35299;&#30340;&#31034;&#20363;&#65292;&#35777;&#26126;&#20102; PINNs &#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#27714;&#35299;&#27969;&#20307;&#26041;&#31243;&#30340;&#19981;&#31283;&#23450;&#33258;&#30456;&#20284;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#20540;&#26694;&#26550;&#26082;&#24378;&#20581;&#21448;&#36866;&#24212;&#21508;&#31181;&#19981;&#21516;&#30340;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether there exist finite time blow-up solutions for the 2-D Boussinesq and the 3-D Euler equations are of fundamental importance to the field of fluid mechanics. We develop a new numerical framework, employing physics-informed neural networks (PINNs), that discover, for the first time, a smooth self-similar blow-up profile for both equations. The solution itself could form the basis of a future computer-assisted proof of blow-up for both equations. In addition, we demonstrate PINNs could be successfully applied to find unstable self-similar solutions to fluid equations by constructing the first example of an unstable self-similar solution to the C\'ordoba-C\'ordoba-Fontelos equation. We show that our numerical framework is both robust and adaptable to various other equations.
&lt;/p&gt;</description></item><item><title>CausalSim&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2201.01811</link><description>&lt;p&gt;
CausalSim: &#19968;&#31181;&#29992;&#20110;&#26080;&#20559;&#24046;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation. (arXiv:2201.01811v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01811
&lt;/p&gt;
&lt;p&gt;
CausalSim&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CausalSim&#65292;&#19968;&#31181;&#29992;&#20110;&#26080;&#20559;&#24046;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#30340;&#22240;&#26524;&#26694;&#26550;&#12290;&#24403;&#21069;&#30340;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#20551;&#35774;&#36827;&#34892;&#20223;&#30495;&#30340;&#24178;&#39044;&#65288;&#20363;&#22914;&#65292;&#26032;&#31639;&#27861;&#65289;&#19981;&#20250;&#24433;&#21709;&#36861;&#36394;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36861;&#36394;&#24120;&#24120;&#20250;&#21463;&#21040;&#31639;&#27861;&#22312;&#36861;&#36394;&#25910;&#38598;&#26399;&#38388;&#36827;&#34892;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#65292;&#22312;&#24178;&#39044;&#19979;&#37325;&#28436;&#36861;&#36394;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;CausalSim&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#25429;&#33719;&#36861;&#36394;&#25910;&#38598;&#26399;&#38388;&#22522;&#30784;&#31995;&#32479;&#26465;&#20214;&#30340;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#23427;&#20351;&#29992;&#22266;&#23450;&#31639;&#27861;&#38598;&#19979;&#30340;&#21021;&#22987;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#26469;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27169;&#25311;&#26032;&#31639;&#27861;&#26102;&#24212;&#29992;&#23427;&#20204;&#26469;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CausalSim, a causal framework for unbiased trace-driven simulation. Current trace-driven simulators assume that the interventions being simulated (e.g., a new algorithm) would not affect the validity of the traces. However, real-world traces are often biased by the choices algorithms make during trace collection, and hence replaying traces under an intervention may lead to incorrect results. CausalSim addresses this challenge by learning a causal model of the system dynamics and latent factors capturing the underlying system conditions during trace collection. It learns these models using an initial randomized control trial (RCT) under a fixed set of algorithms, and then applies them to remove biases from trace data when simulating new algorithms.  Key to CausalSim is mapping unbiased trace-driven simulation to a tensor completion problem with extremely sparse observations. By exploiting a basic distributional invariance property present in RCT data, CausalSim enables a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#24615;&#30340;&#24314;&#26500;&#26694;&#26550;&#65292;&#20219;&#20309;nAI&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;&#26222;&#36866;&#30340;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#32479;&#19968;&#12289;&#26500;&#36896;&#24615;&#21644;&#26032;&#35270;&#35282;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2112.14877</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#24615;&#30340;&#32479;&#19968;&#24314;&#26500;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#24615;&#30340;&#24314;&#26500;&#26694;&#26550;&#65292;&#20219;&#20309;nAI&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;&#26222;&#36866;&#30340;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#32479;&#19968;&#12289;&#26500;&#36896;&#24615;&#21644;&#26032;&#35270;&#35282;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20043;&#25152;&#20197;&#33021;&#22815;&#22797;&#21046;&#22797;&#26434;&#30340;&#20219;&#21153;&#25110;&#20989;&#25968;&#20043;&#19968;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#12290;&#34429;&#28982;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#21333;&#19968;&#30340;&#24314;&#26500;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#20026;&#22823;&#22810;&#25968;&#24050;&#26377;&#28608;&#27963;&#20989;&#25968;&#25552;&#20379;&#32479;&#19968;&#30340;&#24314;&#26500;&#26694;&#26550;&#20197;&#35299;&#37322;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#30340;&#23581;&#35797;&#12290;&#22312;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#24658;&#31561;&#65288;nAI&#65289;&#30340;&#27010;&#24565;&#12290;&#20027;&#35201;&#30340;&#32467;&#26524;&#26159;&#65306;\emph{&#20219;&#20309;nAI&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;&#26222;&#36866;&#30340;}&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#28608;&#27963;&#20989;&#25968;&#37117;&#26159;nAI&#65292;&#22240;&#27492;&#22312;&#32039;&#33268;&#31354;&#38388;&#36830;&#32493;&#20989;&#25968;&#31354;&#38388;&#20869;&#26159;&#26222;&#36866;&#30340;&#12290;&#35813;&#26694;&#26550;&#27604;&#29616;&#26377;&#30340;&#23545;&#24212;&#29289;&#20855;&#26377;\textbf{&#20960;&#20010;&#20248;&#21183;}&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;&#20174;&#21151;&#33021;&#20998;&#26512;&#12289;&#27010;&#29575;&#35770;&#21644;&#25968;&#20540;&#20998;&#26512;&#30340;&#22522;&#26412;&#25163;&#27573;&#20043;&#19978;&#30340;&#26500;&#36896;&#24615;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#21253;&#25324;&#22823;&#22810;&#25968;&#24050;&#26377;&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#22823;&#31867;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#31532;&#19977;&#65292;&#23427;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#24615;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the reasons why many neural networks are capable of replicating complicated tasks or functions is their universal property. Though the past few decades have seen tremendous advances in theories of neural networks, a single constructive framework for neural network universality remains unavailable. This paper is the first effort to provide a unified and constructive framework for the universality of a large class of activation functions including most of existing ones. At the heart of the framework is the concept of neural network approximate identity (nAI). The main result is: {\em any nAI activation function is universal}. It turns out that most of existing activation functions are nAI, and thus universal in the space of continuous functions on compacta. The framework induces {\bf several advantages} over the contemporary counterparts. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first un
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#23433;&#20840;&#28388;&#27874;&#22120;&#65292;&#38024;&#23545;&#24102;&#26377;&#26410;&#30693;&#27169;&#22411;&#21644;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#22122;&#22768;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#32039;&#23433;&#20840;&#32422;&#26463;&#21644;&#26500;&#24314;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;&#21517;&#20041;&#25511;&#21046;&#21160;&#20316;&#65292;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.00631</link><description>&lt;p&gt;
&#23398;&#20064;&#26410;&#30693;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#30340;&#23433;&#20840;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Safety Filters for Unknown Discrete-Time Linear Systems. (arXiv:2111.00631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#23433;&#20840;&#28388;&#27874;&#22120;&#65292;&#38024;&#23545;&#24102;&#26377;&#26410;&#30693;&#27169;&#22411;&#21644;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#22122;&#22768;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#32039;&#23433;&#20840;&#32422;&#26463;&#21644;&#26500;&#24314;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;&#21517;&#20041;&#25511;&#21046;&#21160;&#20316;&#65292;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#26377;&#26410;&#30693;&#27169;&#22411;&#21644;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#22122;&#22768;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#23433;&#20840;&#28388;&#27874;&#22120;&#12290;&#23433;&#20840;&#24615;&#36890;&#36807;&#23545;&#29366;&#24577;&#21644;&#25511;&#21046;&#36755;&#20837;&#26045;&#21152;&#22810;&#38754;&#20307;&#32422;&#26463;&#26469;&#25551;&#36848;&#12290;&#32463;&#39564;&#24615;&#22320;&#23398;&#20064;&#27169;&#22411;&#21644;&#36807;&#31243;&#22122;&#22768;&#21327;&#26041;&#24046;&#21450;&#20854;&#32622;&#20449;&#21306;&#38388;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;&#21517;&#20041;&#25511;&#21046;&#21160;&#20316;&#65292;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#20248;&#21270;&#38382;&#39064;&#20381;&#36182;&#20110;&#25910;&#32039;&#21407;&#22987;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#12290;&#30001;&#20110;&#26368;&#21021;&#32570;&#20047;&#21487;&#38752;&#27169;&#22411;&#26500;&#24314;&#25152;&#38656;&#20449;&#24687;&#65292;&#22240;&#27492;&#25910;&#32039;&#30340;&#24133;&#24230;&#36739;&#22823;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#36880;&#28176;&#32553;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
A learning-based safety filter is developed for discrete-time linear time-invariant systems with unknown models subject to Gaussian noises with unknown covariance. Safety is characterized using polytopic constraints on the states and control inputs. The empirically learned model and process noise covariance with their confidence bounds are used to construct a robust optimization problem for minimally modifying nominal control actions to ensure safety with high probability. The optimization problem relies on tightening the original safety constraints. The magnitude of the tightening is larger at the beginning since there is little information to construct reliable models, but shrinks with time as more data becomes available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20108;&#20056;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#21452;&#26354;&#22411;&#23432;&#24658;&#24459;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#65292;&#21487;&#29992;&#20110;&#36924;&#36817;&#26410;&#30693;&#30028;&#38754;&#20301;&#32622;&#30340;&#19981;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#20445;&#25345;&#19981;&#36830;&#32493;&#24615;&#35299;&#21644;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.10895</link><description>&lt;p&gt;
&#26368;&#23567;&#20108;&#20056;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#27714;&#35299;&#38750;&#32447;&#24615;&#21452;&#26354;&#22411;&#23432;&#24658;&#24459;&#65306;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Least-Squares Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Laws: Discrete Divergence Operator. (arXiv:2110.10895v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20108;&#20056;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#21452;&#26354;&#22411;&#23432;&#24658;&#24459;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#65292;&#21487;&#29992;&#20110;&#36924;&#36817;&#26410;&#30693;&#30028;&#38754;&#20301;&#32622;&#30340;&#19981;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#20445;&#25345;&#19981;&#36830;&#32493;&#24615;&#35299;&#21644;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26368;&#23567;&#20108;&#20056;&#31070;&#32463;&#32593;&#32476;&#65288;LSNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#26631;&#37327;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#21452;&#26354;&#22411;&#23432;&#24658;&#24459;&#65288;HCLs&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#31561;&#20215;&#30340;&#26368;&#23567;&#20108;&#20056;&#65288;LS&#65289;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36924;&#36817;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#36924;&#36817;&#26410;&#30693;&#30028;&#38754;&#20301;&#32622;&#30340;&#19981;&#36830;&#32493;&#20989;&#25968;&#12290;&#22312;LSNN&#26041;&#27861;&#35774;&#35745;&#20013;&#65292;&#24494;&#20998;&#36816;&#31639;&#30340;&#25968;&#20540;&#36924;&#36817;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#26631;&#20934;&#30340;&#25968;&#20540;&#25110;&#33258;&#21160;&#24494;&#20998;&#27839;&#22352;&#26631;&#36724;&#26041;&#21521;&#36827;&#34892;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22833;&#36133;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;HCLs&#36716;&#21270;&#20026;&#26102;&#31354;&#25955;&#24230;&#24418;&#24335;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#65292;&#20174;&#32780;&#20351;&#25152;&#25552;&#20986;&#30340;LSNN&#26041;&#27861;&#19981;&#21463;&#20154;&#24037;&#31896;&#24615;&#24809;&#32602;&#12290;&#29702;&#35770;&#19978;&#65292;&#21363;&#20351;&#26159;&#19981;&#36830;&#32493;&#30340;&#35299;&#65292;&#20063;&#21487;&#20197;&#20272;&#35745;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#30340;&#31934;&#24230;&#12290;&#25968;&#20540;&#19978;&#65292;&#22312;&#19968;&#32452;&#22522;&#20934;&#38382;&#39064;&#20013;&#27979;&#35797;&#20102;&#24102;&#26377;&#26032;&#30340;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#30340;LSNN&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#36816;&#21160;&#19981;&#36830;&#32493;&#24615;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#31163;&#25955;&#25955;&#24230;&#31639;&#23376;&#30340;LSNN&#26041;&#27861;&#22312;&#20445;&#25345;&#19981;&#36830;&#32493;&#24615;&#35299;&#21644;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A least-squares neural network (LSNN) method was introduced for solving scalar linear and nonlinear hyperbolic conservation laws (HCLs) in [7, 6]. This method is based on an equivalent least-squares (LS) formulation and uses ReLU neural network as approximating functions, making it ideal for approximating discontinuous functions with unknown interface location. In the design of the LSNN method for HCLs, the numerical approximation of differential operators is a critical factor, and standard numerical or automatic differentiation along coordinate directions can often lead to a failed NN-based method. To overcome this challenge, this paper rewrites HCLs in their divergence form of space and time and introduces a new discrete divergence operator. As a result, the proposed LSNN method is free of penalization of artificial viscosity. Theoretically, the accuracy of the discrete divergence operator is estimated even for discontinuous solutions. Numerically, the LSNN method with the new discre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2110.07292</link><description>&lt;p&gt;
&#31614;&#21517;&#21644;&#30456;&#20851;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sign and Relevance learning. (arXiv:2110.07292v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#29983;&#29289;&#20223;&#30495;&#21644;&#29983;&#29289;&#21551;&#21457;&#24335;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20840;&#23616;&#35823;&#24046;&#20449;&#21495;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#27973;&#23618;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#20801;&#35768;&#20351;&#29992;&#22810;&#23618;&#32593;&#32476;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20165;&#20256;&#25773;&#21487;&#22609;&#24615;&#21464;&#21270;&#30340;&#31526;&#21495;&#65288;&#21363;LTP / LTD&#65289;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#35843;&#21046;&#26469;&#25511;&#21046;&#23398;&#20064;&#36895;&#29575;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#31070;&#32463;&#35843;&#21046;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#20462;&#27491;&#30340;&#35823;&#24046;&#25110;&#30456;&#20851;&#20449;&#21495;&#65292;&#32780;&#38169;&#35823;&#20449;&#21495;&#30340;&#33258;&#19978;&#32780;&#19979;&#31526;&#21495;&#20915;&#23450;&#38271;&#26399;&#22686;&#24378;&#36824;&#26159;&#38271;&#26399;&#25233;&#21046;&#23558;&#21457;&#29983;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#33539;&#20363;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#20855;&#26377;&#22810;&#23618;&#34920;&#31034;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard models of biologically realistic or biologically inspired reinforcement learning employ a global error signal, which implies the use of shallow networks. On the other hand, error backpropagation allows the use of networks with multiple layers. However, precise error backpropagation is difficult to justify in biologically realistic networks because it requires precise weighted error backpropagation from layer to layer. In this study, we introduce a novel network that solves this problem by propagating only the sign of the plasticity change (i.e., LTP/LTD) throughout the whole network, while neuromodulation controls the learning rate. Neuromodulation can be understood as a rectified error or relevance signal, while the top-down sign of the error signal determines whether long-term potentiation or long-term depression will occur. To demonstrate the effectiveness of this approach, we conducted a real robotic task as proof of concept. Our results show that this paradigm can success
&lt;/p&gt;</description></item><item><title>GRAPE&#26159;&#19968;&#31181;&#36719;&#20214;&#36164;&#28304;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#22270;&#65292;&#24182;&#21033;&#29992;&#19987;&#19994;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#31639;&#27861;&#21644;&#24555;&#36895;&#24182;&#34892;&#23454;&#29616;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#36164;&#28304;&#26356;&#39640;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#33410;&#28857;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.06196</link><description>&lt;p&gt;
GRAPE: &#29992;&#20110;&#24555;&#36895;&#21487;&#25193;&#23637;&#22270;&#22788;&#29702;&#21644;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#23884;&#20837;&#30340;&#36719;&#20214;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
GRAPE for Fast and Scalable Graph Processing and random walk-based Embedding. (arXiv:2110.06196v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06196
&lt;/p&gt;
&lt;p&gt;
GRAPE&#26159;&#19968;&#31181;&#36719;&#20214;&#36164;&#28304;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#22411;&#22270;&#65292;&#24182;&#21033;&#29992;&#19987;&#19994;&#21644;&#26234;&#33021;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#31639;&#27861;&#21644;&#24555;&#36895;&#24182;&#34892;&#23454;&#29616;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#36164;&#28304;&#26356;&#39640;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#33410;&#28857;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20026;&#35299;&#20915;&#30001;&#22270;&#34920;&#31034;&#30340;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#22270;&#20013;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#33410;&#28857;&#21644;&#25968;&#21313;&#20159;&#20010;&#36793;&#65292;&#24050;&#32463;&#36229;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#21644;&#36719;&#20214;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRAPE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#22788;&#29702;&#21644;&#23884;&#20837;&#30340;&#36719;&#20214;&#36164;&#28304;&#65292;&#21487;&#20197;&#20351;&#29992;&#19987;&#19994;&#32780;&#26234;&#33021;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#31639;&#27861;&#21644;&#24555;&#36895;&#24182;&#34892;&#23454;&#29616;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#25193;&#23637;&#22823;&#22411;&#22270;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#36164;&#28304;&#30456;&#27604;&#65292;GRAPE&#22312;&#32463;&#39564;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#37117;&#26174;&#31034;&#20986;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#65292;&#20197;&#21450;&#31454;&#20105;&#24615;&#30340;&#33410;&#28857;&#26631;&#31614;&#39044;&#27979;&#24615;&#33021;&#12290;GRAPE&#21253;&#25324;&#32422;170&#19975;&#34892;Python&#21644;Rust&#20195;&#30721;&#65292;&#25552;&#20379;69&#31181;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#12289;25&#31181;&#25512;&#26029;&#27169;&#22411;&#12289;&#19968;&#31995;&#21015;&#39640;&#25928;&#30340;&#22270;&#22788;&#29702;&#24037;&#20855;&#21644;&#36229;&#36807;80,000&#20010;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#26469;&#28304;&#30340;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) methods opened new avenues for addressing complex, real-world problems represented by graphs. However, many graphs used in these applications comprise millions of nodes and billions of edges and are beyond the capabilities of current methods and software implementations. We present GRAPE, a software resource for graph processing and embedding that can scale with big graphs by using specialized and smart data structures, algorithms, and a fast parallel implementation of random walk-based methods. Compared with state-of-the-art software resources, GRAPE shows an improvement of orders of magnitude in empirical space and time complexity, as well as a competitive edge and node label prediction performance. GRAPE comprises about 1.7 million well-documented lines of Python and Rust code and provides 69 node embedding methods, 25 inference models, a collection of efficient graph processing utilities and over 80,000 graphs from the literature and other source
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#21483;&#20570;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24207;&#21015;&#26368;&#23567;&#21270;&#26041;&#27861;&#65288;BVFSM&#65289;&#65292;&#36890;&#36807;&#23558;BLO&#37325;&#26500;&#20026;&#36817;&#20284;&#30340;&#21333;&#23618;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#25152;&#38656;&#30340;&#37325;&#22797;&#35745;&#31639;&#24490;&#29615;&#26799;&#24230;&#21644;Hessian&#36870;&#30340;&#26102;&#38388;&#28040;&#32791;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#20219;&#21153;&#20197;&#21450;&#20855;&#26377;&#39069;&#22806;&#21151;&#33021;&#32422;&#26463;&#30340;BLO&#12290;</title><link>http://arxiv.org/abs/2110.04974</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24207;&#21015;&#26368;&#23567;&#21270;&#26041;&#27861;&#22312;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Value-Function-based Sequential Minimization for Bi-level Optimization. (arXiv:2110.04974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#21483;&#20570;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24207;&#21015;&#26368;&#23567;&#21270;&#26041;&#27861;&#65288;BVFSM&#65289;&#65292;&#36890;&#36807;&#23558;BLO&#37325;&#26500;&#20026;&#36817;&#20284;&#30340;&#21333;&#23618;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#25152;&#38656;&#30340;&#37325;&#22797;&#35745;&#31639;&#24490;&#29615;&#26799;&#24230;&#21644;Hessian&#36870;&#30340;&#26102;&#38388;&#28040;&#32791;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#20219;&#21153;&#20197;&#21450;&#20855;&#26377;&#39069;&#22806;&#21151;&#33021;&#32422;&#26463;&#30340;BLO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#29616;&#20195;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31574;&#30053;&#37117;&#26159;&#22522;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#19979;&#19968;&#23618;&#23376;&#38382;&#39064;&#30340;&#20984;&#24615;&#65289;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#32500;&#20219;&#21153;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#22312;&#37027;&#20123;&#20855;&#26377;&#21151;&#33021;&#32422;&#26463;&#21644;&#24754;&#35266;BLO&#31561;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;BLO&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;BLO&#37325;&#26500;&#20026;&#36817;&#20284;&#30340;&#21333;&#23618;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#23618;&#20215;&#20540;&#20989;&#25968;&#24207;&#21015;&#26368;&#23567;&#21270;&#65288;BVFSM&#65289;&#30340;&#26032;&#31639;&#27861;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BVFSM&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#36817;&#20284;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#29616;&#26377;&#26041;&#27861;&#25152;&#38656;&#30340;&#37325;&#22797;&#35745;&#31639;&#24490;&#29615;&#26799;&#24230;&#21644;Hessian&#36870;&#30340;&#26102;&#38388;&#28040;&#32791;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;BVFSM&#20197;&#35299;&#20915;&#20855;&#26377;&#39069;&#22806;&#21151;&#33021;&#32422;&#26463;&#30340;BLO&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Bi-Level Optimization (BLO) methods have been widely applied to handle modern learning tasks. However, most existing strategies are theoretically designed based on restrictive assumptions (e.g., convexity of the lower-level sub-problem), and computationally not applicable for high-dimensional tasks. Moreover, there are almost no gradient-based methods able to solve BLO in those challenging scenarios, such as BLO with functional constraints and pessimistic BLO. In this work, by reformulating BLO into approximated single-level problems, we provide a new algorithm, named Bi-level Value-Function-based Sequential Minimization (BVFSM), to address the above issues. Specifically, BVFSM constructs a series of value-function-based approximations, and thus avoids repeated calculations of recurrent gradient and Hessian inverse required by existing approaches, time-consuming especially for high-dimensional tasks. We also extend BVFSM to address BLO with additional functional constrai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#27010;&#24565;&#26469;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#65292;&#22312;&#23558;&#24038;&#21491;&#27893;&#20114;&#25442;&#35282;&#33394;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#26368;&#23567;&#21270;&#21387;&#21147;&#33033;&#20914;&#12290;</title><link>http://arxiv.org/abs/2109.08717</link><description>&lt;p&gt;
&#20351;&#29992;RBF&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#24494;&#22411;&#27893;
&lt;/p&gt;
&lt;p&gt;
The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network. (arXiv:2109.08717v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#27010;&#24565;&#26469;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#65292;&#22312;&#23558;&#24038;&#21491;&#27893;&#20114;&#25442;&#35282;&#33394;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#26368;&#23567;&#21270;&#21387;&#21147;&#33033;&#20914;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20248;&#21270;&#20855;&#26377;&#24182;&#32852;&#27893;&#33108;&#21644;&#34987;&#21160;&#27490;&#22238;&#38400;&#30340;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#20219;&#21153;&#26159;&#22312;&#24038;&#21491;&#27893;&#20114;&#25442;&#21560;&#20837;&#21644;&#36755;&#36865;&#35282;&#33394;&#26102;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#65292;&#23558;&#30001;&#21453;&#27969;&#24341;&#36215;&#30340;&#21387;&#21147;&#33033;&#20914;&#26368;&#23567;&#21270;&#65292;&#36825;&#23545;&#31283;&#23450;&#30340;&#24658;&#27969;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#34987;&#21160;&#27490;&#22238;&#38400;&#30340;&#26426;&#26800;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20174;&#25511;&#21046;&#29702;&#35770;&#35282;&#24230;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#21147;&#33033;&#20914;&#22312;0.15-0.25 MPa&#30340;&#33539;&#22260;&#20869;&#24471;&#21040;&#20102;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#26368;&#22823;&#27893;&#24037;&#20316;&#21387;&#21147;40 MPa&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to optimize the performance of a constant flow parallel mechanical displacement micropump, which has parallel pump chambers and incorporates passive check valves. The critical task is to minimize the pressure pulse caused by regurgitation, which negatively impacts the constant flow rate, during the reciprocating motion when the left and right pumps interchange their role of aspiration and transfusion. Previous works attempt to solve this issue via the mechanical design of passive check valves. In this work, the novel concept of overlap time is proposed, and the issue is solved from the aspect of control theory by implementing a RBF neural network trained by both unsupervised and supervised learning. The experimental results indicate that the pressure pulse is optimized in the range of 0.15 - 0.25 MPa, which is a significant improvement compared to the maximum pump working pressure of 40 MPa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;&#35813;&#27169;&#22411;&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2103.06615</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#24067;&#26009;&#25805;&#20316;&#30340;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.06615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;&#35813;&#27169;&#22411;&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22788;&#29702;&#38750;&#21018;&#24615;&#29289;&#20307;&#65288;&#22914;&#24067;&#26009;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#19982;&#38750;&#21018;&#24615;&#29289;&#20307;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#26159;&#19981;&#30830;&#23450;&#21644;&#22797;&#26434;&#30340;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#20174;&#26679;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24314;&#27169;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29366;&#24577;&#34920;&#31034;&#30340;&#39640;&#32500;&#24615;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#20854;&#23884;&#20837;&#21040;&#20302;&#32500;&#27969;&#24418;&#20013;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290; CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;&#32771;&#34385;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20808;&#39564;&#20998;&#24067;&#26469;&#36793;&#32536;&#21270;&#20004;&#20010;&#26144;&#23556;&#30340;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;CGPDM&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years, significant advances have been made in robotic manipulation, but still, the handling of non-rigid objects, such as cloth garments, is an open problem. Physical interaction with non-rigid objects is uncertain and complex to model. Thus, extracting useful information from sample data can considerably improve modeling performance. However, the training of such models is a challenging task due to the high-dimensionality of the state representation. In this paper, we propose Controlled Gaussian Process Dynamical Model (CGPDM) for learning high-dimensional, nonlinear dynamics by embedding it in a low-dimensional manifold. A CGPDM is constituted by a low-dimensional latent space, with an associated dynamics where external control variables can act and a mapping to the observation space. The parameters of both maps are marginalized out by considering Gaussian Process (GP) priors. Hence, a CGPDM projects a high-dimensional state space into a smaller dimension latent space, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#21644;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.04047</link><description>&lt;p&gt;
&#36880;&#27493;&#20998;&#26512;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.04047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#21644;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#23601;&#12290;&#25968;&#25454;&#25928;&#29575;&#26159;&#23558;&#36825;&#31181;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#39640;&#25928;&#20195;&#29702;&#30340;&#35774;&#35745;&#38656;&#35201;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20449;&#24687;&#33719;&#21462;&#21644;&#34920;&#31034;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27010;&#24565;&#21644;&#36951;&#25022;&#20998;&#26512;&#65292;&#20849;&#21516;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#12290;&#36825;&#31181;&#24605;&#36335;&#25581;&#31034;&#20102;&#20851;&#20110;&#23547;&#27714;&#20160;&#20040;&#20449;&#24687;&#12289;&#22914;&#20309;&#23547;&#27714;&#35813;&#20449;&#24687;&#20197;&#21450;&#20445;&#30041;&#21738;&#20123;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#31361;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We discuss concepts and regret analysis that together offer principled guidance. This line of thinking sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that highlight data efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#38590;&#20197;&#22312;&#32447;&#39034;&#24207;&#26356;&#26032;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#25311;&#21512;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#36136;&#30340;&#20989;&#25968;&#12290;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1905.10003</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#38750;&#24179;&#31283;&#20989;&#25968;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#38590;&#20197;&#22312;&#32447;&#39034;&#24207;&#26356;&#26032;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#25311;&#21512;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#36136;&#30340;&#20989;&#25968;&#12290;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#22312;&#20272;&#35745;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#20013;&#24471;&#21040;&#35299;&#20915;&#65292;&#36890;&#24120;&#36825;&#20123;&#20989;&#25968;&#26159;&#26102;&#38388;&#30456;&#20851;&#30340;&#20989;&#25968;&#65292;&#24182;&#19988;&#26159;&#23454;&#26102;&#22320;&#38543;&#30528;&#35266;&#27979;&#30340;&#21040;&#26469;&#32780;&#20272;&#35745;&#30340;&#12290;&#39640;&#26031;&#36807;&#31243;&#26159;&#24314;&#27169;&#23454;&#20540;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#23384;&#22312;&#33509;&#24178;&#19981;&#36275;&#65306;1&#65289;&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#25512;&#26029;&#30340;&#22797;&#26434;&#24230;$O(N^{3})$&#38543;&#30528;&#35266;&#27979;&#20540;&#30340;&#20010;&#25968;N&#25104;&#22686;&#38271;&#65307;2&#65289;&#36880;&#27493;&#26356;&#26032;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#19981;&#23481;&#26131;&#65307;3&#65289;&#21327;&#26041;&#24046;&#26680;&#36890;&#24120;&#23545;&#20989;&#25968;&#26045;&#21152;&#24179;&#31283;&#24615;&#32422;&#26463;&#65292;&#32780;&#20855;&#26377;&#38750;&#24179;&#31283;&#21327;&#26041;&#24046;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#36890;&#24120;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#26469;&#25311;&#21512;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#65292;&#20197;&#25429;&#25417;&#38750;&#24179;&#31283;&#34892;&#20026;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#32447;&#12289;&#20998;&#24067;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo
&lt;/p&gt;</description></item></channel></rss>