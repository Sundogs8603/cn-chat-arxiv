<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13948</link><description>&lt;p&gt;
&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Decoupled Kullback-Leibler Divergence Loss. (arXiv:2305.13948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#20102;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#19982;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#31561;&#20215;&#65292;&#21518;&#32773;&#30001;&#21152;&#26435;&#22343;&#26041;&#24046;&#25439;&#22833;&#21644;&#21253;&#21547;&#36719;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32452;&#25104;&#12290;&#36890;&#36807;&#23545;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#33976;&#39311;&#31561;&#22330;&#26223;&#19979;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#38382;&#39064;&#12290;&#36825;&#20010;&#25913;&#36827;&#20445;&#35777;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;wMSE&#32452;&#20214;&#22987;&#32456;&#26377;&#25928;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#26500;&#36896;&#24615;&#26263;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#20840;&#23616;&#20449;&#24687;&#24341;&#20837;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#29992;&#20110;&#31867;&#20869;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#25913;&#36827;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#26159;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#65292;&#31639;&#27861;&#26174;&#31034;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.13946</link><description>&lt;p&gt;
&#26080;&#38656;Lipschitzness&#21644;Smoothness&#30340;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness. (arXiv:2305.13946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#65292;&#31639;&#27861;&#26174;&#31034;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#20013;&#30340;&#31532;&#19968;&#31181;&#23567;&#25439;&#22833;&#21644;&#24179;&#31283;&#21464;&#21270;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#26631;&#24535;&#30528;&#22312;&#32447;&#20984;&#20248;&#21270;&#20855;&#26377;&#38750;Lipschitz&#12289;&#38750;&#20809;&#28369;&#25439;&#22833;&#30340;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#30340;&#39318;&#27425;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#20960;&#20046;&#26159;&#25237;&#36164;&#36873;&#25321;&#25968;&#37327;&#30340;&#32447;&#24615;&#12290;&#36951;&#25022;&#19978;&#30028;&#26159;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#30340;&#26032;&#22411;&#20809;&#28369;&#24615;&#34920;&#24449;&#12289;&#36981;&#24490;&#20855;&#26377;&#33258;&#20849;&#36717;&#27491;&#21017;&#21270;&#22120;&#30340;&#27491;&#21017;&#21270;&#39046;&#34966;&#65288;FTRL&#65289;&#30340;&#23616;&#37096;&#33539;&#25968;&#20998;&#26512;&#12289;&#23427;&#20204;&#19981;&#19968;&#23450;&#26159;&#38556;&#30861;&#30340;&#21644;&#20855;&#26377;log&#38556;&#30861;&#30340;&#20048;&#35266;FTRL&#30340;&#38544;&#24335;&#21464;&#20307;&#26469;&#25512;&#23548;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-iteration time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13941</link><description>&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#21644;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language. (arXiv:2305.13941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#28151;&#21512;&#27169;&#22411;&#31561;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#19968;&#31181;&#35270;&#35273;&#35821;&#35328;&#65292;&#22686;&#24378;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#19988;&#32463;&#24120;&#20316;&#20026;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#20027;&#35201;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#25163;&#35821;&#30340;&#20808;&#22825;&#24615;&#21548;&#21147;&#20007;&#22833;&#32773;&#24182;&#19981;&#22810;&#65292;&#20182;&#20204;&#32463;&#24120;&#38754;&#20020;&#31038;&#20132;&#23396;&#31435;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21019;&#24314;&#20154;&#26426;&#30028;&#38754;&#31995;&#32479;&#65292;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#25552;&#20379;&#31038;&#20132;&#24179;&#21488;&#12290;&#24066;&#22330;&#19978;&#22823;&#22810;&#25968;&#21830;&#29992;&#25163;&#35821;&#32763;&#35793;&#31995;&#32479;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#65292;&#20215;&#26684;&#26114;&#36149;&#65292;&#20351;&#29992;&#36215;&#26469;&#20063;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#36843;&#20999;&#38656;&#35201;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20294;&#39318;&#20808;&#24517;&#39035;&#20811;&#26381;&#20960;&#20010;&#25361;&#25112;&#12290;&#26089;&#26399;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#29616;&#22312;&#27491;&#22312;&#24212;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23558;&#25163;&#37096;&#21644;&#25163;&#35821;&#21160;&#20316;&#36716;&#21270;&#20026;&#21475;&#35821;&#25110;&#20070;&#38754;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20844;&#20849;&#30340;&#20013;&#22269;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#34701;&#21512;&#20102;CNN&#21644;LSTM&#30340;&#28151;&#21512;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language is a visual language that enhances communication between people and is frequently used as the primary form of communication by people with hearing loss. Even so, not many people with hearing loss use sign language, and they frequently experience social isolation. Therefore, it is necessary to create human-computer interface systems that can offer hearing-impaired people a social platform. Most commercial sign language translation systems now on the market are sensor-based, pricey, and challenging to use. Although vision-based systems are desperately needed, they must first overcome several challenges. Earlier continuous sign language recognition techniques used hidden Markov models, which have a limited ability to include temporal information. To get over these restrictions, several machine learning approaches are being applied to transform hand and sign language motions into spoken or written language. In this study, we compare various deep learning techniques for recogn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.13938</link><description>&lt;p&gt;
&#36890;&#36807;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35270;&#35282;&#35762;&#36848;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#65306;&#25110;&#35859;&#27861;&#24459;&#38750;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree. (arXiv:2305.13938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#32654;&#24863;&#21040;&#19981;&#20844;&#24179;&#21644;&#27495;&#35270;&#30340;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#27861;&#24459;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#32773;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#20197;&#21450;&#27861;&#24459;&#19978;&#30340;&#27495;&#35270;&#21644;&#24179;&#31561;&#27010;&#24565;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#36890;&#24120;&#19981;&#28165;&#26970;&#65292;&#23548;&#33268;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#20043;&#38388;&#30340;&#35823;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#27010;&#24565;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#21512;&#20197;&#21450;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#25214;&#20986;&#19982;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#31867;&#27604;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#21644;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#31526;&#21512;&#27431;&#30431;&#30340;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACPro&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20998;&#35299;&#31574;&#30053;&#26469;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#25345;&#32493;&#21327;&#20316;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22522;&#20110;&#21508;&#33258;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#24773;&#22659;&#36880;&#27493;&#25193;&#23637;&#65292;&#24182;&#19988;&#36824;&#32467;&#21512;&#20102;&#36741;&#21161;&#20219;&#21153;&#21644;CTDE&#27010;&#24565;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#32463;&#20856;&#26827;&#30424;&#28216;&#25103;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13937</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#25345;&#32493;&#21327;&#20316;&#65306;&#36880;&#27493;&#20219;&#21153;&#24773;&#22659;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Continual Coordination via Progressive Task Contextualization. (arXiv:2305.13937v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACPro&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20998;&#35299;&#31574;&#30053;&#26469;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#25345;&#32493;&#21327;&#20316;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22522;&#20110;&#21508;&#33258;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#24773;&#22659;&#36880;&#27493;&#25193;&#23637;&#65292;&#24182;&#19988;&#36824;&#32467;&#21512;&#20102;&#36741;&#21161;&#20219;&#21153;&#21644;CTDE&#27010;&#24565;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#32463;&#20856;&#26827;&#30424;&#28216;&#25103;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACPro&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20998;&#35299;&#31574;&#30053;&#65292;&#20351;&#29992;&#20849;&#20139;&#29305;&#24449;&#25552;&#21462;&#23618;&#20294;&#26159;&#20998;&#31163;&#30340;&#29420;&#31435;&#20219;&#21153;&#22836;&#65292;&#27599;&#20010;&#20219;&#21153;&#22836;&#20998;&#21035;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#24773;&#22659;&#36880;&#27493;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#20808;&#36827;&#30340;&#36741;&#21161;&#20219;&#21153;&#21644;CTDE&#27010;&#24565;&#65292;&#20197;&#36827;&#19968;&#27493;&#31283;&#23450;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#22312;&#21508;&#31181;&#29615;&#22659;&#19979;&#39564;&#35777;&#20102;&#27492;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#22797;&#26434;&#30340;Atari&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-agent Reinforcement Learning (MARL) has attracted significant attention and played the potential for many real-world applications. Previous arts mainly focus on facilitating the coordination ability from different aspects (e.g., non-stationarity, credit assignment) in single-task or multi-task scenarios, ignoring the stream of tasks that appear in a continual manner. This ignorance makes the continual coordination an unexplored territory, neither in problem formulation nor efficient algorithms designed. Towards tackling the mentioned issue, this paper proposes an approach Multi-Agent Continual Coordination via Progressive Task Contextualization, dubbed MACPro. The key point lies in obtaining a factorized policy, using shared feature extraction layers but separated independent task heads, each specializing in a specific class of tasks. The task heads can be progressively expanded based on the learned task contextualization. Moreover, to cater to the popular CTDE paradi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; CroMAC &#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#28040;&#24687;&#35748;&#35777;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#31574;&#30053;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#25509;&#25910;&#21040;&#21463;&#25200;&#21160;&#28040;&#24687;&#26102;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20248;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.13936</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20449;&#24687;&#35748;&#35777;&#19979;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-agent Communication via Multi-view Message Certification. (arXiv:2305.13936v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; CroMAC &#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#28040;&#24687;&#35748;&#35777;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#31574;&#30053;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#25509;&#25910;&#21040;&#21463;&#25200;&#21160;&#28040;&#24687;&#26102;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20248;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#28040;&#24687;&#20849;&#20139;&#20419;&#36827;&#21327;&#35843;&#65292;&#21152;&#24555;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#28040;&#24687;&#21463;&#25200;&#21160;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35768;&#22810;&#30456;&#20851;&#24037;&#20316;&#37117;&#22522;&#20110;&#29305;&#23450;&#20551;&#35774;&#65292;&#22914;&#26377;&#38480;&#30340;&#28040;&#24687;&#36890;&#36947;&#21487;&#20197;&#25215;&#21463;&#25200;&#21160;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; CroMAC &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#28040;&#24687;&#35748;&#35777;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#31574;&#30053;&#12290;&#22312; CroMAC &#19979;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#33719;&#24471;&#29366;&#24577;&#21160;&#20316;&#20540;&#30340;&#20445;&#35777;&#19979;&#38480;&#65292;&#20197;&#22312;&#25509;&#25910;&#21040;&#21463;&#25200;&#21160;&#28040;&#24687;&#26102;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20248;&#20915;&#31574;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#39318;&#20808;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#35270;&#22270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#28040;&#24687;&#20195;&#34920;&#19968;&#20010;&#29366;&#24577;&#35270;&#22270;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MVAE&#65289;&#25552;&#21462;&#32463;&#36807;&#35748;&#35777;&#30340;&#32852;&#21512;&#28040;&#24687;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many multi-agent scenarios require message sharing among agents to promote coordination, hastening the robustness of multi-agent communication when policies are deployed in a message perturbation environment. Major relevant works tackle this issue under specific assumptions, like a limited number of message channels would sustain perturbations, limiting the efficiency in complex scenarios. In this paper, we take a further step addressing this issue by learning a robust multi-agent communication policy via multi-view message certification, dubbed CroMAC. Agents trained under CroMAC can obtain guaranteed lower bounds on state-action values to identify and choose the optimal action under a worst-case deviation when the received messages are perturbed. Concretely, we first model multi-agent communication as a multi-view problem, where every message stands for a view of the state. Then we extract a certificated joint message representation by a multi-view variational autoencoder (MVAE) that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2305.13935</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#39564;&#35777;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#32452;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65288;&#31216;&#20026;DistroFair&#65289;&#65292;&#36890;&#36807;&#23558;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#24341;&#20837;&#21040;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#65292;&#36890;&#36807;&#19977;&#31181;&#35821;&#20041;&#20445;&#30041;&#22270;&#20687;&#21464;&#25442; - &#23545;&#35937;&#21024;&#38500;&#65292;&#23545;&#35937;&#25554;&#20837;&#21644;&#23545;&#35937;&#26059;&#36716;&#26469;&#31995;&#32479;&#24615;&#22320;&#26292;&#38706;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#31867;&#32423;&#21035;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;CityScapes&#21644;MS-COCO&#65289;&#21644;&#19977;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#65288;&#21363;Amazon Rekognition&#65292;Google Cloud Vision&#21644;Azure&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#23545;DistroFair&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DistroFair&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#65292;&#32422;&#26377;21&#65285;&#36890;&#36807;&#30495;&#23454;&#26631;&#20934;&#25110;&#20803;&#27979;&#35797;&#26631;&#20934;&#26174;&#38706;&#20986;&#20102;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27627;&#31859;&#27874;&#32593;&#32476;&#20013;&#29992;&#25143;&#21644;&#36710;&#36742;&#31227;&#21160;&#12289;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20302;&#24320;&#38144;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.13929</link><description>&lt;p&gt;
&#27627;&#31859;&#27874;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#24341;&#23548;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and Image Super-Resolution-Guided Beam and Power Allocation for mmWave Networks. (arXiv:2305.13929v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27627;&#31859;&#27874;&#32593;&#32476;&#20013;&#29992;&#25143;&#21644;&#36710;&#36742;&#31227;&#21160;&#12289;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20302;&#24320;&#38144;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27627;&#31859;&#27874;&#32593;&#32476;&#20013;&#29992;&#25143;&#21644;&#36710;&#36742;&#30340;&#31227;&#21160;&#20197;&#21450;&#19981;&#24517;&#35201;&#30340;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#23454;&#29616;&#20302;&#24320;&#38144;&#30340;&#27874;&#26463;&#21644;&#21151;&#29575;&#20998;&#37197;&#65292;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27874;&#26463;&#36136;&#37327;&#39044;&#27979;&#21644;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a deep learning (DL)-guided hybrid beam and power allocation approach for multiuser millimeter-wave (mmWave) networks, which facilitates swift beamforming at the base station (BS). The following persisting challenges motivated our research: (i) User and vehicular mobility, as well as redundant beam-reselections in mmWave networks, degrade the efficiency; (ii) Due to the large beamforming dimension at the BS, the beamforming weights predicted by the cutting-edge DL-based methods often do not suit the channel distributions; (iii) Co-located user devices may cause a severe beam conflict, thus deteriorating system performance. To address the aforementioned challenges, we exploit the synergy of supervised learning and super-resolution technology to enable low-overhead beam- and power allocation. In the first step, we propose a method for beam-quality prediction. It is based on deep learning and explores the relationship between high- and low-resolution beam images 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20174;&#21407;&#22987;&#23556;&#39057;&#25968;&#25454;&#20013;&#29983;&#25104;&#36719;&#27979;&#36317;&#20449;&#24687;&#26469;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#30452;&#32447;&#35270;&#36317;&#26816;&#27979;&#21644;&#27979;&#36317;&#35823;&#24046;&#25511;&#21046;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.13911</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23556;&#39057;&#25968;&#25454;&#36719;&#27979;&#36317;&#20449;&#24687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Generating Soft Range Information from RF Data. (arXiv:2305.13911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20174;&#21407;&#22987;&#23556;&#39057;&#25968;&#25454;&#20013;&#29983;&#25104;&#36719;&#27979;&#36317;&#20449;&#24687;&#26469;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#30452;&#32447;&#35270;&#36317;&#26816;&#27979;&#21644;&#27979;&#36317;&#35823;&#24046;&#25511;&#21046;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22312;&#20174;&#23556;&#39057;&#27979;&#37327;&#20013;&#25552;&#21462;&#36275;&#22815;&#20449;&#24687;&#30340;&#25361;&#25112;&#19979;&#65292;&#23556;&#39057;&#25216;&#26415;&#20173;&#28982;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23460;&#20869;&#23450;&#20301;&#12290;&#36719;&#27979;&#36317;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39640;&#31934;&#24230;&#23450;&#20301;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#32473;&#20986;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#36317;&#31163;&#20272;&#35745;&#20540;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#36317;&#31163;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23556;&#39057;&#25968;&#25454;&#20013;&#29983;&#25104;&#20934;&#30830;&#30340;&#36719;&#27979;&#36317;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#31070;&#32463;&#27169;&#22359;&#32452;&#25104;&#30340;&#32593;&#32476;&#23454;&#29616;&#65292;&#24182;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#37327;&#21270;&#22312;&#19981;&#21516;&#23460;&#20869;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#36719;&#27979;&#36317;&#20449;&#24687;&#65292;&#24182;&#22312;&#38750;&#30452;&#32447;&#35270;&#36317;&#26816;&#27979;&#21644;&#27979;&#36317;&#35823;&#24046;&#25511;&#21046;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio frequency (RF)-based techniques are widely adopted for indoor localization despite the challenges in extracting sufficient information from measurements. Soft range information (SRI) offers a promising alternative for highly accurate localization that gives all probable range values rather than a single estimate of distance. We propose a deep learning approach to generate accurate SRI from RF measurements. In particular, the proposed approach is implemented by a network with two neural modules and conducts the generation directly from raw data. Extensive experiments on a case study with two public datasets are conducted to quantify the efficiency in different indoor localization tasks. The results show that the proposed approach can generate highly accurate SRI, and significantly outperforms conventional techniques in both non-line-of-sight (NLOS) detection and ranging error mitigation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;UWB&#27979;&#36317;&#35823;&#24046;&#65292;&#24182;&#23558;&#27010;&#29575;&#24314;&#27169;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#24369;&#30417;&#30563;&#19979;&#23545;UWB&#27979;&#36317;&#35823;&#24046;&#30340;&#40065;&#26834;&#24615;&#32531;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.13904</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;GEM&#32593;&#32476;&#30340;&#24369;&#30417;&#30563;UWB&#27979;&#36317;&#35823;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep GEM-Based Network for Weakly Supervised UWB Ranging Error Mitigation. (arXiv:2305.13904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13904
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;UWB&#27979;&#36317;&#35823;&#24046;&#65292;&#24182;&#23558;&#27010;&#29575;&#24314;&#27169;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#24369;&#30417;&#30563;&#19979;&#23545;UWB&#27979;&#36317;&#35823;&#24046;&#30340;&#40065;&#26834;&#24615;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#25216;&#26415;&#34429;&#28982;&#25104;&#20026;&#39640;&#31934;&#24230;&#23450;&#20301;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#20294;&#22312;&#24694;&#21155;&#29615;&#22659;&#19979;&#24120;&#21463;&#21040;&#27979;&#36317;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#26032;&#20852;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35823;&#24046;&#32531;&#35299;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21407;&#22987;&#25968;&#25454;&#20013;&#30340;&#39640;&#35821;&#20041;&#29305;&#24449;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#23545;&#23436;&#20840;&#26631;&#27880;&#25968;&#25454;&#36807;&#20110;&#20381;&#36182;&#65292;&#23548;&#33268;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;UWB&#27979;&#36317;&#35823;&#24046;&#32531;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;GEM&#65289;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24369;&#30417;&#30563;&#19979;&#32531;&#35299;UWB&#27979;&#36317;&#35823;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#27010;&#29575;&#24314;&#27169;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#26631;&#31614;&#20316;&#20026;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#30417;&#30563;&#22330;&#26223;&#19979;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultra-wideband (UWB)-based techniques, while becoming mainstream approaches for high-accurate positioning, tend to be challenged by ranging bias in harsh environments. The emerging learning-based methods for error mitigation have shown great performance improvement via exploiting high semantic features from raw data. However, these methods rely heavily on fully labeled data, leading to a high cost for data acquisition. We present a learning framework based on weak supervision for UWB ranging error mitigation. Specifically, we propose a deep learning method based on the generalized expectation-maximization (GEM) algorithm for robust UWB ranging error mitigation under weak supervision. Such method integrate probabilistic modeling into the deep learning scheme, and adopt weakly supervised labels as prior information. Extensive experiments in various supervision scenarios illustrate the superiority of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#22823;&#22411;&#25968;&#25454;&#29615;&#22659;&#19979;&#20351;&#29992;&#23376;&#37319;&#26679;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20999;&#25442;&#25968;&#25454;&#23376;&#38598;&#24182;&#21487;&#29992;&#20110;&#25193;&#25955;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13882</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#25193;&#25955;&#20013;&#30340;&#23376;&#37319;&#26679;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Subsampling Error in Stochastic Gradient Langevin Diffusions. (arXiv:2305.13882v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#22312;&#22823;&#22411;&#25968;&#25454;&#29615;&#22659;&#19979;&#20351;&#29992;&#23376;&#37319;&#26679;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20999;&#25442;&#25968;&#25454;&#23376;&#38598;&#24182;&#21487;&#29992;&#20110;&#25193;&#25955;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#36890;&#24120;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#12290;&#19982;&#35768;&#22810;&#24120;&#35268;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31639;&#27861;&#19981;&#21516;&#65292;SGLD &#23545;&#20110;&#21518;&#39564;&#20998;&#24067;&#19981;&#26159;&#31283;&#23450;&#30340;&#12290;&#23427;&#26377;&#20004;&#20010;&#38169;&#35823;&#26469;&#28304;&#65306;&#31532;&#19968;&#20010;&#38169;&#35823;&#26159;&#30001; Euler-Maruyama &#31163;&#25955;&#21270; Langevin &#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#30340;&#65292;&#31532;&#20108;&#20010;&#38169;&#35823;&#26469;&#33258;&#20110;&#25968;&#25454;&#23376;&#37319;&#26679;&#65292;&#36825;&#20351;&#24471;&#23427;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102; SGLD &#30340;&#29702;&#24819;&#21270;&#29256;&#26412;&#65292;&#20197;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#32431;&#23376;&#37319;&#26679;&#35823;&#24046;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#23376;&#37319;&#26679; MCMC &#26041;&#27861;&#30340;&#26368;&#20339;&#24773;&#20917;&#35823;&#24046;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#25193;&#25955; (SGLDiff)&#65292;&#36825;&#26159;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#23427;&#36981;&#24490;&#19982;&#25968;&#25454;&#23376;&#38598;&#30456;&#24212;&#30340; Langevin &#25193;&#25955;&#65292;&#24182;&#22312;&#25351;&#25968;&#31561;&#24453;&#26102;&#38388;&#21518;&#20999;&#25442;&#35813;&#25968;&#25454;&#23376;&#38598;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163; (Was)
&lt;/p&gt;
&lt;p&gt;
The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler--Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method's pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show that the Was
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#20844;&#24179;&#20840;&#23616;&#27169;&#22411;&#21644;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.13878</link><description>&lt;p&gt;
&#12298;&#20844;&#24179;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Fair Differentially Private Federated Learning Framework. (arXiv:2305.13878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#20844;&#24179;&#20840;&#23616;&#27169;&#22411;&#21644;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#21442;&#19982;&#32773;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#20010;&#20154;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#24182;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#22312;FL&#20013;&#65292;&#38544;&#31169;&#24615;&#21644;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;FL&#36890;&#36807;&#26368;&#23567;&#21270;&#20648;&#23384;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#30340;&#29992;&#25143;&#25968;&#25454;&#37327;&#26469;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#65292;&#20294;&#20173;&#23384;&#22312;&#38656;&#35201;&#35299;&#20915;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20026;&#20102;&#30830;&#20445;FL&#30340;&#38544;&#31169;&#24615;&#65292;&#38656;&#35201;&#36981;&#24490;&#24046;&#20998;&#38544;&#31169;&#12289;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#12289;&#21516;&#24577;&#21152;&#23494;&#21644;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#31561;&#34892;&#19994;&#26631;&#20934;&#12290;FL&#20013;&#30340;&#20844;&#24179;&#24615;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#27169;&#22411;&#21487;&#33021;&#20250;&#32487;&#25215;&#23616;&#37096;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#24179;&#34913;&#38544;&#31169;&#24615;&#21644;&#20844;&#24179;&#24615;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#38544;&#31169;&#24615;&#35201;&#27714;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#20844;&#24179;&#24615;&#38656;&#35201;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#20844;&#24179;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#8221;&#65292;&#35299;&#20915;&#20102;&#22312;&#19981;&#20351;&#29992;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20844;&#24179;&#20840;&#23616;&#27169;&#22411;&#21644;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#65292;&#30830;&#20445;&#26368;&#32456;&#30340;&#20840;&#23616;&#27169;&#22411;&#27809;&#26377;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#30830;&#20445;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;FL&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning strategy that enables participants to collaborate and train a shared model without sharing their individual datasets. Privacy and fairness are crucial considerations in FL. While FL promotes privacy by minimizing the amount of user data stored on central servers, it still poses privacy risks that need to be addressed. Industry standards such as differential privacy, secure multi-party computation, homomorphic encryption, and secure aggregation protocols are followed to ensure privacy in FL. Fairness is also a critical issue in FL, as models can inherit biases present in local datasets, leading to unfair predictions. Balancing privacy and fairness in FL is a challenge, as privacy requires protecting user data while fairness requires representative training data. This paper presents a "Fair Differentially Private Federated Learning Framework" that addresses the challenges of generating a fair global model without validation data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#19988;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.13875</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fair Oversampling Technique using Heterogeneous Clusters. (arXiv:2305.13875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32858;&#31867;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#19988;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#34987;&#35748;&#20026;&#26159;&#22952;&#30861;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#26435;&#34913;&#30340;&#20004;&#20010;&#21407;&#22240;&#12290;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#25552;&#20986;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#26469;&#20849;&#21516;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#32452;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#36136;&#24615;&#32676;&#38598;&#25968;&#25454;&#30340;&#20844;&#24179;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#31867;&#29305;&#24449;&#25110;&#28151;&#21512;&#32452;&#29305;&#24449;&#65292;&#20351;&#20998;&#31867;&#22120;&#33021;&#22815;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance and group (e.g., race, gender, and age) imbalance are acknowledged as two reasons in data that hinder the trade-off between fairness and utility of machine learning classifiers. Existing techniques have jointly addressed issues regarding class imbalance and group imbalance by proposing fair over-sampling techniques. Unlike the common oversampling techniques, which only address class imbalance, fair oversampling techniques significantly improve the abovementioned trade-off, as they can also address group imbalance. However, if the size of the original clusters is too small, these techniques may cause classifier overfitting. To address this problem, we herein develop a fair oversampling technique using data from heterogeneous clusters. The proposed technique generates synthetic data that have class-mix features or group-mix features to make classifiers robust to overfitting. Moreover, we develop an interpolation method that can enhance the validity of generated synthetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#65292;&#24182;&#19988;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#12290;&#20316;&#32773;&#37492;&#23450;&#20102;&#19968;&#20123;&#25991;&#26412;&#25552;&#31034;&#22240;&#32032;&#21644;&#27169;&#22411;&#20542;&#21521;&#22240;&#32032;&#65292;&#20197;&#25581;&#31034;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#26426;&#29702;&#65292;&#24182;&#19988;&#20984;&#26174;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13873</link><description>&lt;p&gt;
&#19981;&#23433;&#20840;&#25193;&#25955;&#65306;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;
&lt;/p&gt;
&lt;p&gt;
Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. (arXiv:2305.13873v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#65292;&#24182;&#19988;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#12290;&#20316;&#32773;&#37492;&#23450;&#20102;&#19968;&#20123;&#25991;&#26412;&#25552;&#31034;&#22240;&#32032;&#21644;&#27169;&#22411;&#20542;&#21521;&#22240;&#32032;&#65292;&#20197;&#25581;&#31034;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#26426;&#29702;&#65292;&#24182;&#19988;&#20984;&#26174;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stable Diffusion&#21644;DALLE&#183;2&#31561;&#26368;&#26032;&#30340;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20154;&#20204;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#26041;&#24335;&#12290;&#21516;&#26102;&#65292;&#31038;&#20250;&#23545;&#23545;&#25163;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#25285;&#24551;&#30340;&#27169;&#22240;&#23384;&#22312;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#25581;&#31034;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20116;&#31181;&#31867;&#21035;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#20998;&#31867;&#27861;(&#24615;&#26292;&#21147;&#12289;&#26292;&#21147;&#12289;&#20196;&#20154;&#19981;&#23433;&#12289;&#20196;&#20154;&#24974;&#24694;&#21644;&#25919;&#27835;)&#65292;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#27604;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#65307;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#22235;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#20013;&#65292;&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26377;14.56%&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#22312;&#27604;&#36739;&#36825;&#22235;&#31181;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19981;&#21516;&#30340;&#39118;&#38505;&#27700;&#24179;&#65292;&#20854;&#20013;Stable Diffusion&#26159;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#26368;&#23481;&#26131;&#30340;(&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26377;18.92%&#26159;&#19981;&#23433;&#20840;&#30340;)&#12290;&#37492;&#20110;Stable Diffusion&#30340;&#27969;&#34892;&#21644;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;Stable Diffusion&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#20854;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#20542;&#21521;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#32463;&#24120;&#23548;&#33268;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#21450;&#27169;&#22411;&#29983;&#25104;&#26576;&#20123;&#31867;&#22411;&#20869;&#23481;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#29983;&#25104;&#20197;&#21450;&#24320;&#21457;&#24378;&#26377;&#21147;&#30340;&#23545;&#31574;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\cdot$2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that these models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#29992;&#26041;&#26696;&#65292;&#20351;&#29992;&#26412;&#22320;&#20998;&#31867;&#22120;&#21644;&#36741;&#21161;&#27169;&#22411;&#36827;&#34892;&#37325;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#26041;&#20132;&#21449;&#29109;&#25439;&#22833;&#29992;&#20110;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13871</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#29992;&#26041;&#26696;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Heterogeneous Model Reuse by Density Estimation. (arXiv:2305.13871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#29992;&#26041;&#26696;&#65292;&#20351;&#29992;&#26412;&#22320;&#20998;&#31867;&#22120;&#21644;&#36741;&#21161;&#27169;&#22411;&#36827;&#34892;&#37325;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22810;&#26041;&#20132;&#21449;&#29109;&#25439;&#22833;&#29992;&#20110;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26041;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#29992;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#31169;&#26377;&#25968;&#25454;&#26469;&#23398;&#20064;&#27169;&#22411;&#12290;&#27169;&#22411;&#37325;&#29992;&#26159;&#22810;&#26041;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20551;&#35774;&#27599;&#20010;&#21442;&#19982;&#26041;&#37117;&#35757;&#32451;&#20102;&#26412;&#22320;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65292;&#19968;&#20123;&#24322;&#26500;&#27169;&#22411;&#37325;&#29992;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26412;&#22320;&#20998;&#31867;&#22120;&#65292;&#20294;&#26412;&#22320;&#25968;&#25454;&#30340;&#29305;&#24449;&#24182;&#27809;&#26377;&#34987;&#24456;&#22909;&#22320;&#21033;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#20272;&#35745;&#26412;&#22320;&#25968;&#25454;&#30340;&#23494;&#24230;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#36741;&#21161;&#27169;&#22411;&#19982;&#26412;&#22320;&#20998;&#31867;&#22120;&#19968;&#36215;&#37325;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#22810;&#26041;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#26657;&#20934;&#12290;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20174;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#24322;&#26500;&#27169;&#22411;&#37325;&#29992;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20102;&#23494;&#24230;&#20272;&#35745;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies multiparty learning, aiming to learn a model using the private data of different participants. Model reuse is a promising solution for multiparty learning, assuming that a local model has been trained for each party. Considering the potential sample selection bias among different parties, some heterogeneous model reuse approaches have been developed. However, although pre-trained local classifiers are utilized in these approaches, the characteristics of the local data are not well exploited. This motivates us to estimate the density of local data and design an auxiliary model together with the local classifiers for reuse. To address the scenarios where some local models are not well pre-trained, we further design a multiparty cross-entropy loss for calibration. Upon existing works, we address a challenging problem of heterogeneous model reuse from a decision theory perspective and take advantage of recent advances in density estimation. Experimental results on both s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.13869</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator. (arXiv:2305.13869v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#26159;&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#39640;&#24230;&#28789;&#27963;&#30340;&#35774;&#26045;&#65292;&#38656;&#35201;&#27599;&#21608;&#37325;&#26032;&#37197;&#32622;&#21644;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;&#35774;&#32622;&#26102;&#38388;&#23545;&#20110;&#25552;&#20379;&#20805;&#36275;&#30340;&#23454;&#39564;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36719; actor-critic(TBSAC)&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#27491;&#30340;&#21152;&#36895;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#25511;&#21046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#22312;&#20013;&#22269;&#36229;&#37325;&#20803;&#32032;&#21152;&#36895;&#22120;&#35774;&#26045;(CAFe II)&#21644;&#19968;&#20010;&#36731;&#36136;&#31890;&#23376;&#27880;&#20837;&#22120;(LPI)&#20013;&#25191;&#34892;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20856;&#22411;&#26463;&#27969;&#25511;&#21046;&#20219;&#21153;&#12290;&#22312;CAFe II&#30340;&#19977;&#20010;&#20302;&#28201;&#27169;&#22359;&#20013;&#20998;&#21035;&#25191;&#34892;&#20102;&#36712;&#36947;&#26657;&#27491;&#20219;&#21153;&#65292;&#35843;&#35856;&#25152;&#38656;&#26102;&#38388;&#24050;&#32463;&#20943;&#23569;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#26657;&#27491;&#21518;&#30340;RMS&#20540;&#37117;&#23567;&#20110;1&#27627;&#31859;&#12290;&#21478;&#19968;&#20010;&#20256;&#36755;&#25928;&#29575;&#20248;&#21270;&#20219;&#21153;&#22312;CAFe II&#30340;&#21152;&#36895;&#22120;&#27573;LPI&#20013;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13865</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#24494;&#35843;&#30340;&#26377;&#36873;&#25321;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Selective Pre-training for Private Fine-tuning. (arXiv:2305.13865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#24819;&#22312;&#30005;&#23376;&#37038;&#20214;&#23458;&#25143;&#31471;&#25110;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#35757;&#32451;&#25991;&#26412;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#36981;&#23432;&#29305;&#23450;&#30340;&#22266;&#23450;&#22823;&#23567;&#65292;&#20197;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;D_pub&#21644;&#19968;&#20010;&#23545;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;T&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;D_priv&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;D_pub&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;M&#65292;&#24182;&#22312;D_priv&#19978;&#24494;&#35843;&#23427;&#65292;&#20351;&#24471;M&#30456;&#23545;&#20110;T&#30340;&#24615;&#33021;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;M&#30456;&#23545;&#20110;D_priv&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;D_pub&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#23558;&#20844;&#20849;&#20998;&#24067;&#19982;&#31169;&#26377;&#20998;&#24067;&#38752;&#36817;&#65292;&#26159;&#26368;&#22823;&#21270;M&#39044;&#35757;&#32451;&#21518;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#38500;&#20102;&#24615;&#33021;&#25913;&#36827;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we want to train text prediction models in email clients or word processors. The models must preserve the privacy of user data and adhere to a specific fixed size to meet memory and inference time requirements. We introduce a generic framework to solve this problem. Specifically, we are given a public dataset $D_\text{pub}$ and a private dataset $D_\text{priv}$ corresponding to a downstream task $T$. How should we pre-train a fixed-size model $M$ on $D_\text{pub}$ and fine-tune it on $D_\text{priv}$ such that performance of $M$ with respect to $T$ is maximized and $M$ satisfies differential privacy with respect to $D_\text{priv}$? We show that pre-training on a {\em subset} of dataset $D_\text{pub}$ that brings the public distribution closer to the private distribution is a crucial ingredient to maximize the transfer learning abilities of $M$ after pre-training, especially in the regimes where model sizes are relatively small. Besides performance improvements, our framework als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#22312;&#25308;&#21344;&#24237;&#23481;&#38169;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#24403;&#26799;&#24230;&#35745;&#31639;&#24635;&#25968;&#22266;&#23450;&#26102;&#65292;&#26368;&#20339;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#38543;&#25308;&#21344;&#24237;&#24037;&#20154;&#30340;&#27604;&#20363;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.13856</link><description>&lt;p&gt;
&#35770;&#25308;&#21344;&#24237;&#23481;&#38169;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#26368;&#20339;&#25209;&#22788;&#29702;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
On the Optimal Batch Size for Byzantine-Robust Distributed Learning. (arXiv:2305.13856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#22312;&#25308;&#21344;&#24237;&#23481;&#38169;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#24403;&#26799;&#24230;&#35745;&#31639;&#24635;&#25968;&#22266;&#23450;&#26102;&#65292;&#26368;&#20339;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#38543;&#25308;&#21344;&#24237;&#24037;&#20154;&#30340;&#27604;&#20363;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#24847;&#22806;&#22833;&#35823;&#25110;&#24694;&#24847;&#25915;&#20987;&#23548;&#33268;&#35745;&#31639;&#35774;&#22791;&#24322;&#24120;&#34892;&#20026;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;BRDL&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#22823;&#26041;&#24046;&#65292;&#29616;&#26377;&#30340;BRDL&#26041;&#27861;&#20173;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#12290;&#22686;&#21152;&#25209;&#22788;&#29702;&#22823;&#23567;&#26159;&#20943;&#23569;&#26041;&#24046;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#35745;&#31639;&#24635;&#25968;&#22266;&#23450;&#26102;&#65292;&#36807;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#20250;&#23548;&#33268;&#36845;&#20195;&#27425;&#25968;&#36807;&#23569;&#65288;&#26356;&#26032;&#27425;&#25968;&#65289;&#65292;&#21487;&#33021;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#20934;&#30830;&#29575;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#22266;&#23450;&#26799;&#24230;&#35745;&#31639;&#24635;&#25968;&#30340;&#24773;&#20917;&#19979;&#26368;&#20339;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#32463;&#39564;&#19978;&#34920;&#26126;&#65292;&#24403;&#26799;&#24230;&#35745;&#31639;&#24635;&#25968;&#22266;&#23450;&#26102;&#65292;BRDL&#20013;&#26368;&#20339;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#38543;&#25308;&#21344;&#24237;&#24037;&#20154;&#30340;&#27604;&#20363;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byzantine-robust distributed learning (BRDL), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing BRDL methods will suffer from a significant drop on model accuracy due to the large variance of stochastic gradients. Increasing batch sizes is a simple yet effective way to reduce the variance. However, when the total number of gradient computation is fixed, a too-large batch size will lead to a too-small iteration number (update number), which may also degrade the model accuracy. In view of this challenge, we mainly study the optimal batch size when the total number of gradient computation is fixed in this work. In particular, we theoretically and empirically show that when the total number of gradient computation is fixed, the optimal batch size in BRDL increases with the fraction of Byzantine workers. Ther
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#25324;&#21644;&#20998;&#31867;&#20102;&#22823;&#35268;&#27169;GNN&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#24314;&#31435;&#20102;GNN&#31995;&#32479;&#12289;&#22270;&#24418;&#22788;&#29702;&#31995;&#32479;&#21644;DL&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.13854</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31995;&#32479;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36827;&#21270;&#21450;&#20854;&#28304;&#20110;&#22270;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;: &#19968;&#39033;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Evolution of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey. (arXiv:2305.13854v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#25324;&#21644;&#20998;&#31867;&#20102;&#22823;&#35268;&#27169;GNN&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#24314;&#31435;&#20102;GNN&#31995;&#32479;&#12289;&#22270;&#24418;&#22788;&#29702;&#31995;&#32479;&#21644;DL&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#31181;&#19987;&#38376;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20307;&#31995;&#32467;&#26500;&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#22635;&#34917;&#20102;&#22270;&#24418;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30001;&#20110;&#22270;&#24418;&#26080;&#22788;&#19981;&#22312;&#65292;GNNs&#33021;&#22815;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#31561;&#12290;&#38543;&#30528;&#30495;&#23454;&#19990;&#30028;&#22270;&#24418;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#38656;&#35201;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;GNN&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#25552;&#20986;GNN&#31995;&#32479;&#30340;&#20316;&#21697;&#12290;&#20294;&#26159;&#65292;&#32570;&#20047;&#36825;&#20123;&#31995;&#32479;&#30340;&#27010;&#36848;&#12289;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#21644;&#20998;&#31867;&#22823;&#35268;&#27169;GNN&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;GNN&#31995;&#32479;&#12289;&#22270;&#24418;&#22788;&#29702;&#31995;&#32479;&#21644;DL&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are an emerging research field. This specialized Deep Neural Network (DNN) architecture is capable of processing graph structured data and bridges the gap between graph processing and Deep Learning (DL). As graphs are everywhere, GNNs can be applied to various domains including recommendation systems, computer vision, natural language processing, biology and chemistry. With the rapid growing size of real world graphs, the need for efficient and scalable GNN training solutions has come. Consequently, many works proposing GNN systems have emerged throughout the past few years. However, there is an acute lack of overview, categorization and comparison of such systems. We aim to fill this gap by summarizing and categorizing important methods and techniques for large-scale GNN solutions. In addition, we establish connections between GNN systems, graph processing systems and DL systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13849</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39640;&#26031;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#20570;&#20986;&#22823;&#30340;&#25913;&#21464;&#65292;&#24182;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#20272;&#35745;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#21644;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#26679;&#26412;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#36866;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33391;&#22909;&#27491;&#21017;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#20102;&#37325;&#22823;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;&#12289;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#26550;&#26500;&#30340;&#25913;&#21160;&#35201;&#27714;&#26368;&#23567;&#12290;&#20026;&#20102;&#24471;&#21040;&#36866;&#29992;&#20110;&#39532;&#27663;&#36317;&#31163;&#35745;&#31639;&#30340;&#39640;&#26031;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31867;&#20869;&#34920;&#31034;&#20998;&#20026;&#22810;&#20010;&#39640;&#26031;&#12290;&#20855;&#26377;&#38750;&#39640;&#26031;&#34920;&#31034;&#30340;&#31867;&#21035;&#34987;&#33258;&#21160;&#35782;&#21035;&#24182;&#21160;&#24577;&#32858;&#31867;&#20026;&#22810;&#20010;&#22823;&#27010;&#29575;&#26159;&#39640;&#26031;&#20998;&#24067;&#30340;&#31867;&#21035;&#12290;&#22312;&#26631;&#20934;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.13825</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#38548;&#31163;&#30340;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Dynamic Graphs via Parameter Isolation. (arXiv:2305.13825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13825
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#23398;&#20064;&#26032;&#27169;&#24335;&#21644;&#20445;&#30041;&#26087;&#27169;&#24335;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#26032;&#33410;&#28857;&#21644;&#36793;&#20986;&#29616;&#30340;&#21160;&#24577;&#22270;&#12290;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36973;&#36935;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21363;&#20026;&#20197;&#21069;&#30340;&#22270;&#25152;&#23398;&#30340;&#30693;&#35782;&#20250;&#34987;&#26032;&#22270;&#30340;&#26356;&#26032;&#35206;&#30422;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#24182;&#32500;&#25252;&#26087;&#30340;&#27169;&#24335;&#65292;&#20294;&#20351;&#29992;&#30456;&#21516;&#22266;&#23450;&#22823;&#23567;&#30340;&#21442;&#25968;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20004;&#31181;&#30446;&#26631;&#20043;&#38388;&#30340;&#26681;&#26412;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Parameter Isolation GNN (PI-GNN)&#65292;&#29992;&#20110;&#21160;&#24577;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#21442;&#25968;&#38548;&#31163;&#21644;&#25193;&#23637;&#26469;&#36991;&#20813;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22312;&#20110;&#19981;&#21516;&#30340;&#21442;&#25968;&#23545;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#22270;&#27169;&#24335;&#26377;&#36129;&#29486;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#27169;&#22411;&#21442;&#25968;&#20197;&#25345;&#32493;&#23398;&#20064;&#20986;&#29616;&#30340;&#22270;&#27169;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20445;&#23384;&#26410;&#21463;&#24433;&#21709;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25214;&#21040;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13824</link><description>&lt;p&gt;
&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning for Dynamic Material Handling. (arXiv:2305.13824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#26009;&#25644;&#36816;&#26159;&#26580;&#24615;&#21046;&#36896;&#31995;&#32479;&#30340;&#26680;&#24515;&#37096;&#20998;&#20043;&#19968;&#65292;&#28041;&#21450;&#33258;&#21160;&#21270;&#36710;&#36742;&#22312;&#24037;&#20316;&#31449;&#20043;&#38388;&#30340;&#29289;&#26009;&#23384;&#20648;&#21644;&#25644;&#36816;&#12290;&#29289;&#26009;&#25644;&#36816;&#30340;&#25913;&#21892;&#21487;&#20197;&#20419;&#36827;&#21046;&#36896;&#31995;&#32479;&#30340;&#25972;&#20307;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23433;&#25490;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#21160;&#24577;&#20107;&#20214;&#23545;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#21160;&#24577;&#29289;&#26009;&#25644;&#36816;&#20013;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;&#36827;&#34892;&#35843;&#24230;&#12290;&#22312;&#19968;&#20123;&#30495;&#23454;&#22330;&#26223;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#30340;&#26032;&#20219;&#21153;&#21644;&#24847;&#22806;&#30340;&#36710;&#36742;&#25925;&#38556;&#35270;&#20026;&#25105;&#20204;&#38382;&#39064;&#20013;&#30340;&#21160;&#24577;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#32771;&#34385;&#21040;&#36831;&#21040;&#21644;&#21487;&#29992;&#36710;&#36742;&#20316;&#20026;&#32047;&#31215;&#32422;&#26463;&#21644;&#30636;&#26102;&#32422;&#26463;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCPOM&#65292;&#23427;&#32467;&#21512;&#20102;Lagrangian&#26494;&#24347;&#21644;&#26080;&#25928;&#21160;&#20316;&#23631;&#34109;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22788;&#29702;&#21160;&#24577;&#20107;&#20214;&#21644;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the core parts of flexible manufacturing systems, material handling involves storage and transportation of materials between workstations with automated vehicles. The improvement in material handling can impulse the overall efficiency of the manufacturing system. However, the occurrence of dynamic events during the optimisation of task arrangements poses a challenge that requires adaptability and effectiveness. In this paper, we aim at the scheduling of automated guided vehicles for dynamic material handling. Motivated by some real-world scenarios, unknown new tasks and unexpected vehicle breakdowns are regarded as dynamic events in our problem. We formulate the problem as a constrained Markov decision process which takes into account tardiness and available vehicles as cumulative and instantaneous constraints, respectively. An adaptive constrained reinforcement learning algorithm that combines Lagrangian relaxation and invalid action masking, named RCPOM, is proposed to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#31163;&#32447;&#20219;&#21153;&#24207;&#21015;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#32463;&#39564;&#37325;&#25918; (ER) &#26159;&#26368;&#36866;&#21512; CORL &#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#24341;&#20837; ER &#21518;&#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13804</link><description>&lt;p&gt;
&#31163;&#32447;&#20307;&#39564;&#37325;&#25918;&#29992;&#20110;&#36830;&#32493;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Experience Replay for Continual Offline Reinforcement Learning. (arXiv:2305.13804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#31163;&#32447;&#20219;&#21153;&#24207;&#21015;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#32463;&#39564;&#37325;&#25918; (ER) &#26159;&#26368;&#36866;&#21512; CORL &#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#24341;&#20837; ER &#21518;&#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#20808;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#26029;&#23398;&#20064;&#26032;&#25216;&#33021;&#26159;&#29702;&#24819;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36830;&#32493;&#23398;&#20064;&#19968;&#31995;&#21015;&#31163;&#32447;&#20219;&#21153;&#24456;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#23567;&#30340;&#22238;&#25918;&#32531;&#20914;&#21306;&#23398;&#20064;&#19968;&#31995;&#21015;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23398;&#20064;&#20219;&#21153;&#20013;&#36861;&#27714;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#25506;&#32034;&#25152;&#26377;&#39034;&#24207;&#20219;&#21153;&#30340;&#20219;&#20309;&#29615;&#22659;&#12290;&#20026;&#20102;&#22312;&#25152;&#26377;&#39034;&#24207;&#20219;&#21153;&#19978;&#25345;&#32493;&#23398;&#20064;&#65292;&#20195;&#29702;&#38656;&#35201;&#20197;&#31163;&#32447;&#26041;&#24335;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#26087;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23454;&#39564;&#21457;&#29616;&#32463;&#39564;&#37325;&#25918; (ER) &#26159; CORL &#38382;&#39064;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23558; ER &#24341;&#20837; CORL &#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65306;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#29366;&#24577;&#20998;&#24067;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of continuously learning new skills via a sequence of pre-collected offline datasets is desired for an agent. However, consecutively learning a sequence of offline tasks likely leads to the catastrophic forgetting issue under resource-limited scenarios. In this paper, we formulate a new setting, continual offline reinforcement learning (CORL), where an agent learns a sequence of offline reinforcement learning tasks and pursues good performance on all learned tasks with a small replay buffer without exploring any of the environments of all the sequential tasks. For consistently learning on all sequential tasks, an agent requires acquiring new knowledge and meanwhile preserving old knowledge in an offline manner. To this end, we introduced continual learning algorithms and experimentally found experience replay (ER) to be the most suitable algorithm for the CORL problem. However, we observe that introducing ER into CORL encounters a new distribution shift problem: the mism
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;NORM&#65292;&#36890;&#36807;&#19968;&#31181;&#29305;&#24449;&#21464;&#25442;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#20445;&#30041;&#25945;&#24072;&#32593;&#32476;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20351;&#24471;&#23398;&#29983;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#36924;&#36817;&#25945;&#24072;&#32593;&#32476;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13803</link><description>&lt;p&gt;
&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
NORM: Knowledge Distillation via N-to-One Representation Matching. (arXiv:2305.13803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;N&#21040;&#19968;&#30340;&#34920;&#31034;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;NORM&#65292;&#36890;&#36807;&#19968;&#31181;&#29305;&#24449;&#21464;&#25442;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#20445;&#30041;&#25945;&#24072;&#32593;&#32476;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#20351;&#24471;&#23398;&#29983;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#36924;&#36817;&#25945;&#24072;&#32593;&#32476;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#39044;&#36873;&#30340;&#24072;&#29983;&#23618;&#23545;&#20043;&#38388;&#30340;&#19968;&#23545;&#19968;&#34920;&#31034;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;N&#21040;&#19968;&#34920;&#31034;&#65288;NORM&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#30001;&#20004;&#20010;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31616;&#21333;&#29305;&#24449;&#21464;&#25442;&#65288;FT&#65289;&#27169;&#22359;&#12290;&#20026;&#20102;&#20445;&#30041;&#30001;&#25945;&#24072;&#32593;&#32476;&#23398;&#20064;&#30340;&#23436;&#25972;&#20449;&#24687;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FT&#27169;&#22359;&#20165;&#25554;&#20837;&#22312;&#23398;&#29983;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#21367;&#31215;&#23618;&#20043;&#21518;&#12290;&#31532;&#19968;&#23618;&#32447;&#24615;&#23618;&#23558;&#23398;&#29983;&#34920;&#31034;&#25237;&#23556;&#21040;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#35813;&#29305;&#24449;&#31354;&#38388;&#30340;&#29305;&#24449;&#36890;&#36947;&#25968;&#26159;&#26368;&#21518;&#19968;&#20010;&#21367;&#31215;&#23618;&#20013;&#25945;&#24072;&#34920;&#31034;&#30340;N&#20493;&#65292;&#31532;&#20108;&#20010;&#32447;&#24615;&#23618;&#23558;&#25193;&#23637;&#30340;&#36755;&#20986;&#25910;&#32553;&#22238;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;&#25193;&#23637;&#30340;&#23398;&#29983;&#34920;&#31034;&#39034;&#24207;&#20998;&#25104;N&#20010;&#19981;&#37325;&#21472;&#30340;&#29305;&#24449;&#27573;&#65292;&#27599;&#20010;&#27573;&#20855;&#26377;&#19982;&#25945;&#24072;&#30340;&#30456;&#21516;&#25968;&#37327;&#30340;&#29305;&#24449;&#36890;&#36947;&#65292;&#23427;&#20204;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24378;&#21046;&#36817;&#20284;&#20110;&#25945;&#24072;&#30340;&#34920;&#31034;.
&lt;/p&gt;
&lt;p&gt;
Existing feature distillation methods commonly adopt the One-to-one Representation Matching between any pre-selected teacher-student layer pair. In this paper, we present N-to-One Representation (NORM), a new two-stage knowledge distillation method, which relies on a simple Feature Transform (FT) module consisting of two linear layers. In view of preserving the intact information learnt by the teacher network, during training, our FT module is merely inserted after the last convolutional layer of the student network. The first linear layer projects the student representation to a feature space having N times feature channels than the teacher representation from the last convolutional layer, and the second linear layer contracts the expanded output back to the original feature space. By sequentially splitting the expanded student representation into N non-overlapping feature segments having the same number of feature channels as the teacher's, they can be readily forced to approximate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21270;&#26041;&#27861;&#29992;&#20110;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23545;&#31216;&#21270;&#36807;&#31243;&#20013;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13797</link><description>&lt;p&gt;
SNEkhorn: &#23545;&#31216;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities. (arXiv:2305.13797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13797
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#21270;&#26041;&#27861;&#29992;&#20110;&#29109;&#20146;&#21644;&#21147;&#19979;&#30340;&#38477;&#32500;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23545;&#31216;&#21270;&#36807;&#31243;&#20013;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21152;&#26435;&#22270;&#26469;&#32534;&#30721;&#25968;&#25454;&#38598;&#20013;&#26679;&#26412;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#29109;&#20146;&#21644;&#21147;&#65288;EAs&#65289;&#26159;&#36825;&#31867;&#22270;&#30340;&#29305;&#20363;&#65292;&#23427;&#36890;&#24120;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#31639;&#27861; t-SNE &#20013;&#12290;&#20026;&#20102;&#20445;&#35777;&#23545;&#19981;&#21516;&#37319;&#26679;&#23494;&#24230;&#30340;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;EAs &#25353;&#19968;&#23450;&#26041;&#24335;&#23545;&#27599;&#20010;&#26679;&#26412;&#20998;&#37197;&#19968;&#20010;&#26680;&#24102;&#23485;&#21442;&#25968;&#65292;&#20197;&#20351;&#24471;&#20146;&#21644;&#21147;&#30697;&#38453;&#20013;&#27599;&#19968;&#34892;&#30340;&#29109;&#37117;&#20445;&#25345;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25351;&#25968;&#21442;&#25968;&#19979;&#12290;EAs&#20855;&#26377;&#19981;&#23545;&#31216;&#24615;&#21644;&#25353;&#34892;&#38543;&#26426;&#24615;&#65292;&#20294;&#26159;&#22312;&#32463;&#36807;&#21551;&#21457;&#24335;&#23545;&#31216;&#21270;&#26041;&#27861;&#22788;&#29702;&#21518;&#65292;&#21448;&#34987;&#29992;&#20110;&#38477;&#32500;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;EAs&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#24418;&#24335;&#65292;&#35270;&#20854;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#23545;&#31216;&#21270;&#65292;&#24182;&#19988;&#21487;&#29992;&#21452;&#37325;&#19978;&#21319;&#27861;&#39640;&#25928;&#35745;&#31639;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#20146;&#21644;&#21147;&#30697;&#38453;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#23545;&#31216;&#21270;&#25152;&#24102;&#26469;&#30340;&#29109;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13795</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#26641;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning. (arXiv:2305.13795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22521;&#35757;&#36890;&#24120;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;(QD-RL)&#26159;&#19968;&#31867;&#26032;&#20852;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#21644;RL&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19968;&#31995;&#21015;&#20851;&#20110;&#34892;&#20026;&#23884;&#20837;&#30340;&#39640;&#24615;&#33021;&#21644;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#31574;&#30053;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QD-RL&#26041;&#27861;&#36804;&#20170;&#20026;&#27490;&#21033;&#29992;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#30340;&#36827;&#27493;&#24050;&#32463;&#25171;&#24320;&#20102;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#31639;&#27861;&#30340;&#22823;&#38376;&#65292;&#32780;&#23558;&#29616;&#26377;&#30340;&#31163;&#31574;&#30053;QD-RL&#26041;&#27861;&#25193;&#23637;&#21040;&#36825;&#20123;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#20102;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#31561;&#31574;&#30053;&#26041;&#27861;&#19982;QD&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD-RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generally capable agents that perform well in unseen dynamic environments is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging class of reinforcement learning (RL) algorithms that blend insights from Quality Diversity (QD) and RL to produce a collection of high performing and behaviorally diverse policies with respect to a behavioral embedding. Existing QD-RL approaches have thus far taken advantage of sample-efficient off-policy RL algorithms. However, recent advances in high-throughput, massively parallelized robotic simulators have opened the door for algorithms that can take advantage of such parallelism, and it is unclear how to scale existing off-policy QD-RL methods to these new data-rich regimes. In this work, we take the first steps to combine on-policy RL methods, specifically Proximal Policy Optimization (PPO), that can leverage massive parallelism, with QD, and propose a new QD-RL method with these high-throughput s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13786</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#12298;&#24863;&#30693;&#27979;&#35797;&#65306;&#22810;&#27169;&#24577;&#35270;&#39057;&#27169;&#22411;&#30340;&#35786;&#26029;&#22522;&#20934;&#12299;
&lt;/p&gt;
&lt;p&gt;
Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#24863;&#30693;&#27979;&#35797;&#8221;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27979;&#35797;&#28085;&#30422;&#20102;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#31561;&#25512;&#29702;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#22522;&#20934;&#8212;&#8212;&#24863;&#30693;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914; Flamingo&#12289;BEiT-3 &#25110; GPT-4&#65289;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#25216;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20391;&#37325;&#20110;&#35745;&#31639;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#12289;&#26816;&#27979;&#25110;&#36319;&#36394;&#65289;&#19981;&#21516;&#65292;&#24863;&#30693;&#27979;&#35797;&#20391;&#37325;&#20110;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#36328;&#36234;&#35760;&#24518;&#12289;&#25277;&#35937;&#12289;&#29289;&#29702;&#12289;&#35821;&#20041;&#31561;&#25216;&#33021;&#21644;&#25512;&#29702;&#31867;&#22411;&#65288;&#25551;&#36848;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#39044;&#27979;&#24615;&#12289;&#21453;&#20107;&#23454;&#24615;&#65289;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#32780;&#39640;&#25928;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#25110;&#26377;&#38480;&#24494;&#35843;&#19979;&#25361;&#36873;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#65292;&#24863;&#30693;&#27979;&#35797;&#20171;&#32461;&#20102;11.6k&#31181;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#65292;&#24179;&#22343;&#38271;&#24230;&#20026;23&#31186;&#65292;&#26088;&#22312;&#23637;&#31034;&#24863;&#30693;&#19978;&#26377;&#36259;&#30340;&#24773;&#22659;&#65292;&#30001;&#20840;&#29699;&#32422;100&#21517;&#21442;&#19982;&#32773;&#25293;&#25668;&#12290;&#36825;&#20123;&#35270;&#39057;&#23494;&#38598;&#22320;&#24102;&#26377;&#20845;&#31181;&#26631;&#31614;&#65288;&#22810;&#39033;&#36873;&#25321;&#21644;&#22522;&#20110;&#35270;&#39057;&#38382;&#39064;&#22238;&#31572;&#65292;&#23545;&#35937;a&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#26041;&#27861;--&#19968;&#27493;&#24494;&#20998;&#29992;&#20110;&#24555;&#36895;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#20687;&#33258;&#21160;&#24494;&#20998;&#19968;&#26679;&#31616;&#21333;&#65292;&#20687;&#38544;&#24335;&#24494;&#20998;&#19968;&#26679;&#39640;&#25928;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13768</link><description>&lt;p&gt;
&#36845;&#20195;&#31639;&#27861;&#30340;&#19968;&#27493;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
One-step differentiation of iterative algorithms. (arXiv:2305.13768v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#26041;&#27861;--&#19968;&#27493;&#24494;&#20998;&#29992;&#20110;&#24555;&#36895;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#20687;&#33258;&#21160;&#24494;&#20998;&#19968;&#26679;&#31616;&#21333;&#65292;&#20687;&#38544;&#24335;&#24494;&#20998;&#19968;&#26679;&#39640;&#25928;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36866;&#24403;&#30340;&#26694;&#26550;&#20013;&#65292;&#33258;&#21160;&#24494;&#20998;&#23545;&#29992;&#25143;&#36879;&#26126;&#65292;&#20294;&#22312;&#25805;&#20316;&#25968;&#37327;&#22823;&#26102;&#20195;&#20215;&#26114;&#36149;&#12290;&#23545;&#20110;&#36845;&#20195;&#31639;&#27861;&#65292;&#38544;&#24335;&#24494;&#20998;&#21487;&#20197;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#33258;&#23450;&#20041;&#23454;&#29616;&#38597;&#21508;&#27604;&#30697;&#38453;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#27493;&#24494;&#20998;&#65292;&#20063;&#31216;&#20026;&#26080;&#38597;&#21508;&#27604;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#26159;&#19968;&#31181;&#20687;&#33258;&#21160;&#24494;&#20998;&#19968;&#26679;&#31616;&#21333;&#19988;&#20687;&#38544;&#24335;&#24494;&#20998;&#19968;&#26679;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24555;&#36895;&#31639;&#27861;&#65288;&#20363;&#22914;&#65292;&#36229;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#36817;&#20284;&#20998;&#26512;&#21644;&#20855;&#20307;&#31034;&#20363;&#65288;&#29275;&#39039;&#27861;&#65292;&#26799;&#24230;&#19979;&#38477;&#65289;&#65292;&#20197;&#21450;&#22312;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#20960;&#20010;&#25968;&#23383;&#31034;&#20363;&#35828;&#26126;&#20102;&#19968;&#27493;&#20272;&#35745;&#22120;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In appropriate frameworks, automatic differentiation is transparent to the user at the cost of being a significant computational burden when the number of operations is large. For iterative algorithms, implicit differentiation alleviates this issue but requires custom implementation of Jacobian evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free backpropagation, a method as easy as automatic differentiation and as performant as implicit differentiation for fast algorithms (e.g., superlinear optimization methods). We provide a complete theoretical approximation analysis with specific examples (Newton's method, gradient descent) along with its consequences in bilevel optimization. Several numerical examples illustrate the well-foundness of the one-step estimator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#26469;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#39069;&#22806;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13764</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise through Data Ambiguation. (arXiv:2305.13764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#26469;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#39069;&#22806;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20855;&#26377;&#39640;&#34920;&#29616;&#33021;&#21147;&#30340;&#22823;&#22411;&#27169;&#22411;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#36825;&#31181;&#27169;&#22411;&#23481;&#26131;&#35760;&#24518;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#24378;&#20581;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#26356;&#22797;&#26434;&#30340;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#31616;&#21333;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#32780;&#26631;&#31614;&#26657;&#27491;&#36890;&#24120;&#20250;&#22686;&#21152;&#35757;&#32451;&#35774;&#32622;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#8220;&#27169;&#31946;&#21270;&#8221;&#30446;&#26631;&#20449;&#24687;&#26469;&#35299;&#20915;&#20004;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#26631;&#31614;&#19981;&#36275;&#22815;&#21487;&#20449;&#26102;&#65292;&#28155;&#21152;&#38468;&#21152;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26469;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#25552;&#20379;&#19981;&#31934;&#30830;&#20294;&#26356;&#21487;&#38752;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by "ambiguating" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;&#38382;&#39064;&#30340;L-SA&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;L-SA&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13741</link><description>&lt;p&gt;
L-SA&#65306;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
L-SA: Learning Under-Explored Targets in Multi-Target Reinforcement Learning. (arXiv:2305.13741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13741
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#23398;&#20064;&#38382;&#39064;&#30340;L-SA&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;L-SA&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22810;&#20010;&#30446;&#26631;&#36827;&#34892;&#20132;&#20114;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#22810;&#30446;&#26631;&#20219;&#21153;&#12290;&#24403;&#24212;&#29992;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#36825;&#26679;&#30340;&#20219;&#21153;&#26102;&#65292;&#26576;&#20123;&#38590;&#20197;&#35775;&#38382;&#25110;&#20132;&#20114;&#30340;&#30446;&#26631;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#24573;&#35270;-&#36825;&#31181;&#22256;&#22659;&#31216;&#20026;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#38382;&#39064;&#65288;UTP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#30340;&#23398;&#20064;&#26694;&#26550;L-SA&#65288;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#20027;&#21160;&#26597;&#35810;&#36827;&#34892;&#23398;&#20064;&#65289;&#12290;&#22312;L-SA&#26694;&#26550;&#20013;&#65292;&#33258;&#36866;&#24212;&#37319;&#26679;&#21160;&#24577;&#22320;&#20174;&#26368;&#39640;&#25104;&#21151;&#29575;&#30446;&#26631;&#20013;&#37319;&#26679;&#65292;&#20351;&#24471;&#23398;&#20064;&#20174;&#23481;&#26131;&#21040;&#22256;&#38590;&#30340;&#30446;&#26631;&#65292;&#20027;&#21160;&#26597;&#35810;&#21017;&#20419;&#20351;&#20195;&#29702;&#19982;&#38656;&#35201;&#26356;&#22810;&#32463;&#39564;&#25110;&#25506;&#32034;&#30340;&#25506;&#32034;&#22256;&#38590;&#30446;&#26631;&#26356;&#39057;&#32321;&#22320;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;L-SA&#26694;&#26550;&#25552;&#39640;&#20102;&#22810;&#20010;UTP&#22810;&#30446;&#26631;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25104;&#21151;&#29575;&#12290;&#21478;&#22806;&#65292;&#39044;&#35745;&#35813;&#25552;&#20986;&#30340;L-SA&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#21040;&#20854;&#20182;&#28041;&#21450;&#22810;&#20010;&#23384;&#22312;UTP&#30340;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tasks that involve interaction with various targets are called multi-target tasks. When applying general reinforcement learning approaches for such tasks, certain targets that are difficult to access or interact with may be neglected throughout the course of training - a predicament we call Under-explored Target Problem (UTP). To address this problem, we propose L-SA (Learning by adaptive Sampling and Active querying) framework that includes adaptive sampling and active querying. In the L-SA framework, adaptive sampling dynamically samples targets with the highest increase of success rates at a high proportion, resulting in curricular learning from easy to hard targets. Active querying prompts the agent to interact more frequently with under-explored targets that need more experience or exploration. Our experimental results on visual navigation tasks show that the L-SA framework improves sample efficiency as well as success rates on various multi-target tasks with UTP. Also, it is expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25552;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; ChatGPT &#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#30456;&#24403;&#20110;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13724</link><description>&lt;p&gt;
ChatGPT-EDSS: &#22522;&#20110; ChatGPT &#34893;&#29983;&#20986;&#30340;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#30340;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings. (arXiv:2305.13724v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25552;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#23454;&#29616;&#20855;&#26377;&#24773;&#24863;&#30340;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; ChatGPT &#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#30456;&#24403;&#20110;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; ChatGPT-EDSS&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992; ChatGPT &#25277;&#21462;&#23545;&#35805;&#19978;&#19979;&#25991;&#36827;&#34892;&#24773;&#24863;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#65288;EDSS&#65289;&#30340;&#26041;&#27861;&#12290; ChatGPT &#26159;&#19968;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#36755;&#20837;&#25552;&#31034;&#30340;&#20869;&#23481;&#21644;&#30446;&#30340;&#65292;&#24182;&#36866;&#24403;&#22320;&#22238;&#24212;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110; ChatGPT &#30340;&#38405;&#35835;&#29702;&#35299;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837; EDSS&#65292;&#36825;&#26159;&#19968;&#39033;&#21512;&#25104;&#20855;&#26377;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#32842;&#22825;&#21382;&#21490;&#35760;&#24405;&#25552;&#20379;&#32473; ChatGPT&#65292;&#24182;&#35201;&#27714;&#20854;&#20026;&#27599;&#34892;&#29983;&#25104;&#34920;&#31034;&#24847;&#22270;&#12289;&#24773;&#24863;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#19977;&#20010;&#21333;&#35789;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;ChatGPT&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#30340;&#23884;&#20837;&#20316;&#20026;&#35843;&#33410;&#29305;&#24449;&#65292;&#35757;&#32451;EDSS&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#24773;&#24863;&#26631;&#31614;&#25110;&#20174;&#32842;&#22825;&#21382;&#21490;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34893;&#29983;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#34893;&#29983;&#30340; ChatGPT &#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#22312; https://sarulab-speech.github.io/demo_ChatGPT_EDSS/ &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS) method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that can deeply understand the content and purpose of an input prompt and appropriately respond to the user's request. We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion. Our method first gives chat history to ChatGPT and asks it to generate three words representing the intention, emotion, and speaking style for each line in the chat. Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features. The experimental results demonstrate that our method performs comparably to ones using emotion labels or neural network-derived context embeddings learned from chat histories. The collected ChatGPT-derived context information is available at https://sarulab-speech.github.io/demo_ChatGPT_EDSS/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#20998;&#27010;&#29575;&#27979;&#37327;&#36827;&#34892;&#21327;&#21464;&#37327;&#24179;&#34913;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#26080;&#38656;&#27491;&#30830;&#35268;&#23450;&#20542;&#21521;&#24471;&#20998;&#25110;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#21363;&#21487;&#20445;&#35777;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13715</link><description>&lt;p&gt;
&#21033;&#29992;&#31215;&#20998;&#27010;&#29575;&#27979;&#37327;&#36827;&#34892;&#21327;&#21464;&#37327;&#24179;&#34913;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariate balancing using the integral probability metric for causal inference. (arXiv:2305.13715v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#20998;&#27010;&#29575;&#27979;&#37327;&#36827;&#34892;&#21327;&#21464;&#37327;&#24179;&#34913;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#26080;&#38656;&#27491;&#30830;&#35268;&#23450;&#20542;&#21521;&#24471;&#20998;&#25110;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#21363;&#21487;&#20445;&#35777;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#65292;&#21152;&#26435;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#21327;&#21464;&#37327;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21152;&#26435;&#26041;&#27861;&#21482;&#26377;&#22312;&#26576;&#31181;&#27169;&#22411;&#65288;&#22914;&#20542;&#21521;&#24471;&#20998;&#25110;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#65289;&#34987;&#27491;&#30830;&#35268;&#23450;&#26102;&#25165;&#20855;&#26377;&#29702;&#24819;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#34987;&#27491;&#30830;&#35268;&#23450;&#65292;&#30456;&#24212;&#30340;&#20272;&#35745;&#22120;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#19981;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#36827;&#34892;&#21327;&#21464;&#37327;&#24179;&#34913;&#12290;&#30830;&#23450;&#26368;&#20339;&#26435;&#37325;&#65292;&#20351;&#24471;&#38024;&#23545;&#32473;&#23450;&#30340;&#21028;&#21035;&#22120;&#65292;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#30340;&#21152;&#26435;&#32463;&#39564;&#20998;&#24067;&#20855;&#26377;&#26368;&#23567;&#30340;IPM&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#24212;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26159;&#19968;&#33268;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#27491;&#30830;&#22320;&#35268;&#23450;&#20219;&#20309;&#27169;&#22411;&#65288;&#26082;&#19981;&#26159;&#20542;&#21521;&#24471;&#20998;&#27169;&#22411;&#20063;&#19981;&#26159;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24050;&#26377;&#30340;&#21152;&#26435;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighting methods in causal inference have been widely used to achieve a desirable level of covariate balancing. However, the existing weighting methods have desirable theoretical properties only when a certain model, either the propensity score or outcome regression model, is correctly specified. In addition, the corresponding estimators do not behave well for finite samples due to large variance even when the model is correctly specified. In this paper, we consider to use the integral probability metric (IPM), which is a metric between two probability measures, for covariate balancing. Optimal weights are determined so that weighted empirical distributions for the treated and control groups have the smallest IPM value for a given set of discriminators. We prove that the corresponding estimator can be consistent without correctly specifying any model (neither the propensity score nor the outcome regression model). In addition, we empirically show that our proposed method outperforms e
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211; - CALLS&#65292;&#23427;&#26088;&#22312;&#23558;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#30340;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#39046;&#22495;&#12290;&#23545;&#20110;&#25193;&#23637;&#35813;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#35813;&#35821;&#26009;&#24211;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.13713</link><description>&lt;p&gt;
CALLS: &#20855;&#26377;&#20849;&#24773;&#23545;&#35805;&#26041;&#27861;&#30340;&#26085;&#26412;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center. (arXiv:2305.13713v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13713
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211; - CALLS&#65292;&#23427;&#26088;&#22312;&#23558;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#24212;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#20013;&#24515;&#30340;&#25237;&#35785;&#22788;&#29702;&#21644;&#20851;&#27880;&#20542;&#21548;&#39046;&#22495;&#12290;&#23545;&#20110;&#25193;&#23637;&#35813;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#35813;&#35821;&#26009;&#24211;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CALLS&#65292;&#36825;&#26159;&#19968;&#20010;&#26085;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#23558;&#23458;&#25143;&#20013;&#24515;&#20013;&#30340;&#30005;&#35805;&#21628;&#21483;&#31216;&#20026;&#20849;&#24773;&#21475;&#35821;&#23545;&#35805;&#30340;&#26032;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;STUDIES&#35821;&#26009;&#24211;&#20165;&#28085;&#30422;&#23398;&#26657;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#20026;&#20102;&#25193;&#23637;&#20849;&#24773;&#23545;&#35805;&#35821;&#38899;&#21512;&#25104;&#65288;EDSS&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#35821;&#26009;&#24211;&#65292;&#20197;&#21253;&#25324;&#19982;STUDIES&#25945;&#24072;&#30456;&#21516;&#30340;&#22899;&#24615;&#35762;&#36848;&#32773;&#65292;&#22312;&#27169;&#25311;&#30340;&#30005;&#35805;&#21628;&#21483;&#20013;&#25285;&#20219;&#25805;&#20316;&#21592;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#26009;&#24211;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24405;&#21046;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;CALLS&#21644;STUDIES&#35821;&#26009;&#24211;&#36827;&#34892;EDSS&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#35821;&#26009;&#24211;&#20250;&#23548;&#33268;&#21512;&#25104;&#35821;&#38899;&#36136;&#37327;&#30340;&#20559;&#24046;&#25913;&#36827;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#29616;&#31243;&#24230;&#19981;&#21516;&#25152;&#23548;&#33268;&#30340;&#12290;&#26412;&#35821;&#26009;&#24211;&#30340;&#39033;&#30446;&#39029;&#38754;&#26159;http&#32593;&#22336;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CALLS, a Japanese speech corpus that considers phone calls in a customer center as a new domain of empathetic spoken dialogue. The existing STUDIES corpus covers only empathetic dialogue between a teacher and student in a school. To extend the application range of empathetic dialogue speech synthesis (EDSS), we designed our corpus to include the same female speaker as the STUDIES teacher, acting as an operator in simulated phone calls. We describe a corpus construction methodology and analyze the recorded speech. We also conduct EDSS experiments using the CALLS and STUDIES corpora to investigate the effect of domain differences. The results show that mixing the two corpora during training causes biased improvements in the quality of synthetic speech due to the different degrees of expressiveness. Our project page of the corpus is this http URL
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13706</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#30340;&#20256;&#36755;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#65292;&#38656;&#35201;&#35821;&#20041;&#20256;&#36755;&#26469;&#36830;&#25509;&#20998;&#24067;&#24335;&#35774;&#22791;&#65292;&#20197;&#20445;&#35777;&#24212;&#29992;&#23618;&#24615;&#33021;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#20013;&#20110;&#36890;&#20449;&#23618;&#24615;&#33021;&#12290;&#35821;&#20041;&#22312;&#36825;&#37324;&#26159;&#20449;&#24687;&#20256;&#36755;&#26377;&#29992;&#24615;&#30340;&#34913;&#37327;&#12290;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#24120;&#24120;&#28041;&#21450;&#24222;&#22823;&#30340;&#20915;&#31574;&#31354;&#38388;&#65292;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#26368;&#20248;&#35821;&#20041;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#29702;&#35770;&#25351;&#23548;&#21407;&#21017;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22312;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;EAT&#65289;&#65292;&#21487;&#20197;&#36991;&#20813;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#25915;&#20987;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25152;&#23548;&#33268;&#30340;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#25197;&#26354;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13678</link><description>&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing Accuracy and Robustness through Adversarial Training in Class Incremental Continual Learning. (arXiv:2305.13678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;EAT&#65289;&#65292;&#21487;&#20197;&#36991;&#20813;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#25915;&#20987;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25152;&#23548;&#33268;&#30340;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#25197;&#26354;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26159;&#19968;&#20010;&#33268;&#21629;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#65288;Class Incremental Continual Learning&#65292;CICL&#65289;&#20013;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#23545;&#25239;&#24615;&#35757;&#32451;&#24212;&#29992;&#20110;CICL&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#25928;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#12290;CICL&#24050;&#30693;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#23427;&#36890;&#36807;&#21069;&#20960;&#27425;&#20219;&#21153;&#30340;&#23569;&#37327;&#26679;&#26412;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#12290;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#36935;&#26102;&#65292;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#20219;&#21153;&#30340;&#25915;&#20987;&#27425;&#25968;&#20986;&#29616;&#21478;&#19968;&#31181;&#19981;&#24179;&#34913;&#12290;&#30001;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#32570;&#20047;&#23569;&#25968;&#31867;&#30340;&#24178;&#20928;&#25968;&#25454;&#65292;&#24182;&#19988;&#30001;&#20110;&#31532;&#20108;&#31181;&#19981;&#24179;&#34913;&#22686;&#21152;&#20102;&#26469;&#33258;&#22810;&#25968;&#31867;&#30340;&#25915;&#20987;&#27425;&#25968;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20250;&#25197;&#26354;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#12290;&#36825;&#31181;&#25197;&#26354;&#26368;&#32456;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#26159;&#25552;&#39640;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26174;&#33879;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#22806;&#37096;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;External Adversarial Training&#65292;EAT&#65289;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#21040;CICL&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real life, adversarial attack to deep learning models is a fatal security issue. However, the issue has been rarely discussed in a widely used class-incremental continual learning (CICL). In this paper, we address problems of applying adversarial training to CICL, which is well-known defense method against adversarial attack. A well-known problem of CICL is class-imbalance that biases a model to the current task by a few samples of previous tasks. Meeting with the adversarial training, the imbalance causes another imbalance of attack trials over tasks. Lacking clean data of a minority class by the class-imbalance and increasing of attack trials from a majority class by the secondary imbalance, adversarial training distorts optimal decision boundaries. The distortion eventually decreases both accuracy and robustness than adversarial training. To exclude the effects, we propose a straightforward but significantly effective method, External Adversarial Training (EAT) which can be appli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.13672</link><description>&lt;p&gt;
&#32852;&#37030;&#21464;&#24322;&#25512;&#26029;&#65306;&#36808;&#21521;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Federated Variational Inference: Towards Improved Personalization and Generalization. (arXiv:2305.13672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#29983;&#25104;&#20998;&#24067;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#19981;&#36866;&#24403;&#22320;&#36817;&#20284;&#39044;&#27979;&#36807;&#31243;&#12289;&#25910;&#25947;&#21040;&#26368;&#20248;&#29366;&#24577;&#25110;&#27867;&#21270;&#21040;&#26032;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#30740;&#31350;&#22312;&#20551;&#35774;&#23458;&#25143;&#25968;&#25454;&#20998;&#24067;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#30340;&#29366;&#24577;&#19979;&#65292;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#21152;&#20197;&#35268;&#33539;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#27492;&#31639;&#27861;&#20026;&#32852;&#37030;&#21464;&#20998;&#25512;&#26029;&#65288;FedVI&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;PAC-Bayes&#20998;&#26512;&#20026;FedVI&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;FEMNIST&#21644;CIFAR-100&#22270;&#20687;&#20998;&#31867;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;FedVI&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#22343;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional federated learning algorithms train a single global model by leveraging all participating clients' data. However, due to heterogeneity in client generative distributions and predictive models, these approaches may not appropriately approximate the predictive process, converge to an optimal state, or generalize to new clients. We study personalization and generalization in stateless cross-device federated learning setups assuming heterogeneity in client data distributions and predictive models. We first propose a hierarchical generative model and formalize it using Bayesian Inference. We then approximate this process using Variational Inference to train our model efficiently. We call this algorithm Federated Variational Inference (FedVI). We use PAC-Bayes analysis to provide generalization bounds for FedVI. We evaluate our model on FEMNIST and CIFAR-100 image classification and show that FedVI beats the state-of-the-art on both tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#20307;&#27169;&#25311;&#20013;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#32467;&#26524;&#21457;&#29616;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13668</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#20284;&#24230;&#23398;&#20064;&#22312;&#20855;&#20307;&#27169;&#25311;&#20013;&#23545;&#27010;&#24565;&#35789;&#27719;&#36827;&#34892;&#23450;&#20301;&#21644;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations. (arXiv:2305.13668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#20307;&#27169;&#25311;&#20013;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#32467;&#26524;&#21457;&#29616;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#36807;&#20855;&#20307;&#27169;&#25311;&#25910;&#38598;&#30340;&#26234;&#33021;&#20307;&#32463;&#39564;&#65292;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;&#35789;&#21521;&#37327;&#25509;&#22320;&#21040;&#29289;&#20307;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#26469;&#27604;&#36739;&#19981;&#21516;&#23545;&#35937;&#31867;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#21462;&#19982;&#29289;&#20307;&#34892;&#20026;&#30456;&#20851;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#20223;&#23556;&#21464;&#25442;&#26469;&#35745;&#31639;&#20174;&#19981;&#21516;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#21040;&#36825;&#20010;&#23398;&#20064;&#31354;&#38388;&#30340;&#25237;&#24433;&#30697;&#38453;&#65292;&#24182;&#35780;&#20272;&#26159;&#21542;&#23558;&#36716;&#25442;&#21518;&#30340;&#26631;&#35760;&#21521;&#37327;&#30340;&#26032;&#27979;&#35797;&#23454;&#20363;&#27491;&#30830;&#22320;&#35782;&#21035;&#20026;&#23545;&#35937;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#27491;&#30830;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22235;&#31181;&#19981;&#21516;&#36716;&#25442;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#29305;&#24615;&#65292;&#24182;&#34920;&#26126;&#25509;&#22320;&#23545;&#35937;&#26631;&#35760;&#21521;&#37327;&#36890;&#24120;&#27604;&#25509;&#22320;&#21160;&#35789;&#21644;&#23646;&#24615;&#26631;&#35760;&#21521;&#37327;&#26356;&#26377;&#24110;&#21161;&#65292;&#36825;&#21453;&#26144;&#20102;&#26089;&#26399;&#31867;&#27604;&#25512;&#29702;&#21644;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for using agent experiences gathered through an embodied simulation to ground contextualized word vectors to object representations. We use similarity learning to make comparisons between different object types based on their properties when interacted with, and to extract common features pertaining to the objects' behavior. We then use an affine transformation to calculate a projection matrix that transforms contextualized word vectors from different transformer-based language models into this learned space, and evaluate whether new test instances of transformed token vectors identify the correct concept in the object embedding space. Our results expose properties of the embedding spaces of four different transformer models and show that grounding object token vectors is usually more helpful to grounding verb and attribute token vectors than the reverse, which reflects earlier conclusions in the analogical reasoning and psycholinguistic literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#31639;&#27861;&#65292;Gelato&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#23558;&#23646;&#24615;&#29305;&#24449;&#38598;&#25104;&#21040;&#25299;&#25169;&#29305;&#24449;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#31867;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.13656</link><description>&lt;p&gt;
&#19981;&#38656;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction without Graph Neural Networks. (arXiv:2305.13656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#36335;&#39044;&#27979;&#31639;&#27861;&#65292;Gelato&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#23558;&#23646;&#24615;&#29305;&#24449;&#38598;&#25104;&#21040;&#25299;&#25169;&#29305;&#24449;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#31867;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#35768;&#22810;&#22270;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23427;&#22522;&#20110;&#22270;&#29305;&#24449;&#26469;&#39044;&#27979;&#36793;&#12290;&#23545;&#20110;&#20960;&#20010;&#30456;&#20851;&#38382;&#39064;&#65292;&#22522;&#20110;&#23646;&#24615;&#20013;&#24515;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#38142;&#36335;&#39044;&#27979;&#30340;&#20027;&#23548;&#26694;&#26550;&#12290;GNN&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#26159;&#26159;&#20160;&#20040;&#35753;&#23427;&#20204;&#34920;&#29616;&#20248;&#31168;&#21602;&#65311;&#26159;&#21542;&#26377;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#23454;&#29616;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;GNN-based&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#22914;&#20309;&#22788;&#29702;&#38382;&#39064;&#26412;&#36136;&#19978;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#24615;&#65288;&#30001;&#20110;&#22270;&#30340;&#31232;&#30095;&#24615;&#23548;&#33268;&#65289;&#22312;&#20854;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gelato&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#25299;&#25169;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22270;&#23398;&#20064;&#23558;&#23646;&#24615;&#20449;&#24687;&#24212;&#29992;&#20110;&#22686;&#24378;&#30340;&#22270;&#20013;&#30340;&#25299;&#25169;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;N-pair&#25439;&#22833;&#22312;&#19981;&#20559;&#30340;&#35757;&#32451;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction, which consists of predicting edges based on graph features, is a fundamental task in many graph applications. As for several related problems, Graph Neural Networks (GNNs), which are based on an attribute-centric message-passing paradigm, have become the predominant framework for link prediction. GNNs have consistently outperformed traditional topology-based heuristics, but what contributes to their performance? Are there simpler approaches that achieve comparable or better results? To answer these questions, we first identify important limitations in how GNN-based link prediction methods handle the intrinsic class imbalance of the problem -- due to the graph sparsity -- in their training and evaluation. Moreover, we propose Gelato, a novel topology-centric framework that applies a topological heuristic to a graph enhanced by attribute information via graph learning. Our model is trained end-to-end with an N-pair loss on an unbiased training set to address class imbala
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#19978;&#30340;&#34920;&#29616;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.13651</link><description>&lt;p&gt;
&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#23545;&#25239;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Adversarial Defenses via Vector Quantization. (arXiv:2305.13651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#19978;&#30340;&#34920;&#29616;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#31163;&#25955;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21033;&#29992;&#30690;&#37327;&#37327;&#21270;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;pRD&#21644;swRD&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#22312;&#35777;&#26126;&#20934;&#30830;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#19988;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#24403;&#21069;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#31168;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#19968;&#31181;&#29256;&#26412;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#20998;&#31867;&#22120;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building upon Randomized Discretization, we develop two novel adversarial defenses against white-box PGD attacks, utilizing vector quantization in higher dimensional spaces. These methods, termed pRD and swRD, not only offer a theoretical guarantee in terms of certified accuracy, they are also shown, via abundant experiments, to perform comparably or even superior to the current art of adversarial defenses. These methods can be extended to a version that allows further training of the target classifier and demonstrates further improved performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.13650</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#22343;&#34913;&#25968;&#25454;&#30340;&#40065;&#26834;&#22522;&#20110;&#27169;&#22411;&#30340;&#35774;&#35745;&#30340;&#23646;&#24615;&#24341;&#23548;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#36890;&#36807;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;MBO&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#31283;&#20581;&#22320;&#23547;&#25214;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#25506;&#32034;&#20855;&#26377;&#26497;&#24230;&#31232;&#30095;&#30340;&#26377;&#24847;&#20041;&#21306;&#22495;&#30340;&#39640;&#32500;&#34507;&#30333;&#36136;&#24207;&#21015;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#20351;&#29992;&#30001;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;&#25628;&#32034;&#27169;&#22411;&#26469;&#36741;&#21161;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#19981;&#24179;&#34913;&#24615;&#20351;&#24471;&#29616;&#26377;&#30340;MBO&#26041;&#27861;&#24456;&#38590;&#25110;&#26681;&#26412;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#24341;&#23548;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;PGVAE&#65289;&#65292;&#20854;&#28508;&#22312;&#31354;&#38388;&#30001;&#23646;&#24615;&#20540;&#26126;&#30830;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#25353;&#29031;&#36825;&#20123;&#23646;&#24615;&#20540;&#20248;&#20808;&#32771;&#34385;&#26679;&#26412;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#21644;&#21322;&#21512;&#25104;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MBO&#19982;PGVAE&#31283;&#20581;&#22320;&#21457;&#29616;&#20855;&#26377;&#25913;&#36827;&#23646;&#24615;&#30340;&#24207;&#21015;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35774;&#35745;&#31354;&#38388;&#30340;&#26222;&#36866;&#24615;&#21450;&#20854;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25351;&#26631;&#8212;&#8212;Snow Drought Response Index&#65288;SnoDRI&#65289;&#65292;&#23427;&#22522;&#20110;&#21508;&#31181;&#31215;&#38634;&#30456;&#20851;&#21464;&#37327;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#27169;&#22411;&#20013;&#30340;&#20114;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#37327;&#21270;&#31215;&#38634;&#24178;&#26097;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#31215;&#38634;&#24178;&#26097;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
An Autoencoder-based Snow Drought Index. (arXiv:2305.13646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25351;&#26631;&#8212;&#8212;Snow Drought Response Index&#65288;SnoDRI&#65289;&#65292;&#23427;&#22522;&#20110;&#21508;&#31181;&#31215;&#38634;&#30456;&#20851;&#21464;&#37327;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#27169;&#22411;&#20013;&#30340;&#20114;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#37327;&#21270;&#31215;&#38634;&#24178;&#26097;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#30340;&#20960;&#20010;&#22320;&#21306;&#65292;&#31215;&#38634;&#23545;&#27700;&#25991;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#31215;&#38634;&#30340;&#34701;&#21270;&#20250;&#23548;&#33268;&#22320;&#38754;&#28183;&#27700;&#21450;&#24452;&#27969;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#30740;&#31350;&#31215;&#38634;&#34701;&#21270;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#35199;&#37096;&#22320;&#21306;&#31561;&#31215;&#38634;&#21344;&#20027;&#23548;&#30340;&#27969;&#22495;&#20013;&#65292;&#31215;&#38634;&#20648;&#37327;&#30340;&#20943;&#23569;&#24341;&#21457;&#30340;&#31215;&#38634;&#24178;&#26097;&#20250;&#20005;&#37325;&#24433;&#21709;&#27700;&#36164;&#28304;&#20379;&#24212;&#12290;&#22240;&#27492;&#65292;&#39640;&#25928;&#22320;&#26816;&#27979;&#31215;&#38634;&#24178;&#26097;&#30340;&#21457;&#29983;&#26102;&#38388;&#21644;&#20005;&#37325;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Snow Drought Response Index&#25110;SnoDRI&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#25351;&#26631;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#21644;&#37327;&#21270;&#31215;&#38634;&#24178;&#26097;&#20107;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#21508;&#31181;&#31215;&#38634;&#30456;&#20851;&#21464;&#37327;&#20013;&#35745;&#31639;&#20986;&#20102;&#25351;&#26631;&#12290;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#27169;&#22411;&#20013;&#30340;&#20114;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;SnoDRI&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#35780;&#20272;&#27599;&#20010;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;1981&#24180;&#33267;202&#24180;&#30340;&#20877;&#20998;&#26512;&#25968;&#25454;&#65288;NLDAS-2&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In several regions across the globe, snow has a significant impact on hydrology. The amounts of water that infiltrate the ground and flow as runoff are driven by the melting of snow. Therefore, it is crucial to study the magnitude and effect of snowmelt. Snow droughts, resulting from reduced snow storage, can drastically impact the water supplies in basins where snow predominates, such as in the western United States. Hence, it is important to detect the time and severity of snow droughts efficiently. We propose Snow Drought Response Index or SnoDRI, a novel indicator that could be used to identify and quantify snow drought occurrences. Our index is calculated using cutting-edge ML algorithms from various snow-related variables. The self-supervised learning of an autoencoder is combined with mutual information in the model. In this study, we use random forests for feature extraction for SnoDRI and assess the importance of each variable. We use reanalysis data (NLDAS-2) from 1981 to 202
&lt;/p&gt;</description></item><item><title>SMAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#65292;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13634</link><description>&lt;p&gt;
SMAP&#65306;&#19968;&#31181;&#38754;&#21521;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#30340;&#26032;&#22411;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
SMAP: A Novel Heterogeneous Information Framework for Scenario-based Optimal Model Assignment. (arXiv:2305.13634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13634
&lt;/p&gt;
&lt;p&gt;
SMAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#65292;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#24212;&#29992;&#26085;&#30410;&#25104;&#29087;&#65292;&#23548;&#33268;&#21516;&#19968;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#30456;&#21516;&#30446;&#26631;&#30340;&#27169;&#22411;&#25968;&#37327;&#19981;&#26029;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#27169;&#22411;&#29305;&#24449;&#21644;&#29305;&#23450;&#35201;&#27714;&#21644;&#32422;&#26463;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20559;&#37325;&#20110;&#22522;&#20110;&#20247;&#21253;&#30340;&#24037;&#20154;-&#20219;&#21153;&#20998;&#37197;&#65292;&#24573;&#30053;&#20102;&#22330;&#26223;-&#25968;&#25454;&#38598;-&#27169;&#22411;&#20998;&#37197;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#26368;&#20248;&#27169;&#22411;&#20998;&#37197;&#65288;SOMA&#65289;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22330;&#26223;&#21644;&#27169;&#22411;&#32852;&#21512;&#24863;&#30693;&#65288;SMAP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290; SMAP&#26159;&#19968;&#31181;&#24322;&#26500;&#20449;&#24687;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21508;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#20197;&#26234;&#33021;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#24182;&#20026;&#29305;&#23450;&#22330;&#26223;&#20998;&#37197;&#26368;&#20339;&#27169;&#22411;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#24471;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#23558;SMAP&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMAP&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing maturity of big data applications has led to a proliferation of models targeting the same objectives within the same scenarios and datasets. However, selecting the most suitable model that considers model's features while taking specific requirements and constraints into account still poses a significant challenge. Existing methods have focused on worker-task assignments based on crowdsourcing, they neglect the scenario-dataset-model assignment problem. To address this challenge, a new problem named the Scenario-based Optimal Model Assignment (SOMA) problem is introduced and a novel framework entitled Scenario and Model Associative percepts (SMAP) is developed. SMAP is a heterogeneous information framework that can integrate various types of information to intelligently select a suitable dataset and allocate the optimal model for a specific scenario. To comprehensively evaluate models, a new score function that utilizes multi-head attention mechanisms is proposed. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#28145;&#24230;RKHM&#65292;&#36890;&#36807;&#20351;&#29992;$C^*$&#20195;&#25968;&#33719;&#24471;&#26356;&#28201;&#21644;&#30340;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13588</link><description>&lt;p&gt;
&#36890;&#36807;RKHM&#21644;Perron-Frobenius&#31639;&#23376;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator. (arXiv:2305.13588v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#28145;&#24230;RKHM&#65292;&#36890;&#36807;&#20351;&#29992;$C^*$&#20195;&#25968;&#33719;&#24471;&#26356;&#28201;&#21644;&#30340;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;$C^*$-&#27169;(RKHM)&#36890;&#36807;$C^*$&#20195;&#25968;&#23545;&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#36827;&#34892;&#20102;&#27867;&#21270;&#65292;&#32780;Perron-Frobenius&#31639;&#23376;&#26159;&#19982;&#20989;&#25968;&#32452;&#21512;&#30456;&#20851;&#30340;&#32447;&#24615;&#31639;&#23376;&#12290;&#23558;&#36825;&#20004;&#20010;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;RKHM&#65292;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;Rademacher&#24191;&#20041;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;Perron-Frobenius&#31639;&#23376;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#30001;&#20110;$C^*$&#20195;&#25968;&#30340;&#20248;&#21183;&#65292;&#35813;&#30028;&#38480;&#23545;&#36755;&#20986;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#36739;&#29616;&#26377;&#30028;&#38480;&#26356;&#21152;&#28201;&#21644;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$C^*$&#20195;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#26680;&#24515;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#31639;&#23376;&#30340;&#20056;&#31215;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26126;&#30830;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#28145;&#24230;&#26680;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert $C^*$-module (RKHM) is a generalization of reproducing kernel Hilbert space (RKHS) by means of $C^*$-algebra, and the Perron-Frobenius operator is a linear operator related to the composition of functions. Combining these two concepts, we present deep RKHM, a deep learning framework for kernel methods. We derive a new Rademacher generalization bound in this setting and provide a theoretical interpretation of benign overfitting by means of Perron-Frobenius operators. By virtue of $C^*$-algebra, the dependency of the bound on output dimension is milder than existing bounds. We show that $C^*$-algebra is a suitable tool for deep learning with kernels, enabling us to take advantage of the product structure of operators and to provide a clear connection with convolutional neural networks. Our theoretical analysis provides a new lens through which one can design and analyze deep kernel methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30456;&#20851;&#32858;&#31867;&#30340;&#21333;&#36941;Pivot&#31639;&#27861;&#65292;&#20351;&#29992;O(n/{\epsilon})&#23383;&#30340;&#35760;&#24518;&#31354;&#38388;&#19979;&#65292;&#21487;&#20197;&#32473;&#20986;(3+{\epsilon})&#30340;&#36817;&#20284;&#20540;&#12290;&#36825;&#20010;&#31639;&#27861;&#23481;&#26131;&#23454;&#29616;&#65292;&#24182;&#19988;&#26159;&#31616;&#21333;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.13560</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30456;&#20851;&#32858;&#31867;&#30340;&#21333;&#36941;Pivot&#31639;&#27861;&#12290;&#20445;&#25345;&#31616;&#21333;&#65281;
&lt;/p&gt;
&lt;p&gt;
Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!. (arXiv:2305.13560v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30456;&#20851;&#32858;&#31867;&#30340;&#21333;&#36941;Pivot&#31639;&#27861;&#65292;&#20351;&#29992;O(n/{\epsilon})&#23383;&#30340;&#35760;&#24518;&#31354;&#38388;&#19979;&#65292;&#21487;&#20197;&#32473;&#20986;(3+{\epsilon})&#30340;&#36817;&#20284;&#20540;&#12290;&#36825;&#20010;&#31639;&#27861;&#23481;&#26131;&#23454;&#29616;&#65292;&#24182;&#19988;&#26159;&#31616;&#21333;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#36941;&#21322;&#27969;Pivot&#31639;&#27861;&#30340;&#30456;&#20851;&#32858;&#31867;&#65292;&#22312;&#20351;&#29992;O(n/{\epsilon})&#23383;&#30340;&#35760;&#24518;&#31354;&#38388;&#19979;&#65292;&#21487;&#20197;&#32473;&#20986;(3+{\epsilon})&#30340;&#36817;&#20284;&#20540;&#12290;&#36825;&#26159;&#23545;Cambus, Kuhn, Lindy, Pai&#21644;Uitto&#26368;&#36817;&#30340;&#32467;&#26524;&#30340;&#36731;&#24494;&#25913;&#36827;&#65292;&#20182;&#20204;&#20351;&#29992;O(n log n)&#20010;&#23383;&#30340;&#35760;&#24518;&#31354;&#38388;&#25552;&#20379;&#20102;(3+{\epsilon})&#30340;&#36924;&#36817;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#21482;&#20351;&#29992;O(n)&#23383;&#30340;&#35760;&#24518;&#31354;&#38388;&#30340;Behnezhad, Charikar, Ma&#21644;Tan&#32473;&#20986;&#20102;5-&#36817;&#20284;&#20540;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#20043;&#19968;&#26159;&#65292;&#31639;&#27861;&#21644;&#20854;&#20998;&#26512;&#37117;&#38750;&#24120;&#31616;&#21333;&#65292;&#24182;&#19988;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that a simple single-pass semi-streaming variant of the Pivot algorithm for Correlation Clustering gives a (3 + {\epsilon})-approximation using O(n/{\epsilon}) words of memory. This is a slight improvement over the recent results of Cambus, Kuhn, Lindy, Pai, and Uitto, who gave a (3 + {\epsilon})-approximation using O(n log n) words of memory, and Behnezhad, Charikar, Ma, and Tan, who gave a 5-approximation using O(n) words of memory. One of the main contributions of this paper is that both the algorithm and its analysis are very simple, and also the algorithm is easy to implement.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13552</link><description>&lt;p&gt;
&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65306;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;
&lt;/p&gt;
&lt;p&gt;
Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#24067;&#30340;&#28789;&#27963;&#27169;&#22411;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#65292;&#31216;&#20026;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65288;SNEFY&#65289;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#24182;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#31867;&#20284;&#20110;&#26080;&#31351;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#24191;&#27867;&#32852;&#31995;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#65292;SNEFY&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#22240;&#27492;&#26159;&#28789;&#27963;&#19988;&#23436;&#20840;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#12290;SNEFY&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#30340;&#25351;&#25968;&#26063;&#65292;&#23545;&#20110;&#26465;&#20214;&#25512;&#26029;&#20855;&#26377;&#38381;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23494;&#24230;&#20272;&#35745;&#21644;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13546</link><description>&lt;p&gt;
&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Functional Transformers. (arXiv:2305.13546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#30340;&#38544;&#24335;&#34920;&#31034;&#26041;&#24335;&#30340;&#25104;&#21151;&#65292;&#25512;&#21160;&#20102;&#23545;&#31070;&#32463;&#21151;&#33021;&#30340;&#22686;&#38271;&#20852;&#36259;&#65306;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25805;&#20316;&#20854;&#26435;&#37325;&#31354;&#38388;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36755;&#20837;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#26435;&#37325;&#31354;&#38388;&#23545;&#35937;&#30340;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#24615;&#30340;&#31070;&#32463;&#21151;&#33021;&#20307;&#31995;&#32467;&#26500;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23450;&#20041;&#19968;&#31181;&#26032;&#30340;&#32622;&#25442;&#31561;&#21464;&#30340;&#26435;&#37325;&#31354;&#38388;&#23618;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#28145;&#24230;&#31561;&#21464;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#36716;&#25442;&#22120;(NFTs)&#12290;NFTs&#23562;&#37325;&#26435;&#37325;&#31354;&#38388;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#27880;&#24847;&#21147;&#30340;&#20248;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#22788;&#29702;&#21069;&#39304;MLPs&#21644;CNNs&#30340;&#26435;&#37325;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;NFTs&#30340;&#24615;&#33021;&#19982;&#25110;&#20248;&#20110;&#20808;&#21069;&#30340;&#26435;&#37325;&#31354;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;NFTs&#24320;&#21457;&#20102;Inr2Array&#65292;&#19968;&#31181;&#35745;&#31639;&#32622;&#25442;&#19981;&#21464;&#28508;&#21464;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of neural networks as implicit representation of data has driven growing interest in neural functionals: models that can process other neural networks as input by operating directly over their weight spaces. Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called neural functional Transformers (NFTs). NFTs respect weight-space permutation symmetries while incorporating the advantages of attention, which have exhibited remarkable success across multiple domains. In experiments processing the weights of feedforward MLPs and CNNs, we find that NFTs match or exceed the performance of prior weight-space methods. We also leverage NFTs to develop Inr2Array, a novel method for computing permutation invariant laten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ConvBoost&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#23618;&#32467;&#26500;&#27169;&#22411;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#32531;&#35299;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13541</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;ConvBoost&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ConvBoost: Boosting ConvNets for Sensor-based Activity Recognition. (arXiv:2305.13541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ConvBoost&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#23618;&#32467;&#26500;&#27169;&#22411;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20256;&#24863;&#22120;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#32531;&#35299;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#26222;&#21450;&#21644;&#21487;&#31359;&#25140;&#35745;&#31639;&#30340;&#26680;&#24515;&#30740;&#31350;&#20027;&#39064;&#20043;&#19968;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#26512;&#26041;&#27861;&#30340;&#27969;&#34892;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#21644;&#36827;&#34892;&#20998;&#31867;&#12290;&#20294;&#30001;&#20110;&#20856;&#22411;HAR&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#29992;&#30340;&#26631;&#35760;&#26679;&#26412;&#25968;&#25454;&#25968;&#37327;&#24448;&#24448;&#38750;&#24120;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#36825;&#20351;&#24471;DL-Based HAR&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvBoost-&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#30340;HAR&#30340;&#26032;&#22411;&#12289;&#19977;&#23618;&#12289;&#32467;&#26500;&#21270;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#22686;&#24378;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;HAR&#65292;&#26088;&#22312;&#32531;&#35299;&#39046;&#22495;&#20869;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is one of the core research themes in ubiquitous and wearable computing. With the shift to deep learning (DL) based analysis approaches, it has become possible to extract high-level features and perform classification in an end-to-end manner. Despite their promising overall capabilities, DL-based HAR may suffer from overfitting due to the notoriously small, often inadequate, amounts of labeled sample data that are available for typical HAR applications. In response to such challenges, we propose ConvBoost -- a novel, three-layer, structured model architecture and boosting framework for convolutional network based HAR. Our framework generates additional training data from three different perspectives for improved HAR, aiming to alleviate the shortness of labeled training data in the field. Specifically, with the introduction of three conceptual layers--Sampling Layer, Data Augmentation Layer, and Resilient Layer -- we develop three "boosters" -R-Frame,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13536</link><description>&lt;p&gt;
&#29992;&#20302;&#32500;&#21442;&#25968;&#23376;&#31354;&#38388;&#34920;&#31034;&#36755;&#20837;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Representing Input Transformations by Low-Dimensional Parameter Subspaces. (arXiv:2305.13536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#33258;&#23450;&#20041;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#20302;&#32500;&#32467;&#26500;&#21487;&#20197;&#22312;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#23545;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#21464;&#25442;&#65288;&#22914;&#26059;&#36716;&#12289;&#32553;&#25918;&#21644;&#24179;&#31227;&#65289;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#38500;&#38750;&#23427;&#20204;&#20855;&#26377;&#29305;&#23450;&#30340;&#19981;&#21464;&#32467;&#26500;&#25110;&#32463;&#36807;&#29305;&#23450;&#30340;&#35757;&#32451;&#21518;&#65288;&#20363;&#22914;&#20174;&#25968;&#25454;&#22686;&#24378;&#20013;&#23398;&#20064;&#25152;&#38656;&#30340;&#40065;&#26834;&#24615;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#37197;&#32622;&#23376;&#31354;&#38388;&#20551;&#35774;&#65292;&#21363;&#20026;&#36830;&#32493;&#21442;&#25968;&#21270;&#21464;&#25442;&#26368;&#20248;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#23376;&#31354;&#38388;&#21487;&#37197;&#32622;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#23376;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#23427;&#20204;&#22312;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#30340;&#25152;&#26377;&#27979;&#35797;&#21464;&#25442;&#12289;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#20013;&#30340;&#32467;&#26500;&#21644;&#24778;&#20154;&#30340;&#20302;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#36328;&#19981;&#21516;&#39046;&#22495;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#36716;&#31227;&#36755;&#20837;&#21464;&#25442;&#30693;&#35782;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#26356;&#20581;&#22766;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models lack robustness to simple input transformations such as rotation, scaling, and translation, unless they feature a particular invariant architecture or undergo specific training, e.g., learning the desired robustness from data augmentations. Alternatively, input transformations can be treated as a domain shift problem, and solved by post-deployment model adaptation. Although a large number of methods deal with transformed inputs, the fundamental relation between input transformations and optimal model weights is unknown. In this paper, we put forward the configuration subspace hypothesis that model weights optimal for parameterized continuous transformations can reside in low-dimensional linear subspaces. We introduce subspace-configurable networks to learn these subspaces and observe their structure and surprisingly low dimensionality on all tested transformations, datasets and architectures from computer vision and audio signal processing domains. Our findings enable effic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#26469;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13535</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#29983;&#25104;&#25104;&#23545;&#21453;&#20107;&#23454;&#25968;&#25454;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Classifier Robustness through Active Generation of Pairwise Counterfactuals. (arXiv:2305.13535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#26469;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#24182;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;CDA&#65289;&#26159;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#24182;&#26377;&#25928;&#22320;&#26631;&#35760;&#23427;&#20204;&#26159;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65292;&#38656;&#35201;&#23613;&#21487;&#33021;&#38477;&#20302;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#35268;&#27169;&#65292;&#35201;&#20040;&#38544;&#21547;&#22320;&#20551;&#35774;&#26631;&#31614;&#19981;&#21464;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#20174;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#20027;&#21160;&#25277;&#26679;&#29983;&#25104;&#22823;&#37327;&#19981;&#21516;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#23398;&#20064;&#30340;&#25104;&#23545;&#20998;&#31867;&#22120;&#33258;&#21160;&#26631;&#35760;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25104;&#23545;&#20998;&#31867;&#22120;&#26469;&#25554;&#20540;&#21407;&#22987;&#26679;&#20363;&#21644;&#21453;&#20107;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#26631;&#35760;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classifiers. However, one fundamental challenge is how to discover meaningful counterfactuals and efficiently label them, with minimal human labeling cost. Most existing methods either completely rely on human-annotated labels, an expensive process which limits the scale of counterfactual data, or implicitly assume label invariance, which may mislead the model with incorrect labels. In this paper, we present a novel framework that utilizes counterfactual generative models to generate a large number of diverse counterfactuals by actively sampling from regions of uncertainty, and then automatically label them with a learned pairwise classifier. Our key insight is that we can more correctly label the generated counterfactuals by training a pairwise classifier that interpolates the relationship between the original example and the counterfactual. We demonstrate that with a small
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>Tied-Augment&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24456;&#22810;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#12290;</title><link>http://arxiv.org/abs/2305.13520</link><description>&lt;p&gt;
Tied-Augment&#65306;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Tied-Augment: Controlling Representation Similarity Improves Data Augmentation. (arXiv:2305.13520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13520
&lt;/p&gt;
&lt;p&gt;
Tied-Augment&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#34920;&#31034;&#30456;&#20284;&#24615;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24456;&#22810;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#25104;&#20026;&#21322;&#30417;&#30563;&#12289;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#35757;&#32451;&#20013;&#26368;&#20808;&#36827;&#27169;&#22411;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tied-Augment&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#19968;&#20010;&#31616;&#21333;&#30340;&#39033;&#28155;&#21152;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#25511;&#21046;&#25197;&#26354;&#19979;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#25552;&#39640;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#12290;Tied-Augment&#21487;&#20197;&#25913;&#21892;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#65292;&#20248;&#21270;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;&#20363;&#22914;RandAugment&#65292;mixup&#21644;SAM&#65289;&#12290;&#20363;&#22914;&#65292;Tied-RandAugment&#21487;&#20197;&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods have played an important role in the recent advance of deep learning models, and have become an indispensable component of state-of-the-art models in semi-supervised, self-supervised, and supervised training for vision. Despite incurring no additional latency at test time, data augmentation often requires more epochs of training to be effective. For example, even the simple flips-and-crops augmentation requires training for more than 5 epochs to improve performance, whereas RandAugment requires more than 90 epochs. We propose a general framework called Tied-Augment, which improves the efficacy of data augmentation in a wide range of applications by adding a simple term to the loss that can control the similarity of representations under distortions. Tied-Augment can improve state-of-the-art methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM), and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30005;&#24359;&#28809;&#29076;&#28195;&#30005;&#23548;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#26631;&#20934;&#20559;&#24046;&#35745;&#31639;&#21450;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.13519</link><description>&lt;p&gt;
&#21457;&#23637;&#29992;&#20110;&#39044;&#27979;&#30789;&#37240;&#30416;&#30005;&#23548;&#29575;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Development of Non-Linear Equations for Predicting Electrical Conductivity in Silicates. (arXiv:2305.13519v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30005;&#24359;&#28809;&#29076;&#28195;&#30005;&#23548;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#26631;&#20934;&#20559;&#24046;&#35745;&#31639;&#21450;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23548;&#29575;&#22312;&#30005;&#24359;&#28809;(EAF)&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#23427;&#19982;&#29076;&#28195;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#33021;&#37327;&#25439;&#22833;&#21644;&#20302;&#25928;&#29575;&#12290;&#25968;&#23398;&#24314;&#27169;&#26377;&#21161;&#20110;&#29702;&#35299;&#36825;&#31181;&#29616;&#35937;&#30340;&#34892;&#20026;&#65292;&#30740;&#31350;&#32773;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;EAF&#29076;&#28195;&#30340;&#30005;&#23548;&#29575;&#12290;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#23618;&#26377;100&#20010;&#31070;&#32463;&#20803;&#65292;&#20351;&#29992;6&#20010;&#39044;&#27979;&#21464;&#37327;&#21644;&#19968;&#20010;&#39044;&#27979;&#21464;&#37327;&#30005;&#23548;&#29575;&#12290;&#35745;&#31639;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#26631;&#20934;&#20559;&#24046;&#65292;&#24182;&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23545;&#27599;&#20010;&#39044;&#27979;&#21464;&#37327;&#30340;&#24433;&#21709;&#19982;&#39044;&#27979;&#21464;&#37327;&#36827;&#34892;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical conductivity is of fundamental importance in electric arc furnaces (EAF) and the interaction of this phenomenon with the process slag results in energy losses and low optimization. As mathematical modeling helps in understanding the behavior of phenomena and it was used to predict the electrical conductivity of EAF slags through artificial neural networks. The best artificial neural network had 100 neurons in the hidden layer, with 6 predictor variables and the predicted variable, electrical conductivity. Mean absolute error and standard deviation of absolute error were calculated, and sensitivity analysis was performed to correlate the effect of each predictor variable with the predicted variable.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13514</link><description>&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#37325;&#20889;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#24040;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#20307;&#31215;&#21644;&#36890;&#36807;API&#30340;&#21463;&#38480;&#35775;&#38382;&#20351;&#24471;&#38024;&#23545;&#20219;&#21153;&#30340;&#24494;&#35843;&#19981;&#20999;&#23454;&#38469;&#12290;&#32780;&#19988;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#28436;&#31034;&#30340;&#36873;&#25321;&#21644;&#39034;&#24207;&#65289;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20854;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;LLM&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#26679;&#26412;&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#20505;&#36873;&#27744;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;LM-corrector&#65288;LMCor&#65289;&#26469;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;LMCor&#34987;&#35757;&#32451;&#29992;&#20110;&#23545;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#21517;&#12289;&#32452;&#21512;&#21644;&#37325;&#20889;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#30446;&#26631;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#23567;&#30340;LMCor&#27169;&#22411;&#65288;250M&#65289;&#65292;&#20063;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;LLMs&#65288;62B&#65289;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;LMCor&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25913;&#36827;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#25913;&#21892;LLMs&#23454;&#38469;&#21487;&#29992;&#24615;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13509</link><description>&lt;p&gt;
ColMix -- &#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
ColMix -- A Simple Data Augmentation Framework to Improve Object Detector Performance and Robustness in Aerial Images. (arXiv:2305.13509v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#25552;&#39640;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#35777;&#26126;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#26816;&#27979;&#25991;&#29486;&#26159;&#22312;MS COCO&#31561;&#25968;&#25454;&#38598;&#19978;&#21457;&#23637;&#36825;&#31181;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#24050;&#34987;&#35777;&#26126;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#20063;&#24456;&#26377;&#25928;&#12290;&#32780;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#22914;&#26631;&#27880;&#23545;&#35937;&#25968;&#37327;&#23569;&#21644;&#20302;&#30446;&#26631;&#23494;&#24230;&#65292;&#38459;&#30861;&#20102;&#24635;&#20307;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#65292;&#29992;&#20110;&#22686;&#21152;&#30446;&#26631;&#23494;&#24230;&#65292;&#26080;&#38656;&#20998;&#21106;&#25513;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#31867;&#20284;&#30340;&#26041;&#27861;&#65288;&#22914;&#39532;&#36187;&#20811;&#22686;&#24378;&#65289;&#30456;&#27604;&#65292;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#65292;&#24182;&#23454;&#29616;&#26356;&#22823;&#30340;&#30446;&#26631;&#23494;&#24230;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#25340;&#36148;&#25340;&#36148;&#65288;collage pasting&#65289;&#23481;&#26131;&#21463;&#21040;&#26576;&#20123;&#20998;&#24067;&#22806;&#31227;&#20301;&#65288;&#22914;&#22270;&#20687;&#25439;&#22351;&#65289;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In the last decade, Convolutional Neural Network (CNN) and transformer based object detectors have achieved high performance on a large variety of datasets. Though the majority of detection literature has developed this capability on datasets such as MS COCO, these detectors have still proven effective for remote sensing applications. Challenges in this particular domain, such as small numbers of annotated objects and low object density, hinder overall performance. In this work, we present a novel augmentation method, called collage pasting, for increasing the object density without a need for segmentation masks, thereby improving the detector performance. We demonstrate that collage pasting improves precision and recall beyond related methods, such as mosaic augmentation, and enables greater control of object density. However, we find that collage pasting is vulnerable to certain out-of-distribution shifts, such as image corruptions. To address this, we introduce two simple approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;DeepBern-Nets&#65292;&#20351;&#29992;Bernstein&#22810;&#39033;&#24335;&#20195;&#26367;ReLU&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#19981;&#23436;&#25972;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#33021;&#20135;&#29983;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#21487;&#29992;&#20110;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#12289;&#20844;&#27491;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13508</link><description>&lt;p&gt;
DeepBern-Nets: &#20351;&#29992;Bernstein&#22810;&#39033;&#24335;&#28608;&#27963;&#21644;&#31934;&#20934;&#36793;&#30028;&#20256;&#25773;&#39535;&#21270;&#31070;&#32463;&#32593;&#32476;&#35748;&#35777;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
DeepBern-Nets: Taming the Complexity of Certifying Neural Networks using Bernstein Polynomial Activations and Precise Bound Propagation. (arXiv:2305.13508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;DeepBern-Nets&#65292;&#20351;&#29992;Bernstein&#22810;&#39033;&#24335;&#20195;&#26367;ReLU&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#19981;&#23436;&#25972;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#33021;&#20135;&#29983;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#21487;&#29992;&#20110;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#12289;&#20844;&#27491;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#27491;&#24335;&#35748;&#35777;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#23433;&#20840;&#12289;&#20844;&#27491;&#21644;&#40065;&#26834;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;ReLU&#30340;NN&#30340;&#23436;&#25972;&#35748;&#35777;&#31639;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;NN&#12290;&#32780;&#22522;&#20110;&#19981;&#23436;&#25972;&#35748;&#35777;&#31639;&#27861;&#26131;&#20110;&#35745;&#31639;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#30340;&#30028;&#38480;&#20250;&#38543;&#30528;NN&#30340;&#28145;&#24230;&#32780;&#21464;&#24471;&#23485;&#26494;&#65292;&#36825;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;DeepBern-Nets&#8221;&#65292;&#36825;&#26159;&#19968;&#31867;&#20351;&#29992;Bernstein&#22810;&#39033;&#24335;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#32780;&#38750;&#24120;&#29992;&#30340;ReLU&#30340;NN&#12290;Bernstein&#22810;&#39033;&#24335;&#26159;&#20809;&#28369;&#19988;&#21487;&#24494;&#30340;&#20989;&#25968;&#65292;&#20855;&#26377;&#31216;&#20026;"&#21306;&#38388;&#21253;&#22260;"&#21644;"&#32454;&#20998;"&#23646;&#24615;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#8220;Bern-IB&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formal certification of Neural Networks (NNs) is crucial for ensuring their safety, fairness, and robustness. Unfortunately, on the one hand, sound and complete certification algorithms of ReLU-based NNs do not scale to large-scale NNs. On the other hand, incomplete certification algorithms are easier to compute, but they result in loose bounds that deteriorate with the depth of NN, which diminishes their effectiveness. In this paper, we ask the following question; can we replace the ReLU activation function with one that opens the door to incomplete certification algorithms that are easy to compute but can produce tight bounds on the NN's outputs? We introduce DeepBern-Nets, a class of NNs with activation functions based on Bernstein polynomials instead of the commonly used ReLU activation. Bernstein polynomials are smooth and differentiable functions with desirable properties such as the so-called range enclosure and subdivision properties. We design a novel algorithm, called Bern-IB
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#24212;&#29992;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36755;&#20837;&#24773;&#20917;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24050;&#25506;&#32034;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.13504</link><description>&lt;p&gt;
&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation for Code Generation. (arXiv:2305.13504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27010;&#36848;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#24212;&#29992;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36755;&#20837;&#24773;&#20917;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24050;&#25506;&#32034;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#21457;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#33258;&#21160;&#32763;&#35793;&#33258;&#28982;&#35821;&#35328;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;NMT&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#21040;&#31243;&#24207;&#20195;&#30721;&#30340;&#29983;&#25104;&#20013;&#12290;&#22312;NMT&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;&#20219;&#21153;&#26159;&#29983;&#25104;&#28385;&#36275;&#36755;&#20837;&#20013;&#34920;&#36798;&#30340;&#32422;&#26463;&#26465;&#20214;&#30340;&#36755;&#20986;&#28304;&#20195;&#30721;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#36755;&#20837;&#24773;&#20917;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#36739;&#20302;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#22914;&#20108;&#36827;&#21046;&#25110;&#27719;&#32534;&#65288;&#31070;&#32463;&#21453;&#27719;&#32534;&#65289;&#65292;&#28304;&#20195;&#30721;&#30340;&#37096;&#20998;&#34920;&#31034;&#65288;&#20195;&#30721;&#23436;&#25104;&#21644;&#20462;&#22797;&#65289;&#65292;&#20197;&#21450;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#65288;&#20195;&#30721;&#32763;&#35793;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NMT&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#25991;&#29486;&#36827;&#34892;&#27010;&#36848;&#65292;&#25353;&#29031;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#65292;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#29992;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#23545;&#24050;&#25506;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;NMT-based&#26041;&#27861;&#29983;&#25104;&#20195;&#30721;&#30340;&#29616;&#26377;&#38480;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13503</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#22810;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#65306;&#29702;&#35770;&#12289;&#24314;&#27169;&#19982;&#20248;&#21270;(arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#30446;&#21069;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#20851;&#27880;&#21333;&#19968;&#20219;&#21153;/&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#21516;&#27493;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#23427;&#32771;&#34385;&#21033;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#19968;&#26063;&#35843;&#24230;&#24352;&#37327;&#26469;&#25429;&#25417;&#35774;&#22791;&#30340;&#35843;&#24230;&#65292;&#24182;&#23545;MA-FL&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#27425;&#25968;&#65289;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#65288;&#21363;&#39044;&#28909;&#19982;&#20919;&#21551;&#21160;&#21021;&#22987;&#21270;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;MA-FL&#21046;&#23450;&#20102;&#19968;&#20010;&#38750;&#20984;&#28151;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#20849;&#21516;&#37197;&#32622;&#36164;&#28304;&#20998;&#37197;&#21644;&#35774;&#22791;&#35843;&#24230;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13498</link><description>&lt;p&gt;
&#29992;&#20110;&#24102;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise. (arXiv:2305.13498v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#23545;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20056;&#24615;&#22122;&#22768;&#21644;&#28909;&#22122;&#22768;&#23545;&#20449;&#21495;&#20998;&#31163;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#21306;&#20998;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12289;&#25913;&#21892;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#30340;&#31639;&#27861;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20056;&#24615;&#21644;&#28909;&#22122;&#22768;&#23545;&#23454;&#38469;&#20449;&#21495;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#20998;&#31163;&#28909;&#22122;&#22768;&#30340;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;Hamilton Monte Carlo (HMC)&#30456;&#23218;&#32654;&#65292;&#20294;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#35777;&#26126;&#20102;HMC&#26080;&#27861;&#38548;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#39069;&#22806;&#20102;&#35299;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#20043;&#38388;&#27604;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#20272;&#35745;&#21442;&#25968;&#21644;&#20998;&#31163;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to investigate the impact of noise on parameter fitting for an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and thermal noise on the accuracy of signal separation. To address these issues, we propose algorithms and methods that can effectively distinguish between thermal and multiplicative noise and improve the precision of parameter estimation for optimal data analysis. Specifically, we explore the impact of both multiplicative and thermal noise on the obfuscation of the actual signal and propose methods to resolve them. Firstly, we present an algorithm that can effectively separate thermal noise with comparable performance to Hamilton Monte Carlo (HMC) but with significantly improved speed. Subsequently, we analyze multiplicative noise and demonstrate that HMC is insufficient for isolating thermal and multiplicative noise. However, we show that, with additional knowledge of the ratio between thermal and multiplicative noise, we can accuratel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#21644;&#27169;&#25311;&#24314;&#27169;&#65292;&#24378;&#35843;&#20102;&#23558;&#25968;&#25454;&#21644;&#21307;&#30103;&#20445;&#20581;&#20107;&#21153;&#30340;&#35752;&#35770;&#32622;&#20110;&#20154;&#20204;&#20197;&#21450;&#20182;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#31185;&#23398;&#26041;&#38754;&#30340;&#32463;&#39564;&#21644;&#35748;&#35782;&#65292;&#24182;&#35748;&#35782;&#21040;&#31639;&#27861;&#36816;&#34892;&#25152;&#22788;&#30340;&#31038;&#20250;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#25152;&#23548;&#33268;&#30340;&#31181;&#26063;&#27495;&#35270;&#21644;&#20581;&#24247;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13485</link><description>&lt;p&gt;
&#25512;&#36827;&#31038;&#21306;&#21442;&#19982;&#26041;&#27861;&#20197;&#35782;&#21035;&#20581;&#24247;&#35786;&#26029;&#31639;&#27861;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;&#30340;&#32467;&#26500;&#39537;&#21160;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Advancing Community Engaged Approaches to Identifying Structural Drivers of Racial Bias in Health Diagnostic Algorithms. (arXiv:2305.13485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#21644;&#27169;&#25311;&#24314;&#27169;&#65292;&#24378;&#35843;&#20102;&#23558;&#25968;&#25454;&#21644;&#21307;&#30103;&#20445;&#20581;&#20107;&#21153;&#30340;&#35752;&#35770;&#32622;&#20110;&#20154;&#20204;&#20197;&#21450;&#20182;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#31185;&#23398;&#26041;&#38754;&#30340;&#32463;&#39564;&#21644;&#35748;&#35782;&#65292;&#24182;&#35748;&#35782;&#21040;&#31639;&#27861;&#36816;&#34892;&#25152;&#22788;&#30340;&#31038;&#20250;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#25152;&#23548;&#33268;&#30340;&#31181;&#26063;&#27495;&#35270;&#21644;&#20581;&#24247;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20351;&#29992;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#21644;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#25345;&#32493;&#31181;&#26063;&#27495;&#35270;&#21644;&#20581;&#24247;&#24046;&#36317;&#26041;&#38754;&#12290;&#22312;2019&#24180;1&#26376;&#20030;&#21150;&#30340;Data for Black Lives II&#20250;&#35758;&#19978;&#30340;&#21021;&#22987;&#31995;&#32479;&#21160;&#21147;&#23398;&#30740;&#35752;&#20250;&#21518;&#65292;&#19968;&#32676;&#23545;&#20351;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#26469;&#29702;&#35299;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#20250;&#35758;&#21442;&#19982;&#32773;&#27599;&#26376;&#32858;&#38598;&#19968;&#27425;&#65292;&#25506;&#35752;&#19982;AI&#30340;&#31181;&#26063;&#20559;&#35265;&#20197;&#21450;&#23545;&#20581;&#24247;&#24046;&#36317;&#30340;&#24433;&#21709;&#26377;&#20851;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#27169;&#25311;&#24314;&#27169;&#26469;&#23637;&#31034;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#32467;&#26524;&#21644;&#35265;&#35299;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#22260;&#32469;&#25968;&#25454;&#21644;&#21307;&#30103;&#20445;&#20581;&#30340;&#35752;&#35770;&#22260;&#32469;&#20154;&#20204;&#21450;&#20854;&#19982;&#21307;&#30103;&#20445;&#20581;&#21644;&#31185;&#23398;&#30340;&#32463;&#39564;&#20197;&#21450;&#35782;&#21035;&#31639;&#27861;&#36816;&#34892;&#30340;&#31038;&#20250;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36127;&#38754;&#32463;&#39564;&#21644;&#31038;&#21306;&#21019;&#20260;&#30340;&#38598;&#20307;&#35760;&#24518;&#65292;&#23548;&#33268;&#27515;&#20129;&#24402;&#22240;&#20110;&#21307;&#30103;&#20445;&#20581;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much attention and concern has been raised recently about bias and the use of machine learning algorithms in healthcare, especially as it relates to perpetuating racial discrimination and health disparities. Following an initial system dynamics workshop at the Data for Black Lives II conference hosted at MIT in January of 2019, a group of conference participants interested in building capabilities to use system dynamics to understand complex societal issues convened monthly to explore issues related to racial bias in AI and implications for health disparities through qualitative and simulation modeling. In this paper we present results and insights from the modeling process and highlight the importance of centering the discussion of data and healthcare on people and their experiences with healthcare and science, and recognizing the societal context where the algorithm is operating. Collective memory of community trauma, through deaths attributed to poor healthcare, and negative experie
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#39537;&#20351;&#27169;&#22411;&#20248;&#21270;&#21152;&#26435;&#20998;&#31867;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#12289;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#20540;&#21152;&#26435;&#25216;&#33021;&#24471;&#20998;&#31561;&#24050;&#30830;&#31435;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13472</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#24615;&#33021;&#20248;&#21270;&#22522;&#20110;&#21152;&#26435;&#24230;&#37327;&#30340;&#32508;&#21512;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A comprehensive theoretical framework for the optimization of neural networks classification performance with respect to weighted metrics. (arXiv:2305.13472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#39537;&#20351;&#27169;&#22411;&#20248;&#21270;&#21152;&#26435;&#20998;&#31867;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#12289;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#20540;&#21152;&#26435;&#25216;&#33021;&#24471;&#20998;&#31561;&#24050;&#30830;&#31435;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#38656;&#35201;&#35774;&#35745;&#23450;&#21046;&#21270;&#21644;&#21152;&#26435;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#20013;&#26368;&#22823;&#21270;&#36825;&#20123;&#35780;&#20998;&#19982;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#20102;&#21152;&#26435;&#20998;&#31867;&#24230;&#37327;&#65292;&#24182;&#20801;&#35768;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#20197;&#39537;&#20351;&#27169;&#22411;&#20248;&#21270;&#36825;&#20123;&#26377;&#36259;&#30340;&#25351;&#26631;&#12290;&#32463;&#36807;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#32463;&#20856;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#12289;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#20540;&#21152;&#26435;&#25216;&#33021;&#24471;&#20998;&#31561;&#24050;&#30830;&#31435;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many contexts, customized and weighted classification scores are designed in order to evaluate the goodness of the predictions carried out by neural networks. However, there exists a discrepancy between the maximization of such scores and the minimization of the loss function in the training phase. In this paper, we provide a complete theoretical setting that formalizes weighted classification metrics and then allows the construction of losses that drive the model to optimize these metrics of interest. After a detailed theoretical analysis, we show that our framework includes as particular instances well-established approaches such as classical cost-sensitive learning, weighted cross entropy loss functions and value-weighted skill scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.13471</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#20998;&#25968;&#25454;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence in Learning Two-Layer Neural Networks with Separable Data. (arXiv:2305.13471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#21487;&#20998;&#25968;&#25454;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#19978;&#65292;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#22312;&#21152;&#36895;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65288;&#21253;&#25324;&#25351;&#25968;&#21644;&#36923;&#36753;&#25439;&#22833;&#65289;&#25910;&#25947;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24402;&#19968;&#21270; GD &#23545;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20316;&#29992;&#36229;&#36234;&#20102;&#32447;&#24615;&#27169;&#22411;&#12290;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270; GD &#23548;&#33268;&#35757;&#32451;&#25439;&#22833;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23637;&#31034;&#19968;&#23450;&#30340;&#26799;&#24230;&#33258;&#38480;&#21046;&#26465;&#20214;&#21644;&#23545;&#25968;&#21033;&#26222;&#24076;&#33576;&#29305;&#24615;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#29992;&#20110;&#20984;&#30446;&#26631;&#30340;&#24402;&#19968;&#21270; GD &#30340;&#27867;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#35777;&#26126;&#20102;&#35757;&#32451;&#26399;&#38388;&#24402;&#19968;&#21270; GD &#19981;&#20250;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13453</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#25512;&#24191;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23460;&#20869;&#23450;&#20301;&#22240;&#20854;&#22312;&#26234;&#33021;&#23478;&#23621;&#12289;&#24037;&#19994;&#33258;&#21160;&#21270;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20381;&#36182;&#20854;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#26080;&#32447;&#21442;&#25968;&#65288;&#22914;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21644;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#65289;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#26080;&#32447;&#35774;&#22791;&#30340;&#20301;&#32622;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36731;&#26494;&#37096;&#32626;&#21040;&#26032;&#29615;&#22659;&#25110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23450;&#20301;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#22810;&#20803;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.13447</link><description>&lt;p&gt;
&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;&#20197;&#21860;&#37202;&#33457;&#20998;&#31867;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularization Through Simultaneous Learning: A Case Study for Hop Classification. (arXiv:2305.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#23398;&#20064;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#20173;&#28982;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#65292;&#23548;&#33268;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#37319;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26159;&#25269;&#21046;&#36825;&#19968;&#25361;&#25112;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65306;Simultaneous Learning&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#29702;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#21860;&#37202;&#29983;&#20135;&#20013;&#30340;&#21860;&#37202;&#33457;&#21697;&#31181;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#19982;&#30446;&#26631;&#25968;&#25454;&#38598;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#33719;&#21462;&#39640;&#24230;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#26368;&#32456;&#23618;&#36827;&#34892;&#25112;&#30053;&#24615;&#20462;&#25913;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20998;&#31867;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#35270;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#32452;&#38388;&#24809;&#32602;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;InceptionV3&#21644;ResNet50&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#25351;&#23450;&#20102;UFOP-HVD&#21860;&#37202;&#33457;&#21494;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting remains a prevalent challenge in deep neural networks, leading to suboptimal real-world performance. Employing regularization techniques is a common strategy to counter this challenge, improving model generalization. This paper proposes Simultaneous Learning, a novel regularization approach drawing on Transfer Learning and Multi-task Learning principles, applied specifically to the classification of hop varieties - an integral component of beer production. Our approach harnesses the power of auxiliary datasets in synergy with the target dataset to amplify the acquisition of highly relevant features. Through a strategic modification of the model's final layer, we enable the simultaneous classification of both datasets without the necessity to treat them as disparate tasks. To realize this, we formulate a loss function that includes an inter-group penalty. We conducted experimental evaluations using the InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#30149;&#24577;&#25968;&#25454;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20302;&#30340;&#20013;&#20301;&#25968;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915; Bun &#31561;&#20154;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13440</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#30149;&#24577;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#20013;&#20301;&#25968;&#21644;&#20869;&#37096;&#28857;&#26500;&#36896;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Medians and Interior Points for Non-Pathological Data. (arXiv:2305.13440v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#30149;&#24577;&#25968;&#25454;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20302;&#30340;&#20013;&#20301;&#25968;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915; Bun &#31561;&#20154;&#30340;&#36127;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#36896;&#20102;&#20855;&#26377;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20272;&#35745;&#28385;&#36275;&#38750;&#24120;&#28201;&#21644;&#30340;&#30697;&#26465;&#20214;&#30340;&#20219;&#24847;&#20998;&#24067;&#22312; $\mathbb{R}$ &#19978;&#30340;&#20013;&#20301;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982; Bun&#31561;&#20154;(FOCS 2015) &#30340;&#24778;&#20154;&#36127;&#38754;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#35813;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#20855;&#26377;&#20219;&#20309;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#36820;&#22238;&#20219;&#20309;&#23545;&#20219;&#24847;&#20998;&#24067;&#30340;&#20013;&#20301;&#25968;&#30340;&#38750;&#24179;&#20961;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We construct differentially private estimators with low sample complexity that estimate the median of an arbitrary distribution over $\mathbb{R}$ satisfying very mild moment conditions. Our result stands in contrast to the surprising negative result of Bun et al. (FOCS 2015) that showed there is no differentially private estimator with any finite sample complexity that returns any non-trivial approximation to the median of an arbitrary distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13426</link><description>&lt;p&gt;
&#23545;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24517;&#39035;&#38754;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20197;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26681;&#25454;&#22312;&#25972;&#20010;&#30740;&#31350;&#26102;&#38388;&#27573;&#38543;&#26426;&#25277;&#21462;&#30340;&#24739;&#32773;&#26469;&#25286;&#20998;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#21463;&#21040;&#21453;&#21521;&#27979;&#35797;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;EMDOT&#27169;&#25311;&#23454;&#36341;&#32773;&#21487;&#33021;&#33021;&#22815;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#25191;&#34892;&#30340;&#28508;&#22312;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#22312;&#25152;&#26377;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#25968;&#25454;&#28304;&#65288;&#34920;&#26684;&#21644;&#25104;&#20687;&#65289;&#19978;&#35780;&#20272;&#32447;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25152;&#26377;&#21382;&#21490;&#25968;&#25454;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#29702;&#24819;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#30340;&#31383;&#21475;&#21487;&#33021;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#27169;&#22411;&#31361;&#28982;&#21463;&#21040;&#24433;&#21709;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#22312;&#30456;&#23545;&#36739;&#36817;&#30340;&#25968;&#25454;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models deployed in healthcare systems must face data drawn from continually evolving environments. However, researchers proposing such models typically evaluate them in a time-agnostic manner, splitting datasets according to patients sampled randomly throughout the entire study time period. This work proposes the Evaluation on Medical Datasets Over Time (EMDOT) framework, which evaluates the performance of a model class across time. Inspired by the concept of backtesting, EMDOT simulates possible training procedures that practitioners might have been able to execute at each point in time and evaluates the resulting models on all future time points. Evaluating both linear and more complex models on six distinct medical data sources (tabular and imaging), we show how depending on the dataset, using all historical data may be ideal in many cases, whereas using a window of the most recent data could be advantageous in others. In datasets where models suffer from sudde
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13409</link><description>&lt;p&gt;
&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36890;&#36807;&#20811;&#21033;&#31119;&#24503;&#38376;&#21644;$O(\log(n))$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26368;&#22810;&#20351;&#29992;$t$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;$n$&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;$|\psi\rangle$&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#29992;$\mathsf{poly}(n,2^t,1/\epsilon)$&#26102;&#38388;&#21644;$|\psi\rangle$&#30340;&#22797;&#21046;&#26469;&#23398;&#20064;$|\psi\rangle$&#65292;&#20351;&#20854;&#36319;&#30495;&#23454;&#29366;&#24577;&#30340;&#36317;&#31163;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#35813;&#32467;&#26524;&#26159;&#19968;&#20010;&#31283;&#23450;&#32500;&#25968;&#36739;&#22823;&#30340;&#29366;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#29305;&#20363;&#65292;&#24403;&#19968;&#20010;&#37327;&#23376;&#29366;&#24577;&#30340;&#31283;&#23450;&#23376;&#32500;&#25968;&#20026;$k$&#65292;&#34920;&#31034;&#23427;&#34987;&#19968;&#20010;&#30001;$2^k$&#20010;Pauli&#31639;&#23376;&#30340;Abel&#32676;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#21333;&#20010;Conformer&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39046;&#22495;&#29305;&#24322;&#24615;&#65292;&#36890;&#36807;&#22312;Conformer&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22312;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#65289;&#20013;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13408</link><description>&lt;p&gt;
&#22522;&#20110; Conformer &#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22359;&#21270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Modular Domain Adaptation for Conformer-Based Streaming ASR. (arXiv:2305.13408v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13408
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#21333;&#20010;Conformer&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39046;&#22495;&#29305;&#24322;&#24615;&#65292;&#36890;&#36807;&#22312;Conformer&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22312;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#65289;&#20013;&#36798;&#21040;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#39046;&#22495;&#30340;&#35821;&#38899;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;&#36890;&#24120;&#22312;&#25152;&#26377;&#39046;&#22495;&#30340;&#28151;&#21512;&#25968;&#25454;&#19978;&#35757;&#32451;&#21333;&#20010;&#22810;&#22495;&#27169;&#22411;&#65292;&#22914;Conformer transducer&#35821;&#38899;&#35782;&#21035;&#22120;&#12290;&#20294;&#26159;&#65292;&#26356;&#25913;&#19968;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#25110;&#28155;&#21152;&#26032;&#39046;&#22495;&#20250;&#35201;&#27714;&#37325;&#26032;&#35757;&#32451;&#22810;&#39046;&#22495;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#22359;&#21270;&#39046;&#22495;&#36866;&#24212;&#65288;MDA&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20351;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22810;&#39046;&#22495;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#21442;&#25968;&#29305;&#23450;&#20110;&#39046;&#22495;&#65292;&#21363;&#27599;&#20010;&#21442;&#25968;&#20165;&#30001;&#19968;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#35757;&#32451;&#12290;&#22312;&#20165;&#20351;&#29992;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#35757;&#32451;&#30340;&#27969;&#24335;Conformer transducer&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#22312;Conformer encoder&#20013;&#28155;&#21152;&#27599;&#20010;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21644;&#36880;&#39046;&#22495;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#22522;&#20110;MDA&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#22810;&#39046;&#22495;&#27169;&#22411;&#31867;&#20284;&#30340;&#24615;&#33021;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#35821;&#38899;&#25628;&#32034;&#21644;&#21548;&#20889;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech data from different domains has distinct acoustic and linguistic characteristics. It is common to train a single multidomain model such as a Conformer transducer for speech recognition on a mixture of data from all domains. However, changing data in one domain or adding a new domain would require the multidomain model to be retrained. To this end, we propose a framework called modular domain adaptation (MDA) that enables a single model to process multidomain data while keeping all parameters domain-specific, i.e., each parameter is only trained by data from one domain. On a streaming Conformer transducer trained only on video caption data, experimental results show that an MDA-based model can reach similar performance as the multidomain model on other domains such as voice search and dictation by adding per-domain adapters and per-domain feed-forward networks in the Conformer encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13404</link><description>&lt;p&gt;
&#21033;&#29992;&#21442;&#25968;&#23545;&#31216;&#24615;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving Convergence and Generalization Using Parameter Symmetries. (arXiv:2305.13404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#21152;&#36895;&#20248;&#21270;&#24182;&#22312;&#24635;&#20307;&#19978;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#26102;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#21442;&#25968;&#30340;&#19981;&#21516;&#20540;&#21487;&#33021;&#23548;&#33268;&#30456;&#21516;&#30340;&#25439;&#22833;&#20540;&#12290;&#21442;&#25968;&#31354;&#38388;&#23545;&#31216;&#24615;&#26159;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#32780;&#20445;&#25345;&#25439;&#22833;&#19981;&#21464;&#30340;&#21464;&#25442;&#12290;&#20256;&#36865;&#24212;&#29992;&#36825;&#26679;&#30340;&#21464;&#25442;&#26469;&#21152;&#36895;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31639;&#27861;&#25104;&#21151;&#30340;&#30830;&#20999;&#26426;&#21046;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#19981;&#20165;&#21487;&#20197;&#22312;&#30701;&#26399;&#20869;&#21152;&#36895;&#20248;&#21270;&#65292;&#32780;&#19988;&#21487;&#20197;&#20351;&#24635;&#20307;&#25910;&#25947;&#26102;&#38388;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#36865;&#21040;&#20855;&#26377;&#19981;&#21516;&#26354;&#29575;&#30340;&#26368;&#23567;&#20540;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26368;&#23567;&#20540;&#26354;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20256;&#36865;&#38598;&#25104;&#21040;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#20013;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#22312;&#23384;&#22312;&#26377;&#38480;&#23545;&#25239;&#38169;&#35823;&#26102;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35299;&#26512;&#26694;&#26550;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.13402</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#30340;&#23481;&#38169;&#31934;&#30830;&#26597;&#35810;&#23398;&#20064;&#26377;&#38480;&#38598;&#21512;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Error-Tolerant Exact Query Learning of Finite Set Partitions with Same-Cluster Oracle. (arXiv:2305.13402v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#22312;&#23384;&#22312;&#26377;&#38480;&#23545;&#25239;&#38169;&#35823;&#26102;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35299;&#26512;&#26694;&#26550;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#26377;&#38480;&#30340;&#23545;&#25239;&#38169;&#35823;&#26102;&#65292;&#20165;&#36890;&#36807;&#21516;&#31751;&#39044;&#35328;&#26426;&#26469;&#31215;&#26497;&#23398;&#20064;&#23436;&#20840;&#24674;&#22797;&#21010;&#20998;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#31361;&#20986;&#20102;&#23398;&#20064;&#21010;&#20998;&#21644;&#30456;&#20851;&#32858;&#31867;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#20026;&#36825;&#20010;&#38382;&#39064;&#24314;&#31435;&#20102;&#19968;&#20010;R&#233;nyi-Ulam&#26679;&#24335;&#30340;&#35299;&#26512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38480;&#21046;&#20102;&#30456;&#20851;&#38543;&#26426;&#31639;&#27861;&#30340;&#26399;&#26395;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24230;&#22312;&#35813;&#38382;&#39064;&#21644;&#30456;&#20851;&#21464;&#20307;&#20013;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper initiates the study of active learning for exact recovery of partitions exclusively through access to a same-cluster oracle in the presence of bounded adversarial error. We first highlight a novel connection between learning partitions and correlation clustering. Then we use this connection to build a R\'enyi-Ulam style analytical framework for this problem, and prove upper and lower bounds on its worst-case query complexity. Further, we bound the expected performance of a relevant randomized algorithm. Finally, we study the relationship between adaptivity and query complexity for this problem and related variants.
&lt;/p&gt;</description></item><item><title>nnDetection&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#23450;&#20301;&#39045;&#20869;&#21160;&#33033;&#30244;&#30340;3D&#22352;&#26631;&#65292;&#24182;&#36890;&#36807;&#33258;&#30001;&#21709;&#24212;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13398</link><description>&lt;p&gt;
nnDetection&#29992;&#20110;&#39045;&#20869;&#21160;&#33033;&#30244;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
nnDetection for Intracranial Aneurysms Detection and Localization. (arXiv:2305.13398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13398
&lt;/p&gt;
&lt;p&gt;
nnDetection&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#23450;&#20301;&#39045;&#20869;&#21160;&#33033;&#30244;&#30340;3D&#22352;&#26631;&#65292;&#24182;&#36890;&#36807;&#33258;&#30001;&#21709;&#24212;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#21160;&#33033;&#30244;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21361;&#21450;&#29983;&#21629;&#30340;&#30142;&#30149;&#65292;&#22823;&#32422;&#24433;&#21709;&#30528;3.2%&#30340;&#20154;&#21475;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#20123;&#21160;&#33033;&#30244;&#22312;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30149;&#21464;&#26816;&#27979;&#28041;&#21450;&#21040;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21516;&#26102;&#23450;&#20301;&#21644;&#20998;&#31867;&#24322;&#24120;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;nnDetection&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;3D&#21307;&#23398;&#29289;&#20307;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#33258;&#25105;&#37197;&#32622;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#23450;&#20301;&#20102;&#21160;&#33033;&#30244;&#30340;3D&#22352;&#26631;&#12290;&#20026;&#20102;&#25429;&#33719;&#21644;&#25552;&#21462;&#19982;&#21160;&#33033;&#30244;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;ADAM&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;TOF-MRA&#21644;&#32467;&#26500;MRI&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#33258;&#30001;&#21709;&#24212;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#36827;&#34892;&#35780;&#20272;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;TOF-MRA&#36793;&#30028;&#26694;&#30340;3D&#39044;&#27979;&#21487;&#22312;https://github.com/orouskhani/AneurysmDetection&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial aneurysms are a commonly occurring and life-threatening condition, affecting approximately 3.2% of the general population. Consequently, detecting these aneurysms plays a crucial role in their management. Lesion detection involves the simultaneous localization and categorization of abnormalities within medical images. In this study, we employed the nnDetection framework, a self-configuring framework specifically designed for 3D medical object detection, to detect and localize the 3D coordinates of aneurysms effectively. To capture and extract diverse features associated with aneurysms, we utilized TOF-MRA and structural MRI, both obtained from the ADAM dataset. The performance of our proposed deep learning model was assessed through the utilization of free-response receiver operative characteristics for evaluation purposes. The model's weights and 3D prediction of the bounding box of TOF-MRA are publicly available at https://github.com/orouskhani/AneurysmDetection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#21457;&#23637;&#24615;&#22909;&#22855;&#24515;&#20026;&#22522;&#30784;&#30340;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#36827;&#34892;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;</title><link>http://arxiv.org/abs/2305.13396</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#20013;&#30340;&#21457;&#23637;&#22909;&#22855;&#24515;&#21644;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Developmental Curiosity and Social Interaction in Virtual Agents. (arXiv:2305.13396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#21457;&#23637;&#24615;&#22909;&#22855;&#24515;&#20026;&#22522;&#30784;&#30340;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#36827;&#34892;&#25506;&#31350;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#20250;&#26377;&#24847;&#35782;&#22320;&#25506;&#31350;&#22797;&#26434;&#30340;&#29289;&#29702;&#21644;&#31038;&#20250;&#29615;&#22659;&#12290;&#20026;&#20102;&#25506;&#23547;&#20869;&#22312;&#21160;&#26426;&#22914;&#20309;&#24110;&#21161;&#32452;&#32455;&#25506;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#34394;&#25311;&#23156;&#20799;&#20195;&#29702;&#20154;&#24182;&#23558;&#20854;&#32622;&#20110;&#21463;&#21551;&#21457;&#30340;3D&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#12290;&#35813;&#29615;&#22659;&#26377;&#19968;&#20010;&#34394;&#25311;&#30340;&#29031;&#30475;&#20195;&#29702;&#20154;&#65292;&#33021;&#22815;&#19982;&#23156;&#20799;&#20195;&#29702;&#20132;&#20114;&#24182;&#20197;&#31867;&#20284;&#28216;&#25103;&#30340;&#26041;&#24335;&#20114;&#21160;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#31867;&#20284;&#20110;&#25512;&#21160;&#20154;&#31867;&#25506;&#32034;&#30340;&#21160;&#26426;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65306;&#24778;&#22855;&#65292;&#19981;&#30830;&#23450;&#24615;&#65292;&#26032;&#22855;&#24615;&#21644;&#23398;&#20064;&#36827;&#24230;&#12290;&#36825;&#20123;&#36890;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#24341;&#23548;&#23156;&#20799;&#20195;&#29702;&#20154;&#25506;&#32034;&#20854;&#29615;&#22659;&#65292;&#24182;&#21457;&#29616;&#20869;&#23884;&#22312;&#29031;&#30475;&#20195;&#29702;&#20013;&#30340;&#24212;&#21464;&#12290;&#20195;&#34920;&#26032;&#22855;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#26368;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#22810;&#26679;&#30340;&#20307;&#39564;&#65292;&#24182;&#28608;&#27963;&#20102;&#29615;&#22659;&#20013;&#30340;&#24212;&#21464;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#23384;&#22312;&#19968;&#20010;&#20851;&#27880;&#19988;&#21453;&#24212;&#36805;&#36895;&#30340;&#29031;&#30475;&#20195;&#29702;&#20154;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentiv
&lt;/p&gt;</description></item><item><title>EnSiam&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#38598;&#25104;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35757;&#32451;&#37197;&#32622;&#25913;&#21464;&#26102;SimSiam&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#22823;&#22810;&#25968;&#23454;&#39564;&#20013;&#65292;EnSiam&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13391</link><description>&lt;p&gt;
EnSiam: &#24102;&#26377;&#38598;&#25104;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EnSiam: Self-Supervised Learning With Ensemble Representations. (arXiv:2305.13391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13391
&lt;/p&gt;
&lt;p&gt;
EnSiam&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#38598;&#25104;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35757;&#32451;&#37197;&#32622;&#25913;&#21464;&#26102;SimSiam&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#22823;&#22810;&#25968;&#23454;&#39564;&#20013;&#65292;EnSiam&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23545;&#26679;&#26412;&#30340;&#36523;&#20221;&#21028;&#26029;&#26469;&#30830;&#23450;&#34920;&#31034;&#30456;&#20284;&#24615;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;SimSiam&#26159;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#33879;&#21517;&#31034;&#20363;&#65292;&#20197;&#20854;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#29305;&#24449;&#65292;&#23427;&#23545;&#20110;&#35757;&#32451;&#37197;&#32622;&#65288;&#22914;&#36229;&#21442;&#25968;&#21644;&#22686;&#24191;&#35774;&#32622;&#65289;&#30340;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20851;&#27880;&#23545;&#27604;&#23398;&#20064;&#19982;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#24072;&#29983;&#26694;&#26550;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#21463;&#38598;&#25104;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EnSiam&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#29992;&#38598;&#25104;&#34920;&#31034;&#26469;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#12290; &#36825;&#21487;&#20197;&#25552;&#20379;&#31283;&#23450;&#30340;&#20266;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290; &#23454;&#39564;&#35777;&#26126;&#65292;EnSiam&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;ImageNet&#19978;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive self-supervised learning, where the proximity of representations is determined based on the identities of samples, has made remarkable progress in unsupervised representation learning. SimSiam is a well-known example in this area, known for its simplicity yet powerful performance. However, it is known to be sensitive to changes in training configurations, such as hyperparameters and augmentation settings, due to its structural characteristics. To address this issue, we focus on the similarity between contrastive learning and the teacher-student framework in knowledge distillation. Inspired by the ensemble-based knowledge distillation approach, the proposed method, EnSiam, aims to improve the contrastive learning procedure using ensemble representations. This can provide stable pseudo labels, providing better performance. Experiments demonstrate that EnSiam outperforms previous state-of-the-art methods in most cases, including the experiments on ImageNet, which sho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#35201;&#23454;&#29616;&#21453;&#21521;&#20256;&#25773;&#30340;&#25193;&#23637;&#38656;&#35201;&#35775;&#38382;&#19968;&#20010;&#29366;&#24577;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#32570;&#23569;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22522;&#20110;&#38452;&#24433;&#27979;&#37327;&#30340;&#31639;&#27861;&#21487;&#20197;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.13362</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#23376;&#21453;&#21521;&#20256;&#25773;&#12289;&#20449;&#24687;&#37325;&#29992;&#21644;&#27450;&#39575;&#27979;&#37327;&#22349;&#22604;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On quantum backpropagation, information reuse, and cheating measurement collapse. (arXiv:2305.13362v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#35201;&#23454;&#29616;&#21453;&#21521;&#20256;&#25773;&#30340;&#25193;&#23637;&#38656;&#35201;&#35775;&#38382;&#19968;&#20010;&#29366;&#24577;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#32570;&#23569;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22522;&#20110;&#38452;&#24433;&#27979;&#37327;&#30340;&#31639;&#27861;&#21487;&#20197;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24314;&#31435;&#22312;&#33021;&#22815;&#22823;&#35268;&#27169;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#19978;&#12290;&#36890;&#36807;&#24039;&#22937;&#37325;&#29992;&#20013;&#38388;&#20449;&#24687;&#65292;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#35745;&#31639;&#20989;&#25968;&#30340;&#26799;&#24230;&#26469;&#23454;&#29616;&#35757;&#32451;&#65292;&#24635;&#25104;&#26412;&#22823;&#33268;&#19982;&#36816;&#34892;&#20989;&#25968;&#25104;&#27604;&#20363;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#19982;&#21442;&#25968;&#25968;&#37327;&#25104;&#27604;&#20363;&#30340;&#39069;&#22806;&#22240;&#32032;&#65292;&#36825;&#20010;&#25968;&#37327;&#29616;&#22312;&#21487;&#33021;&#39640;&#36798;&#25968;&#19975;&#20159;&#12290;&#24456;&#33258;&#28982;&#22320;&#65292;&#20154;&#20204;&#35748;&#20026;&#37327;&#23376;&#27979;&#37327;&#22349;&#22604;&#23436;&#20840;&#25490;&#38500;&#20102;&#22914;&#21453;&#21521;&#20256;&#25773;&#20013;&#30340;&#37327;&#23376;&#20449;&#24687;&#37325;&#29992;&#12290;&#20294;&#26368;&#36817;&#38452;&#24433;&#27979;&#37327;&#30340;&#21457;&#23637;&#25361;&#25112;&#20102;&#36825;&#19968;&#27010;&#24565;&#65292;&#38452;&#24433;&#27979;&#37327;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#37327;&#23376;&#24577;&#30340;&#22810;&#20010;&#21103;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#21442;&#25968;&#21270;&#37327;&#23376;&#27169;&#22411;&#26159;&#21542;&#33021;&#20687;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#37027;&#26679;&#39640;&#25928;&#22320;&#35757;&#32451;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#21453;&#21521;&#20256;&#25773;&#30340;&#25193;&#23637;&#38656;&#35201;&#35775;&#38382;&#19968;&#20010;&#29366;&#24577;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#32570;&#23569;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28304;&#20110;&#38452;&#24433;&#27979;&#37327;&#30340;&#31639;&#27861;&#65292;&#23427;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters - which can now be in the trillions. Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion. Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state. With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21442;&#25968;&#32447;&#24615;&#23610;&#24230;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#32452;&#26377;&#29992;&#30340;&#24615;&#36136;&#21644;&#19968;&#20010;&#26032;&#30340;&#26500;&#24314;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#32500;&#36830;&#32493;&#20449;&#21495;&#30340;&#20998;&#31867;&#21644;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.13350</link><description>&lt;p&gt;
&#19968;&#32500;&#20449;&#21495;&#20998;&#31867;&#30340;&#22810;&#21442;&#25968;&#32447;&#24615;&#23610;&#24230;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
A Multiple Parameter Linear Scale-Space for one dimensional Signal Classification. (arXiv:2305.13350v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21442;&#25968;&#32447;&#24615;&#23610;&#24230;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#32452;&#26377;&#29992;&#30340;&#24615;&#36136;&#21644;&#19968;&#20010;&#26032;&#30340;&#26500;&#24314;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#32500;&#36830;&#32493;&#20449;&#21495;&#30340;&#20998;&#31867;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#21442;&#25968;&#32447;&#24615;&#23610;&#24230;&#31354;&#38388;&#30340;&#26368;&#22823;&#26680;&#38598;&#65292;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#31867;&#20284;&#20110;&#39640;&#26031;&#32447;&#24615;&#23610;&#24230;&#31354;&#38388;&#26041;&#27861;&#30340;&#19968;&#32500;&#36830;&#32493;&#20449;&#21495;&#20998;&#31867;&#21644;&#35782;&#21035;&#26641;&#12290;&#25552;&#20379;&#20102;&#20613;&#37324;&#21494;&#21464;&#25442;&#20844;&#24335;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#25512;&#23548;&#20986;&#20102;&#26368;&#22823;&#26680;&#38598;&#30340;&#19968;&#20123;&#26377;&#29992;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#21152;&#24378;&#21644;&#25512;&#24191;&#20102;&#19968;&#20123;&#20851;&#20110;&#39640;&#26031;&#26680;&#20998;&#31867;&#30340;&#20808;&#21069;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;&#26500;&#24314;&#26641;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we construct a maximal set of kernels for a multi-parameter linear scale-space that allow us to construct trees for classification and recognition of one-dimensional continuous signals similar the Gaussian linear scale-space approach. Fourier transform formulas are provided and used for quick and efficient computations. A number of useful properties of the maximal set of kernels are derived. We also strengthen and generalize some previous results on the classification of Gaussian kernels. Finally, a new topologically invariant method of constructing trees is introduced.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;(mfDNN)&#65292;&#21487;&#20197;&#35299;&#20915;&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#26080;&#38480;&#32500;&#29305;&#24449;&#38590;&#20197;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;ReLU&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#27492;&#32593;&#32476;&#21487;&#20197;&#22312;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;mfDNN&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13349</link><description>&lt;p&gt;
&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#30340;&#22810;&#31867;&#20998;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiclass classification for multidimensional functional data through deep neural networks. (arXiv:2305.13349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;(mfDNN)&#65292;&#21487;&#20197;&#35299;&#20915;&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#26080;&#38480;&#32500;&#29305;&#24449;&#38590;&#20197;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;ReLU&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#27492;&#32593;&#32476;&#21487;&#20197;&#22312;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#19979;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;mfDNN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#35266;&#27979;&#20540;&#22312;&#22810;&#32500;&#22495;&#19978;&#30340;&#20869;&#22312;&#26080;&#38480;&#32500;&#29305;&#24449;&#20351;&#24471;&#26631;&#20934;&#20998;&#31867;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31867;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(mfDNN)&#20998;&#31867;&#22120;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#31867;&#24037;&#20855;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24102;&#26377;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;(ReLU)&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#23454;&#29616;&#20013;&#20351;&#29992;&#29616;&#20195;&#35745;&#31639;&#24037;&#20855;&#12290;&#23545;&#20110;&#23436;&#20840;&#35266;&#23519;&#21644;&#31163;&#25955;&#35266;&#23519;&#30340;&#22810;&#32500;&#20989;&#25968;&#25968;&#25454;&#65292;&#36824;&#25512;&#23548;&#20986;&#20102;&#35823;&#20998;&#31867;&#39118;&#38505;&#20989;&#25968;&#30340;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;mfDNN&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intrinsically infinite-dimensional features of the functional observations over multidimensional domains render the standard classification methods effectively inapplicable. To address this problem, we introduce a novel multiclass functional deep neural network (mfDNN) classifier as an innovative data mining and classification tool. Specifically, we consider sparse deep neural network architecture with rectifier linear unit (ReLU) activation function and minimize the cross-entropy loss in the multiclass classification setup. This neural network architecture allows us to employ modern computational tools in the implementation. The convergence rates of the misclassification risk functions are also derived for both fully observed and discretely observed multidimensional functional data. We demonstrate the performance of mfDNN on simulated data and several benchmark datasets from different application domains.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#27169;&#25311;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35686;&#21578;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#33021;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.13342</link><description>&lt;p&gt;
&#35770;&#27169;&#25311;&#20027;&#21160;&#23398;&#20064;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Simulating Active Learning. (arXiv:2305.13342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13342
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#20027;&#21160;&#23398;&#20064;&#27169;&#25311;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35686;&#21578;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#33021;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26159;&#19968;&#31181;&#20154;&#19982;&#27169;&#22411;&#20132;&#20114;&#24490;&#29615;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#36845;&#20195;&#22320;&#36873;&#25321;&#20449;&#24687;&#24615;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#20379;&#20154;&#31867;&#27880;&#37322;&#65292;&#26088;&#22312;&#25913;&#21892;&#38543;&#26426;&#25277;&#26679;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#27969;&#31243;&#20013;&#23454;&#26102;&#36827;&#34892;&#24102;&#20154;&#31867;&#27880;&#37322;&#30340;AL&#23454;&#39564;&#26159;&#19968;&#39033;&#32321;&#29712;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;&#24050;&#26631;&#35760;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#20316;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#27744;&#26469;&#27169;&#25311;AL&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#26368;&#36817;&#30340;&#25991;&#29486;&#24182;&#31361;&#20986;&#26174;&#31034;AL&#24490;&#29615;&#20013;&#25152;&#26377;&#19981;&#21516;&#27493;&#39588;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23454;&#39564;&#35774;&#32622;&#20013;&#34987;&#24573;&#35270;&#30340;&#27880;&#24847;&#20107;&#39033;&#65292;&#36825;&#20123;&#27880;&#24847;&#20107;&#39033;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;AL&#30740;&#31350;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25509;&#30528;&#25506;&#35752;&#20102;&#27169;&#25311;&#35774;&#32622;&#22914;&#20309;&#25903;&#37197;&#32463;&#39564;&#21457;&#29616;&#65292;&#35748;&#20026;&#36825;&#21487;&#33021;&#26159;&#8220;&#20026;&#20160;&#20040;&#26377;&#26102;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#32988;&#36807;&#38543;&#26426;&#25277;&#26679;&#8221;&#30340;&#26435;&#34913;&#20043;&#19968;&#12290;&#25105;&#20204;&#35748;&#20026;&#20165;&#22522;&#20110;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#24471;&#20986;&#24378;&#28872;&#32467;&#35770;&#21487;&#20197;&#23548;&#33268;&#35780;&#20272;AL&#31639;&#27861;&#30340;&#35823;&#23548;&#65292;&#22240;&#27492;&#25552;&#20986;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating A
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#23398;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#65292;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20022;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#26159;&#20854;&#22522;&#30784;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21457;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;&#36880;&#28176;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#20173;&#38754;&#20020;&#22810;&#39033;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13341</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Causal Relations and Equations from Data. (arXiv:2305.13341v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13341
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#65292;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20022;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#26159;&#20854;&#22522;&#30784;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21457;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#21644;&#26041;&#31243;&#24335;&#36880;&#28176;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#20173;&#38754;&#20020;&#22810;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#26159;&#19968;&#38376;&#21033;&#29992;&#31185;&#23398;&#26041;&#27861;&#22238;&#31572;&#33258;&#28982;&#29616;&#35937;&#20135;&#29983;&#21407;&#22240;&#24182;&#24314;&#31435;&#21487;&#39564;&#35777;&#27169;&#22411;&#26469;&#35299;&#37322;&#36825;&#20123;&#29616;&#35937;&#30340;&#23398;&#31185;&#12290;&#20960;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#21457;&#29616;&#19981;&#21464;&#12289;&#24378;&#20581;&#21644;&#22240;&#26524;&#35299;&#37322;&#19990;&#30028;&#30340;&#26041;&#31243;&#12289;&#27861;&#21017;&#21644;&#21407;&#21017;&#19968;&#30452;&#26159;&#29289;&#29702;&#23398;&#20013;&#30340;&#22522;&#30784;&#12290;&#36825;&#20123;&#21457;&#29616;&#28304;&#20110;&#23545;&#19990;&#30028;&#30340;&#35266;&#23519;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#25152;&#30740;&#31350;&#30340;&#31995;&#32479;&#20013;&#36827;&#34892;&#24178;&#39044;&#30740;&#31350;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#22240;&#26524;&#21644;&#26041;&#31243;&#24335;&#21457;&#29616;&#39046;&#22495;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#32479;&#35745;&#23398;&#12289;&#21746;&#23398;&#20197;&#21450;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#25152;&#26377;&#36825;&#20123;&#39046;&#22495;&#37117;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#21487;&#29992;&#20110;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12289;&#29289;&#29702;&#23450;&#24459;&#21644;&#26041;&#31243;&#24335;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29289;&#29702;&#23398;&#24191;&#27867;&#39046;&#22495;&#20013;&#30340;&#22240;&#26524;&#21644;&#26041;&#31243;&#24335;&#21457;&#29616;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#30456;&#20851;&#20316;&#21697;&#65292;&#24182;&#27010;&#36848;&#20102;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;LeNet&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#32954;&#37096;&#32959;&#30244;&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;99.51&#65285;&#30340;&#25104;&#21151;&#29575;&#12289;93&#65285;&#30340;&#28789;&#25935;&#24230;&#21644;95&#65285;&#30340;&#29305;&#24322;&#24230;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13333</link><description>&lt;p&gt;
&#22312;&#20234;&#25289;&#20811;&#32959;&#30244;&#25945;&#23398;&#21307;&#38498;/&#22269;&#23478;&#30284;&#30151;&#30142;&#30149;&#20013;&#24515;&#20013;&#27979;&#35797;LeNet&#31639;&#27861;&#20197;&#20998;&#31867;&#32954;&#30284;
&lt;/p&gt;
&lt;p&gt;
Evaluating LeNet Algorithms in Classification Lung Cancer from Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases. (arXiv:2305.13333v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;LeNet&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#32954;&#37096;&#32959;&#30244;&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;99.51&#65285;&#30340;&#25104;&#21151;&#29575;&#12289;93&#65285;&#30340;&#28789;&#25935;&#24230;&#21644;95&#65285;&#30340;&#29305;&#24322;&#24230;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#23545;&#20110;&#20154;&#31867;&#30142;&#30149;&#30340;&#20020;&#24202;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#32954;&#30284;&#20316;&#20026;&#19968;&#31181;&#24433;&#21709;&#30007;&#22899;&#30340;&#30142;&#30149;&#65292;&#20854;&#30149;&#27515;&#29575;&#26497;&#39640;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;LeNet&#26816;&#27979;&#32954;&#37096;&#32959;&#30244;&#12290;&#23454;&#39564;&#29992;&#30340;&#26159;&#20844;&#24320;&#30340;CT&#22270;&#20687;&#25968;&#25454;&#38598;(IQ-OTH / NCCD)&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#20234;&#25289;&#20811;&#32959;&#30244;&#25945;&#23398;&#21307;&#38498;/&#22269;&#23478;&#30284;&#30151;&#30142;&#30149;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25104;&#21151;&#29575;&#20026;99.51&#65285;&#65292;&#28789;&#25935;&#24230;(93&#65285;)&#21644;&#29305;&#24322;&#24230;(95&#65285;)&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20687;&#25105;&#20204;&#36825;&#26679;&#30340;&#31639;&#27861;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#26159;&#24320;&#21457;&#21487;&#22312;&#24120;&#35268;&#30149;&#29702;&#23398;&#20013;&#37319;&#29992;&#30340;&#36719;&#20214;&#22871;&#20214;&#30340;&#37325;&#35201;&#21021;&#22987;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of computer-aided detection systems had a significant impact on clinical analysis and decision-making on human disease. Lung cancer requires more attention among the numerous diseases being examined because it affects both men and women, increasing the mortality rate. LeNet, a deep learning model, is used in this study to detect lung tumors. The studies were run on a publicly available dataset made up of CT image data (IQ-OTH/NCCD). Convolutional neural networks (CNNs) were employed in the experiment for feature extraction and classification. The proposed system was evaluated on Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases datasets the success percentage was calculated as 99.51%, sensitivity (93%) and specificity (95%), and better results were obtained compared to the existing methods. Development and validation of algorithms such as ours are important initial steps in the development of software suites that could be adopted in routine pathologica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#26356;&#26032;&#20851;&#38190;&#35789;&#35782;&#21035;&#22120;&#65292;&#22312;&#21160;&#24577;&#38899;&#39057;&#27969;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;34&#65285;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2305.13332</link><description>&lt;p&gt;
&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#26377;&#26465;&#20214;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditional Online Learning for Keyword Spotting. (arXiv:2305.13332v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#26356;&#26032;&#20851;&#38190;&#35789;&#35782;&#21035;&#22120;&#65292;&#22312;&#21160;&#24577;&#38899;&#39057;&#27969;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;34&#65285;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22823;&#22411;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#25152;&#24471;&#27169;&#22411;&#24448;&#24448;&#20250;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;SGD&#22312;&#35774;&#22791;&#19978;&#26356;&#26032;&#20851;&#38190;&#35789;&#35782;&#21035;&#22120;&#65292;&#20197;&#20415;&#22312;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#26356;&#26032;&#27169;&#22411;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#20391;&#37325;&#20110;&#23398;&#20064;&#30456;&#21516;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#21830;&#19994;&#24212;&#29992;&#12290;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23545;&#21160;&#24577;&#38899;&#39057;&#27969;&#36827;&#34892;&#23454;&#39564;&#26102;&#65292;&#35813;&#26041;&#27861;&#23558;&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;34&#65285;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26420;&#32032;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#30456;&#27604;&#65292;&#22522;&#20110;&#35757;&#32451;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#23567;&#22411;&#20445;&#30041;&#38598;&#20013;&#20854;&#24615;&#33021;&#36827;&#34892;&#26465;&#20214;&#27169;&#22411;&#26356;&#26032;&#21487;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern approaches for keyword spotting rely on training deep neural networks on large static datasets with i.i.d. distributions. However, the resulting models tend to underperform when presented with changing data regimes in real-life applications. This work investigates a simple but effective online continual learning method that updates a keyword spotter on-device via SGD as new data becomes available. Contrary to previous research, this work focuses on learning the same KWS task, which covers most commercial applications. During experiments with dynamic audio streams in different scenarios, that method improves the performance of a pre-trained small-footprint model by 34%. Moreover, experiments demonstrate that, compared to a naive online learning implementation, conditional model updates based on its performance in a small hold-out set drawn from the training distribution mitigate catastrophic forgetting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;Poincar&#233;&#22270;&#20013;&#30340;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#35757;&#32451;&#38598;&#21644;&#36716;&#25442;&#22352;&#26631;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13329</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;Poincar&#233;&#22270;&#20013;&#30340;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Orbits in Poincar\'e Maps using Machine Learning. (arXiv:2305.13329v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13329
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;Poincar&#233;&#22270;&#20013;&#30340;&#36712;&#36947;&#36827;&#34892;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#35757;&#32451;&#38598;&#21644;&#36716;&#25442;&#22352;&#26631;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Poincar&#233;&#22270;&#26159;&#31561;&#31163;&#23376;&#29289;&#29702;&#23398;&#23478;&#29992;&#20110;&#29702;&#35299;&#25176;&#21345;&#39532;&#20811;&#25968;&#20540;&#27169;&#25311;&#20013;&#30913;&#22266; confinement plasma &#34892;&#20026;&#30340;&#12290;&#36825;&#20123;&#22270;&#36890;&#36807;&#30913;&#22330;&#32447;&#19982;&#34920;&#31034;&#25176;&#21345;&#39532;&#20811;&#30340;&#29615;&#24418;&#36724;&#32447;&#22402;&#30452;&#30340;&#20108;&#32500;&#26497;&#21521;&#24179;&#38754;&#30340;&#20132;&#28857;&#21019;&#24314;&#32780;&#25104;&#12290;&#19968;&#20010;&#22270;&#30001;&#22810;&#20010;&#36712;&#36947;&#32452;&#25104;&#65292;&#27599;&#20010;&#36712;&#36857;&#30001;&#19981;&#21516;&#30340;&#30913;&#22330;&#32447;&#32469;&#25176;&#21345;&#39532;&#20811;&#32780;&#25104;&#12290;&#27599;&#20010;&#36712;&#36857;&#21487;&#20197;&#26377;&#22235;&#31181;&#19981;&#21516;&#30340;&#24418;&#29366;&#25110;&#31867;&#21035;&#65292;&#34920;&#31034;&#38480;&#21046;&#31561;&#31163;&#23376;&#20307;&#30340;&#30913;&#22330;&#25299;&#25169;&#30340;&#21464;&#21270;&#12290;&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#35299;&#20915;&#27492;&#38382;&#39064;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#38598;&#24182;&#23558;&#28857;&#30340;&#22352;&#26631;&#36716;&#25442;&#20026;&#26426;&#22120;&#23398;&#20064;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poincar\'e plots, also called Poincar\'e maps, are used by plasma physicists to understand the behavior of magnetically confined plasma in numerical simulations of a tokamak. These plots are created by the intersection of field lines with a two-dimensional poloidal plane that is perpendicular to the axis of the torus representing the tokamak. A plot is composed of multiple orbits, each created by a different field line as it goes around the torus. Each orbit can have one of four distinct shapes, or classes, that indicate changes in the topology of the magnetic fields confining the plasma. Given the (x,y) coordinates of the points that form an orbit, the analysis task is to assign a class to the orbit, a task that appears ideally suited for a machine learning approach. In this paper, we describe how we overcame two major challenges in solving this problem - creating a high-quality training set, with few mislabeled orbits, and converting the coordinates of the points into features that a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13318</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#26041;&#27861;&#36827;&#34892;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A principled deep learning approach for geological facies generation. (arXiv:2305.13318v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#38024;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#20102;&#26377;&#26465;&#20214;&#27169;&#25311;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22320;&#29699;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#27169;&#25311;&#19981;&#21487;&#35266;&#27979;&#20307;&#31215;&#20013;&#30340;&#22320;&#36136;&#30456;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#32771;&#34385;&#21040;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#28145;&#24230;&#29983;&#25104;&#23398;&#20064;&#26159;&#20811;&#26381;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#23616;&#38480;&#24615;&#65288;&#29305;&#21035;&#26159;&#32570;&#20047;&#29289;&#29702;&#36924;&#30495;&#24615;&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#28145;&#24230;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#24212;&#29992;&#65292;&#20197;&#20415;&#26377;&#26465;&#20214;&#22320;&#23545;&#22320;&#19979;&#28192;&#36947;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#25239;&#24615;&#26041;&#27861;&#21644;&#26088;&#22312;&#20419;&#36827;&#20854;&#35757;&#32451;&#30340;&#31283;&#23450;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20197;&#38543;&#26426;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;Flumy&#27169;&#22411;&#29983;&#25104;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#27169;&#25311;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#21033;&#29992;&#24418;&#24577;&#23398;&#25351;&#26631;&#27604;&#36739;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#36845;&#20195;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#31283;&#23450;&#25216;&#26415;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22320;&#36136;&#23721;&#30456;&#29983;&#25104;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#22320;&#36136;&#32479;&#35745;&#27169;&#22411;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#29289;&#29702;&#36924;&#30495;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of geological facies in an unobservable volume is essential in various geoscience applications. Given the complexity of the problem, deep generative learning is a promising approach to overcome the limitations of traditional geostatistical simulation models, in particular their lack of physical realism. This research aims to investigate the application of generative adversarial networks and deep variational inference for conditionally simulating meandering channels in underground volumes. In this paper, we review the generative deep learning approaches, in particular the adversarial ones and the stabilization techniques that aim to facilitate their training. The proposed approach is tested on 2D and 3D simulations generated by the stochastic process-based model Flumy. Morphological metrics are utilized to compare our proposed method with earlier iterations of generative adversarial networks. The results indicate that by utilizing recent stabilization techniques, generati
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13316</link><description>&lt;p&gt;
KineticNet: &#28145;&#24230;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#30340;&#21160;&#33021;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
KineticNet: Deep learning a transferable kinetic energy functional for orbital-free density functional theory. (arXiv:2305.13316v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13316
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;OF-DFT&#65289;&#26377;&#26395;&#20197;&#26368;&#23567;&#20195;&#20215;&#35745;&#31639;&#22522;&#24577;&#20998;&#23376;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#23558;&#21160;&#33021;&#35745;&#31639;&#20026;&#30005;&#23376;&#23494;&#24230;&#30340;&#20989;&#25968;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20174;&#26356;&#26114;&#36149;&#30340;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#33021;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#26469;&#20419;&#36827;&#36712;&#36947;&#33258;&#30001;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#23454;&#38469;&#36816;&#29992;&#12290;KineticNet&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#27492;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbital-free density functional theory (OF-DFT) holds the promise to compute ground state molecular properties at minimal cost. However, it has been held back by our inability to compute the kinetic energy as a functional of the electron density only. We here set out to learn the kinetic energy functional from ground truth provided by the more expensive Kohn-Sham density functional theory. Such learning is confronted with two key challenges: Giving the model sufficient expressivity and spatial context while limiting the memory footprint to afford computations on a GPU; and creating a sufficiently broad distribution of training data to enable iterative density optimization even when starting from a poor initial guess. In response, we introduce KineticNet, an equivariant deep neural network architecture based on point convolutions adapted to the prediction of quantities on molecular quadrature grids. Important contributions include convolution filters with sufficient spatial resolution i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#22270;&#24418;&#24182;&#20511;&#21161;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#39044;&#27979;&#20998;&#23376;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#65292;&#20248;&#20110;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13315</link><description>&lt;p&gt;
&#20351;&#29992;2D&#22270;&#24418;&#36827;&#34892;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
3D Molecular Geometry Analysis with 2D Graphs. (arXiv:2305.13315v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#22270;&#24418;&#24182;&#20511;&#21161;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#39044;&#27979;&#20998;&#23376;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#65292;&#20248;&#20110;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#20998;&#23376;&#20998;&#26512;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#37327;&#23376;&#21147;&#23398;&#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#20934;&#30830;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#20174;2D&#22270;&#24418;&#35745;&#31639;&#22522;&#24577;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#20998;&#23376;&#22270;&#24418;&#39044;&#27979;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24179;&#34913;&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPNN&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#20174;&#20998;&#23376;&#22270;&#24418;&#20013;&#25429;&#25417;&#22522;&#24577;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#20998;&#26512;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20998;&#23376;&#20960;&#20309;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#20998;&#21106;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMPNN&#21487;&#20197;&#27604;RDKit&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#26356;&#20934;&#30830;&#30340;&#22522;&#24577;&#19977;&#32500;&#32467;&#26500;&#12290;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-state 3D geometries of molecules are essential for many molecular analysis tasks. Modern quantum mechanical methods can compute accurate 3D geometries but are computationally prohibitive. Currently, an efficient alternative to computing ground-state 3D molecular geometries from 2D graphs is lacking. Here, we propose a novel deep learning framework to predict 3D geometries from molecular graphs. To this end, we develop an equilibrium message passing neural network (EMPNN) to better capture ground-state geometries from molecular graphs. To provide a testbed for 3D molecular geometry analysis, we develop a benchmark that includes a large-scale molecular geometry dataset, data splits, and evaluation protocols. Experimental results show that EMPNN can efficiently predict more accurate ground-state 3D geometries than RDKit and other deep learning methods. Results also show that the proposed framework outperforms self-supervised learning methods on property prediction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.13314</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Simplifying Full Waveform Inversion via Domain-Independent Self-Supervised Learning. (arXiv:2305.13314v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#20013;&#24212;&#29992;&#24471;&#21040;&#25104;&#21151;&#65292;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#29616;&#35937;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21508;&#33258;&#30340;&#39046;&#22495;&#20013;&#20998;&#21035;&#35757;&#32451;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#36328;&#39046;&#22495;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SimFWI&#65292;&#19968;&#20010;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#30340;&#26032;&#33539;&#24335;&#65306;(a)&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#20998;&#21035;&#23398;&#20064;&#22320;&#38663;&#32534;&#30721;&#22120;&#21644;&#36895;&#24230;&#35299;&#30721;&#22120;&#65307;(b)&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Geophysics has witnessed success in applying deep learning to one of its core problems: full waveform inversion (FWI) to predict subsurface velocity maps from seismic data. It is treated as an image-to-image translation problem, jointly training an encoder for seismic data and a decoder for the velocity map from seismic-velocity pairs. In this paper, we report a surprising phenomenon: when training an encoder and decoder separately in their own domains via self-supervised learning, a linear relationship is observed across domains in the latent spaces. Moreover, this phenomenon connects multiple FWI datasets in an elegant manner: these datasets can share the self-learned encoder and decoder with different linear mappings.  Based on these findings, we develop SimFWI, a new paradigm that includes two steps: (a) learning a seismic encoder and a velocity decoder separately by masked image modeling over multiple datasets; (b) learning a linear mapping per dataset. Experimental results show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13301</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#19979;&#28216;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#26377;&#25928;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28789;&#27963;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#30340;&#36817;&#20284;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#24182;&#19981;&#20851;&#27880;&#20284;&#28982;&#65292;&#32780;&#26159;&#20851;&#27880;&#20154;&#31867;&#24863;&#30693;&#30340;&#22270;&#20687;&#36136;&#37327;&#25110;&#33647;&#29289;&#25928;&#21147;&#31561;&#19979;&#28216;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#27492;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#21435;&#22122;&#35270;&#20026;&#22810;&#27493;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#31216;&#20043;&#20026;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;DDPO&#65289;&#30340;&#19968;&#31867;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;&#26367;&#20195;&#30340;&#22870;&#21169;&#21152;&#26435;&#20284;&#28982;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;DDPO&#33021;&#22815;&#36866;&#24212;&#38590;&#20197;&#36890;&#36807;&#25552;&#31034;&#34920;&#36798;&#30340;&#22270;&#20687;&#21387;&#32553;&#31561;&#30446;&#26631;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#24471;&#20986;&#30340;&#32654;&#23398;&#36136;&#37327;&#31561;&#30446;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;DDPO&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#21453;&#39304;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#40784;&#26041;&#24335;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#32423;&#21035;&#19978;ChatGPT&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22797;&#26434;&#30340;&#25925;&#38556;&#65292;&#24182;&#25351;&#20986;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.13276</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#23545;&#22810;&#35821;&#35328;&#21644;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection. (arXiv:2305.13276v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#32423;&#21035;&#19978;ChatGPT&#27169;&#22411;&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22797;&#26434;&#30340;&#25925;&#38556;&#65292;&#24182;&#25351;&#20986;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#24433;&#21709;&#35768;&#22810;&#22312;&#32447;&#24179;&#21488;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#24050;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#20197;&#24320;&#21457;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#12290;&#36817;&#26469;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#20197;&#24314;&#31435;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;ChatGPT&#27169;&#22411;&#22312;&#36328;11&#31181;&#35821;&#35328;&#30340;&#31890;&#24230;&#32423;&#21035;&#19978;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37319;&#29992;&#19968;&#31995;&#21015;&#21151;&#33021;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21508;&#31181;&#22797;&#26434;&#25925;&#38556;&#65292;&#32780;&#32858;&#21512;&#25351;&#26631;&#22914;&#23439;F1&#25110;&#20934;&#30830;&#24615;&#21017;&#26080;&#27861;&#23637;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#21253;&#25324;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#22312;&#20869;&#30340;&#22797;&#26434;&#24773;&#24863;&#23545;ChatGPT&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26816;&#27979;&#26576;&#20123;&#31867;&#22411;&#30340;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#32570;&#28857;&#65292;&#24182;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is a severe issue that affects many online platforms. So far, several studies have been performed to develop robust hate speech detection systems. Large language models like ChatGPT have recently shown a great promise in performing several tasks, including hate speech detection. However, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. To bridge this gap, our study aims to evaluate the strengths and weaknesses of the ChatGPT model in detecting hate speech at a granular level across 11 languages. Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold. In addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the ChatGPT model. Our analysis highlights the shortcomings of the generative models in detecting certain types of h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#30340;&#20445;&#23432;&#22411;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26500;&#24314;&#38750;&#23432;&#24658;&#24418;&#24335;&#19979;&#21452;&#26354;&#32447;&#26631;&#37327;&#23432;&#24658;&#24459;&#30340;Riemann&#38382;&#39064;&#30340;&#24369;&#35299;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#23380;&#38553;&#24230;&#30340;&#24191;&#20041;Buckley-Leverett&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12817</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#23432;&#24658;&#36229;&#38480;&#23432;&#24658;&#23450;&#24459;&#20020;&#30028;&#29366;&#24577;&#30340;&#20445;&#23432;&#22411;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Conservative Physics-Informed Neural Networks for Non-Conservative Hyperbolic Conservation Laws Near Critical States. (arXiv:2305.12817v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#29256;&#30340;&#20445;&#23432;&#22411;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26500;&#24314;&#38750;&#23432;&#24658;&#24418;&#24335;&#19979;&#21452;&#26354;&#32447;&#26631;&#37327;&#23432;&#24658;&#24459;&#30340;Riemann&#38382;&#39064;&#30340;&#24369;&#35299;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#23380;&#38553;&#24230;&#30340;&#24191;&#20041;Buckley-Leverett&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20462;&#27491;&#29256;&#30340;&#20445;&#23432;&#22411;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(cPINN)&#65292;&#29992;&#20110;&#26500;&#24314;&#38750;&#23432;&#24658;&#24418;&#24335;&#19979;&#21452;&#26354;&#32447;&#26631;&#37327;&#23432;&#24658;&#24459;&#30340;Riemann&#38382;&#39064;&#30340;&#24369;&#35299;&#12290;&#20026;&#20102;&#35777;&#26126;&#32467;&#26524;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#23380;&#38553;&#24230;&#30340;&#24191;&#20041;Buckley-Leverett&#26041;&#31243;(GBL)&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26410;&#30693;&#37327;&#65292;GBL&#26041;&#31243;&#36716;&#21270;&#20026;&#20108;&#20056;&#20108;&#20849;&#25391;&#21452;&#26354;&#32447;&#23432;&#24658;&#23450;&#24459;&#30340;&#20445;&#23432;&#24418;&#24335;&#12290;&#20026;&#20102;&#20811;&#26381;&#23380;&#38553;&#24230;&#19981;&#36830;&#32493;&#21644;Riemann&#25968;&#25454;&#20013;&#20851;&#38190;&#29366;&#24577;(&#25509;&#36817;&#30495;&#31354;)&#30340;&#20986;&#29616;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#21457;&#26126;&#20102;&#20462;&#25913;&#21518;&#30340;cPINN&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;GBL&#26041;&#31243;&#30340;&#20445;&#23432;&#24418;&#24335;&#21644;&#38750;&#23432;&#24658;&#24418;&#24335;&#20197;&#21450;&#20020;&#30028;&#29366;&#24577;&#21644;&#38750;&#20020;&#30028;&#29366;&#24577;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20004;&#31181;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#21442;&#25968;&#30340;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a modified version of conservative Physics-informed Neural Networks (cPINN for short) is provided to construct the weak solutions of Riemann problem for the hyperbolic scalar conservation laws in non-conservative form. To demonstrate the results, we use the model of generalized Buckley-Leverett equation (GBL equation for short) with discontinuous porosity in porous media. By inventing a new unknown, the GBL equation is transformed into a two-by-two resonant hyperbolic conservation laws in conservative form. The modified method of cPINN is invented to overcome the difficulties due to the discontinuity of the porosity and the appearance of the critical states (near vacuum) in the Riemann data. We experiment with our idea by using a deep learning algorithm to solve the GBL equation in both conservative and non-conservative forms, as well as the cases of critical and non-critical states. This method provides a combination of two different neural networks and corresponding lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.12715</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65306;&#23398;&#20064;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;ILL&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#23558;&#31934;&#30830;&#26631;&#31614;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19982;&#20197;&#21069;&#35797;&#22270;&#20174;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;ILL&#26694;&#26550;&#32771;&#34385;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#24378;&#21152;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#20219;&#20309;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ILL&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20197;&#21450;&#36825;&#20123;&#37197;&#32622;&#30340;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#26631;&#24535;&#30528;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
&lt;/p&gt;</description></item><item><title>ParticleWNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#24369;&#24418;&#24335;&#19979;&#27714;&#35299;PDE&#12290;&#23427;&#37319;&#29992;DNN&#20316;&#20026;&#35797;&#39564;&#31354;&#38388;&#65292;&#29992;&#30001;&#31890;&#23376;&#20026;&#20013;&#24515;&#30340;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#32039;&#23494;&#25903;&#25345;&#30340;&#20989;&#25968;&#26500;&#25104;&#30340;&#27979;&#35797;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;R&#33258;&#36866;&#24212;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#26131;&#20110;&#25193;&#23637;&#21644;&#24182;&#34892;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.12433</link><description>&lt;p&gt;
ParticleWNN&#65306;&#19968;&#31181;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ParticleWNN: a Novel Neural Networks Framework for Solving Partial Differential Equations. (arXiv:2305.12433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12433
&lt;/p&gt;
&lt;p&gt;
ParticleWNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#24369;&#24418;&#24335;&#19979;&#27714;&#35299;PDE&#12290;&#23427;&#37319;&#29992;DNN&#20316;&#20026;&#35797;&#39564;&#31354;&#38388;&#65292;&#29992;&#30001;&#31890;&#23376;&#20026;&#20013;&#24515;&#30340;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#32039;&#23494;&#25903;&#25345;&#30340;&#20989;&#25968;&#26500;&#25104;&#30340;&#27979;&#35797;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;R&#33258;&#36866;&#24212;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#26131;&#20110;&#25193;&#23637;&#21644;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24191;&#27867;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;Particle Weak-form&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;ParticleWNN&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#24369;&#24418;&#24335;&#30340;PDE&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#35797;&#39564;&#31354;&#38388;&#36873;&#25321;&#20026;DNN&#30340;&#31354;&#38388;&#65292;&#27979;&#35797;&#31354;&#38388;&#30001;&#32039;&#23494;&#25903;&#25345;&#22312;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#20989;&#25968;&#26500;&#25104;&#65292;&#36825;&#20123;&#20989;&#25968;&#30340;&#20013;&#24515;&#26159;&#31890;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;R&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#33258;&#36866;&#24212;&#20462;&#25913;&#21306;&#22495;&#30340;&#21322;&#24452;&#12290;ParticleWNN&#32487;&#25215;&#20102;&#24369;/&#21464;&#20998;&#20844;&#24335;&#30340;&#20248;&#28857;&#65292;&#20363;&#22914;&#35201;&#27714;&#36739;&#23569;&#30340;&#35299;&#30340;&#27491;&#21017;&#24615;&#21644;&#35745;&#31639;&#31215;&#20998;&#30340;&#23569;&#37327;&#31215;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27979;&#35797;&#20989;&#25968;&#30340;&#29305;&#27530;&#26500;&#36896;&#65292;ParticleWNN&#20801;&#35768;&#26412;&#22320;&#35757;&#32451;&#32593;&#32476;&#12289;&#24182;&#34892;&#23454;&#29616;&#21644;&#20165;&#22312;&#26497;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31215;&#20998;&#35745;&#31639;&#12290;&#35813;&#26694;&#26550;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#29616;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been widely used to solve partial differential equations (PDEs) in recent years. In this work, a novel deep learning-based framework named Particle Weak-form based Neural Networks (ParticleWNN) is developed for solving PDEs in the weak form. In this framework, the trial space is chosen as the space of DNNs, and the test space is constructed by functions compactly supported in extremely small regions whose centers are particles. To train the neural networks, an R-adaptive strategy is designed to adaptively modify the radius of regions during training. The ParticleWNN inherits the advantages of weak/variational formulation, such as requiring less regularity of the solution and a small number of quadrature points for computing the integrals. Moreover, due to the special construction of the test functions, the ParticleWNN allows local training of networks, parallel implementation, and integral calculations only in extremely small regions. The framework is p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#31561;&#22810;&#20010;&#36807;&#31243;&#65292;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#26368;&#26032;&#30340;&#20960;&#39033;&#30740;&#31350;&#65292;&#24182;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25913;&#36827;&#28857;&#30340;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2305.11994</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey on software ISP methods based on Deep Learning. (arXiv:2305.11994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#31561;&#22810;&#20010;&#36807;&#31243;&#65292;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#26368;&#26032;&#30340;&#20960;&#39033;&#30740;&#31350;&#65292;&#24182;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25913;&#36827;&#28857;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#30340;&#25972;&#20010;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISP&#65289;&#20381;&#38752;&#22810;&#20010;&#36807;&#31243;&#23558;&#26469;&#33258;&#24425;&#33394;&#28388;&#27874;&#38453;&#21015;&#65288;CFA&#65289;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#36716;&#25442;&#65292;&#20363;&#22914;&#21435;&#39532;&#36187;&#20811;&#12289;&#38477;&#22122;&#21644;&#22686;&#24378;&#12290;&#36825;&#20123;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26576;&#20123;&#30828;&#20214;&#25110;&#36719;&#20214;&#26469;&#25191;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20102;&#20854;&#20013;&#19968;&#20123;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29978;&#33267;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25972;&#20010;ISP&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#20869;&#30340;&#20960;&#39033;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The entire Image Signal Processor (ISP) of a camera relies on several processes to transform the data from the Color Filter Array (CFA) sensor, such as demosaicing, denoising, and enhancement. These processes can be executed either by some hardware or via software. In recent years, Deep Learning has emerged as one solution for some of them or even to replace the entire ISP using a single neural network for the task. In this work, we investigated several recent pieces of research in this area and provide deeper analysis and comparison among them, including results and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11831</link><description>&lt;p&gt;
&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment. (arXiv:2305.11831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#31574;&#30053;&#35780;&#20272;&#12289;&#31574;&#30053;&#25913;&#36827;&#21644;&#28201;&#24230;&#35843;&#25972;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#21644;&#20462;&#25913;&#65292;&#20197;&#26356;&#21152;&#26126;&#30830;&#22320;&#38416;&#36848;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.11509</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25628;&#32034;&#21040;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38543;&#26426;&#25628;&#32034;&#21450;&#20854;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#30028;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#36755;&#20986;&#20998;&#21035;&#20197;&#19968;&#23450;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#26159;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#24456;&#23569;&#26377;&#38750;&#21551;&#21457;&#24335;&#30340;&#29702;&#35770;&#29992;&#20110;&#25551;&#36848;&#20854;&#24037;&#20316;&#26426;&#21046;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20851;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#25955;&#23556;&#32500;&#24230;&#8221;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#24213;&#23618;&#20989;&#25968;&#30340;&#29366;&#24577;&#65292;&#24182;&#37327;&#21270;&#20102;&#38543;&#26426;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29615;&#22659;&#27809;&#26377;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#20854;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $&#65292;&#20854;&#20013;$ d_s \ge 0 $&#26159;&#24213;&#23618;&#20989;&#25968;&#30340;&#25955;&#23556;&#32500;&#24230;&#12290;&#24403;&#35266;&#23519;&#21040;&#30340;&#20989;&#25968;&#20540;&#21463;&#21040;&#26377;&#30028;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;&#38543;&#26426;&#25628;&#32034;&#30340;&#36755;&#20986;&#20197;&#27010;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#20540;&#65292;&#36895;&#29575;&#20026;$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#30340;&#25915;&#20987;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10358</link><description>&lt;p&gt;
NUANCE: &#32593;&#32476;&#36890;&#20449;&#29615;&#22659;&#19979;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#36827;&#34892;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NUANCE: Near Ultrasound Attack On Networked Communication Environments. (arXiv:2305.10358v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#30340;&#25915;&#20987;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#36817;&#36229;&#22768;&#27874;&#29305;&#27931;&#20234;&#26408;&#39532;&#23545;&#20122;&#39532;&#36874;Alexa&#35821;&#38899;&#26381;&#21153;&#30340;&#20027;&#35201;&#19981;&#21487;&#21548;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#30528;&#37325;&#34920;&#24449;&#20102;&#25915;&#20987;&#38754;&#24182;&#32771;&#23519;&#20102;&#21457;&#20986;&#19981;&#21487;&#21548;&#35821;&#38899;&#25351;&#20196;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#23558;&#27599;&#20010;&#25915;&#20987;&#21521;&#37327;&#26144;&#23556;&#21040;MITRE ATT&#65286;CK&#30697;&#38453;&#20013;&#30340;&#19968;&#31181;&#31574;&#30053;&#25110;&#25216;&#26415;&#65292;&#28085;&#30422;&#20225;&#19994;&#12289;&#31227;&#21160;&#21644;&#24037;&#25511;&#31995;&#32479;&#65288;ICS&#65289;&#26694;&#26550;&#12290;&#23454;&#39564;&#28041;&#21450;&#29983;&#25104;&#21644;&#35843;&#26597;50&#20010;&#36817;&#36229;&#22768;&#27874;&#38899;&#39057;&#20197;&#35780;&#20272;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#26410;&#32463;&#22788;&#29702;&#30340;&#25351;&#20196;&#20855;&#26377;100&#65285;&#30340;&#25104;&#21151;&#29575;&#65292;&#22788;&#29702;&#21518;&#30340;&#25351;&#20196;&#23454;&#29616;&#20102;58&#65285;&#30340;&#24635;&#20307;&#25104;&#21151;&#29575;&#12290;&#35813;&#31995;&#32479;&#24615;&#26041;&#27861;&#21050;&#28608;&#20102;&#20197;&#21069;&#26410;&#24471;&#21040;&#35299;&#20915;&#30340;&#25915;&#20987;&#38754;&#65292;&#30830;&#20445;&#20102;&#20840;&#38754;&#30340;&#26816;&#27979;&#21644;&#25915;&#20987;&#35774;&#35745;&#65292;&#24182;&#23558;&#27599;&#20010;ATT&#65286;CK&#26631;&#35782;&#31526;&#19982;&#27979;&#35797;&#36807;&#30340;&#38450;&#24481;&#26041;&#27861;&#25645;&#37197;&#65292;&#20026;&#24555;&#36895;&#21709;&#24212;&#25552;&#20379;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36873;&#39033;&#12290;&#20027;&#35201;&#21457;&#29616;&#25581;&#31034;&#20102;&#35813;&#25915;&#20987;&#26041;&#27861;&#37319;&#29992;&#21333;&#36793;&#24102;&#24133;&#24230;&#35843;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates a primary inaudible attack vector on Amazon Alexa voice services using near ultrasound trojans and focuses on characterizing the attack surface and examining the practical implications of issuing inaudible voice commands. The research maps each attack vector to a tactic or technique from the MITRE ATT&amp;CK matrix, covering enterprise, mobile, and Industrial Control System (ICS) frameworks. The experiment involved generating and surveying fifty near-ultrasonic audios to assess the attacks' effectiveness, with unprocessed commands having a 100% success rate and processed ones achieving a 58% overall success rate. This systematic approach stimulates previously unaddressed attack surfaces, ensuring comprehensive detection and attack design while pairing each ATT&amp;CK Identifier with a tested defensive method, providing attack and defense tactics for prompt-response options. The main findings reveal that the attack method employs Single Upper Sideband Amplitude Modulatio
&lt;/p&gt;</description></item><item><title>&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.09241</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#32473;&#20986;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20363;&#23376;&#31359;&#36879;&#37027;&#20123;&#26080;&#27861;&#21033;&#29992;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09241
&lt;/p&gt;
&lt;p&gt;
&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#19979;&#38543;&#22788;&#21487;&#35265;&#30340;&#23433;&#20840;&#28431;&#27934;&#20013;&#65292;&#20445;&#25252;&#25968;&#25454;&#20813;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#21033;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21483;&#20570;&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#65288;UEs&#65289;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#22312;&#21407;&#22987;&#30340;&#24178;&#20928;&#20998;&#24067;&#19978;&#20934;&#30830;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; UEs &#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#26159;&#34394;&#20551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#21033;&#29992;&#20854;&#20182;&#26410;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#26469;&#21435;&#38500;&#20445;&#25252;&#65292;&#23558;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#36716;&#20026;&#21487;&#23398;&#20064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#23041;&#32961;&#65292;&#24341;&#20837;&#20102;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#65288;LEs&#65289;&#65292;&#36825;&#20123;&#26159;&#24050;&#32463;&#21435;&#38500;&#20445;&#25252;&#30340;UEs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#23558;UEs&#25237;&#23556;&#21040;LEs&#30340;&#27969;&#24418;&#19978;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;UEs&#36827;&#34892;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09126</link><description>&lt;p&gt;
&#30693;&#35782;&#36801;&#31227;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;: &#36716;&#31227;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer. (arXiv:2305.09126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\ell_1$-TCL&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#36801;&#31227;&#21644;Lasso&#22238;&#24402;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#30456;&#21516;&#30340;&#21327;&#21464;&#37327;&#65288;&#25110;&#29305;&#24449;&#65289;&#31354;&#38388;&#35774;&#32622;&#19979;&#36890;&#36807;&#30693;&#35782;&#36801;&#31227;&#26469;&#25552;&#39640;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#31934;&#24230;&#65292;&#21363;&#21516;&#31867;&#21035;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#65292;&#23558;&#20854;&#31216;&#20026;&#36716;&#31227;&#22240;&#26524;&#23398;&#20064;&#65288;TCL&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;$\ell_1$-TCL&#65292;&#20854;&#20013;&#21253;&#21547;$\ell_1$&#27491;&#21017;&#21270;TL&#26469;&#36827;&#34892;&#33510;&#20107;&#21442;&#25968;&#20272;&#35745;&#21644;&#19979;&#28216;&#25554;&#20214;ACE&#20272;&#35745;&#22120;&#65292;&#21253;&#25324;&#32467;&#26524;&#22238;&#24402;&#12289;&#36870;&#27010;&#29575;&#21152;&#26435;&#21644;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#20511;&#21161;&#20110;Lasso&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#24674;&#22797;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel problem of improving causal effect estimation accuracy with the help of knowledge transfer under the same covariate (or feature) space setting, i.e., homogeneous transfer learning (TL), is studied, referred to as the Transfer Causal Learning (TCL) problem. While most recent efforts in adapting TL techniques to estimate average causal effect (ACE) have been focused on the heterogeneous covariate space setting, those methods are inadequate for tackling the TCL problem since their algorithm designs are based on the decomposition into shared and domain-specific covariate spaces. To address this issue, we propose a generic framework called \texttt{$\ell_1$-TCL}, which incorporates $\ell_1$ regularized TL for nuisance parameter estimation and downstream plug-in ACE estimators, including outcome regression, inverse probability weighted, and doubly robust estimators. Most importantly, with the help of Lasso for high-dimensional regression, we establish non-asymptotic recovery guarantee
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35745;&#31639;&#30340;&#32570;&#38519;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#38146;&#37329;&#23646;&#22312;&#30005;&#27744;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06925</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Accurate Surface and Finite Temperature Bulk Properties of Lithium Metal at Large Scales using Machine Learning Interaction Potentials. (arXiv:2305.06925v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06925
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#23610;&#24230;&#19978;&#20934;&#30830;&#39044;&#27979;&#38146;&#37329;&#23646;&#34920;&#38754;&#21644;&#26377;&#38480;&#28201;&#24230;&#19979;&#30340;&#25209;&#37327;&#24615;&#36136;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35745;&#31639;&#30340;&#32570;&#38519;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#38146;&#37329;&#23646;&#22312;&#30005;&#27744;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#37329;&#23646;&#30340;&#24615;&#36136;&#26159;&#35774;&#35745;&#38146;&#31163;&#23376;&#21644;&#38146;&#37329;&#23646;&#30005;&#27744;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#30001;&#20110;&#38146;&#30340;&#39640;&#21453;&#24212;&#24615;&#21644;&#20302;&#29076;&#28857;&#20197;&#21450;&#38146;&#22312;&#30005;&#27744;&#20013;&#23384;&#22312;&#20110;&#24494;&#35266;&#23610;&#24230;&#19979;&#65292;&#24456;&#38590;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#25506;&#27979;&#12290;&#35745;&#31639;&#19978;&#65292;&#32570;&#23569;&#33021;&#22815;&#22312;&#25152;&#26377;&#24615;&#36136;&#19978;&#19968;&#33268;&#19988;&#23450;&#37327;&#20934;&#30830;&#30340;&#32463;&#39564;&#21183;&#65292;&#32780;&#20174;&#22836;&#35745;&#31639;&#30340;&#25104;&#26412;&#21448;&#22826;&#39640;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183; (MLIPs) &#23545;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770; (DFT) &#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22312;&#22823;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#19979;&#20197;&#29366;&#20917;-of-the-art &#31934;&#24230;&#22797;&#29616;&#23454;&#39564;&#21644;&#20174;&#22836;&#35745;&#31639;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20934;&#30830;&#39044;&#27979;&#28909;&#21147;&#23398;&#24615;&#36136;&#12289;&#22768;&#23376;&#20809;&#35889;&#12289;&#24377;&#24615;&#24120;&#25968;&#30340;&#28201;&#24230;&#20381;&#36182;&#24615;&#20197;&#21450;&#21508;&#31181;&#34920;&#38754;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#22312; DFT &#20013;&#26080;&#27861;&#33719;&#24471;&#12290;&#25105;&#20204;&#35748;&#20026;&#19981;&#21516;&#30340; DFT &#27867;&#20989;&#23384;&#22312;&#24494;&#22937;&#20294;&#26174;&#30528;&#30340;&#23450;&#37327;&#24046;&#24322;&#65292;&#24433;&#21709;&#20102;&#20851;&#38190;&#24615;&#36136;&#22914;&#34920;&#38754;&#33021;&#12290;&#36890;&#36807; MLIPs &#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#23545;&#38146;&#37329;&#23646;&#30340;&#22823;&#23610;&#24230;&#22810;&#23610;&#24230;&#27169;&#25311;&#65292;&#36825;&#26159;&#30740;&#31350;&#30005;&#27744;&#20013;&#38146;&#37329;&#23646;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The properties of lithium metal are key parameters in the design of lithium ion and lithium metal batteries. They are difficult to probe experimentally due to the high reactivity and low melting point of lithium as well as the microscopic scales at which lithium exists in batteries where it is found to have enhanced strength, with implications for dendrite suppression strategies. Computationally, there is a lack of empirical potentials that are consistently quantitatively accurate across all properties and ab-initio calculations are too costly. In this work, we train Machine Learning Interaction Potentials (MLIPs) on Density Functional Theory (DFT) data to state-of-the-art accuracy in reproducing experimental and ab-initio results across a wide range of simulations at large length and time scales. We accurately predict thermodynamic properties, phonon spectra, temperature dependence of elastic constants and various surface properties inaccessible using DFT. We establish that there exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04532</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36235;&#21183;&#65306;&#19968;&#20010;&#33539;&#22260;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latest Trends in Artificial Intelligence Technology: A Scoping Review. (arXiv:2305.04532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#26234;&#33021;&#25163;&#26426;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#31561;&#24212;&#29992;&#31243;&#24207;&#37117;&#21033;&#29992;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031; PRISMA &#26694;&#26550;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#30446;&#26631;&#26159;&#23547;&#25214;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30740;&#31350;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36873;&#21462;&#20102;&#19977;&#20010;&#30693;&#21517;&#26399;&#21002;&#65306;&#12298;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26434;&#24535;&#12299;&#12289;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26434;&#24535;&#12299;&#21644;&#12298;&#26426;&#22120;&#23398;&#20064;&#12299;&#65292;&#24182;&#35266;&#23519;&#20102;2022&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#19968;&#23450;&#30340;&#36164;&#26684;&#35201;&#27714;&#65306;&#25216;&#26415;&#24517;&#39035;&#38024;&#23545;&#21487;&#27604;&#36739;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#65292;&#24517;&#39035;&#20351;&#29992;&#20844;&#35748;&#25110;&#20854;&#20182;&#20805;&#20998;&#35777;&#26126;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#65292;&#24182;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2305.02749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#35299;&#37322;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#34892;&#21160;&#21487;&#33021;&#23545;&#26410;&#26469;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#32780;&#19981;&#39044;&#20808;&#30693;&#36947;&#29615;&#22659;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#21160;&#20316;&#30340;&#24433;&#21709;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#38142;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#34892;&#21160;&#26159;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#20302;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#35299;&#37322;&#24615;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;$k$&#20010;&#33218;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#24773;&#22659;&#12290;&#22312;&#20551;&#35774;&#29615;&#22659;&#36739;&#20026;&#28201;&#21644;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20851;&#20110;Learner's Losses $V_T$&#30340;&#20108;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#21644;&#20851;&#20110;&#26368;&#20339;&#31574;&#30053;$L_T^*$&#30340;&#19968;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#30340;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.00832</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
First- and Second-Order Bounds for Adversarial Linear Contextual Bandits. (arXiv:2305.00832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20801;&#35768;$k$&#20010;&#33218;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#24773;&#22659;&#12290;&#22312;&#20551;&#35774;&#29615;&#22659;&#36739;&#20026;&#28201;&#21644;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20851;&#20110;Learner's Losses $V_T$&#30340;&#20108;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#21644;&#20851;&#20110;&#26368;&#20339;&#31574;&#30053;$L_T^*$&#30340;&#19968;&#38454;&#25439;&#22833;&#20540;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#30340;&#24773;&#22659;&#65292;&#35813;&#24773;&#22659;&#20801;&#35768;&#19982;K&#20010;&#33218;&#30456;&#20851;&#32852;&#30340;&#25439;&#22833;&#20989;&#25968;&#38543;&#26102;&#38388;&#32780;&#33258;&#30001;&#21464;&#21270;&#12290; &#20551;&#35774;d&#32500;&#19978;&#19979;&#25991;&#20174;&#24050;&#30693;&#20998;&#24067;&#20013;&#32472;&#21046;&#65292;&#37027;&#20040;&#22312;T&#36718;&#28216;&#25103;&#26399;&#38388;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#36951;&#25022;&#23558;&#20197;$\tilde O(\sqrt{Kd T})$&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#22312;&#20551;&#35774;&#19978;&#19979;&#25991;&#30340;&#23494;&#24230;&#26159;&#23545;&#25968;&#20985;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20108;&#38454;&#30028;&#65292;&#20854;&#22312;&#32047;&#31215;&#25439;&#22833;&#30340;&#20108;&#27425;&#30697;$V_T$&#26041;&#38754;&#30340;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d V_T})$&#65292;&#20197;&#21450;&#19968;&#20010;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#19968;&#38454;&#30028;&#65292;&#20854;&#22312;&#26368;&#20339;&#31574;&#30053;&#30340;&#32047;&#31215;&#25439;&#22833;$L_T^*$&#26041;&#38754;&#30340;&#37327;&#32423;&#20026;$\tilde O(K\sqrt{d L_T^*})$&#12290;&#30001;&#20110;$V_T$&#25110;$L_T^*$&#21487;&#33021;&#26126;&#26174;&#23567;&#20110;$T$&#65292;&#22240;&#27492;&#27599;&#24403;&#29615;&#22659;&#30456;&#23545;&#28201;&#21644;&#26102;&#65292;&#20415;&#20250;&#25913;&#21892;&#26368;&#22351;&#24773;&#20917;&#30340;&#36951;&#25022;&#12290;&#26412;&#25991;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#31639;&#27861;&#30340;&#25130;&#26029;&#29256;&#26412;&#26469;&#33719;&#24471;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of $K$ arms to change over time without restriction. Assuming the $d$-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of $T$ rounds is known to scale as $\tilde O(\sqrt{Kd T})$. Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order $\tilde O(K\sqrt{d V_T})$ in terms of the cumulative second moment of the learner's losses $V_T$, and a closely related first-order bound of order $\tilde O(K\sqrt{d L_T^*})$ in terms of the cumulative loss of the best policy $L_T^*$. Since $V_T$ or $L_T^*$ may be significantly smaller than $T$, these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.00621</link><description>&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#30340;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proper Scoring Rules for Survival Analysis. (arXiv:2305.00621v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#20272;&#35745;&#26410;&#26469;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#20851;&#20110;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#22522;&#26412;&#29702;&#35770;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#20854;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#22235;&#31181;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#25193;&#23637;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#25193;&#23637;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#36825;&#20123;&#25193;&#23637;&#35780;&#20998;&#35268;&#21017;&#30340;&#20272;&#35745;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13534</link><description>&lt;p&gt;
&#29992;&#22343;&#22330;&#21338;&#24328;&#20026;&#29983;&#25104;&#27169;&#22411;&#25645;&#24314;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22343;&#22330;&#21338;&#24328;&#20316;&#20026;&#23454;&#39564;&#23460;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#19982;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#21516;&#26102;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22343;&#22330;&#21338;&#24328; (MFGs) &#20316;&#20026;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#12289;&#22686;&#24378;&#21644;&#35774;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102; MFGs &#19982;&#20027;&#35201;&#27969;&#21160;&#21644;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#21644;&#20195;&#20215;&#20989;&#25968;&#25512;&#23548;&#20102;&#36825;&#19977;&#20010;&#31867;&#21035;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30456;&#20851;&#30340; MFG &#30340;&#26368;&#20248;&#26465;&#20214;&#8212;&#8212;&#19968;&#32452;&#32806;&#21512;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#27599;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#21644;&#29305;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#21452;&#20154; MFG &#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#21512;&#25104;&#26679;&#26412;&#65292;&#21478;&#19968;&#20010;&#20195;&#29702;&#23545;&#26679;&#26412;&#36827;&#34892;&#35782;&#21035;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#19988;&#36924;&#30495;&#65292;&#21516;&#26102;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#21892;&#20102;&#35299;&#32544;&#32467;&#21644;&#20844;&#24179;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#31361;&#26174;&#20102; MFGs &#20316;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#39564;&#23460;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00354</link><description>&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;RL&#37319;&#29992;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#25512;&#26029;&#20219;&#21153;&#34920;&#31034;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#28982;&#21518;&#26681;&#25454;&#25512;&#26029;&#20986;&#30340;&#20219;&#21153;&#34920;&#31034;&#35843;&#25972;&#34892;&#21160;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65292;&#29305;&#21035;&#26159;OMRL&#20013;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#21487;&#33021;&#20250;&#36973;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#20581;&#30340;&#20219;&#21153;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#21033;&#29992;&#23548;&#33268;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#32047;&#31215;&#22238;&#25253;&#27604;&#22522;&#20934;&#26041;&#27861;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16524</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;BP&#26550;&#26500;&#30340;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification. (arXiv:2303.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29028;&#26609;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;ANN-BP&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#23558;&#26609;&#23376;&#31283;&#23450;&#24615;&#25193;&#23637;&#21040;&#22235;&#20010;&#31867;&#21035;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29028;&#26609;&#26159;&#30830;&#20445;&#22320;&#19979;&#30828;&#23721;&#30719;&#23665;&#23433;&#20840;&#30340;&#37325;&#35201;&#32467;&#26500;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#22320;&#19979;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#36827;&#34892;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#35780;&#20272;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#25351;&#26631;&#26159;&#23433;&#20840;&#31995;&#25968;&#65288;SF&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20351;&#29992;SF&#36827;&#34892;&#26609;&#23376;&#31283;&#23450;&#24615;&#35780;&#20272;&#26102;&#65292;&#24120;&#24120;&#20986;&#29616;&#28165;&#26224;&#30340;&#36793;&#30028;&#19981;&#21487;&#38752;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#65288;ANN-BP&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#22312;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#26609;&#23376;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#26377;&#19977;&#31181;ANN-BP&#65292;&#20998;&#21035;&#30001;&#20854;&#28608;&#27963;&#20989;&#25968;&#21306;&#20998;&#65306;ANN-BP ReLU&#12289;ANN-BP ELU&#21644;ANN-BP GELU&#12290;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;SF&#30340;&#36866;&#24212;&#24615;&#26469;&#32771;&#34385;&#26609;&#23376;&#30340;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26609;&#23376;&#31283;&#23450;&#24615;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#12289;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12289;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#22833;&#36133;&#21644;&#19981;&#20855;&#26377;&#36866;&#24403;&#30340;&#23433;&#20840;&#31995;&#25968;&#32780;&#23436;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pillars are important structural units used to ensure mining safety in underground hard rock mines. Therefore, precise predictions regarding the stability of underground pillars are required. One common index that is often used to assess pillar stability is the Safety Factor (SF). Unfortunately, such crisp boundaries in pillar stability assessment using SF are unreliable. This paper presents a novel application of Artificial Neural Network-Backpropagation (ANN-BP) and Deep Ensemble Learning for pillar stability classification. There are three types of ANN-BP used for the classification of pillar stability distinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, and ANN-BP GELU. This research also presents a new labeling alternative for pillar stability by considering its suitability with the SF. Thus, pillar stability is expanded into four categories: failed with a suitable safety factor, intact with a suitable safety factor, failed without a suitable safety factor, and in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.16458</link><description>&lt;p&gt;
&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65311;&#22522;&#20110;&#25968;&#25454;&#29983;&#25104;&#35270;&#35282;&#30340;&#22238;&#31572;&#65281;
&lt;/p&gt;
&lt;p&gt;
When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#65292;&#26088;&#22312;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20351;&#29992;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#65292;&#20294;&#36127;&#38754;&#36801;&#31227;&#26159;&#23558;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#20309;&#26102;&#39044;&#35757;&#32451;&#21644;&#22914;&#20309;&#39044;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#26080;&#35770;&#31574;&#30053;&#22914;&#20309;&#20808;&#36827;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#20173;&#28982;&#26080;&#27861;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;W2PGNN&#26469;&#22238;&#31572;&#20309;&#26102;&#39044;&#35757;&#32451;&#30340;&#20851;&#38190;&#38382;&#39064;&#65288;&#21363;&#25105;&#20204;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21487;&#20197;&#21033;&#29992;&#22270;&#39044;&#35757;&#32451;&#65289;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#36153;&#21147;&#30340;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#19979;&#28216;&#25968;&#25454;&#30340;&#22797;&#26434;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>NESS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08958</link><description>&lt;p&gt;
NESS&#65306;&#20174;&#38745;&#24577;&#23376;&#22270;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NESS: Learning Node Embeddings from Static SubGraphs. (arXiv:2303.08958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08958
&lt;/p&gt;
&lt;p&gt;
NESS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20351;&#29992;&#25972;&#20010;&#22270;&#25110;&#38543;&#26426;&#23376;&#22270;&#30340;&#24403;&#21069;&#33258;&#32534;&#30721;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#38745;&#24577;&#23376;&#22270;&#21152;&#19978;GAE&#25913;&#21892;&#20102;&#33410;&#28857;&#34920;&#31034;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;NESS&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20351;&#29992;&#38543;&#26426;&#36793;&#32536;&#25286;&#20998;&#65288;RES&#65289;&#23558;&#35757;&#32451;&#22270;&#21010;&#20998;&#20026;&#23376;&#22270;&#65292;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#65292;2&#65289;&#32858;&#21512;&#20174;&#27599;&#20010;&#23376;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#33719;&#24471;&#22270;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NESS&#25913;&#36827;&#20102;&#24191;&#27867;&#30340;&#22270;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#26368;&#26032;&#32467;&#26524;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework for learning Node Embeddings from Static Subgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using static subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph at test time. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MUX-PLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2302.12441</link><description>&lt;p&gt;
MUX-PLMs&#65306;&#39640;&#21534;&#21520;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
MUX-PLMs: Data Multiplexing for High-throughput Language Models. (arXiv:2302.12441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MUX-PLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#38656;&#27714;&#12290;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#25152;&#38656;&#30340;&#25512;&#26029;&#25104;&#26412;&#20197;&#21450;&#30828;&#20214;&#30701;&#32570;&#65292;&#38480;&#21046;&#20102;&#32463;&#27982;&#23454;&#24800;&#30340;&#35775;&#38382;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#21534;&#21520;&#37327;&#21644;&#39640;&#24615;&#33021;&#30340;&#25928;&#29575;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31639;&#27861;&#65288;&#20363;&#22914;&#25968;&#25454;&#22797;&#29992;&#65289;&#36890;&#36807;&#23545;&#22810;&#20010;&#36755;&#20837;&#25191;&#34892;&#25512;&#26029;&#65292;&#20197;&#21333;&#20010;&#36755;&#20837;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#22810;&#37325;&#21534;&#21520;&#37327;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30446;&#21069;&#30340;&#34920;&#29616;&#36824;&#19981;&#36275;&#20197;&#37096;&#32626;&#22312;&#29616;&#20195;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;MUX-PLMs&#65292;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#22797;&#29992;&#35757;&#32451;&#30340;&#39640;&#21534;&#21520;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#21487;&#20197;&#24494;&#35843;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#20135;&#29983;&#39640;&#21534;&#21520;&#37327;&#21644;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#22797;&#29992;&#21644;&#35299;&#22797;&#29992;&#27169;&#22359;&#33021;&#22815;&#26377;&#25928;&#22320;&#32416;&#32544;&#21644;&#35299;&#32544;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MIMO&#26679;&#24335;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#22810;&#20010;&#23618;&#38754;&#30340;&#40065;&#26834;&#24615;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#31532;&#19968;&#20010;&#22810;&#25915;&#20987;&#35780;&#20272;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35780;&#20272;&#20102;16&#20010;&#38450;&#24481;&#27169;&#22411;&#38024;&#23545;9&#31181;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#21644;20&#31181;&#19981;&#21516;&#25915;&#20987;&#24378;&#24230;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.10980</link><description>&lt;p&gt;
MultiRobustBench: &#23545;&#25239;&#22810;&#31181;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#22810;&#20010;&#23618;&#38754;&#30340;&#40065;&#26834;&#24615;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#31532;&#19968;&#20010;&#22810;&#25915;&#20987;&#35780;&#20272;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35780;&#20272;&#20102;16&#20010;&#38450;&#24481;&#27169;&#22411;&#38024;&#23545;9&#31181;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#21644;20&#31181;&#19981;&#21516;&#25915;&#20987;&#24378;&#24230;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#38450;&#24481;&#39046;&#22495;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#38450;&#24481;&#21333;&#19968;&#65288;&#36890;&#24120;&#26159;&#26377;&#30028;&#30340;Lp&#33539;&#25968;&#65289;&#25915;&#20987;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23545;&#21508;&#31181;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22810;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#23398;&#20064;&#22120;&#23545;&#27979;&#35797;&#26102;&#25915;&#20987;&#32773;&#30340;&#19981;&#21516;&#20102;&#35299;&#27700;&#24179;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26410;&#30693;&#25915;&#20987;&#21644;&#25915;&#20987;&#38598;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#25915;&#20987;&#35780;&#20272;&#30340;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35813;&#25490;&#34892;&#27036;&#33021;&#22815;&#25429;&#25417;&#25915;&#20987;&#31867;&#22411;&#21644;&#25915;&#20987;&#24378;&#24230;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;16&#20010;&#38450;&#24481;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#38024;&#23545;9&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#21253;&#25324;Lp&#33539;&#25968;&#23041;&#32961;&#27169;&#22411;&#12289;&#31354;&#38388;&#36716;&#25442;&#21644;&#39068;&#33394;&#25913;&#21464;&#31561;&#65292;&#22312;20&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#24378;&#24230;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65288;&#24635;&#20849;180&#27425;&#25915;&#20987;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#22810;&#25915;&#20987;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#29992;SE(3)&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#27969;&#36895;&#20272;&#35745;&#65292;&#36895;&#24230;&#24555;&#65292;&#20943;&#23569;&#20102;&#20351;&#29992;CFD&#27169;&#25311;&#30340;&#38656;&#35201;</title><link>http://arxiv.org/abs/2302.08780</link><description>&lt;p&gt;
SE(3)&#23545;&#31216;&#24615;&#35753;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#34880;&#27969;&#36895;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SE(3) symmetry lets graph neural networks learn arterial velocity estimation from small datasets. (arXiv:2302.08780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08780
&lt;/p&gt;
&lt;p&gt;
&#29992;SE(3)&#31561;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21160;&#33033;&#27969;&#36895;&#20272;&#35745;&#65292;&#36895;&#24230;&#24555;&#65292;&#20943;&#23569;&#20102;&#20351;&#29992;CFD&#27169;&#25311;&#30340;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#20013;&#30340;&#34880;&#28082;&#36895;&#24230;&#22330;&#21487;&#33021;&#26159;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26377;&#20215;&#20540;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#34880;&#27969;&#21160;&#21147;&#23398;&#27169;&#25311;&#38656;&#35201;&#19987;&#23478;&#30340;&#32454;&#33268;&#35774;&#32622;&#65292;&#32791;&#26102;&#19988;&#38590;&#20197;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#22823;&#35268;&#27169;&#25509;&#21463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#40657;&#30418;&#23376;&#20195;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#33033;&#27969;&#20307;&#20013;&#39030;&#28857;&#26144;&#23556;&#30340;3D&#36895;&#24230;&#22330;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21160;&#33033;&#27169;&#22411;&#21644;&#22522;&#20110;CFD&#30340;&#22320;&#38754;&#30495;&#23454;&#36895;&#24230;&#22330;&#23545;&#36825;&#20123;GNN&#36827;&#34892;&#22521;&#35757;&#12290;&#19968;&#26086;GNN&#35757;&#32451;&#23436;&#25104;&#65292;&#19982;CFD&#30456;&#27604;&#65292;&#21487;&#20197;36&#20493;&#21152;&#36895;&#33719;&#24471;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#21160;&#33033;&#30340;&#36895;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;SE(3)&#31561;&#21464;&#30340;GNN&#65292;&#23427;&#29420;&#31435;&#20110;&#36755;&#20837;&#32593;&#26684;&#30340;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#20943;&#23569;&#24517;&#35201;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Hemodynamic velocity fields in coronary arteries could be the basis of valuable biomarkers for diagnosis, prognosis and treatment planning in cardiovascular disease. Velocity fields are typically obtained from patient-specific 3D artery models via computational fluid dynamics (CFD). However, CFD simulation requires meticulous setup by experts and is time-intensive, which hinders large-scale acceptance in clinical practice. To address this, we propose graph neural networks (GNN) as an efficient black-box surrogate method to estimate 3D velocity fields mapped to the vertices of tetrahedral meshes of the artery lumen. We train these GNNs on synthetic artery models and CFD-based ground truth velocity fields. Once the GNN is trained, velocity estimates in a new and unseen artery can be obtained with 36-fold speed-up compared to CFD. We demonstrate how to construct an SE(3)-equivariant GNN that is independent of the spatial orientation of the input mesh and show how this reduces the necessar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#19968;&#20010;&#20998;&#24067;&#30340;&#22810;&#20010;&#20998;&#20301;&#25968;&#12290;&#23427;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#31169;&#26377;&#22320;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.06943</link><description>&lt;p&gt;
&#22810;&#20010;&#20998;&#20301;&#25968;&#30340;&#31169;&#26377;&#32479;&#35745;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Private Statistical Estimation of Many Quantiles. (arXiv:2302.06943v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#19968;&#20010;&#20998;&#24067;&#30340;&#22810;&#20010;&#20998;&#20301;&#25968;&#12290;&#23427;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#36890;&#36807;&#31169;&#26377;&#22320;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#26469;&#20272;&#35745;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#19979;&#20272;&#35745;&#35768;&#22810;&#32479;&#35745;&#20998;&#20301;&#25968;&#30340;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#32473;&#23450;&#19968;&#20010;&#20998;&#24067;&#24182;&#19988;&#33021;&#22815;&#35775;&#38382;&#26469;&#33258;&#20854;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29305;&#23450;&#28857;&#19978;&#20272;&#35745;&#20854;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#36870;&#20989;&#25968;&#65288;&#20998;&#20301;&#25968;&#20989;&#25968;&#65289;&#12290;&#20363;&#22914;&#65292;&#36825;&#39033;&#20219;&#21153;&#22312;&#31169;&#26377;&#25968;&#25454;&#29983;&#25104;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#31169;&#19979;&#20272;&#35745;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#20301;&#25968;&#65292;&#24182;&#23558;&#27492;&#32467;&#26524;&#29992;&#20316;&#20998;&#24067;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#22120;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102; Kaplan&#31561;&#20154;&#26368;&#36817;&#21457;&#34920;&#30340;&#36882;&#24402;&#20272;&#35745;&#20998;&#20301;&#25968;&#30340;&#38544;&#31169;&#31639;&#27861;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#36827;&#34892;&#22343;&#21248;&#38388;&#38548;&#20869;&#30340;&#20998;&#20301;&#25968;&#20989;&#25968;&#20272;&#35745;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24403;&#25105;&#20204;&#24819;&#35201;&#20272;&#35745;&#35768;&#22810;&#20998;&#20301;&#25968;&#26102;&#65292;&#26368;&#22909;&#20351;&#29992;&#31532;&#19968;&#31181;&#26041;&#27861;&#21333;&#29420;&#20272;&#35745;&#23427;&#20204;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#25105;&#20204;&#24819;&#35201;&#22312;&#22823;&#21306;&#38388;&#19978;&#20272;&#35745;&#20998;&#20301;&#25968;&#20989;&#25968;&#26102;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the estimation of many statistical quantiles under differential privacy. More precisely, given a distribution and access to i.i.d. samples from it, we study the estimation of the inverse of its cumulative distribution function (the quantile function) at specific points. For instance, this task is of key importance in private data generation. We present two different approaches. The first one consists in privately estimating the empirical quantiles of the samples and using this result as an estimator of the quantiles of the distribution. In particular, we study the statistical properties of the recently published algorithm introduced by Kaplan et al. 2022 that privately estimates the quantiles recursively. The second approach is to use techniques of density estimation in order to uniformly estimate the quantile function on an interval. In particular, we show that there is a tradeoff between the two methods. When we want to estimate many quantiles, it is better to estim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#36523;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#26368;&#20339;&#36866;&#21512;&#22359;&#30340;&#21069;&#36523;&#32452;&#21512;&#26469;&#25913;&#21892;&#26657;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06245</link><description>&lt;p&gt;
&#29992;&#21069;&#36523;&#31070;&#32463;&#32593;&#32476;&#26469;&#26657;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Calibrating a Deep Neural Network with Its Predecessors. (arXiv:2302.06245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06245
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21069;&#36523;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#26368;&#20339;&#36866;&#21512;&#22359;&#30340;&#21069;&#36523;&#32452;&#21512;&#26469;&#25913;&#21892;&#26657;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#32780;&#35328;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#8212;&#8212;&#21363;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#8212;&#8212;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#39564;&#35777;&#20102;&#26657;&#20934;&#19981;&#36275;&#19982;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#24120;&#29992;&#25216;&#26415;&#65292;&#26089;&#26399;&#20572;&#27490;&#19981;&#33021;&#22815;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26089;&#26399;&#20572;&#27490;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#20102;&#32593;&#32476;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#27599;&#20010;&#21333;&#29420;&#30340;&#27169;&#22359;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#8220;&#21069;&#36523;&#32452;&#21512;&#25628;&#32034;&#8221;&#65288;PCS&#65289;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#36866;&#21512;&#22359;&#30340;&#21069;&#36523;&#32452;&#21512;&#26469;&#25913;&#21892;&#26657;&#20934;&#12290;&#22359;&#30340;&#21069;&#36523;&#26159;&#20855;&#26377;&#36739;&#26089;&#35757;&#32451;&#38454;&#27573;&#30340;&#26435;&#37325;&#21442;&#25968;&#30340;&#30456;&#24212;&#32593;&#32476;&#22359;&#12290;PCS&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;PCS&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence calibration - the process to calibrate the output probability distribution of neural networks - is essential for safety-critical applications of such networks. Recent works verify the link between mis-calibration and overfitting. However, early stopping, as a well-known technique to mitigate overfitting, fails to calibrate networks. In this work, we study the limitions of early stopping and comprehensively analyze the overfitting problem of a network considering each individual block. We then propose a novel regularization method, predecessor combination search (PCS), to improve calibration by searching a combination of best-fitting block predecessors, where block predecessors are the corresponding network blocks with weight parameters from earlier training stages. PCS achieves the state-of-the-art calibration performance on multiple datasets and architectures. In addition, PCS improves model robustness under dataset distribution shift.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.04062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04062
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#20302;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27424;&#25311;&#21512;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#30417;&#31649;&#38382;&#39064;&#38590;&#20197;&#35775;&#38382;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20197;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26080;&#27861;&#20570;&#21040;&#30340;&#26041;&#24335;&#36827;&#34892;&#20849;&#20139;&#21644;&#20351;&#29992;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
&lt;/p&gt;</description></item><item><title>OPORP&#20351;&#29992;&#19968;&#31181;"&#35745;&#25968;&#33609;&#22270;"&#31867;&#22411;&#30340;&#25968;&#25454;&#38477;&#32500;/&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#65292;&#22312;&#20445;&#35777;&#36739;&#23569;&#30340;&#20449;&#24687;&#25439;&#22833;&#30340;&#21069;&#25552;&#19979;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#30340;&#25104;&#26412;</title><link>http://arxiv.org/abs/2302.03505</link><description>&lt;p&gt;
OPORP&#65306;&#19968;&#27425;&#32622;&#25442;+&#19968;&#27425;&#38543;&#26426;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
OPORP: One Permutation + One Random Projection. (arXiv:2302.03505v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03505
&lt;/p&gt;
&lt;p&gt;
OPORP&#20351;&#29992;&#19968;&#31181;"&#35745;&#25968;&#33609;&#22270;"&#31867;&#22411;&#30340;&#25968;&#25454;&#38477;&#32500;/&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#65292;&#22312;&#20445;&#35777;&#36739;&#23569;&#30340;&#20449;&#24687;&#25439;&#22833;&#30340;&#21069;&#25552;&#19979;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20004;&#20010;$D$&#32500;&#25968;&#25454;&#21521;&#37327;&#65288;&#20363;&#22914;&#23884;&#20837;&#65289;&#65306;$u, v$&#12290;&#22312;&#35768;&#22810;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#32034;&#65288;EBR&#65289;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;$D=256\sim 1024$&#24456;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;OPORP&#65288;&#19968;&#27425;&#32622;&#25442;+&#19968;&#27425;&#38543;&#26426;&#25237;&#24433;&#65289;&#20351;&#29992;&#19968;&#31181;&#8220;&#35745;&#25968;&#33609;&#22270;&#8221;&#31867;&#22411;&#30340;&#25968;&#25454;&#32467;&#26500;&#21464;&#20307;&#36827;&#34892;&#25968;&#25454;&#38477;&#32500;/&#21387;&#32553;&#12290;&#20351;&#29992;OPORP&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#25968;&#25454;&#21521;&#37327;&#36827;&#34892;&#32622;&#25442;&#12290;&#29983;&#25104;&#38543;&#26426;&#21521;&#37327;$r$&#65292;i.i.d. &#65292;&#28385;&#36275;&#65306;$E&#65288;r_i&#65289;=0&#65292;E&#65288;r_i^2&#65289;=1&#65292;E&#65288;r_i^3&#65289;=0&#65292;E&#65288;r_i^4&#65289;=s$&#12290;&#25105;&#20204;&#23558;$r$&#19982;&#25152;&#26377;&#32622;&#25442;&#25968;&#25454;&#21521;&#37327;&#30456;&#20056;&#65288;&#20316;&#20026;&#28857;&#31215;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;$D$&#21015;&#20998;&#25104;$k$&#20010;&#30456;&#31561;&#38271;&#24230;&#30340;&#31665;&#65288;bin&#65289;&#65292;&#24182;&#27719;&#24635;&#65288;&#21363;&#27714;&#21644;&#65289;&#27599;&#20010;&#31665;&#20013;&#30340;&#20540;&#20197;&#20174;&#27599;&#20010;&#25968;&#25454;&#21521;&#37327;&#20013;&#33719;&#21462;$k$&#20010;&#26679;&#26412;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#27493;&#39588;&#26159;&#23558;$k$&#20010;&#26679;&#26412;&#26631;&#20934;&#21270;&#20026;&#21333;&#20301;$l_2$&#33539;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20272;&#35745;&#26041;&#24046;&#26412;&#36136;&#19978;&#26159;&#65306;$(s-1)A + \frac{D-k}{D-1}\frac{1}{k}\left[ (1-\rho^2)^2 -2A\right]$&#65292;&#20854;&#20013;$A\geq 0$&#26159;&#25968;&#25454;&#65288;$u,v$&#65289;&#30340;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Consider two $D$-dimensional data vectors (e.g., embeddings): $u, v$. In many embedding-based retrieval (EBR) applications where the vectors are generated from trained models, $D=256\sim 1024$ are common. In this paper, OPORP (one permutation + one random projection) uses a variant of the ``count-sketch'' type of data structures for achieving data reduction/compression. With OPORP, we first apply a permutation on the data vectors. A random vector $r$ is generated i.i.d. with moments: $E(r_i) = 0, E(r_i^2)=1, E(r_i^3) =0, E(r_i^4)=s$. We multiply (as dot product) $r$ with all permuted data vectors. Then we break the $D$ columns into $k$ equal-length bins and aggregate (i.e., sum) the values in each bin to obtain $k$ samples from each data vector. One crucial step is to normalize the $k$ samples to the unit $l_2$ norm. We show that the estimation variance is essentially: $(s-1)A + \frac{D-k}{D-1}\frac{1}{k}\left[ (1-\rho^2)^2 -2A\right]$, where $A\geq 0$ is a function of the data ($u,v$)
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.03098</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;ially private&#65288;DP&#65289;&#31639;&#27861;&#30340;&#38544;&#31169;&#20272;&#35745;&#25216;&#26415;&#21487;&#29992;&#20110;&#19982;&#20998;&#26512;&#19978;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#22312;&#24050;&#30693;&#20998;&#26512;&#19978;&#30028;&#19981;&#32039;&#30340;&#24773;&#20917;&#19979;&#23454;&#39564;&#27979;&#37327;&#38544;&#31169;&#25439;&#22833;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25216;&#26415;&#36890;&#24120;&#23545;&#23545;&#25163;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#20013;&#38388;&#27169;&#22411;&#36845;&#20195;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#22810;&#27425;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#21315;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#22823;&#35268;&#27169;&#37096;&#32626;&#27492;&#31867;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#27425;&#8221;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#36816;&#34892;&#26399;&#38388;&#39640;&#25928;&#22320;&#23457;&#35745;&#25110;&#20272;&#35745;&#27169;&#22411;&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;DP&#31639;&#27861;&#65292;&#24182;&#30001;&#23454;&#39564;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20854;&#25552;&#20379;&#30340;&#20934;&#30830;&#38544;&#31169;&#25439;&#22833;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SE&#65288;3&#65289;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20351;&#29992;FrameDiff&#26694;&#26550;&#22312;&#22810;&#20010;&#26694;&#26550;&#19978;&#23398;&#20064;SE&#65288;3&#65289;&#31561;&#21464;&#20998;&#25968;&#65292;&#25104;&#21151;&#29983;&#25104;&#21487;&#35774;&#35745;&#30340;&#38271;&#36798;500&#20010;&#27688;&#22522;&#37240;&#30340;&#21333;&#20307;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.02277</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#30340;SE&#65288;3&#65289;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SE(3) diffusion model with application to protein backbone generation. (arXiv:2302.02277v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SE&#65288;3&#65289;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20351;&#29992;FrameDiff&#26694;&#26550;&#22312;&#22810;&#20010;&#26694;&#26550;&#19978;&#23398;&#20064;SE&#65288;3&#65289;&#31561;&#21464;&#20998;&#25968;&#65292;&#25104;&#21151;&#29983;&#25104;&#21487;&#35774;&#35745;&#30340;&#38271;&#36798;500&#20010;&#27688;&#22522;&#37240;&#30340;&#21333;&#20307;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26032;&#22411;&#34507;&#30333;&#36136;&#32467;&#26500;&#20173;&#28982;&#26159;&#29983;&#29289;&#21307;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#25361;&#25112;&#12290;&#22312;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#20013;&#65292;&#19968;&#20010;&#19977;&#32500;&#21018;&#24615;&#20307;&#65288;&#31216;&#20026;&#26694;&#26550;&#65289;&#19978;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#22312;&#33258;&#28982;&#30028;&#20013;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#26032;&#22411;&#21151;&#33021;&#34507;&#30333;&#20027;&#38142;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#20445;&#25345;&#21018;&#24615;&#36816;&#21160;&#30340;SE&#65288;3&#65289;&#25193;&#25955;&#19978;&#32570;&#20047;&#26126;&#30830;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26694;&#26550;&#25805;&#20316;&#20013;&#20445;&#25345;&#32676;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#22810;&#20010;&#26694;&#26550;&#19978;SE&#65288;3&#65289;&#19981;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;FrameDiff&#65292;&#26469;&#23398;&#20064;&#22810;&#20010;&#26694;&#26550;&#19978;SE&#65288;3&#65289;&#31561;&#21464;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21333;&#20307;&#32972;&#26223;&#29983;&#25104;&#19978;&#24212;&#29992;FrameDiff&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#29983;&#25104;&#21487;&#35774;&#35745;&#30340;&#21333;&#20307;&#32972;&#26223;&#65292;&#38271;&#36798;500&#20010;&#27688;&#22522;&#37240;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20043;&#21069;&#26041;&#27861;&#20013;&#24517;&#35201;&#30340;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#32593;&#32476;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;sa
&lt;/p&gt;
&lt;p&gt;
The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for learning the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#36827;&#34892;&#25512;&#26029;&#30340;&#24037;&#20316;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20272;&#35745;&#22120;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12553</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#30340;&#39640;&#32500;&#29305;&#24449;&#28176;&#36817;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Inference for Multi-Stage Stationary Treatment Policy with High Dimensional Features. (arXiv:2301.12553v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#36827;&#34892;&#25512;&#26029;&#30340;&#24037;&#20316;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20272;&#35745;&#22120;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#35268;&#21017;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#37327;&#36523;&#23450;&#21046;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#20989;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#19968;&#31867;&#37325;&#35201;&#30340;&#27835;&#30103;&#31574;&#30053;&#26159;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#65292;&#20854;&#20351;&#29992;&#30456;&#21516;&#30340;&#20915;&#31574;&#20989;&#25968;&#26469;&#25351;&#23450;&#27835;&#30103;&#20998;&#37197;&#27010;&#29575;&#65292;&#22312;&#20915;&#31574;&#26102;&#22522;&#20110;&#21516;&#26102;&#21253;&#25324;&#22522;&#32447;&#21464;&#37327;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#21644;&#26102;&#21464;&#21464;&#37327;&#65288;&#20363;&#22914;&#24120;&#35268;&#26816;&#27979;&#21040;&#30340;&#30142;&#30149;&#29983;&#29289;&#26631;&#24535;&#29289;&#65289;&#30340;&#19968;&#32452;&#29305;&#24449;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#25991;&#29486;&#23545;&#19982;&#21160;&#24577;&#27835;&#30103;&#31574;&#30053;&#30456;&#20851;&#30340;&#20215;&#20540;&#20989;&#25968;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#20294;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#39033;&#24037;&#20316;&#30340;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#22686;&#24378;&#30340;&#20498;&#25968;&#26435;&#37325;&#20272;&#35745;&#22120;&#20272;&#35745;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic treatment rules or policies are a sequence of decision functions over multiple stages that are tailored to individual features. One important class of treatment policies for practice, namely multi-stage stationary treatment policies, prescribe treatment assignment probabilities using the same decision function over stages, where the decision is based on the same set of features consisting of both baseline variables (e.g., demographics) and time-evolving variables (e.g., routinely collected disease biomarkers). Although there has been extensive literature to construct valid inference for the value function associated with the dynamic treatment policies, little work has been done for the policies themselves, especially in the presence of high dimensional feature variables. We aim to fill in the gap in this work. Specifically, we first estimate the multistage stationary treatment policy based on an augmented inverse probability weighted estimator for the value function to increase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#65292;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#65292;&#20462;&#27491;&#20266;&#36127;&#26679;&#26412;&#65292;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11673</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#33258;&#23398;&#20064;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Self-Supervised Contrastive Learning. (arXiv:2301.11673v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#65292;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#65292;&#20462;&#27491;&#20266;&#36127;&#26679;&#26412;&#65292;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24212;&#29992;&#65292;&#28982;&#32780;&#20854;&#33258;&#30417;&#30563;&#29256;&#26412;&#20173;&#23384;&#22312;&#35768;&#22810;&#28608;&#21160;&#20154;&#24515;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#36127;&#26679;&#26412;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#65292;&#22240;&#27492;&#38543;&#26426;&#36873;&#25321;&#30340;&#26679;&#26412;&#21487;&#33021;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#20266;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#35757;&#32451;&#19981;&#27491;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#8212;&#8212;BCL&#25439;&#22833;&#65292;&#23427;&#20173;&#28982;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#37325;&#35201;&#24615;&#26435;&#37325;&#20462;&#27491;&#23548;&#33268;&#30340;&#20559;&#24046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#35774;&#35745;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#65292;&#20174;&#32780;&#37319;&#26679;&#38590;&#20197;&#24471;&#21040;&#30340;&#30495;&#23454;&#36127;&#26679;&#26412;&#12290;&#31361;&#20986;&#20248;&#28857;&#22312;&#20110;&#25152;&#38656;&#30340;&#37319;&#26679;&#20998;&#24067;&#26159;&#19968;&#20010;&#21442;&#25968;&#32467;&#26500;&#65292;&#20854;&#20013;&#20855;&#26377;&#20301;&#32622;&#21442;&#25968;&#20197;&#32416;&#27491;&#20266;&#36127;&#26679;&#26412;&#20197;&#21450;&#20855;&#26377;&#27987;&#24230;&#21442;&#25968;&#20197;&#37319;&#30719;&#38590;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;BCL&#25439;&#22833;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#26041;&#27861;&#29992;&#20110;&#26680;&#31215;&#20998;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#26041;&#27861;&#65292;&#20351;&#24471;&#25552;&#39640;&#20102;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.09517</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#21644;&#26680;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Nystr\"om Approximation and Kernel Quadrature. (arXiv:2301.09517v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#26679;&#30340;Nystr&#246;m&#36924;&#36817;&#26041;&#27861;&#29992;&#20110;&#26680;&#31215;&#20998;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#26041;&#27861;&#65292;&#20351;&#24471;&#25552;&#39640;&#20102;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#19982;&#27010;&#29575;&#27979;&#37327;&#30456;&#20851;&#30340;&#27491;&#23450;&#26680;&#30340;Nystr&#246;m&#36924;&#36817;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#20256;&#32479;Nystr&#246;m&#36924;&#36817;&#22312;&#36830;&#32493;&#21306;&#38388;&#20013;&#20351;&#29992;i.i.d.&#25277;&#26679;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#65292;&#35777;&#26126;&#25216;&#24039;&#20511;&#37492;&#20102;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;Nystr&#246;m&#36924;&#36817;&#20013;&#30340;&#23376;&#31354;&#38388;&#31934;&#32454;&#36873;&#25321;&#65292;&#36825;&#26159;&#36866;&#29992;&#20110;&#38750;i.i.d.&#22320;&#26631;&#28857;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20984;&#26680;&#31215;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#20445;&#35777;&#20197;&#21450;&#25968;&#20540;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the Nystr\"om approximation of a positive definite kernel associated with a probability measure. We first prove an improved error bound for the conventional Nystr\"om approximation with i.i.d. sampling and singular-value decomposition in the continuous regime; the proof techniques are borrowed from statistical learning theory. We further introduce a refined selection of subspaces in Nystr\"om approximation with theoretical guarantees that is applicable to non-i.i.d. landmark points. Finally, we discuss their application to convex kernel quadrature and give novel theoretical guarantees as well as numerical observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#22270;&#32467;&#26500;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CDGS&#65289;&#26469;&#29983;&#25104;&#20998;&#23376;&#22270;&#65292;&#36890;&#36807;SDE&#26500;&#24314;&#27491;&#21521;&#22270;&#25193;&#25955;&#36807;&#31243;&#21644;ODE&#27714;&#35299;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2301.00427</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#22270;&#32467;&#26500;&#30340;&#20998;&#23376;&#22270;&#29983;&#25104;&#30340;&#26465;&#20214;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Based on Discrete Graph Structures for Molecular Graph Generation. (arXiv:2301.00427v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#22270;&#32467;&#26500;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CDGS&#65289;&#26469;&#29983;&#25104;&#20998;&#23376;&#22270;&#65292;&#36890;&#36807;SDE&#26500;&#24314;&#27491;&#21521;&#22270;&#25193;&#25955;&#36807;&#31243;&#21644;ODE&#27714;&#35299;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#22270;&#30340;&#28508;&#22312;&#20998;&#24067;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#22270;&#32467;&#26500;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CDGS&#65289;&#26469;&#29983;&#25104;&#20998;&#23376;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#22312;&#22270;&#32467;&#26500;&#21644;&#22266;&#26377;&#29305;&#24449;&#19978;&#26500;&#24314;&#27491;&#21521;&#22270;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#23548;&#20986;&#31163;&#25955;&#22270;&#32467;&#26500;&#20316;&#20026;&#21453;&#21521;&#29983;&#25104;&#36807;&#31243;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#28151;&#21512;&#22270;&#22122;&#22768;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#20013;&#38388;&#22270;&#29366;&#24577;&#20013;&#25552;&#21462;&#20840;&#23616;&#19978;&#19979;&#25991;&#21644;&#23616;&#37096;&#33410;&#28857;-&#36793;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#27714;&#35299;&#22120;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#37319;&#26679;&#65292;&#22522;&#20110;&#27969;&#37327;&#30340;&#21322;&#32447;&#24615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the underlying distribution of molecular graphs and generating high-fidelity samples is a fundamental research problem in drug discovery and material science. However, accurately modeling distribution and rapidly generating novel molecular graphs remain crucial and challenging goals. To accomplish these goals, we propose a novel Conditional Diffusion model based on discrete Graph Structures (CDGS) for molecular graph generation. Specifically, we construct a forward graph diffusion process on both graph structures and inherent features through stochastic differential equations (SDE) and derive discrete graph structures as the condition for reverse generative processes. We present a specialized hybrid graph noise prediction model that extracts the global context and the local node-edge dependency from intermediate graph states. We further utilize ordinary differential equation (ODE) solvers for efficient graph sampling, based on the semi-linear structure of the probability flow 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;KITMUS&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#22810;&#28304;&#30693;&#35782;&#36827;&#34892;&#25972;&#21512;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#22312;&#27979;&#35797;&#20013;&#30340;&#26680;&#24515;&#23376;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2212.08192</link><description>&lt;p&gt;
KITMUS&#27979;&#35797;&#65306;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#20013;&#22810;&#28304;&#30693;&#35782;&#25972;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources in Natural Language Understanding Systems. (arXiv:2212.08192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;KITMUS&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#23545;&#22810;&#28304;&#30693;&#35782;&#36827;&#34892;&#25972;&#21512;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#22312;&#27979;&#35797;&#20013;&#30340;&#26680;&#24515;&#23376;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#37117;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#21033;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#31867;&#21035;&#30340;&#25512;&#29702;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#38388;&#25552;&#20379;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#21253;&#21547;&#30340;&#32972;&#26223;&#30693;&#35782;&#20197;&#21450;&#29305;&#23450;&#23454;&#20363;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#28304;&#30693;&#35782;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#25972;&#21512;&#21644;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#25351;&#20195;&#28040;&#35299;&#23376;&#20219;&#21153;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#38656;&#35201;&#38024;&#23545;&#22810;&#20010;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#23376;&#20219;&#21153;&#22312;&#21738;&#20123;&#30693;&#35782;&#26469;&#28304;&#21253;&#21547;&#30456;&#20851;&#30340;&#20107;&#23454;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#25512;&#29702;&#26102;&#38388;&#20165;&#20351;&#29992;&#34394;&#26500;&#30693;&#35782;&#30340;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#26680;&#24515;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#20960;&#20010;&#27169;&#22411;&#38590;&#20197;&#23454;&#26102;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model's pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied. In this work, we propose a test suite of coreference resolution subtasks that require reasoning over multiple facts. These subtasks differ in terms of which knowledge sources contain the relevant facts. We also introduce subtasks where knowledge is present only at inference time using fictional knowledge. We evaluate state-of-the-art coreference resolution models on our dataset. Our results indicate that several models struggle to reason on-the-fly o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#20248;&#21270;&#21160;&#20316;&#35299;&#30721;&#30340;&#37327;&#23376;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#32463;&#20856;&#21518;&#22788;&#29702;&#65292;&#20351;&#20854;&#22312;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22810;&#39046;&#22495;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.06663</link><description>&lt;p&gt;
&#24102;&#26377;&#20248;&#21270;&#21160;&#20316;&#35299;&#30721;&#30340;&#37327;&#23376;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Policy Gradient Algorithm with Optimized Action Decoding. (arXiv:2212.06663v2 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#20248;&#21270;&#21160;&#20316;&#35299;&#30721;&#30340;&#37327;&#23376;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#32463;&#20856;&#21518;&#22788;&#29702;&#65292;&#20351;&#20854;&#22312;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22810;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#19978;&#30340;&#23454;&#29616;&#34987;&#35748;&#20026;&#26159;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26102;&#20195;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27010;&#24565;&#12290;&#38024;&#23545;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#34892;&#21160;&#35299;&#30721;&#31243;&#24207;&#65292;&#29992;&#20110;&#37327;&#23376;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#25152;&#38656;&#30340;&#32463;&#20856;&#21518;&#22788;&#29702;&#20197;&#36827;&#34892;&#34892;&#21160;&#36873;&#25321;&#65292;&#21463;&#21040;&#23616;&#37096;&#21644;&#20840;&#23616;&#37327;&#23376;&#27979;&#37327;&#30340;&#21551;&#21457;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;&#19968;&#20010;5&#37327;&#23376;&#20301;&#30828;&#20214;&#35774;&#22791;&#19978;&#25191;&#34892;&#20102;&#23436;&#25972;&#30340;&#35757;&#32451;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#24341;&#20837;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#30340;&#32463;&#20856;&#24320;&#38144;&#65292;&#24182;&#19988;&#26377;&#21487;&#33021;&#23558;&#22522;&#20110;VQC&#30340;&#31639;&#27861;&#25512;&#24191;&#21040;&#36229;&#20986;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning implemented by variational quantum circuits (VQCs) is considered a promising concept for the noisy intermediate-scale quantum computing era. Focusing on applications in quantum reinforcement learning, we propose a specific action decoding procedure for a quantum policy gradient approach. We introduce a novel quality measure that enables us to optimize the classical post-processing required for action selection, inspired by local and global quantum measurements. The resulting algorithm demonstrates a significant performance improvement in several benchmark environments. With this technique, we successfully execute a full training routine on a 5-qubit hardware device. Our method introduces only negligible classical overhead and has the potential to improve VQC-based algorithms beyond the field of quantum reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21551;&#29992;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#26412;&#22320;&#25968;&#25454;&#30340;&#33337;&#38431;&#33539;&#22260;&#23398;&#20064;&#65292;&#35299;&#20915;&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#25968;&#25454;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2212.03529</link><description>&lt;p&gt;
&#24320;&#21457;&#39118;&#21147;&#28065;&#36718;&#26426;&#26465;&#20214;&#20449;&#24687;&#30340;&#33337;&#38431;&#20849;&#20139;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning. (arXiv:2212.03529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21551;&#29992;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#26412;&#22320;&#25968;&#25454;&#30340;&#33337;&#38431;&#33539;&#22260;&#23398;&#20064;&#65292;&#35299;&#20915;&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#25968;&#25454;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21046;&#36896;&#21830;&#27599;&#22825;&#20174;&#33258;&#24049;&#30340;&#33337;&#38431;&#20013;&#25910;&#38598;&#20102;&#25968;&#21315;&#20806;&#23383;&#33410;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#23454;&#26102;&#20449;&#24687;&#65292;&#29992;&#20110;&#28065;&#36718;&#26426;&#20581;&#24247;&#35786;&#26029;&#12289;&#24615;&#33021;&#30417;&#27979;&#12289;&#39044;&#27979;&#32597;&#35265;&#25925;&#38556;&#21644;&#20851;&#38190;&#37096;&#20214;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39118;&#21147;&#28065;&#36718;&#26426;&#38431;&#30340;&#36825;&#20123;&#25968;&#25454;&#36130;&#23500;&#22240;&#21046;&#36896;&#21830;&#20986;&#20110;&#21830;&#19994;&#25112;&#30053;&#21407;&#22240;&#32780;&#26080;&#27861;&#34987;&#25805;&#20316;&#32773;&#12289;&#20844;&#29992;&#20107;&#19994;&#20844;&#21496;&#21644;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#12290;&#25968;&#25454;&#35775;&#38382;&#30340;&#32570;&#20047;&#22952;&#30861;&#20102;&#21033;&#29992;&#26426;&#20250;&#65292;&#22914;&#25913;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#28065;&#36718;&#26426;&#36816;&#32500;&#31574;&#30053;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#30041;&#22312;&#39118;&#21147;&#28065;&#36718;&#26426;&#19978;&#20197;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#20173;&#28982;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#23454;&#29616;&#33337;&#38431;&#33539;&#22260;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#31232;&#32570;&#20195;&#34920;&#24615;&#22521;&#35757;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terabytes of data are collected every day by wind turbine manufacturers from their fleets. The data contain valuable real-time information for turbine health diagnostics and performance monitoring, for predicting rare failures and the remaining service life of critical parts. And yet, this wealth of data from wind turbine fleets remains inaccessible to operators, utility companies, and researchers as manufacturing companies prefer the privacy of their fleets' turbine data for business strategic reasons. The lack of data access impedes the exploitation of opportunities, such as improving data-driven turbine operation and maintenance strategies and reducing downtimes. We present a distributed federated machine learning approach that leaves the data on the wind turbines to preserve the data privacy, as desired by manufacturers, while still enabling fleet-wide learning on those local data. We demonstrate in two case studies that wind turbines which are scarce in representative training dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#37327;&#21270;&#21518;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21464;&#24471;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02781</link><description>&lt;p&gt;
QEBVerif&#65306;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
QEBVerif: Quantization Error Bound Verification of Neural Networks. (arXiv:2212.02781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22312;&#37327;&#21270;&#21518;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21464;&#24471;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23454;&#38469;&#38480;&#21046;&#65292;&#37327;&#21270;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;DNN&#30340;&#26435;&#37325;&#21644;/&#25110;&#28608;&#27963;&#24352;&#37327;&#37327;&#21270;&#20026;&#36739;&#20302;&#20301;&#23485;&#30340;&#23450;&#28857;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#65292;&#38477;&#20302;&#20102;&#23545;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#36164;&#28304;&#35201;&#27714;&#65292;&#23613;&#31649;&#24050;&#32463;&#32463;&#39564;&#35777;&#26126;&#23427;&#20250;&#24341;&#20837;&#36731;&#24494;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#65292;&#20294;&#26159;&#22312;&#37327;&#21270;&#21518;&#65292;DNN&#30340;&#20851;&#38190;&#39564;&#35777;&#23646;&#24615;&#21487;&#33021;&#21464;&#24471;&#26080;&#25928;&#12290;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19987;&#27880;&#20110;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#25110;QNN&#65289;&#25110;&#37096;&#20998;&#37327;&#21270;&#30340;&#37327;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QEBVerif&#30340;&#37327;&#21270;&#35823;&#24046;&#36793;&#30028;&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#26435;&#37325;&#21644;&#28608;&#27963;&#24352;&#37327;&#37117;&#34987;&#37327;&#21270;&#20102;&#12290;QEBVerif&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#19981;&#21516;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65288;DRA&#65289;&#21644;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#39564;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To alleviate the practical constraints for deploying deep neural networks (DNNs) on edge devices, quantization is widely regarded as one promising technique. It reduces the resource requirements for computational power and storage space by quantizing the weights and/or activation tensors of a DNN into lower bit-width fixed-point numbers, resulting in quantized neural networks (QNNs). While it has been empirically shown to introduce minor accuracy loss, critical verified properties of a DNN might become invalid once quantized. Existing verification methods focus on either individual neural networks (DNNs or QNNs) or quantization error bound for partial quantization. In this work, we propose a quantization error bound verification method, named QEBVerif, where both weights and activation tensors are quantized. QEBVerif consists of two parts, i.e., a differential reachability analysis (DRA) and a mixed-integer linear programming (MILP) based verification method. DRA performs difference an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#25968;&#25454;&#20013;&#23616;&#37096;&#21270;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#32780;&#38750;&#30495;&#27491;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#35782;&#21035;&#21644;&#28040;&#38500;&#35821;&#20041;&#19981;&#30456;&#20851;&#30340;&#32447;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#38752;&#22320;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15510</link><description>&lt;p&gt;
&#26412;&#22320;&#21270;&#30340;&#24555;&#25463;&#26041;&#24335;&#31227;&#38500;
&lt;/p&gt;
&lt;p&gt;
Localized Shortcut Removal. (arXiv:2211.15510v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#25968;&#25454;&#20013;&#23616;&#37096;&#21270;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#32780;&#38750;&#30495;&#27491;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#35782;&#21035;&#21644;&#28040;&#38500;&#35821;&#20041;&#19981;&#30456;&#20851;&#30340;&#32447;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#38752;&#22320;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#39046;&#22495;&#65292;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#22312;&#23398;&#20064;&#25104;&#21151;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#31168;&#24182;&#19981;&#19968;&#23450;&#34920;&#31034;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#24615;&#25110;&#23398;&#20064;&#21040;&#20102;&#26377;&#24847;&#20041;&#30340;&#19996;&#35199;&#12290;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#23384;&#22312;&#26426;&#22120;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;-&#25968;&#25454;&#20013;&#19982;&#38382;&#39064;&#26080;&#20851;&#20294;&#20855;&#26377;&#39044;&#27979;&#24615;&#30340;&#29305;&#24449;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#23616;&#37096;&#21270;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#32780;&#38750;&#30495;&#27491;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;"&#38236;&#22836;"&#26469;&#26816;&#27979;&#21644;&#28040;&#38500;&#22270;&#20687;&#20013;&#39640;&#24230;&#39044;&#27979;&#20294;&#35821;&#20041;&#19978;&#19981;&#30456;&#20851;&#30340;&#32447;&#32034;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#38752;&#22320;&#35782;&#21035;&#24182;&#28040;&#38500;&#36825;&#31867;&#24555;&#25463;&#26041;&#24335;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#24847;&#20041;&#21644;&#27867;&#21270;&#24615;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is a data-driven field, and the quality of the underlying datasets plays a crucial role in learning success. However, high performance on held-out test data does not necessarily indicate that a model generalizes or learns anything meaningful. This is often due to the existence of machine learning shortcuts - features in the data that are predictive but unrelated to the problem at hand. To address this issue for datasets where the shortcuts are smaller and more localized than true features, we propose a novel approach to detect and remove them. We use an adversarially trained lens to detect and eliminate highly predictive but semantically unconnected clues in images. In our experiments on both synthetic and real-world data, we show that our proposed approach reliably identifies and neutralizes such shortcuts without causing degradation of model performance on clean data. We believe that our approach can lead to more meaningful and generalizable machine learning models, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21033;&#29992;&#27492;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35813;&#20915;&#31574;&#26641;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.09894</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Supervised Feature Compression based on Counterfactual Analysis. (arXiv:2211.09894v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#30417;&#30563;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21033;&#29992;&#27492;&#26041;&#27861;&#21487;&#20197;&#26500;&#24314;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35813;&#20915;&#31574;&#26641;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#25104;&#20026;&#20107;&#21518;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#35782;&#21035;&#39044;&#35757;&#32451;&#40657;&#30418;&#27169;&#22411;&#30340;&#37325;&#35201;&#20915;&#31574;&#36793;&#30028;&#12290;&#35813;&#20449;&#24687;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#19968;&#31181;&#21487;&#35843;&#25972;&#32454;&#24230;&#30340;&#29305;&#24449;&#30340;&#30417;&#30563;&#31163;&#25955;&#21270;&#12290;&#20351;&#29992;&#31163;&#25955;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#31867;&#20284;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20294;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations are becoming a de-facto standard in post-hoc interpretable machine learning. For a given classifier and an instance classified in an undesired class, its counterfactual explanation corresponds to small perturbations of that instance that allows changing the classification outcome. This work aims to leverage Counterfactual Explanations to detect the important decision boundaries of a pre-trained black-box model. This information is used to build a supervised discretization of the features in the dataset with a tunable granularity. Using the discretized dataset, an optimal Decision Tree can be trained that resembles the black-box model, but that is interpretable and compact. Numerical results on real-world datasets show the effectiveness of the approach in terms of accuracy and sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20018;&#35762;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#22312;&#28176;&#36827;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#32852;&#21512;&#24212;&#29992;&#65292;&#35777;&#23454;&#32452;&#21512;&#29305;&#24449;&#32423;&#21035;&#21644;&#39044;&#27979;&#32423;&#21035;&#30340;KD&#20250;&#24102;&#26469;&#26368;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20302;&#36164;&#28304;&#35774;&#22791;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.08161</link><description>&lt;p&gt;
&#20018;&#35762;&#21644;&#30693;&#35782;&#33976;&#39311;&#22312;&#35328;&#35821;&#29702;&#35299;&#30340;&#28176;&#36827;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding. (arXiv:2211.08161v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20018;&#35762;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#22312;&#28176;&#36827;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#32852;&#21512;&#24212;&#29992;&#65292;&#35777;&#23454;&#32452;&#21512;&#29305;&#24449;&#32423;&#21035;&#21644;&#39044;&#27979;&#32423;&#21035;&#30340;KD&#20250;&#24102;&#26469;&#26368;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20302;&#36164;&#28304;&#35774;&#22791;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28176;&#36827;&#23398;&#20064;&#26159;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#27169;&#22411;&#38543;&#30528;&#26102;&#38388;&#25509;&#25910;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#22312;&#20445;&#30041;&#20197;&#21069;&#20064;&#24471;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#28385;&#36275;&#36825;&#20004;&#20010;&#26399;&#26395;&#65292;&#20174;&#32780;&#23548;&#33268;&#25152;&#35859;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#31574;&#30053;&#20197;&#20943;&#36731;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#36951;&#24536;&#65292;&#20294;&#22312;&#28041;&#21450;&#35821;&#38899;&#30340;&#20219;&#21153;&#20013;&#65292;&#21364;&#32570;&#20047;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20018;&#35762;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26041;&#27861;&#22312;&#28176;&#36827;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#32852;&#21512;&#20351;&#29992;&#20197;&#36827;&#34892;&#35328;&#35821;&#29702;&#35299;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22810;&#20010;&#32423;&#21035;&#30340;KD&#32452;&#21512;&#65292;&#24182;&#26174;&#31034;&#32452;&#21512;&#29305;&#24449;&#32423;&#21035;&#21644;&#39044;&#27979;&#32423;&#21035;&#30340;KD&#20250;&#24102;&#26469;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#20018;&#35762;&#35760;&#24518;&#22823;&#23567;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20302;&#36164;&#28304;&#35774;&#22791;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#21516;&#26102;&#21253;&#25324;&#35757;&#32451;&#33539;&#24335;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24456;&#23569;&#30340;&#37319;&#26679;&#36845;&#20195;&#20135;&#29983;&#20986;&#32654;&#35266;&#30340;&#22270;&#20687;&#65292;&#20801;&#35768;&#36890;&#36807;&#26377;&#36259;&#30340;&#35843;&#21046;&#26041;&#24335;&#26469;&#35843;&#25972;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07292</link><description>&lt;p&gt;
&#37327;&#21270;&#28508;&#31354;&#38388;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#26032;&#22411;&#37319;&#26679;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces. (arXiv:2211.07292v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#21516;&#26102;&#21253;&#25324;&#35757;&#32451;&#33539;&#24335;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24456;&#23569;&#30340;&#37319;&#26679;&#36845;&#20195;&#20135;&#29983;&#20986;&#32654;&#35266;&#30340;&#22270;&#20687;&#65292;&#20801;&#35768;&#36890;&#36807;&#26377;&#36259;&#30340;&#35843;&#21046;&#26041;&#24335;&#26469;&#35843;&#25972;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25552;&#39640;&#65292;&#21253;&#25324;&#36136;&#37327;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#31561;&#26041;&#38754;&#30340;&#25552;&#39640;&#12290;&#29616;&#20195;&#25216;&#26415;&#20351;&#24471;&#29983;&#25104;&#39640;&#24230;&#22797;&#26434;&#30340;&#35270;&#35273;&#25928;&#26524;&#65292;&#25509;&#36817;&#20110;&#36924;&#30495;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36827;&#23637;&#30340;&#23454;&#29616;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#20174;&#32780;&#21152;&#21095;&#20102;&#39046;&#22495;&#20869;&#22806;&#20010;&#20154;&#20043;&#38388;&#30340;&#29702;&#35299;&#38556;&#30861;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24046;&#24322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#20102;&#35757;&#32451;&#33539;&#24335;&#21644;&#37319;&#26679;&#36807;&#31243;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#26174;&#33879;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24456;&#23569;&#30340;&#37319;&#26679;&#36845;&#20195;&#20135;&#29983;&#20986;&#32654;&#35266;&#30340;&#22270;&#20687;&#65292;&#20801;&#35768;&#37319;&#29992;&#26377;&#36259;&#30340;&#26041;&#24335;&#26469;&#35843;&#33410;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#25216;&#26415;&#25152;&#27809;&#26377;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#29616;&#19982;&#29616;&#26377;&#26041;&#27861;&#21487;&#27604;&#30340;&#32467;&#26524;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24050;&#32463;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements in the domain of text-to-image synthesis have culminated in a multitude of enhancements pertaining to quality, fidelity, and diversity. Contemporary techniques enable the generation of highly intricate visuals which rapidly approach near-photorealistic quality. Nevertheless, as progress is achieved, the complexity of these methodologies increases, consequently intensifying the comprehension barrier between individuals within the field and those external to it.  In an endeavor to mitigate this disparity, we propose a streamlined approach for text-to-image generation, which encompasses both the training paradigm and the sampling process. Despite its remarkable simplicity, our method yields aesthetically pleasing images with few sampling iterations, allows for intriguing ways for conditioning the model, and imparts advantages absent in state-of-the-art techniques. To demonstrate the efficacy of this approach in achieving outcomes comparable to existing works, we have t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;miCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#32467;&#26524;&#65292;&#24182;&#20026;&#26356;&#21152;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2211.04928</link><description>&lt;p&gt;
miCSE&#65306;&#29992;&#20110;&#23569;&#26679;&#26412;&#21477;&#23376;&#23884;&#20837;&#30340;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;miCSE&#26694;&#26550;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#32467;&#26524;&#65292;&#24182;&#20026;&#26356;&#21152;&#40065;&#26834;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;miCSE&#65292;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#21477;&#23376;&#23884;&#20837;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#23398;&#20064;&#26399;&#38388;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#35270;&#22270;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#36827;&#34892;&#23545;&#40784;&#12290;&#20351;&#29992;miCSE&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#21363;&#23545;&#27599;&#20010;&#21477;&#23376;&#30340;&#22686;&#24378;&#35270;&#22270;&#24378;&#21046;&#23454;&#26045;&#32467;&#26500;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#20351;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#19982;&#22810;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20840;&#26679;&#26412;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#27604;&#24403;&#21069;&#30340;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22686;&#24378;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65288;&#36830;&#32493;&#24615;&#12289;&#26234;&#33021;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.01315</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65306;&#22522;&#20110;&#22686;&#24378;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Addressing Data Distribution Shifts in Online Machine Learning Powered Smart City Applications Using Augmented Test-Time Adaptation. (arXiv:2211.01315v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01315
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22686;&#24378;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65288;&#36830;&#32493;&#24615;&#12289;&#26234;&#33021;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#22686;&#21152;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#39640;&#26114;&#19988;&#24615;&#33021;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65288;&#36830;&#32493;&#24615;&#12289;&#26234;&#33021;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#65289;&#30340;&#31995;&#32479;&#21270;&#20027;&#21160;&#24494;&#35843;&#65288;SAF&#65289;&#23618;&#26469;&#22686;&#24378;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#12290;SAF&#23618;&#33021;&#22815;&#36866;&#24212;&#32463;&#24120;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#24847;&#35782;&#21040;&#24494;&#35843;&#26159;&#19968;&#20010;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#36807;&#31243;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#26102;&#38388;&#36827;&#34892;&#65292;&#21516;&#26102;&#21442;&#19982;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#23454;&#38469;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#20154;&#26426;&#21327;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21487;&#38752;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data distribution shift is a common problem in machine learning-powered smart city applications where the test data differs from the training data. Augmenting smart city applications with online machine learning models can handle this issue at test time, albeit with high cost and unreliable performance. To overcome this limitation, we propose to endow test-time adaptation with a systematic active fine-tuning (SAF) layer that is characterized by three key aspects: a continuity aspect that adapts to ever-present data distribution shifts; intelligence aspect that recognizes the importance of fine-tuning as a distribution-shift-aware process that occurs at the appropriate time to address the recently detected data distribution shifts; and cost-effectiveness aspect that involves budgeted human-machine collaboration to make relabeling cost-effective and practical for diverse smart city applications. Our empirical results show that our proposed approach outperforms the traditional test-time a
&lt;/p&gt;</description></item><item><title>SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.15185</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#28210;&#26579;&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. (arXiv:2210.15185v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15185
&lt;/p&gt;
&lt;p&gt;
SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20855;&#26377;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22914;&#20309;&#20174;&#21407;&#22987;&#24863;&#23448;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#33258;&#21160;&#26377;&#25928;&#22320;&#24320;&#21457;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#20219;&#21153;&#65292;&#26159;&#38480;&#21046;MBRL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAM-RL&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#21033;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;SAM-RL&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#36890;&#36807;&#24863;&#30693;&#24863;&#30693;&#23398;&#20064;&#31649;&#36947;&#65292;SAM-RL&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#26469;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;&#19977;&#20010;&#25805;&#20316;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#35013;&#37197;&#65292;&#24037;&#20855;&#25805;&#32437;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;AdaMS&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#21442;&#25968;&#26367;&#25442;&#36229;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#25552;&#39640;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14564</link><description>&lt;p&gt;
AdaMS: &#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#22312;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination. (arXiv:2210.14564v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;AdaMS&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#21442;&#25968;&#26367;&#25442;&#36229;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#25552;&#39640;&#22768;&#23398;&#21333;&#35789;&#36776;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#25439;&#22833;&#20989;&#25968;&#37319;&#29992;&#23545;&#25968;&#21644;&#25351;&#25968;&#24418;&#24335;&#65292;&#38656;&#35201;&#20351;&#29992;&#36793;&#30028;&#21644;&#23610;&#24230;&#31561;&#36229;&#21442;&#25968;&#12290;&#30001;&#20110;&#27599;&#20010;&#25968;&#25454;&#31867;&#37117;&#20855;&#26377;&#22266;&#26377;&#29305;&#24449;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36793;&#30028;&#26469;&#23398;&#20064;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#27809;&#26377;&#20851;&#20110;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#35748;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#35813;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36793;&#30028;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AdaMS&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36793;&#30028;&#21644;&#23610;&#24230;&#30340;&#36229;&#21442;&#25968;&#34987;&#26367;&#25442;&#20026;&#27599;&#20010;&#31867;&#30340;&#33258;&#36866;&#24212;&#36793;&#30028;&#21644;&#33258;&#36866;&#24212;&#23610;&#24230;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#21326;&#23572;&#34903;&#26085;&#25253;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21333;&#35789;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#34920;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent loss functions in deep metric learning are expressed with logarithmic and exponential forms, and they involve margin and scale as essential hyper-parameters. Since each data class has an intrinsic characteristic, several previous works have tried to learn embedding space close to the real distribution by introducing adaptive margins. However, there was no work on adaptive scales at all. We argue that both margin and scale should be adaptively adjustable during the training. In this paper, we propose a method called Adaptive Margin and Scale (AdaMS), where hyper-parameters of margin and scale are replaced with learnable parameters of adaptive margins and adaptive scales for each class. Our method is evaluated on Wall Street Journal dataset, and we achieve outperforming results for word discrimination tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2210.12760</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#21487;&#38752;&#21644;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#65292;&#26420;&#32032;&#30340;&#24230;&#37327;&#26041;&#27861;(&#22914;&#26368;&#21518;&#19968;&#23618;&#20998;&#25968;)&#24050;&#32463;&#34987;&#24191;&#20026;&#20154;&#30693;&#22320;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#28201;&#24230;&#32553;&#25918;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#36125;&#21494;&#26031;&#22788;&#29702;&#65292;&#20197;&#32531;&#35299;&#36807;&#24230;&#33258;&#20449;&#65292;&#36890;&#24120;&#36890;&#36807;&#25968;&#20540;&#35266;&#23519;&#25903;&#25345;&#23427;&#20204;&#20135;&#29983;&#26356;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#23398;&#21487;&#22788;&#29702;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#24120;&#35265;&#19981;&#30830;&#23450;&#24230;&#37327;&#20043;&#38388;&#30340;&#23574;&#38160;&#27604;&#36739;&#65306;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25259;&#38706;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#19982;&#36807;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#30340;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#12290;&#36825;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#30340;&#26657;&#20934;&#26159;&#33391;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2210.09903</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Convex Optimization with Unbounded Memory. (arXiv:2210.09903v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#30340;&#25439;&#22833;&#19981;&#20165;&#21462;&#20915;&#20110;&#24403;&#21069;&#30340;&#20915;&#31574;&#65292;&#36824;&#21462;&#20915;&#20110;&#30452;&#21040;&#37027;&#20010;&#26102;&#38388;&#28857;&#30340;&#25152;&#26377;&#20915;&#31574;&#21382;&#21490;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;OCO&#30340;&#25193;&#23637;&#26694;&#26550;&#65292;&#8220;&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#8221;&#65292;&#26469;&#25429;&#25417;&#23545;&#36807;&#21435;&#20915;&#31574;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#65292;$H_p$&#65292;&#23427;&#37327;&#21270;&#20102;$p$&#38454;&#24433;&#21709;&#30340;&#26368;&#22823;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#36777;&#35770;&#26694;&#26550;&#65292;&#23558;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#24314;&#27169;&#25104;&#19968;&#20010;&#22810;&#20154;&#24207;&#36143;&#38646;&#21644;&#36777;&#35770;&#28216;&#25103;&#65292;&#36890;&#36807;&#25910;&#38598;&#20998;&#31867;&#22120;&#28508;&#22312;&#31354;&#38388;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#35299;&#37322;&#20998;&#31867;&#22120;&#23545;&#20854;&#39044;&#27979;&#30340;&#20869;&#37096;&#25512;&#29702;&#12290;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#65292;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2210.09015</link><description>&lt;p&gt;
&#29992;&#35270;&#35273;&#36777;&#35770;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explaining Image Classification with Visual Debates. (arXiv:2210.09015v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#36777;&#35770;&#26694;&#26550;&#65292;&#23558;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#24314;&#27169;&#25104;&#19968;&#20010;&#22810;&#20154;&#24207;&#36143;&#38646;&#21644;&#36777;&#35770;&#28216;&#25103;&#65292;&#36890;&#36807;&#25910;&#38598;&#20998;&#31867;&#22120;&#28508;&#22312;&#31354;&#38388;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#35299;&#37322;&#20998;&#31867;&#22120;&#23545;&#20854;&#39044;&#27979;&#30340;&#20869;&#37096;&#25512;&#29702;&#12290;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#65292;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#65292;&#20154;&#20204;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#30475;&#24453;&#21516;&#19968;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#23558;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#25512;&#29702;&#24314;&#27169;&#25104;&#19968;&#20010;&#22810;&#20154;&#24207;&#36143;&#38646;&#21644;&#36777;&#35770;&#28216;&#25103;&#65292;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#23545;&#27604;&#24615;&#65292;&#40723;&#21169;&#36777;&#25163;&#25552;&#20986;&#22810;&#26679;&#24615;&#30340;&#35266;&#28857;&#65292;&#25366;&#25496;&#23545;&#25163;&#24573;&#30053;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#24182;&#31361;&#20986;&#20998;&#31867;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#36777;&#25163;&#20174;&#20998;&#31867;&#22120;&#30340;&#31163;&#25955;&#21270;&#28508;&#22312;&#30693;&#35782;&#20013;&#25552;&#20986;&#35770;&#28857;&#65292;&#25903;&#25345;&#25110;&#21453;&#23545;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#30001;&#27492;&#24418;&#25104;&#30340;&#35270;&#35273;&#36777;&#35770;&#25910;&#38598;&#20102;&#20998;&#31867;&#22120;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25903;&#25345;&#21644;&#21453;&#23545;&#29305;&#24449;&#65292;&#20026;&#20998;&#31867;&#22120;&#23545;&#20854;&#39044;&#27979;&#30340;&#20869;&#37096;&#25512;&#29702;&#25552;&#20379;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective way to obtain different perspectives on any given topic is by conducting a debate, where participants argue for and against the topic. Here, we propose a novel debate framework for understanding and explaining a continuous image classifier's reasoning for making a particular prediction by modeling it as a multiplayer sequential zero-sum debate game. The contrastive nature of our framework encourages players to learn to put forward diverse arguments during the debates, picking up the reasoning trails missed by their opponents and highlighting any uncertainties in the classifier. Specifically, in our proposed setup, players propose arguments, drawn from the classifier's discretized latent knowledge, to support or oppose the classifier's decision. The resulting Visual Debates collect supporting and opposing features from the discretized latent space of the classifier, serving as explanations for the internal reasoning of the classifier towards its predictions. We demonstrate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.04325</link><description>&lt;p&gt;
ASDOT&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models. (arXiv:2210.04325v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASDOT&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#25110;&#27809;&#26377;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#20854;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#39046;&#22495;&#65288;&#22914;&#37329;&#34701; vs &#36816;&#21160;&#65289;&#25110;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;&#19981;&#21516;&#30340;&#35859;&#35789;&#65289;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#30340;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#21040;&#28040;&#38500;&#27495;&#20041;&#21644;&#25551;&#36848;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#38382;&#39064;&#24448;&#24448;&#38754;&#20020;&#30528;&#21508;&#31181;&#19981;&#36275;&#26679;&#26412;&#30340;&#38382;&#39064;&#65306;&#21487;&#33021;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25110;&#26681;&#26412;&#27809;&#26377;&#35757;&#32451;&#26679;&#26412;&#65292;&#25110;&#38656;&#35201;&#20381;&#36182;&#20110;&#19981;&#21516;&#39046;&#22495;&#25110;&#26550;&#26500;&#30340;&#26679;&#20363;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Any-Shot Data-to-Text (ASDOT)&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20219;&#20309;&#32473;&#23450;&#65288;&#25110;&#27809;&#26377;&#65289;&#26679;&#26412;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;ASDOT&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#25968;&#25454;&#28040;&#27495;&#21644;&#21477;&#23376;&#34701;&#21512;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#25968;&#25454;&#28040;&#27495;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#24335;GPT-3&#27169;&#22411;&#26469;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27169;&#31946;&#19977;&#20803;&#32452;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21487;&#29992;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#20197;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;de Finetti&#23450;&#29702;&#30340;HMM&#32622;&#20449;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#26080;&#20132;&#25442;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#23558;&#38750;&#20132;&#25442;&#24615;&#30340;&#25968;&#25454;&#20998;&#25104;&#21487;&#20132;&#25442;&#22359;&#65292;&#20445;&#35777;&#20102;&#29702;&#35770;&#19978;&#30340;&#32622;&#20449;&#39044;&#27979;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02271</link><description>&lt;p&gt;
&#22522;&#20110;de Finetti&#23450;&#29702;&#30340;HMM&#32622;&#20449;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extending Conformal Prediction to Hidden Markov Models with Exact Validity via de Finetti's Theorem for Markov Chains. (arXiv:2210.02271v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;de Finetti&#23450;&#29702;&#30340;HMM&#32622;&#20449;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#26080;&#20132;&#25442;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#23558;&#38750;&#20132;&#25442;&#24615;&#30340;&#25968;&#25454;&#20998;&#25104;&#21487;&#20132;&#25442;&#22359;&#65292;&#20445;&#35777;&#20102;&#29702;&#35770;&#19978;&#30340;&#32622;&#20449;&#39044;&#27979;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#39044;&#27979;&#26159;&#37327;&#21270;&#20998;&#31867;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#35201;&#27714;&#25968;&#25454;&#20855;&#26377;&#20132;&#25442;&#24615;&#12290;&#26412;&#25991;&#23558;&#32622;&#20449;&#39044;&#27979;&#25512;&#24191;&#21040;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#25968;&#25454;&#26080;&#20132;&#25442;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;Diaconis&#21644;Freedman&#65288;1980&#65289;&#21457;&#29616;&#30340;de Finetti&#23450;&#29702;&#23558;HMM&#20013;&#38750;&#20132;&#25442;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#20998;&#25104;&#21487;&#20132;&#25442;&#30340;&#22359;&#12290;&#20132;&#25442;&#22359;&#30340;&#25490;&#21015;&#34987;&#35270;&#20026;&#35266;&#23519;&#21040;&#30340;HMM&#25968;&#25454;&#30340;&#38543;&#26426;&#21270;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#32463;&#20856;&#32622;&#20449;&#39044;&#27979;&#26694;&#26550;&#22312;&#21487;&#20132;&#25442;&#21644;&#39532;&#23572;&#21487;&#22827;&#35774;&#32622;&#19979;&#30340;&#25152;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#35813;&#26041;&#27861;&#23558;&#39532;&#23572;&#21487;&#22827;&#26679;&#26412;&#24341;&#20837;&#30340;&#32570;&#20047;&#20132;&#25442;&#24615;&#35270;&#20026;&#19968;&#31181;&#20551;&#35774;&#36829;&#21453;&#65292;&#20294;&#20445;&#35777;&#20102;&#20855;&#26377;&#20132;&#25442;&#22359;&#30340;&#25968;&#25454;&#30340;&#21516;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a widely used method to quantify the uncertainty of a classifier under the assumption of exchangeability (e.g., IID data). We generalize conformal prediction to the Hidden Markov Model (HMM) framework where the assumption of exchangeability is not valid. The key idea of the proposed method is to partition the non-exchangeable Markovian data from the HMM into exchangeable blocks by exploiting the de Finetti's Theorem for Markov Chains discovered by Diaconis and Freedman (1980). The permutations of the exchangeable blocks are viewed as randomizations of the observed Markovian data from the HMM. The proposed method provably retains all desirable theoretical guarantees offered by the classical conformal prediction framework in both exchangeable and Markovian settings. In particular, while the lack of exchangeability introduced by Markovian samples constitutes a violation of a crucial assumption for classical conformal prediction, the proposed method views it as an a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>DiGress&#26159;&#19968;&#31181;&#33021;&#29983;&#25104;&#24102;&#26377;&#20998;&#31867;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#23376;&#21644;&#38750;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#27700;&#24179;&#65292;&#24182;&#22312;&#24179;&#38754;&#22270;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;3&#20493;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#36824;&#26159;&#31532;&#19968;&#20010;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411; GuacaMol &#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.14734</link><description>&lt;p&gt;
DiGress&#65306;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#22270;
&lt;/p&gt;
&lt;p&gt;
DiGress: Discrete Denoising diffusion for graph generation. (arXiv:2209.14734v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14734
&lt;/p&gt;
&lt;p&gt;
DiGress&#26159;&#19968;&#31181;&#33021;&#29983;&#25104;&#24102;&#26377;&#20998;&#31867;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#23376;&#21644;&#38750;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#27700;&#24179;&#65292;&#24182;&#22312;&#24179;&#38754;&#22270;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;3&#20493;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#36824;&#26159;&#31532;&#19968;&#20010;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411; GuacaMol &#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411; DiGress&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;&#20998;&#31867;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#65292;&#36890;&#36807;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#32536;&#21644;&#26356;&#25913;&#31867;&#21035;&#65292;&#36880;&#27493;&#32534;&#36753;&#24102;&#22122;&#22768;&#30340;&#22270;&#24418;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#24418;&#36716;&#25442;&#32593;&#32476;&#26469;&#21453;&#36716;&#27492;&#36807;&#31243;&#65292;&#23558;&#22270;&#24418;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#31995;&#21015;&#33410;&#28857;&#21644;&#36793;&#32536;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20445;&#30041;&#25193;&#25955;&#26399;&#38388;&#33410;&#28857;&#21644;&#36793;&#32536;&#31867;&#22411;&#36793;&#38469;&#20998;&#24067;&#30340;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#27169;&#22411;&#20197;&#21450;&#34701;&#21512;&#36741;&#21161;&#22270;&#24418;&#29305;&#24449;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#26679;&#26412;&#36136;&#37327;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#24418;&#32423;&#21035;&#29305;&#24449;&#19978;&#36827;&#34892;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;DiGress&#22312;&#20998;&#23376;&#21644;&#38750;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#24179;&#38754;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#25552;&#39640;&#20102;3&#20493;&#12290;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411; GuacaMol &#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset contain
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#36866;&#24212;&#23618;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#22495;&#36716;&#31227;&#65292;&#21487;&#36866;&#24212;&#21508;&#31181;&#26410;&#30693;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2209.11820</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#25193;&#23637;&#34892;&#20026;&#39044;&#27979;&#30340;&#37096;&#32626;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Expanding the Deployment Envelope of Behavior Prediction via Adaptive Meta-Learning. (arXiv:2209.11820v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#36866;&#24212;&#23618;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#22495;&#36716;&#31227;&#65292;&#21487;&#36866;&#24212;&#21508;&#31181;&#26410;&#30693;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#34892;&#20026;&#39044;&#27979;&#26041;&#27861;&#22312;&#23454;&#38469;&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#37096;&#32626;&#65292;&#20363;&#22914;&#22312;&#21830;&#19994;&#21270;&#36816;&#33829;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#38431;&#20013;&#24320;&#22987;&#22312;&#19990;&#30028;&#21508;&#22823;&#22478;&#24066;&#20013;&#36816;&#34892;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#39044;&#27979;&#31995;&#32479;&#37117;&#19987;&#38376;&#35774;&#35745;&#20110;&#19968;&#32452;&#32463;&#36807;&#28145;&#20837;&#25506;&#32034;&#30340;&#22320;&#29702;&#21306;&#22495;&#25110;&#25805;&#20316;&#35774;&#35745;&#22495;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37096;&#32626;&#21040;&#20854;&#20182;&#22478;&#24066;&#12289;&#22269;&#23478;&#25110;&#27954;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20803;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#36125;&#21494;&#26031;&#22238;&#24402;&#65292;&#36890;&#36807;&#22686;&#21152;&#33258;&#36866;&#24212;&#23618;&#65292;&#20351;&#24471;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#22495;&#36716;&#31227;&#65292;&#26080;&#35770;&#26159;&#31163;&#32447;&#24494;&#35843;&#12289;&#22312;&#32447;&#36866;&#24212;&#25110;&#20004;&#32773;&#32467;&#21512;&#12290;&#22312;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;&#26410;&#30693;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based behavior prediction methods are increasingly being deployed in real-world autonomous systems, e.g., in fleets of self-driving vehicles, which are beginning to commercially operate in major cities across the world. Despite their advancements, however, the vast majority of prediction systems are specialized to a set of well-explored geographic regions or operational design domains, complicating deployment to additional cities, countries, or continents. Towards this end, we present a novel method for efficiently adapting behavior prediction models to new environments. Our approach leverages recent advances in meta-learning, specifically Bayesian regression, to augment existing behavior prediction models with an adaptive layer that enables efficient domain transfer via offline fine-tuning, online adaptation, or both. Experiments across multiple real-world datasets demonstrate that our method can efficiently adapt to a variety of unseen environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#38236;&#38754;&#20809;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#38236;&#38754;&#20809;&#20135;&#29983;&#23545;&#25239;&#24615;&#25200;&#21160;&#20197;&#23454;&#29616;&#23545;&#20808;&#36827;DNN&#30340;&#38544;&#31192;&#21644;&#33258;&#28982;&#30340;&#25915;&#20987;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.11739</link><description>&lt;p&gt;
&#23545;DNN&#30340;&#25915;&#20987;&#30340;&#26377;&#25928;&#12289;&#38544;&#31192;&#21644;&#24378;&#22823;&#30340;&#29289;&#29702;&#19990;&#30028;&#25915;&#20987;&#65306;&#23545;&#25239;&#24615;&#38236;&#38754;&#20809;
&lt;/p&gt;
&lt;p&gt;
Adversarial Catoptric Light: An Effective, Stealthy and Robust Physical-World Attack to DNNs. (arXiv:2209.11739v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#38236;&#38754;&#20809;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#38236;&#38754;&#20809;&#20135;&#29983;&#23545;&#25239;&#24615;&#25200;&#21160;&#20197;&#23454;&#29616;&#23545;&#20808;&#36827;DNN&#30340;&#38544;&#31192;&#21644;&#33258;&#28982;&#30340;&#25915;&#20987;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24322;&#24120;&#30340;&#25104;&#21151;&#65292;&#20984;&#26174;&#20102;&#35780;&#20272;&#20808;&#36827;DNN&#30340;&#31283;&#20581;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20351;&#29992;&#36148;&#32440;&#20316;&#20026;&#29289;&#29702;&#25200;&#21160;&#27450;&#39575;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#38544;&#31192;&#24615;&#21644;&#21360;&#21047;&#25439;&#22833;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#29289;&#29702;&#25915;&#20987;&#26041;&#38754;&#30340;&#36827;&#23637;&#21033;&#29992;&#20102;&#20809;&#26463;&#65288;&#22914;&#28608;&#20809;&#22120;&#21644;&#25237;&#24433;&#20202;&#65289;&#25191;&#34892;&#25915;&#20987;&#65292;&#25152;&#29983;&#25104;&#30340;&#20809;&#23398;&#22270;&#26696;&#26159;&#20154;&#36896;&#30340;&#65292;&#32780;&#19981;&#26159;&#33258;&#28982;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#21363;&#23545;&#25239;&#24615;&#38236;&#38754;&#20809;&#65288;AdvCL&#65289;&#65292;&#20854;&#20013;&#20351;&#29992;&#24120;&#35265;&#30340;&#33258;&#28982;&#29616;&#35937;&#8212;&#8212;&#38236;&#38754;&#20809;&#20135;&#29983;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#20197;&#22312;&#40657;&#30418;&#27169;&#24335;&#19979;&#23454;&#29616;&#20808;&#36827;DNN&#30340;&#38544;&#31192;&#21644;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;&#26041;&#38754;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65306;&#26377;&#25928;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#33719;&#24471;&#30340;&#23450;&#37327;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated exceptional success across various tasks, underscoring the need to evaluate the robustness of advanced DNNs. However, traditional methods using stickers as physical perturbations to deceive classifiers present challenges in achieving stealthiness and suffer from printing loss. Recent advancements in physical attacks have utilized light beams such as lasers and projectors to perform attacks, where the optical patterns generated are artificial rather than natural. In this study, we introduce a novel physical attack, adversarial catoptric light (AdvCL), where adversarial perturbations are generated using a common natural phenomenon, catoptric light, to achieve stealthy and naturalistic adversarial attacks against advanced DNNs in a black-box setting. We evaluate the proposed method in three aspects: effectiveness, stealthiness, and robustness. Quantitative results obtained in simulated environments demonstrate the effectiveness of the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#24425;&#33394;&#21464;&#24322;&#23545;DNN&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24425;&#33394;&#21464;&#24322;&#19982;&#20934;&#30830;&#24230;&#20007;&#22833;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24425;&#33394;&#21464;&#24322;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;DNN&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.02832</link><description>&lt;p&gt;
&#24425;&#33394;&#21464;&#24322;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Colour Variation on Robustness of Deep Neural Networks. (arXiv:2209.02832v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#24425;&#33394;&#21464;&#24322;&#23545;DNN&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24425;&#33394;&#21464;&#24322;&#19982;&#20934;&#30830;&#24230;&#20007;&#22833;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24425;&#33394;&#21464;&#24322;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;DNN&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#26041;&#38754;&#24050;&#32463;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#25163;&#21160;&#25968;&#23383;&#25200;&#21160;&#65292;&#21363;&#23545;&#25239;&#25915;&#20987;&#65292;&#24456;&#23481;&#26131;&#20135;&#29983;&#28431;&#27934;&#12290;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21463;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#26174;&#33879;&#24433;&#21709;&#12290;&#36755;&#20837;&#22270;&#20687;&#39068;&#33394;&#31354;&#38388;&#30340;&#25197;&#26354;&#25110;&#25200;&#21160;&#20250;&#20135;&#29983;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#26356;&#23481;&#26131;&#23545;&#20854;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24425;&#33394;&#21464;&#24322;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25197;&#26354;&#20854;&#20013;27&#31181;&#19981;&#21516;&#32452;&#21512;&#30340;RGB&#39068;&#33394;&#22312;ImageNet&#30340;&#23376;&#38598;&#19978;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#30740;&#31350;&#24425;&#33394;&#21464;&#24322;&#23545;DNN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;DNN&#26550;&#26500;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#24425;&#33394;&#21464;&#24322;&#19982;&#20934;&#30830;&#24230;&#20007;&#22833;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24425;&#33394;&#21464;&#24322;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;DNN&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have have shown state-of-the-art performance for computer vision applications like image classification, segmentation and object detection. Whereas recent advances have shown their vulnerability to manual digital perturbations in the input data, namely adversarial attacks. The accuracy of the networks is significantly affected by the data distribution of their training dataset. Distortions or perturbations on color space of input images generates out-of-distribution data, which make networks more likely to misclassify them. In this work, we propose a color-variation dataset by distorting their RGB color on a subset of the ImageNet with 27 different combinations. The aim of our work is to study the impact of color variation on the performance of DNNs. We perform experiments on several state-of-the-art DNN architectures on the proposed dataset, and the result shows a significant correlation between color variation and loss of accuracy. Furthermore, based on th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;SVRP &#21644; Catalyzed SVRP&#65292;&#23427;&#20204;&#37117;&#26377;&#36739;&#39640;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#24191;&#27867;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#32479;&#35745;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2209.02257</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#38454;&#30456;&#20284;&#24615;&#30340;&#26356;&#24555;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Faster federated optimization under second-order similarity. (arXiv:2209.02257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;SVRP &#21644; Catalyzed SVRP&#65292;&#23427;&#20204;&#37117;&#26377;&#36739;&#39640;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#24191;&#27867;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#32479;&#35745;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#22312;&#36890;&#20449;&#32422;&#26463;&#19979;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#23581;&#35797;&#22312;&#32593;&#32476;&#19978;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#20108;&#38454;&#20989;&#25968;&#30456;&#20284;&#24615;&#26465;&#20214;&#21644;&#24378;&#20984;&#24615;&#19979;&#30340;&#26377;&#38480;&#21644;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65306;SVRP &#21644;&#20652;&#21270; SVRP&#12290;&#36817;&#24180;&#26469;&#65292;&#20108;&#38454;&#30456;&#20284;&#24615;&#26465;&#20214;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#24182;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#28385;&#36275;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#32479;&#35745;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861; SVRP &#32452;&#21512;&#20102;&#36817;&#20284;&#38543;&#26426;&#36817;&#31471;&#28857;&#35780;&#20272;&#12289;&#23458;&#25143;&#31471;&#25277;&#26679;&#21644;&#26041;&#24046;&#32553;&#20943;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; SVRP &#20855;&#26377;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#20989;&#25968;&#30456;&#20284;&#24615;&#36275;&#22815;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#35768;&#22810;&#29616;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#31639;&#27861;&#65292;Catalyzed SVRP &#26159; SVRP &#30340;&#20652;&#21270;&#21058;&#21152;&#36895;&#21464;&#20307;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#32479;&#19968;&#25913;&#36827;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federate
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#35780;&#36848;&#20102;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#26681;&#25454;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2207.12599</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;&#65306;&#20998;&#31867;&#21644;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics. (arXiv:2207.12599v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#35780;&#36848;&#20102;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#26681;&#25454;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22270;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#20570;&#30340;&#39044;&#27979;&#24448;&#24448;&#38590;&#20197;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20174;GNNExplainer&#12289;XGNN&#21644;PGExplainer&#31561;&#26041;&#38754;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#26426;&#21046;&#12290;&#34429;&#28982;&#36825;&#20123;&#24037;&#20316;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#35299;&#37322;GNN&#30340;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#21487;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#35780;&#36848;&#23578;&#19981;&#21487;&#29992;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20026;GNN&#24320;&#21457;&#30340;&#21487;&#35299;&#37322;&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#36848;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26681;&#25454;&#20351;&#29992;&#21487;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;GNN&#35299;&#37322;&#30340;&#24120;&#35265;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#25351;&#20986;&#20960;&#20010;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#21010;&#26102;&#38388;&#20869;&#26080;&#27861;&#22788;&#29702;&#30340;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2207.04550</link><description>&lt;p&gt;
&#24102;&#26377;&#20002;&#22833;&#38144;&#21806;&#21644;&#19981;&#30830;&#23450;&#20379;&#24212;&#30340;&#24211;&#23384;&#31995;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Order for Inventory Systems with Lost Sales and Uncertain Supplies. (arXiv:2207.04550v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35745;&#21010;&#26102;&#38388;&#20869;&#26080;&#27861;&#22788;&#29702;&#30340;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#35745;&#21010;&#26102;&#38388; $T$ &#20869;&#20855;&#26377;&#23548;&#21521;&#26102;&#38388; $L$ &#30340;&#38543;&#26426;&#20002;&#22833;&#38144;&#21806;&#24211;&#23384;&#25511;&#21046;&#31995;&#32479;&#12290;&#30001;&#20110;&#38543;&#26426;&#20135;&#37327;/&#20135;&#33021;&#31561;&#21407;&#22240;&#65292;&#20379;&#24212;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#26159;&#35746;&#21333;&#25968;&#37327;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270; $T$ &#26399;&#25104;&#26412;&#65292;&#20294;&#21363;&#20351;&#22312;&#38656;&#27714;&#21644;&#20379;&#24212;&#20998;&#24067;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#26159;&#35745;&#31639;&#26080;&#27861;&#22788;&#29702;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38656;&#27714;&#21644;&#20379;&#24212;&#20998;&#24067;&#37117;&#26410;&#30693;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403; $L\geq\log(T)$ &#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102; $O(L+\sqrt{T})$ &#30340;&#21518;&#24724;&#65288;&#21363;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#26412;&#21644; $T$ &#26399;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65289;&#12290;&#25105;&#20204;&#26159;&#36890;&#36807; 1&#65289;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#26412;&#27604;&#23436;&#20840;&#20449;&#24687;&#19979;&#19968;&#20010;&#26368;&#20248;&#24120;&#25968;&#35746;&#21333;&#31574;&#30053;&#39640;&#33267;&#22810; $O(L+\sqrt{T})$&#65288;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65289;; 2&#65289;&#21033;&#29992;&#20854;&#24050;&#30693;&#30340;&#24615;&#33021;&#20445;&#35777;&#20174;&#29616;&#26377;&#30340;&#25991;&#23398;&#20013;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic lost-sales inventory control system with a lead time $L$ over a planning horizon $T$. Supply is uncertain, and is a function of the order quantity (due to random yield/capacity, etc). We aim to minimize the $T$-period cost, a problem that is known to be computationally intractable even under known distributions of demand and supply. In this paper, we assume that both the demand and supply distributions are unknown and develop a computationally efficient online learning algorithm. We show that our algorithm achieves a regret (i.e. the performance gap between the cost of our algorithm and that of an optimal policy over $T$ periods) of $O(L+\sqrt{T})$ when $L\geq\log(T)$. We do so by 1) showing our algorithm cost is higher by at most $O(L+\sqrt{T})$ for any $L\geq 0$ compared to an optimal constant-order policy under complete information (a well-known and widely-used algorithm) and 2) leveraging its known performance guarantee from the existing literature. To the 
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26679;&#23376;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#38598;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26631;&#31614;&#20381;&#36182;&#24615;&#21644;&#25104;&#21592;&#27169;&#22411;&#26469;&#20419;&#36827;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.14477</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26631;&#31614;&#20381;&#36182;&#24615;&#21644;&#25104;&#21592;&#27169;&#22411;&#30340;&#25932;&#23545;&#38598;&#21512;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Ensemble Training by Jointly Learning Label Dependencies and Member Models. (arXiv:2206.14477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14477
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26679;&#23376;&#27169;&#22411;&#26159;&#25552;&#39640;&#20854;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#38598;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26631;&#31614;&#20381;&#36182;&#24615;&#21644;&#25104;&#21592;&#27169;&#22411;&#26469;&#20419;&#36827;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#35777;&#26126;&#65292;&#35757;&#32451;&#22810;&#26679;&#30340;&#23376;&#27169;&#22411;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#38598;&#21512;&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992; one-hot &#21521;&#37327;&#23545;&#22270;&#20687;&#26631;&#31614;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#24573;&#30053;&#20102;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25932;&#23545;&#38598;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20849;&#21516;&#23398;&#20064;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#21644;&#25104;&#21592;&#27169;&#22411;&#65292;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#20419;&#36827;&#25104;&#21592;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598; MNIST&#12289;FashionMNIST &#21644; CIFAR-10 &#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/ZJLAB-AMMI/LSD &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an ensemble of diverse sub-models has been empirically demonstrated as an effective strategy for improving the adversarial robustness of deep neural networks. However, current ensemble training methods for image recognition typically encode image labels using one-hot vectors, which overlook dependency relationships between the labels. In this paper, we propose a novel adversarial en-semble training approach that jointly learns the label dependencies and member models. Our approach adaptively exploits the learned label dependencies to pro-mote diversity among the member models. We evaluate our approach on widely used datasets including MNIST, FashionMNIST, and CIFAR-10, and show that it achieves superior robustness against black-box attacks compared to state-of-the-art methods. Our code is available at https://github.com/ZJLAB-AMMI/LSD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2206.14203</link><description>&lt;p&gt;
&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Latent Combinational Game Design. (arXiv:2206.14203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#8221;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (GMVAEs) &#23545; VAE &#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#35757;&#32451;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#28216;&#25103;&#30340;&#27700;&#24179;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#28151;&#21512;&#28216;&#25103;&#23450;&#20041;&#20026;&#36825;&#20123;&#32452;&#20214;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#26032;&#28216;&#25103;&#65292;&#24182;&#25511;&#21046;&#28151;&#21512;&#20013;&#27599;&#20010;&#28216;&#25103;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26377;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#20197;&#21069;&#30340;&#28151;&#21512;&#24037;&#20316;&#65292;&#24182;&#19982; GMVAE &#36827;&#34892;&#27604;&#36739;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#28151;&#21512;&#26465;&#20214; GMVAE (CGMVAE) &#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#25972;&#20010;&#28151;&#21512;&#27700;&#24179;&#21644;&#24067;&#23616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#36848;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#25353;&#25351;&#23450;&#32452;&#21512;&#28151;&#21512;&#30340;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#24179;&#21488;&#28216;&#25103;&#21644;&#22320;&#19979;&#22478;&#31867;&#28216;&#25103;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present latent combinational game design -- an approach for generating playable games that blend a given set of games in a desired combination using deep generative latent variable models. We use Gaussian Mixture Variational Autoencoders (GMVAEs) which model the VAE latent space via a mixture of Gaussian components. Through supervised training, each component encodes levels from one game and lets us define blended games as linear combinations of these components. This enables generating new games that blend the input games and controlling the relative proportions of each game in the blend. We also extend prior blending work using conditional VAEs and compare against the GMVAE and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture which lets us generate whole blended levels and layouts. Results show that the above approaches can generate playable games that blend the input games in specified combinations. We use both platformers and dungeon-based games to demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvZL&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#21033;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26159;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2206.12251</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#22411;&#29289;&#29702;&#25915;&#20987;&#65306;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;
&lt;/p&gt;
&lt;p&gt;
Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs. (arXiv:2206.12251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdvZL&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#21033;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26159;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#30693;&#36947;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24456;&#33030;&#24369;&#65292;&#20294;&#36824;&#27809;&#26377;&#20154;&#30740;&#31350;&#36807;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#25110;&#32553;&#23567;&#23545;DNNs&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adversarial Zoom Lens&#65288;AdvZL&#65289;&#30340;&#26032;&#22411;&#29289;&#29702;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#25932;&#23545;&#21464;&#28966;&#38236;&#22836;&#23545;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#25918;&#22823;&#21644;&#32553;&#23567;&#65292;&#20174;&#32780;&#27450;&#39575;DNNs&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#30446;&#26631;&#23545;&#35937;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#26159;&#36804;&#20170;&#20026;&#27490;&#21807;&#19968;&#19968;&#31181;&#19981;&#28155;&#21152;&#29289;&#29702;&#25932;&#23545;&#25200;&#21160;&#25915;&#20987;DNNs&#30340;&#25932;&#23545;&#25915;&#20987;&#25216;&#26415;&#12290;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;AdvZL&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39564;&#35777;&#31561;&#27604;&#20363;&#25918;&#22823;&#22270;&#20687;&#23545;DNNs&#30340;&#25932;&#23545;&#24615;&#12290;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#29992;&#21464;&#28966;&#38236;&#22836;&#23545;&#30446;&#26631;&#23545;&#35937;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#29983;&#25104;&#25932;&#23545;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;AdvZL&#22312;&#25968;&#23383;&#29615;&#22659;&#21644;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#25932;&#23545;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural networks (DNNs) are known to be fragile, no one has studied the effects of zooming-in and zooming-out of images in the physical world on DNNs performance. In this paper, we demonstrate a novel physical adversarial attack technique called Adversarial Zoom Lens (AdvZL), which uses a zoom lens to zoom in and out of pictures of the physical world, fooling DNNs without changing the characteristics of the target object. The proposed method is so far the only adversarial attack technique that does not add physical adversarial perturbation attack DNNs. In a digital environment, we construct a data set based on AdvZL to verify the antagonism of equal-scale enlarged images to DNNs. In the physical environment, we manipulate the zoom lens to zoom in and out of the target object, and generate adversarial samples. The experimental results demonstrate the effectiveness of AdvZL in both digital and physical environments. We further analyze the antagonism of the proposed data set 
&lt;/p&gt;</description></item><item><title>&#38634;&#23665;&#22242;&#38431;&#21457;&#24067;&#20102;&#19968;&#20221;&#20302;&#36164;&#28304;&#21271;&#21360;&#24230;&#35821;&#35328;&#22307;&#32463;&#38899;&#39057;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#19968;&#20010;&#22522;&#32447;&#65292;&#20197;&#21033;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;ASR&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.01205</link><description>&lt;p&gt;
&#38634;&#23665;&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#22307;&#32463;&#38899;&#39057;&#35760;&#24405;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Snow Mountain: Dataset of Audio Recordings of The Bible in Low Resource Languages. (arXiv:2206.01205v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01205
&lt;/p&gt;
&lt;p&gt;
&#38634;&#23665;&#22242;&#38431;&#21457;&#24067;&#20102;&#19968;&#20221;&#20302;&#36164;&#28304;&#21271;&#21360;&#24230;&#35821;&#35328;&#22307;&#32463;&#38899;&#39057;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#19968;&#20010;&#22522;&#32447;&#65292;&#20197;&#21033;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;ASR&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#36234;&#26469;&#36234;&#26377;&#29992;&#12290;&#26377;&#35768;&#22810;&#38024;&#23545;&#20687;&#33521;&#35821;&#36825;&#26679;&#35757;&#32451;&#25968;&#25454;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;ASR&#27169;&#22411;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20195;&#34920;&#24615;&#24456;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#25918;&#35768;&#21487;&#21644;&#26684;&#24335;&#21270;&#30340;&#20302;&#36164;&#28304;&#21271;&#21360;&#24230;&#35821;&#35328;&#22307;&#32463;&#38899;&#39057;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35774;&#32622;&#22810;&#20010;&#23454;&#39564;&#25286;&#20998;&#65292;&#24182;&#35757;&#32451;&#21644;&#20998;&#26512;&#20004;&#20010;&#31454;&#20105;&#24615;ASR&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#30340;&#30740;&#31350;&#25552;&#20379;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has increasing utility in the modern world. There are a many ASR models available for languages with large amounts of training data like English. However, low-resource languages are poorly represented. In response we create and release an open-licensed and formatted dataset of audio recordings of the Bible in low-resource northern Indian languages. We setup multiple experimental splits and train and analyze two competitive ASR models to serve as the baseline for future research using this data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25513;&#30721;&#21464;&#37327;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#20851;&#38381;&#19968;&#20123;&#33410;&#28857;&#65292;&#20197;&#20135;&#29983;&#31232;&#30095;&#30340;DNN&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#24471;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MCMC&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#33021;&#22815;&#21457;&#29616;&#31934;&#31616;&#30340;DNN&#32467;&#26500;&#65292;&#20855;&#26377;&#19982;&#22823;&#22411;DNN&#30456;&#20284;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.00853</link><description>&lt;p&gt;
&#25513;&#30721;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;: &#35745;&#31639;&#19982;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;
Masked Bayesian Neural Networks : Computation and Optimality. (arXiv:2206.00853v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25513;&#30721;&#21464;&#37327;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#20851;&#38381;&#19968;&#20123;&#33410;&#28857;&#65292;&#20197;&#20135;&#29983;&#31232;&#30095;&#30340;DNN&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#24471;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MCMC&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#33021;&#22815;&#21457;&#29616;&#31934;&#31616;&#30340;DNN&#32467;&#26500;&#65292;&#20855;&#26377;&#19982;&#22823;&#22411;DNN&#30456;&#20284;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#37327;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26550;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#24222;&#22823;&#65292;&#22240;&#27492;&#38656;&#35201;&#31616;&#21270;&#36825;&#31181;&#22797;&#26434;&#21644;&#24222;&#22823;&#30340;DNN&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#36866;&#24403;&#22797;&#26434;&#24230;&#30340; DNN&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#20351;&#29992;&#25513;&#30721;&#21464;&#37327;&#65292;&#26681;&#25454;&#21518;&#39564;&#20998;&#24067;&#20851;&#38381;&#19968;&#20123;&#33410;&#28857;&#65292;&#20197;&#20135;&#29983;&#31232;&#30095; DNN&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#24471;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#24615;&#65288;&#21363;&#26497;&#23567;&#26497;&#22823;&#20248;&#36234;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;&#65289;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;MCMC&#31639;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;BNN&#34920;&#29616;&#33391;&#22909;&#65292;&#19982;&#22823;&#22411;DNN&#30456;&#27604;&#65292;&#23427;&#21457;&#29616;&#20102;&#31934;&#31616;&#30340;DNN&#32467;&#26500;&#65292;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As data size and computing power increase, the architectures of deep neural networks (DNNs) have been getting more complex and huge, and thus there is a growing need to simplify such complex and huge DNNs. In this paper, we propose a novel sparse Bayesian neural network (BNN) which searches a good DNN with an appropriate complexity. We employ the masking variables at each node which can turn off some nodes according to the posterior distribution to yield a nodewise sparse DNN. We devise a prior distribution such that the posterior distribution has theoretical optimalities (i.e. minimax optimality and adaptiveness), and develop an efficient MCMC algorithm. By analyzing several benchmark datasets, we illustrate that the proposed BNN performs well compared to other existing methods in the sense that it discovers well condensed DNN architectures with similar prediction accuracy and uncertainty quantification compared to large DNNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753; (NeuPSL) &#36825;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#34701;&#21512;&#26694;&#26550;&#12290;&#36890;&#36807;NeSy&#33021;&#37327;&#27169;&#22411;&#24314;&#27169;&#31070;&#32463;&#21644;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26080;&#32541;&#34701;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#22312;&#23454;&#38469;&#35780;&#27979;&#20013;&#65292;&#30456;&#23545;&#20110;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#36798; 30% &#30340;&#25552;&#21319;&#65292;&#22312;MNIST-Addition&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20063;&#39640;&#36798;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;5%&#12290;</title><link>http://arxiv.org/abs/2205.14268</link><description>&lt;p&gt;
NeuPSL: &#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
NeuPSL: Neural Probabilistic Soft Logic. (arXiv:2205.14268v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753; (NeuPSL) &#36825;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#34701;&#21512;&#26694;&#26550;&#12290;&#36890;&#36807;NeSy&#33021;&#37327;&#27169;&#22411;&#24314;&#27169;&#31070;&#32463;&#21644;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26080;&#32541;&#34701;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#22312;&#23454;&#38469;&#35780;&#27979;&#20013;&#65292;&#30456;&#23545;&#20110;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22810;&#36798; 30% &#30340;&#25552;&#21319;&#65292;&#22312;MNIST-Addition&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20063;&#39640;&#36798;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;&#65288;NeuPSL&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23558;&#29616;&#20195;&#31526;&#21495;&#25512;&#29702;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#32423;&#24863;&#30693;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#8212;&#8212;NeSy&#33021;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;&#31070;&#32463;&#21644;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#36275;&#22815;&#36890;&#29992;&#65292;&#21253;&#25324;NeuPSL&#21644;&#35768;&#22810;&#20854;&#20182;NeSy&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26080;&#32541;&#22320;&#38598;&#25104;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;NeSy&#26041;&#27861;&#30340;&#22909;&#22788;&#65292;&#30456;&#23545;&#20110;&#29420;&#31435;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#20102;&#22810;&#36798;30&#65285;&#30340;&#25552;&#21319;&#12290;&#22312;&#19968;&#20010;&#24050;&#32463;&#24314;&#31435;&#30340;NeSy&#20219;&#21153;MNIST-Addition&#19978;&#65292;NeuPSL&#36890;&#36807;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#26368;&#39640;10&#65285;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;NeSy&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#32852;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;NeuPSL&#30456;&#23545;&#20110;&#29616;&#26377;&#39046;&#20808;&#30340;NeSy&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neuro-symbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, we propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. Using this framework, we show how to seamlessly integrate neural and symbolic parameter learning and inference in NeuPSL. Through an extensive empirical evaluation, we demonstrate the benefits of using NeSy methods, achieving upwards of 30% improvement over independent neural network models. On a well-established NeSy task, MNIST-Addition, NeuPSL demonstrates its joint reasoning capabilities by outperforming existing NeSy approaches by up to 10% in low-data settings. Furthermore, NeuPSL achieves a 5% boost in performance over state-of-the-art NeSy methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2205.10055</link><description>&lt;p&gt;
SVM&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Case of Exponential Convergence Rates for SVM. (arXiv:2205.10055v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;SVM&#30340;&#25351;&#25968;&#32423;&#25910;&#25947;&#36895;&#24230;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#36890;&#24120;&#26159;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#20013;&#25551;&#36848;&#30340;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;&#21382;&#21490;&#19978;&#65292;&#29926;&#26222;&#23612;&#20811;-&#20999;&#23572;&#27779;&#24180;&#31185;&#29702;&#35770;&#25552;&#20379;&#20102;&#20998;&#31867;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20445;&#35777;&#22522;&#20110;&#38590;&#20197;&#22788;&#29702;&#30340;&#31639;&#27861;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#31867;&#20013;&#20195;&#29702;&#26041;&#27861;&#30340;&#29702;&#35770;&#12290;&#20195;&#29702;&#26041;&#27861;&#25552;&#20379;&#30340;&#20445;&#35777;&#22522;&#20110;&#26657;&#20934;&#19981;&#31561;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#26576;&#20123;&#36793;&#38469;&#26465;&#20214;&#19979;&#38750;&#24120;&#27425;&#20248;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#25351;&#25968;&#32423;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#20123;"&#36229;"&#24555;&#36895;&#29575;&#29616;&#22312;&#24050;&#32463;&#23545;&#20110;&#20809;&#28369;&#30340;&#20195;&#29702;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#19982;&#33879;&#21517;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30456;&#20851;&#30340;&#38750;&#20809;&#28369;&#25439;&#22833;&#65288;&#22914;&#38128;&#38142;&#25439;&#22833;&#65289;&#65292;&#30011;&#38754;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26426;&#21046;&#26469;&#33719;&#24471;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#30740;&#31350;&#20854;&#29992;&#20110;SVM&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SVM&#21487;&#20197;&#23637;&#29616;&#20986;&#25351;&#25968;&#32423;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21363;&#20351;&#27809;&#26377;&#20551;&#35774;&#30828;Tsybakov&#36793;&#38469;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification is often the first problem described in introductory machine learning classes. Generalization guarantees of classification have historically been offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on intractable algorithms, which has led to the theory of surrogate methods in classification. Guarantees offered by surrogate methods are based on calibration inequalities, which have been shown to be highly sub-optimal under some margin conditions, failing short to capture exponential convergence phenomena. Those "super" fast rates are becoming to be well understood for smooth surrogates, but the picture remains blurry for non-smooth losses such as the hinge loss, associated with the renowned support vector machines. In this paper, we present a simple mechanism to obtain fast convergence rates and we investigate its usage for SVM. In particular, we show that SVM can exhibit exponential convergence rates even without assuming the hard Tsybakov margin conditi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26080;&#30417;&#30563;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#25216;&#26415;&#22312;&#36895;&#24230;&#35889;&#25968;&#25454;&#20013;&#33258;&#21160;&#25342;&#21462;&#39640;&#25928;&#19988;&#21512;&#29702;&#30340;&#36895;&#24230;&#28857;&#65292;&#20174;&#32780;&#26356;&#21487;&#38752;&#21644;&#31934;&#30830;&#22320;&#21152;&#36895;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2205.08372</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#22320;&#38663;&#36895;&#24230;&#25342;&#21462;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Stack Velocity Picking Using an Unsupervised Ensemble Learning Method. (arXiv:2205.08372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#26080;&#30417;&#30563;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#25216;&#26415;&#22312;&#36895;&#24230;&#35889;&#25968;&#25454;&#20013;&#33258;&#21160;&#25342;&#21462;&#39640;&#25928;&#19988;&#21512;&#29702;&#30340;&#36895;&#24230;&#28857;&#65292;&#20174;&#32780;&#26356;&#21487;&#38752;&#21644;&#31934;&#30830;&#22320;&#21152;&#36895;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#22320;&#38663;&#36895;&#24230;&#25342;&#21462;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#24555;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#36895;&#24230;&#65292;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#36895;&#24230;&#35889;&#12290;&#23613;&#31649;&#26377;&#20123;&#22522;&#20110;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25342;&#21462;&#36895;&#24230;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#26631;&#35760;&#36153;&#29992;&#25110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#26469;&#39537;&#21160;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26377;&#26395;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38598;&#25104;&#23398;&#20064;&#65288;UEL&#65289;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#21644;&#25552;&#39640;&#36895;&#24230;&#28857;&#20934;&#30830;&#24230;&#20043;&#38388;&#24179;&#34913;&#30340;&#30446;&#30340;&#65292;&#26088;&#22312;&#30830;&#23450;&#22534;&#26632;&#36895;&#24230;&#12290;UEL&#21033;&#29992;&#38468;&#36817;&#36895;&#24230;&#35889;&#21644;&#20854;&#20182;&#24050;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#32858;&#31867;&#25216;&#26415;&#33719;&#21462;&#39640;&#25928;&#21644;&#21512;&#29702;&#30340;&#36895;&#24230;&#28857;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#22330;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;UEL&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#25342;&#21462;&#26041;&#27861;&#26356;&#21487;&#38752;&#21644;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismic velocity picking algorithms that are both accurate and efficient can greatly speed up seismic data processing, with the primary approach being the use of velocity spectra. Despite the development of some supervised deep learning-based approaches to automatically pick the velocity, they often come with costly manual labeling expenses or lack interpretability. In comparison, using physical knowledge to drive unsupervised learning techniques has the potential to solve this problem in an efficient manner. We suggest an Unsupervised Ensemble Learning (UEL) approach to achieving a balance between reliance on labeled data and picking accuracy, with the aim of determining the stack velocity. UEL makes use of the data from nearby velocity spectra and other known sources to help pick efficient and reasonable velocity points, which are acquired through a clustering technique. Testing on both the synthetic and field data sets shows that UEL is more reliable and precise in auto-picking than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#24182;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#30340;&#26597;&#35810;&#23601;&#33021;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#36798;&#25104;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#26159;&#19968;&#31181;&#21487;&#24597;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.00853</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#29289;&#29702;&#19990;&#30028;&#23545;&#25239;&#25915;&#20987;&#65306;Adversarial Neon Beam&#65288;arXiv:2204.00853v2 [cs.CV] &#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#24182;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#30340;&#26597;&#35810;&#23601;&#33021;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#36798;&#25104;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#26159;&#19968;&#31181;&#21487;&#24597;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#65292;&#20809;&#32447;&#20250;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20135;&#21697;&#24050;&#32463;&#34987;&#24212;&#29992;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#28982;&#32780;&#65292;&#20809;&#32447;&#20135;&#29983;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#23545;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#26497;&#20854;&#21361;&#38505;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Neon Beam&#65288;AdvNB&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#26597;&#35810;&#33719;&#21462;&#23545;&#25239;&#24615;&#27670;&#20809;&#26463;&#30340;&#29289;&#29702;&#21442;&#25968;&#26469;&#25191;&#34892;&#29289;&#29702;&#25915;&#20987;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25968;&#23383;&#27979;&#35797;&#21644;&#29289;&#29702;&#27979;&#35797;&#20013;&#37117;&#21487;&#20197;&#36798;&#21040;&#39046;&#20808;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;99.3&#65285;&#65292;&#22312;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;100&#65285;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#29289;&#29702;&#25200;&#21160;&#38544;&#34109;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
In the physical world, light affects the performance of deep neural networks. Nowadays, many products based on deep neural network have been put into daily life. There are few researches on the effect of light on the performance of deep neural network models. However, the adversarial perturbations generated by light may have extremely dangerous effects on these systems. In this work, we propose an attack method called adversarial neon beam (AdvNB), which can execute the physical attack by obtaining the physical parameters of adversarial neon beams with very few queries. Experiments show that our algorithm can achieve advanced attack effect in both digital test and physical test. In the digital environment, 99.3% attack success rate was achieved, and in the physical environment, 100% attack success rate was achieved. Compared with the most advanced physical attack methods, our method can achieve better physical perturbation concealment. In addition, by analyzing the experimental data, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#26694;&#26550;&#65292;EquiVSet&#65292;&#29992;&#20110;&#22312;&#21482;&#26377;&#26368;&#20248;&#23376;&#38598;&#65288;OS&#65289;&#39044;&#35328;&#26426;&#19979;&#30340;&#24369;&#30417;&#30563;&#24212;&#29992;&#20013;&#23398;&#20064;&#31070;&#32463;&#38598;&#20989;&#25968;&#12290;&#26694;&#26550;&#21516;&#26102;&#28385;&#36275;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20801;&#35768;&#22320;&#38754;&#38598;&#21512;&#30340;&#21464;&#21270;&#12289;&#26368;&#23567;&#20808;&#39564;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2203.01693</link><description>&lt;p&gt;
&#22312;&#26368;&#20248;&#23376;&#38598;&#27491;&#21453;&#39304;&#19979;&#23398;&#20064;&#31070;&#32463;&#38598;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Set Functions Under the Optimal Subset Oracle. (arXiv:2203.01693v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#26694;&#26550;&#65292;EquiVSet&#65292;&#29992;&#20110;&#22312;&#21482;&#26377;&#26368;&#20248;&#23376;&#38598;&#65288;OS&#65289;&#39044;&#35328;&#26426;&#19979;&#30340;&#24369;&#30417;&#30563;&#24212;&#29992;&#20013;&#23398;&#20064;&#31070;&#32463;&#38598;&#20989;&#25968;&#12290;&#26694;&#26550;&#21516;&#26102;&#28385;&#36275;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20801;&#35768;&#22320;&#38754;&#38598;&#21512;&#30340;&#21464;&#21270;&#12289;&#26368;&#23567;&#20808;&#39564;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#38598;&#20989;&#25968;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20363;&#22914;AI&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20135;&#21697;&#25512;&#33616;&#21644;&#21270;&#21512;&#29289;&#36873;&#25321;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#30740;&#31350;&#20102;&#22312;&#20989;&#25968;&#20540;&#39044;&#35328;&#26426;&#19979;&#23398;&#20064;&#38598;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#22240;&#27492;&#23545;&#20110;&#21482;&#26377;&#26368;&#20248;&#23376;&#38598;&#65288;OS&#65289;&#39044;&#35328;&#26426;&#19979;&#30340;&#24369;&#30417;&#30563;&#24212;&#29992;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#27492;&#39046;&#22495;&#30340;&#30740;&#31350;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;EquiVSet&#30340;&#21407;&#21017;&#24615;&#32780;&#23454;&#29992;&#30340;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#28385;&#36275;&#22312;OS&#27491;&#21453;&#39304;&#24773;&#20917;&#19979;&#23398;&#20064;&#38598;&#20989;&#25968;&#30340;&#20197;&#19979;&#35201;&#27714;&#65306;i&#65289;&#24314;&#31435;&#25490;&#21015;&#19981;&#21464;&#30340;&#38598;&#21512;&#36136;&#37327;&#20989;&#25968;&#65307;ii&#65289;&#20801;&#35768;&#22320;&#38754;&#38598;&#21512;&#30340;&#21464;&#21270;&#65307;iii&#65289;&#26368;&#23567;&#20808;&#39564;&#65307;iv&#65289;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural set functions becomes increasingly more important in many applications like product recommendation and compound selection in AI-aided drug discovery. The majority of existing works study methodologies of set function learning under the function value oracle, which, however, requires expensive supervision signals. This renders it impractical for applications with only weak supervisions under the Optimal Subset (OS) oracle, the study of which is surprisingly overlooked. In this work, we present a principled yet practical maximum likelihood learning framework, termed as EquiVSet, that simultaneously meets the following desiderata of learning set functions under the OS oracle: i) permutation invariance of the set mass function being modeled; ii) permission of varying ground set; iii) minimum prior; and iv) scalability. The main components of our framework involve: an energy-based treatment of the set mass function, DeepSet-style architectures to handle permutation invarianc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;PFGE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#39640;&#24615;&#33021;DNN&#38598;&#21512;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#20102;5&#20493;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.06658</link><description>&lt;p&gt;
PFGE: &#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PFGE: Parsimonious Fast Geometric Ensembling of DNNs. (arXiv:2202.06658v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;PFGE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#39640;&#24615;&#33021;DNN&#38598;&#21512;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20869;&#23384;&#25928;&#29575;&#25552;&#39640;&#20102;5&#20493;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#38598;&#25104;&#38656;&#35201;&#39640;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#27492;&#38598;&#25104;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#22914;&#24555;&#36895;&#20960;&#20309;&#38598;&#25104;&#65288;FGE&#65289;&#21644;&#24555;&#29031;&#38598;&#25104;&#65292;&#36890;&#36807;&#22312;&#19982;&#21333;&#20010;&#27169;&#22411;&#30456;&#21516;&#30340;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#38598;&#25104;&#26469;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#19968;&#27169;&#22411;&#30340;&#22522;&#20110;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31616;&#27905;FGE&#65288;PFGE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#30001;&#36830;&#32493;&#30340;&#38543;&#26426;&#26435;&#37325;&#24179;&#22343;&#36807;&#31243;&#29983;&#25104;&#30340;&#39640;&#24615;&#33021;DNN&#30340;&#36731;&#37327;&#32423;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29616;&#20195;DNN&#26550;&#26500;&#30340;CIFAR-{10,100}&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PFGE&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#27604;5&#20493;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used to enhance the generalization performance of machine learning models. However, they present a challenge in deep learning systems due to the high computational overhead required to train an ensemble of deep neural networks (DNNs). Recent advancements such as fast geometric ensembling (FGE) and snapshot ensembles have addressed this issue by training model ensembles in the same time as a single model. Nonetheless, these techniques still require additional memory for test-time inference compared to single-model-based methods. In this paper, we propose a new method called parsimonious FGE (PFGE), which employs a lightweight ensemble of higher-performing DNNs generated through successive stochastic weight averaging procedures. Our experimental results on CIFAR-{10,100} and ImageNet datasets across various modern DNN architectures demonstrate that PFGE achieves 5x memory efficiency compared to previous methods, without compromising on generalization perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#20197;&#39764;&#26041;&#20026;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.03157</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23601;&#36275;&#20197;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervision is All You Need for Solving Rubik's Cube. (arXiv:2106.03157v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#65292;&#24182;&#20197;&#39764;&#26041;&#20026;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#19988;&#38656;&#35201;&#19968;&#23450;&#27700;&#24179;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#39044;&#23450;&#20041;&#30446;&#26631;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#30001;&#39764;&#26041;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#36825;&#20123;&#38382;&#39064;&#65292;&#20165;&#36890;&#36807;&#23545;&#20174;&#30446;&#26631;&#29366;&#24577;&#24320;&#22987;&#30340;&#38543;&#26426;&#28151;&#20081;&#29366;&#24577;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#23601;&#36275;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312; Rubik's Cube&#12289;15 Puzzle &#21644; 7&#215;7 Lights Out &#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#21069;&#27839;&#26041;&#27861; DeepCubeA&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23613;&#31649;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#23569;&#20110;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19982;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30456;&#20851;&#30340; Rubik's Cube &#35299;&#31639;&#22120;&#30340;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing combinatorial search methods are often complex and require some level of expertise. This work introduces a simple and efficient deep learning method for solving combinatorial problems with a predefined goal, represented by Rubik's Cube. We demonstrate that, for such problems, training a deep neural network on random scrambles branching from the goal state is sufficient to achieve near-optimal solutions. When tested on Rubik's Cube, 15 Puzzle, and 7$\times$7 Lights Out, our method outperformed the previous state-of-the-art method DeepCubeA, improving the trade-off between solution optimality and computational cost, despite significantly less training data. Furthermore, we investigate the scaling law of our Rubik's Cube solver with respect to model size and training data volume.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#65292;&#21457;&#29616;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31867;&#65292;&#21516;&#26102;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2012.06694</link><description>&lt;p&gt;
&#24930;&#31070;&#32463;&#21160;&#21147;&#23398;&#23545;&#22686;&#37327;&#23398;&#20064;&#30340;&#21518;&#26524;
&lt;/p&gt;
&lt;p&gt;
Consequences of Slow Neural Dynamics for Incremental Learning. (arXiv:2012.06694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.06694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#65292;&#21457;&#29616;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31867;&#65292;&#21516;&#26102;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#33041;&#20013;&#65292;&#20869;&#37096;&#29366;&#24577;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#30456;&#20114;&#20851;&#32852;&#65288;&#30001;&#20110;&#23616;&#37096;&#24490;&#29615;&#21644;&#20854;&#20182;&#20869;&#22312;&#30005;&#36335;&#29305;&#24615;&#65289;&#65292;&#24182;&#30001;&#31361;&#28982;&#36716;&#25442;&#26029;&#26029;&#32493;&#32493;&#22320;&#21576;&#29616;&#12290;&#20045;&#19968;&#30475;&#65292;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#20250;&#23545;&#23398;&#20064;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#26631;&#31614;&#65289;&#20135;&#29983;&#38382;&#39064;&#65292;&#22240;&#20026;&#36755;&#20837;&#30340;&#20869;&#37096;&#34920;&#31034;&#23558;&#21253;&#21547;&#24403;&#21069;&#36755;&#20837;&#21644;&#20808;&#21069;&#36755;&#20837;&#30340;&#28151;&#21512;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#33258;&#28982;&#25968;&#25454;&#65288;&#20363;&#22914;&#30005;&#24433;&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36755;&#20837;&#20063;&#23384;&#22312;&#26102;&#38388;&#33258;&#30456;&#20851;&#24615;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#20063;&#26159;&#26102;&#38388;&#24179;&#28369;&#30340;&#26102;&#65292;&#20869;&#37096;&#29366;&#24577;&#30340;&#26102;&#38388;&#24179;&#28369;&#24615;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#30340;&#25928;&#29575;&#65311;&#23427;&#22914;&#20309;&#24433;&#21709;&#25152;&#23398;&#30340;&#34920;&#31034;&#31867;&#22411;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20351;&#29992;&#26102;&#38388;&#24179;&#28369;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20855;&#26377;&#8220;&#24930;&#8221;&#31070;&#32463;&#32593;&#32476;&#65288;&#37197;&#22791;&#32447;&#24615;&#24490;&#29615;&#21644;&#38376;&#25511;&#26426;&#21046;&#65289;&#30340;&#32593;&#32476;&#27604;&#21069;&#39304;&#32593;&#32476;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#32447;&#24615;&#24490;&#29615;&#21644;&#22810;&#26102;&#38388;&#23610;&#24230;&#38376;&#25511;&#26426;&#21046;&#30340;&#32593;&#32476;&#23398;&#20250;&#20102;&#34920;&#31034;&#36755;&#20837;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the human brain, internal states are often correlated over time (due to local recurrence and other intrinsic circuit properties), punctuated by abrupt transitions. At first glance, temporal smoothness of internal states presents a problem for learning input-output mappings (e.g. category labels for images), because the internal representation of the input will contain a mixture of current input and prior inputs. However, when training with naturalistic data (e.g. movies) there is also temporal autocorrelation in the input. How does the temporal "smoothness" of internal states affect the efficiency of learning when the training data are also temporally smooth? How does it affect the kinds of representations that are learned? We found that, when trained with temporally smooth data, "slow" neural networks (equipped with linear recurrence and gating mechanisms) learned to categorize more efficiently than feedforward networks. Furthermore, networks with linear recurrence and multi-timesc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#31616;&#21333;&#25193;&#25955;&#27169;&#22411;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#38480;&#65292;&#25351;&#20986;&#22312;&#25193;&#25955;&#30340;&#30456;&#24403;&#26202;&#26399;&#20043;&#21069;&#26080;&#27861;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#65292;&#23545;&#20110;Bass&#27169;&#22411;&#21644;SIR&#27169;&#22411;&#65292;&#35201;&#33267;&#23569;&#32463;&#21382;&#21040;&#26102;&#38388;&#30340;&#19977;&#20998;&#20043;&#20108;&#25165;&#21487;&#20197;&#39044;&#27979;&#20986;&#26368;&#32456;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.06373</link><description>&lt;p&gt;
&#21521;&#31616;&#21333;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#38480;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Limits to Learning a Diffusion Model. (arXiv:2006.06373v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#31616;&#21333;&#25193;&#25955;&#27169;&#22411;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#38480;&#65292;&#25351;&#20986;&#22312;&#25193;&#25955;&#30340;&#30456;&#24403;&#26202;&#26399;&#20043;&#21069;&#26080;&#27861;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#65292;&#23545;&#20110;Bass&#27169;&#22411;&#21644;SIR&#27169;&#22411;&#65292;&#35201;&#33267;&#23569;&#32463;&#21382;&#21040;&#26102;&#38388;&#30340;&#19977;&#20998;&#20043;&#20108;&#25165;&#21487;&#20197;&#39044;&#27979;&#20986;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20272;&#35745;&#31616;&#21333;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#38480;&#65292;&#21253;&#25324; Bass &#27169;&#22411;&#65288;&#29992;&#20110;&#24314;&#27169;&#28040;&#36153;&#32773;&#37319;&#29992;&#65289;&#21644; SIR &#27169;&#22411;&#65288;&#29992;&#20110;&#24314;&#27169;&#27969;&#34892;&#30149;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25193;&#25955;&#30340;&#30456;&#24403;&#26202;&#26399;&#20043;&#21069;&#65292;&#26080;&#27861;&#20687;&#23398;&#20064;&#20854;&#20182;&#27169;&#22411;&#37027;&#26679;&#23398;&#20064;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#25910;&#38598;&#30340;&#35266;&#27979;&#37327;&#38656;&#22823;&#20110;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#38480;&#65292;&#27492;&#26102;&#38388;&#36739;&#38271;&#12290;&#23545;&#20110;&#20302;&#21019;&#26032;&#29575;&#30340;Bass&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#26032;&#37319;&#29992;&#32773;&#36798;&#21040;&#23792;&#20540;&#26102;&#65292;&#38656;&#35201;&#33267;&#23569;&#32463;&#21382;&#21040;&#26102;&#38388;&#30340;&#19977;&#20998;&#20043;&#20108;&#25165;&#21487;&#20197;&#39044;&#27979;&#26368;&#32456;&#37319;&#29992;&#23458;&#25143;&#25968;&#37327;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;SIR&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#22312;&#24863;&#26579;&#29575;&#36798;&#21040;&#23792;&#20540;&#20043;&#21069;&#65292;&#38656;&#35201;&#33267;&#23569;&#32463;&#21382;&#26102;&#38388;&#30340;&#19977;&#20998;&#20043;&#20108;&#25165;&#33021;&#39044;&#27979;&#26368;&#32456;&#24863;&#26579;&#20154;&#25968;&#12290;&#36825;&#31181;&#20272;&#35745;&#19979;&#38480;&#36827;&#19968;&#27493;&#36716;&#21270;&#20026; d &#20010;&#20915;&#31574;&#28857;&#21487;&#33021;&#20250;&#23548;&#33268;&#20302;&#30340;&#36951;&#25022;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides the first sample complexity lower bounds for the estimation of simple diffusion models, including the Bass model (used in modeling consumer adoption) and the SIR model (used in modeling epidemics). We show that one cannot hope to learn such models until quite late in the diffusion. Specifically, we show that the time required to collect a number of observations that exceeds our sample complexity lower bounds is large. For Bass models with low innovation rates, our results imply that one cannot hope to predict the eventual number of adopting customers until one is at least two-thirds of the way to the time at which the rate of new adopters is at its peak. In a similar vein, our results imply that in the case of an SIR model, one cannot hope to predict the eventual number of infections until one is approximately two-thirds of the way to the time at which the infection rate has peaked. This lower bound in estimation further translates into a lower bound in regret for d
&lt;/p&gt;</description></item></channel></rss>