<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01598</link><description>&lt;p&gt;
&#20174;&#20004;&#20010;&#21313;&#24180;&#30340;&#34880;&#21387;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#36328;&#36234;7500&#19975;&#24739;&#32773;&#23601;&#35786;&#30340;&#19981;&#21516;&#20154;&#32676;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#21313;&#24180;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#20551;&#35774;&#65307;&#21516;&#26102;&#65292;&#33298;&#24352;&#21387;&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#25910;&#32553;&#21387;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#20173;&#28982;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#20854;&#24739;&#30149;&#29575;&#19981;&#26029;&#19978;&#21319;&#65292;&#22240;&#27492;&#38656;&#35201;&#26377;&#25928;&#30340;&#30417;&#27979;&#21644;&#29702;&#35299;&#34880;&#21387;&#21160;&#24577;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;&#34880;&#21387;&#27979;&#37327;&#20013;&#33719;&#24471;&#30340;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#26159;&#20102;&#35299;&#39640;&#34880;&#21387;&#36235;&#21183;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#20102;&#34880;&#21387;&#21464;&#21270;&#19982;&#21508;&#31181;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20221;&#28085;&#30422;&#20102;&#20004;&#20010;&#21313;&#24180;&#30340;7500&#19975;&#35760;&#24405;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#20026;&#25506;&#32034;&#21644;&#20998;&#26512;&#19981;&#21516;&#20154;&#32676;&#29305;&#24449;&#65292;&#22914;&#24180;&#40836;&#12289;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#38388;&#30340;&#34880;&#21387;&#21464;&#21270;&#25552;&#20379;&#20102;&#29420;&#29305;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#24615;&#21035;&#30340;&#34880;&#21387;&#21464;&#21270;&#22312;&#32479;&#35745;&#19978;&#24182;&#19981;&#26174;&#33879;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#20551;&#35774;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33298;&#24352;&#21387;&#65288;SBP&#65289;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#25345;&#32493;&#22686;&#21152;&#65292;&#32780;&#33298;&#24352;&#21387;&#65288;DBP&#65289;&#22312;&#22235;&#21313;&#22810;&#23681;&#30340;&#24180;&#40836;&#32452;&#26174;&#31034;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#23792;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#20998;&#24067;&#27169;&#24335;&#20013;&#30340;&#19968;&#20123;&#26377;&#36259;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01364
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#26131;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36171;&#20104;LLMs&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#20351;&#20854;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#31867;&#30693;&#35782;&#20445;&#25345;&#21516;&#27493;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#37492;&#20110;LLMs&#30340;&#29420;&#29305;&#24615;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28041;&#21450;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#23545;&#40784;&#31561;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#19982;&#22312;&#35268;&#27169;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#31616;&#21333;&#36866;&#24212;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#22686;&#24378;&#31574;&#30053;(&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#27169;&#22411;&#32534;&#36753;)&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#23545;&#22522;&#20934;&#21644;&#35780;&#20272;&#30340;&#35752;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#37325;&#35201;&#20219;&#21153;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;POGLM&#23398;&#20064;&#20013;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#21644;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#31561;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01263</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM
&lt;/p&gt;
&lt;p&gt;
A Differentiable POGLM with Forward-Backward Message Passing
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#30340;&#21487;&#24494;&#20998;POGLM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;POGLM&#23398;&#20064;&#20013;&#30340;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#21644;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#21547;&#31070;&#32463;&#20803;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;POGLM&#65289;&#26159;&#29702;&#35299;&#31070;&#32463;&#36830;&#25509;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#23398;&#20064;POGLM&#65292;&#20294;&#23398;&#20064;&#36825;&#31181;&#28508;&#21464;&#37327;&#27169;&#22411;&#23384;&#22312;&#22256;&#38590;&#12290;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;1&#65289;&#37319;&#26679;&#30340;&#27850;&#26494;&#38544;&#34255;&#23574;&#23792;&#25968;&#37327;&#38459;&#30861;&#20102;&#20351;&#29992;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65307;&#65288;2&#65289;&#29616;&#26377;&#30340;&#21464;&#20998;&#27169;&#22411;&#35774;&#35745;&#26082;&#19981;&#20855;&#26377;&#34920;&#36798;&#24615;&#20063;&#19981;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;&#24615;&#33021;&#12290;&#38024;&#23545;&#38382;&#39064;&#65288;1&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;POGLM&#65292;&#21487;&#20197;&#20351;&#29992;&#36335;&#24452;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20248;&#20110;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#24471;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#38024;&#23545;&#38382;&#39064;&#65288;2&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#37319;&#26679;&#26041;&#26696;&#29992;&#20110;&#21464;&#20998;&#27169;&#22411;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;POGLM&#19982;&#25105;&#20204;&#30340;&#21069;&#21521;-&#21518;&#21521;&#28040;&#24687;&#20256;&#36882;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01000</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#30456;&#20851;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic Time Series Forecasting with Correlated Errors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#24230;&#20302;&#12289;&#26657;&#20934;&#39044;&#27979;&#20934;&#30830;&#24615;&#39640;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19982;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22810;&#20803;&#27169;&#22411;&#22312;&#32771;&#34385;&#35823;&#24046;&#20043;&#38388;&#30340;&#21516;&#26102;&#30456;&#20851;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#32479;&#35745;&#31616;&#21270;&#30340;&#30446;&#30340;&#65292;&#23545;&#36825;&#20123;&#35823;&#24046;&#30340;&#24120;&#35265;&#20551;&#35774;&#26159;&#23427;&#20204;&#22312;&#26102;&#38388;&#19978;&#26159;&#29420;&#31435;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#35266;&#27979;&#24448;&#24448;&#20559;&#31163;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#22240;&#20026;&#35823;&#24046;&#36890;&#24120;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#25490;&#38500;&#26102;&#38388;&#30456;&#20851;&#30340;&#21327;&#21464;&#37327;&#65289;&#32780;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#21442;&#25968;&#21270;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21051;&#30011;&#35823;&#24046;&#30340;&#33258;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#21487;&#21462;&#30340;&#29305;&#24615;&#65306;&#22797;&#26434;&#24230;&#19981;&#38543;&#26102;&#38388;&#24207;&#21015;&#25968;&#30446;&#22686;&#21152;&#65292;&#24471;&#21040;&#30340;&#21327;&#26041;&#24046;&#21487;&#20197;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;MIPS&#65292;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#36716;&#21270;&#20026;Python&#20195;&#30721;&#12290;MIPS&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;32&#20010;&#31639;&#27861;&#20219;&#21153;&#65292;&#20854;&#20013;13&#20010;&#26159;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#12290;&#35813;&#26041;&#27861;&#19981;&#20351;&#29992;&#20154;&#24037;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05110</link><description>&lt;p&gt;
&#25171;&#24320;AI&#40657;&#31665;&#65306;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#23454;&#29616;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Opening the AI black box: program synthesis via mechanistic interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;MIPS&#65292;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#36716;&#21270;&#20026;Python&#20195;&#30721;&#12290;MIPS&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;32&#20010;&#31639;&#27861;&#20219;&#21153;&#65292;&#20854;&#20013;13&#20010;&#26159;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#12290;&#35813;&#26041;&#27861;&#19981;&#20351;&#29992;&#20154;&#24037;&#35757;&#32451;&#25968;&#25454;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MIPS&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#26426;&#26800;&#35299;&#37322;&#24615;&#35757;&#32451;&#25152;&#38656;&#20219;&#21153;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#36716;&#21270;&#20026;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;62&#20010;&#21487;&#36890;&#36807;RNN&#23398;&#20064;&#30340;&#31639;&#27861;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;MIPS&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21457;&#29616;&#23427;&#19982;GPT-4&#38750;&#24120;&#20114;&#34917;&#65306;MIPS&#35299;&#20915;&#20102;&#20854;&#20013;32&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;GPT-4&#26080;&#27861;&#35299;&#20915;&#30340;13&#20010;&#38382;&#39064;&#65288;GPT-4&#35299;&#20915;&#20102;30&#20010;&#38382;&#39064;&#65289;&#12290;MIPS&#20351;&#29992;&#25972;&#25968;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;RNN&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#28982;&#21518;&#24212;&#29992;&#24067;&#23572;&#25110;&#25972;&#25968;&#31526;&#21495;&#22238;&#24402;&#26469;&#25429;&#25417;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#12290;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#31243;&#24207;&#21512;&#25104;&#25216;&#26415;&#19981;&#20351;&#29992;&#65288;&#22240;&#27492;&#19981;&#21463;&#38480;&#20110;&#65289;&#22914;GitHub&#19978;&#30340;&#31639;&#27861;&#21644;&#20195;&#30721;&#31561;&#20154;&#24037;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#23637;&#36825;&#31181;&#26041;&#27861;&#20197;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.
&lt;/p&gt;</description></item><item><title>Hydra heads&#26159;&#19968;&#31181;&#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#65292;&#21462;&#20195;&#20102;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;Medusa&#35299;&#30721;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05109</link><description>&lt;p&gt;
Hydra: &#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#29992;&#20110;Medusa&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05109
&lt;/p&gt;
&lt;p&gt;
Hydra heads&#26159;&#19968;&#31181;&#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#65292;&#21462;&#20195;&#20102;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;Medusa&#35299;&#30721;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#33258;&#22238;&#24402;LLM&#25512;&#29702;&#30340;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25512;&#27979;&#35299;&#30721;&#26694;&#26550;&#12290;&#20026;&#20102;&#25191;&#34892;&#25512;&#27979;&#35299;&#30721;&#65292;&#19968;&#20010;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#25552;&#20986;&#20102;&#36755;&#20837;&#24207;&#21015;&#30340;&#20505;&#36873;&#24310;&#32493;&#65292;&#28982;&#21518;&#30001;&#22522;&#30784;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#12290;&#22312;&#26368;&#36817;&#30340;Medusa&#35299;&#30721;&#26694;&#26550;&#20013;&#65292;&#19968;&#31181;&#25351;&#23450;&#33609;&#31295;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#20316;&#20026;&#19968;&#32452;&#31216;&#20026;&#33609;&#31295;&#22836;&#37096;&#30340;&#36731;&#37327;&#32423;&#22836;&#37096;&#65292;&#36825;&#20123;&#22836;&#37096;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#25805;&#20316;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#33609;&#31295;&#22836;&#37096;&#37117;&#26159;&#39034;&#24207;&#29420;&#31435;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#22312;&#20505;&#36873;&#24310;&#32493;&#20013;&#30340;&#20196;&#29260;&#25512;&#27979;&#19982;&#20505;&#36873;&#24310;&#32493;&#20013;&#30340;&#20219;&#20309;&#21069;&#38754;&#30340;&#20196;&#29260;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#24207;&#20381;&#36182;&#30340;Hydra heads&#65292;&#23427;&#20204;&#26159;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#30340;&#21487;&#26367;&#25442;&#32452;&#20214;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;Hydra heads&#36827;&#34892;&#35299;&#30721;&#27604;&#20351;&#29992;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#30340;Medusa&#35299;&#30721;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design spac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#23548;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#22312;&#19981;&#21516;&#27010;&#29575;&#24046;&#24322;&#20043;&#38388;&#36873;&#25321;&#26368;&#20339;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#32039;&#23494;&#24615;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05101</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#23454;&#29616;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Tighter Generalisation Bounds via Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#23548;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#22312;&#19981;&#21516;&#27010;&#29575;&#24046;&#24322;&#20043;&#38388;&#36873;&#25321;&#26368;&#20339;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#32039;&#23494;&#24615;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#25512;&#23548;&#26032;&#30340;&#22522;&#20110;$(f, \Gamma)$-divergence&#30340;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#27010;&#29575;&#24046;&#24322;&#65288;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;KL&#12289;Wasserstein&#21644;&#24635;&#21464;&#24046;&#65289;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#30340;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#65292;&#26681;&#25454;&#21518;&#39564;&#20998;&#24067;&#30340;&#23646;&#24615;&#36873;&#25321;&#26368;&#20339;&#26041;&#26696;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#26089;&#26399;&#32467;&#26524;&#32852;&#31995;&#36215;&#26469;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#29305;&#23450;&#26696;&#20363;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#30028;&#38480;&#23454;&#20363;&#21270;&#20026;&#35757;&#32451;&#30446;&#26631;&#65292;&#20135;&#29983;&#20102;&#38750;&#24179;&#20961;&#30340;&#20445;&#35777;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties. We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases. We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances.
&lt;/p&gt;</description></item><item><title>Hydragen&#26159;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#20998;&#35299;&#20026;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#65292;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#31471;&#21040;&#31471;LLM&#21534;&#21520;&#37327;&#22810;&#36798;32&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05099</link><description>&lt;p&gt;
Hydragen&#65306;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hydragen: High-Throughput LLM Inference with Shared Prefixes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05099
&lt;/p&gt;
&lt;p&gt;
Hydragen&#26159;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#20998;&#35299;&#20026;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#65292;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#31471;&#21040;&#31471;LLM&#21534;&#21520;&#37327;&#22810;&#36798;32&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29616;&#22312;&#24050;&#32463;&#37096;&#32626;&#21040;&#25968;&#20159;&#29992;&#25143;&#19978;&#12290;LLM&#25512;&#29702;&#36890;&#24120;&#22312;&#20849;&#20139;&#21069;&#32512;&#30340;&#24207;&#21015;&#25209;&#27425;&#19978;&#25191;&#34892;&#65292;&#20363;&#22914;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#31034;&#12290;&#22312;&#36825;&#31181;&#22823;&#25209;&#37327;&#35774;&#32622;&#19979;&#65292;&#35299;&#30721;&#21487;&#33021;&#20250;&#21463;&#21040;&#27880;&#24847;&#25805;&#20316;&#30340;&#29942;&#39048;&#65292;&#35813;&#25805;&#20316;&#20174;&#20869;&#23384;&#20013;&#35835;&#21462;&#22823;&#22411;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#24182;&#20026;&#25209;&#27425;&#20013;&#30340;&#27599;&#20010;&#24207;&#21015;&#35745;&#31639;&#20302;&#25928;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Hydragen&#65292;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#30828;&#20214;&#24863;&#30693;&#31934;&#30830;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#12290;Hydragen&#23558;&#27880;&#24847;&#21147;&#20998;&#21035;&#35745;&#31639;&#22312;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#19978;&#12290;&#36825;&#31181;&#20998;&#35299;&#36890;&#36807;&#22312;&#24207;&#21015;&#20043;&#38388;&#25209;&#37327;&#26597;&#35810;&#19968;&#36215;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;&#35835;&#21462;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21069;&#32512;&#27880;&#24847;&#21147;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#30828;&#20214;&#21451;&#22909;&#30340;&#30697;&#38453;&#20056;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#31471;&#21040;&#31471;&#30340;LLM&#21534;&#21520;&#37327;&#25552;&#39640;&#22810;&#36798;32&#20493;&#65292;&#36229;&#36807;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#21644;&#20849;&#20139;&#21069;&#32512;&#30340;&#38271;&#24230;&#22686;&#21152;&#65292;&#36895;&#24230;&#25552;&#39640;&#30340;&#24133;&#24230;&#20063;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.05098</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#23494;&#24230;&#25110;&#33021;&#37327;&#20989;&#25968;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#25193;&#25955;&#32467;&#26500;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#25311;&#30340;&#21464;&#20998;&#26041;&#27861;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#65288;&#36830;&#32493;&#29983;&#25104;&#27969;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#21516;&#26102;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#36136;&#30097;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#25506;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#22312;https://github.com/GFNOrg/gfn-diffusion&#65292;&#20316;&#20026;&#26410;&#26469;&#22312;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#19978;&#24037;&#20316;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
&lt;/p&gt;</description></item><item><title>NITO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Boundary Point Order-Invariant MLP&#65288;BPOM&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#26080;&#20998;&#36776;&#29575;&#21644;&#22495;&#19981;&#21487;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#30701;&#30340;&#26102;&#38388;&#29983;&#25104;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.05073</link><description>&lt;p&gt;
NITO:&#31070;&#32463;&#38544;&#24335;&#22330;&#29992;&#20110;&#26080;&#20998;&#36776;&#29575;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
NITO: Neural Implicit Fields for Resolution-free Topology Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05073
&lt;/p&gt;
&lt;p&gt;
NITO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Boundary Point Order-Invariant MLP&#65288;BPOM&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#26080;&#20998;&#36776;&#29575;&#21644;&#22495;&#19981;&#21487;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20197;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#30701;&#30340;&#26102;&#38388;&#29983;&#25104;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20248;&#21270;&#26159;&#24037;&#31243;&#35774;&#35745;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#31354;&#38388;&#20013;&#20197;&#26368;&#20339;&#26041;&#24335;&#20998;&#24067;&#26448;&#26009;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#38544;&#24335;&#25299;&#25169;&#20248;&#21270;(NITO)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;NITO&#26159;&#31532;&#19968;&#20010;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25299;&#25169;&#20248;&#21270;&#20013;&#25552;&#20379;&#26080;&#20998;&#36776;&#29575;&#21644;&#22495;&#19981;&#21487;&#30693;&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;&#19982;SOTA&#30340;&#25193;&#25955;&#27169;&#22411;&#30456;&#27604;&#65292;NITO&#21512;&#25104;&#30340;&#32467;&#26500;&#20855;&#26377;&#39640;&#36798;7&#20493;&#30340;&#32467;&#26500;&#25928;&#29575;&#65292;&#24182;&#19988;&#25152;&#38656;&#26102;&#38388;&#21482;&#26377;&#21313;&#20998;&#20043;&#19968;&#12290;&#22312;NITO&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#36793;&#30028;&#28857;&#39034;&#24207;&#19981;&#21464;&#30340;MLP&#65288;BPOM&#65289;&#65292;&#20197;&#31232;&#30095;&#21644;&#22495;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#34920;&#31034;&#36793;&#30028;&#26465;&#20214;&#65292;&#36828;&#31163;&#26114;&#36149;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;NITO&#35268;&#36991;&#20102;&#38480;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#22788;&#20110;&#32467;&#26500;&#21270;&#21644;&#22266;&#23450;&#22823;&#23567;&#22495;&#30340;&#39046;&#22495;&#21644;&#20998;&#36776;&#29575;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance. We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning. NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization. NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time. In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches. Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.05071</link><description>&lt;p&gt;
&#25193;&#23637;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#28385;&#36275;rho-&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#25110;&#22312;rho-&#24369;Minty&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;MVI&#65289;&#20013;&#23384;&#22312;&#35299;&#30340;&#32422;&#26463;&#65292;L-&#20809;&#28369;&#30340;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#25968;rho&gt;0&#30340;&#36739;&#22823;&#20540;&#23545;&#24212;&#26356;&#39640;&#30340;&#38750;&#20984;&#24615;&#31243;&#24230;&#12290;&#36825;&#20123;&#38382;&#39064;&#31867;&#21253;&#25324;&#20004;&#20010;&#29609;&#23478;&#24378;&#21270;&#23398;&#20064;&#65292;&#20132;&#20114;&#20027;&#23548;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20197;&#21450;&#26576;&#20123;&#32463;&#20856;&#26497;&#23567;&#26497;&#22823;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#21512;&#25104;&#27979;&#35797;&#38382;&#39064;&#12290;&#24050;&#26377;&#29468;&#24819;&#35748;&#20026;&#19968;&#38454;&#26041;&#27861;&#21487;&#23481;&#24525;&#26368;&#22823;rho&#20026;1/L&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#24050;&#20572;&#28382;&#22312;&#26356;&#20005;&#26684;&#30340;&#35201;&#27714;rho&lt;1/2L&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20855;&#26377;&#36830;&#24102;&#20559;&#24207;&#29305;&#24615;&#25110;&#24369;MVI&#26465;&#20214;&#19979;&#65292;rho&lt;1/L&#30340;&#26368;&#20248;&#25110;&#26368;&#20339;&#24050;&#30693;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#31639;&#27861;&#26159;Halpern&#21644;Krasnosel'ski&#301;-Mann (KM)&#36845;&#20195;&#30340;&#38750;&#31934;&#30830;&#21464;&#31181;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;g...
&lt;/p&gt;
&lt;p&gt;
We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\rho$-cohypomonotonicity or admitting a solution to the $\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\rho&gt;0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\rho &lt; \frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\rho &lt; \frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\u{\i}-Mann (KM) iterations. We also provide algorithms and complexity g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05067</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#65306;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#21040;&#23567;&#23610;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29616;&#35937;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#20934;&#30830;&#26377;&#25928;&#22320;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#26222;&#36941;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#32806;&#26041;&#27861;&#23545;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#36827;&#34892;&#34920;&#24449;&#30340;&#26032;&#30340;&#27714;&#35299;&#27169;&#24335;&#12290;&#36890;&#36807;&#29420;&#31435;&#22320;&#24314;&#27169;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#35270;&#20026;&#20174;&#23646;&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35889;PINN&#26041;&#27861;&#65292;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#25509;&#36817;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#32500;Kuramot-Sivashinsky (KS)&#26041;&#31243;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;Navier-Stokes (NS)&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#32593;&#26684;&#12289;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#12289;&#24102;&#22122;&#22768;&#30340;&#22823;&#23610;&#24230;&#25968;&#25454;&#21644;&#39640;&#32500;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.05052</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#36827;&#34892;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#20010;&#36890;&#29992;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning from Multiple Distributions: A General Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#35774;&#32622;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#20998;&#24067;&#20043;&#38388;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#26080;&#38656;&#20551;&#35774;&#30828;&#24178;&#39044;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#24674;&#22797;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#27979;&#37327;&#21464;&#37327;&#65288;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65289;&#21482;&#26159;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#30340;&#27010;&#24565;&#25110;&#23545;&#35937;&#65289;&#30340;&#25968;&#23398;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#25110;&#23545;&#31995;&#32479;&#36827;&#34892;&#36866;&#24403;&#30340;&#26356;&#25913;&#65292;&#24674;&#22797;&#38544;&#34255;&#30340;&#22240;&#26524;&#21464;&#37327;$Z_i$&#20197;&#21450;&#30001;&#22270;$\mathcal{G}_Z$&#34920;&#31034;&#30340;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#26368;&#36817;&#34987;&#31216;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#20851;&#27880;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#65288;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#25110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#12289;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#35774;&#32622;&#65292;&#19981;&#38656;&#35201;&#20551;&#35774;&#20998;&#24067;&#25913;&#21464;&#32972;&#21518;&#23384;&#22312;&#30828;&#24178;&#39044;&#12290;&#25105;&#20204;&#26088;&#22312;&#22312;&#36825;&#20010;&#22522;&#26412;&#24773;&#20917;&#19979;&#24320;&#21457;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65307;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#26377;&#21161;&#20110;&#30475;&#21040;&#20854;&#20182;&#20551;&#35774;&#65288;&#22914;&#21442;&#25968;&#22240;&#26524;&#27169;&#22411;&#25110;&#30828;&#24178;&#39044;&#65289;&#25552;&#20379;&#30340;&#29420;&#29305;&#22909;&#22788;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#24674;&#22797;&#36807;&#31243;&#20013;&#23545;&#22270;&#30340;&#31232;&#30095;&#24615;&#32422;&#26463;&#19979;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#23398;&#20064;&#20986;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.05050</link><description>&lt;p&gt;
Federated Learning&#33021;&#22815;&#25214;&#21040;&#26377;&#30410;&#30340;&#22909;&#21451;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Can Find Friends That Are Beneficial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Federated Learning (FL)&#20013;&#65292;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#23458;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26082;&#24102;&#26469;&#20102;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#23458;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#21512;&#20316;&#37117;&#26159;&#26377;&#30410;&#30340;&#65307;&#26377;&#20123;&#29978;&#33267;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#20026;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#23458;&#25143;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#35782;&#21035;&#20986;&#25968;&#25454;&#20998;&#24067;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#20165;&#32858;&#21512;&#20855;&#26377;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#25509;&#25910;&#30340;&#26356;&#26032;&#30340;&#26041;&#27861;&#19981;&#30456;&#19978;&#19979;&#12290;&#27492;&#22806;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;&#30001;&#25105;&#20204;&#30340;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#30340;FL&#26041;&#27861;&#12290;&#36825;&#24378;&#35843;&#20102;&#23457;&#24910;&#36873;&#25321;&#23458;&#25143;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20026;&#26410;&#26469;&#26356;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;FL&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#35774;&#35745;&#21644;&#35757;&#32451;GNN&#26102;&#21033;&#29992;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#21644;&#35299;&#37322;&#36741;&#21161;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#23567;&#20110;&#35299;&#37322;&#19981;&#21487;&#30693;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.05039</link><description>&lt;p&gt;
PAC&#23398;&#20064;&#29702;&#35770;&#22312;&#20445;&#25345;&#35299;&#37322;&#30340;&#22270;&#25200;&#21160;&#19979;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
PAC Learnability under Explanation-Preserving Graph Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#35774;&#35745;&#21644;&#35757;&#32451;GNN&#26102;&#21033;&#29992;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#21644;&#35299;&#37322;&#36741;&#21161;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#23567;&#20110;&#35299;&#37322;&#19981;&#21487;&#30693;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27169;&#22411;&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#29983;&#29289;&#23398;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#22270;&#35299;&#37322;&#26159;&#19968;&#20010;&#23376;&#22270;&#65292;&#23427;&#26159;&#30456;&#23545;&#20110;&#20854;&#20998;&#31867;&#26631;&#31614;&#32780;&#35328;&#36755;&#20837;&#22270;&#30340;&#19968;&#20010;&#8220;&#20960;&#20046;&#36275;&#22815;&#8221;&#30340;&#32479;&#35745;&#37327;&#12290;&#22240;&#27492;&#65292;&#20998;&#31867;&#26631;&#31614;&#23545;&#20110;&#19981;&#23646;&#20110;&#35299;&#37322;&#23376;&#22270;&#30340;&#22270;&#36793;&#30340;&#25200;&#21160;&#26159;&#19981;&#21464;&#30340;&#65292;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#27010;&#29575;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#31181;&#21033;&#29992;&#36825;&#31181;&#25200;&#21160;&#19981;&#21464;&#24615;&#35774;&#35745;&#21644;&#35757;&#32451;GNN&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#20102;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#35268;&#21017;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#36741;&#21161;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#23567;&#20110;&#35299;&#37322;&#19981;&#21487;&#30693;&#23398;&#20064;&#12290;&#25509;&#19979;&#26469;&#65292;&#32771;&#34385;&#20102;&#35299;&#37322;&#36741;&#21161;&#25968;&#25454;&#22686;&#24378;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others. Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data. A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs. First, explanation-assisted learning rules are considered. It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. Next, explanation-assisted data augmentation is considered, where 
&lt;/p&gt;</description></item><item><title>&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#26159;&#19968;&#31181;&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20027;&#35201;&#26550;&#26500;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05033</link><description>&lt;p&gt;
&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulated Overparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05033
&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#26159;&#19968;&#31181;&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20027;&#35201;&#26550;&#26500;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#12290;SOP&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;SOP&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#26174;&#33879;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21482;&#20351;&#29992;&#20854;&#20013;&#36739;&#23567;&#12289;&#39640;&#25928;&#30340;&#23376;&#38598;&#36827;&#34892;&#23454;&#38469;&#35745;&#31639;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20027;&#35201;&#26550;&#26500;&#65288;&#21253;&#25324;Transformer&#27169;&#22411;&#65289;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#12290; majority kernels&#20351;&#24471;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#65288;&#22681;&#19978;&#25346;&#38047;&#26102;&#38388;&#65289;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called "majority kernels", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#24615;&#24341;&#23548;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#24378;&#20984;&#24615;&#26469;&#25913;&#21892;&#20854;&#24179;&#22374;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#26041;&#31243;&#26469;&#36817;&#20284;&#35745;&#31639;&#24378;&#20984;&#24615;&#21442;&#25968;&#65292;&#24182;&#20197;&#38543;&#26426;&#21270;&#30340;&#26041;&#24335;&#23547;&#25214;&#26368;&#23567;&#21270;&#35813;&#21442;&#25968;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#30701;&#30340;&#36816;&#34892;&#26102;&#38388;&#19979;&#21462;&#24471;&#20102;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05025</link><description>&lt;p&gt;
&#24378;&#20984;&#24615;&#24341;&#23548;&#30340;&#26356;&#24179;&#22374;&#25439;&#22833;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Strong convexity-guided hyper-parameter optimization for flatter losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#24615;&#24341;&#23548;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#24378;&#20984;&#24615;&#26469;&#25913;&#21892;&#20854;&#24179;&#22374;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#26041;&#31243;&#26469;&#36817;&#20284;&#35745;&#31639;&#24378;&#20984;&#24615;&#21442;&#25968;&#65292;&#24182;&#20197;&#38543;&#26426;&#21270;&#30340;&#26041;&#24335;&#23547;&#25214;&#26368;&#23567;&#21270;&#35813;&#21442;&#25968;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#30701;&#30340;&#36816;&#34892;&#26102;&#38388;&#19979;&#21462;&#24471;&#20102;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30333;&#30418;&#26041;&#27861;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#21463;&#21040;&#26368;&#36817;&#30340;&#20851;&#20110;&#24179;&#22374;&#26368;&#23567;&#20540;&#21644;&#27867;&#21270;&#20043;&#38388;&#20851;&#31995;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#24378;&#20984;&#24615;&#21644;&#20854;&#24179;&#22374;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#24378;&#20984;&#24615;&#26469;&#23547;&#25214;&#25913;&#21892;&#24179;&#22374;&#24615;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#24378;&#20984;&#24615;&#21442;&#25968;&#30340;&#38381;&#24335;&#26041;&#31243;&#65292;&#24182;&#23581;&#35797;&#20197;&#38543;&#26426;&#21270;&#30340;&#26041;&#24335;&#23547;&#25214;&#26368;&#23567;&#21270;&#23427;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;14&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26356;&#30701;&#30340;&#36816;&#34892;&#26102;&#38388;&#19979;&#21462;&#24471;&#20102;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05015</link><description>&lt;p&gt;
&#23545;&#20110;&#26448;&#26009;&#21457;&#29616;&#26469;&#35828;&#65292;&#23545;LLM&#30340;&#25308;&#21344;&#24237;&#20248;&#21270;&#26159;&#21542;&#30495;&#30340;&#26377;&#21033;&#65311;&#19968;&#20010;&#20919;&#38745;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26159;&#24403;&#20195;&#26448;&#26009;&#21457;&#29616;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#36825;&#31181;&#24037;&#20316;&#27969;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20351;&#31185;&#23398;&#23478;&#33021;&#22815;&#23558;&#20808;&#21069;&#30340;&#39046;&#22495;&#30693;&#35782;&#24212;&#29992;&#21040;&#23545;&#22823;&#35268;&#27169;&#20998;&#23376;&#31354;&#38388;&#30340;&#39640;&#25928;&#25506;&#32034;&#20013;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#20808;&#21069;&#30693;&#35782;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#24418;&#24335;&#65292;&#20294;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25152;&#21253;&#21547;&#30340;&#36741;&#21161;&#31185;&#23398;&#30693;&#35782;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#36720;&#21160;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#30740;&#31350;&#20165;&#25506;&#32034;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#26448;&#26009;&#25628;&#32034;&#30340;LLMs&#12290;&#23454;&#38469;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#28857;&#20272;&#35745;&#30340;&#38750;&#36125;&#21494;&#26031;LLMs&#20013;&#33719;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#26159;BO&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#30495;&#30340;&#26377;&#21161;&#20110;&#21152;&#36895;&#22312;&#20998;&#23376;&#31354;&#38388;&#20013;&#30340;&#27491;&#35268;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#37319;&#21462;&#20102;&#20919;&#38745;&#12289;&#23458;&#35266;&#30340;&#31435;&#22330;&#12290;&#36825;&#26159;&#36890;&#36807;&#20180;&#32454;&#22320;&#65288;i&#65289;&#23558;LLMs&#35270;&#20026;&#26631;&#20934;&#20294;&#27491;&#35268;&#30340;BO&#26367;&#20195;&#27169;&#22411;&#30340;&#22266;&#23450;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21033;&#29992;&#21442;&#25968;&#25928;&#33021;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#37319;&#29992;&#27973;&#23618;&#32467;&#26500;&#65292;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20250;&#23436;&#20840;&#24573;&#30053;&#31232;&#30095;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#22120;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#20020;&#30028;&#28857;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.05013</link><description>&lt;p&gt;
&#29992;&#33258;&#32534;&#30721;&#22120;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#65306;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#33258;&#32534;&#30721;&#22120;&#20013;&#65292;&#21363;&#20351;&#37319;&#29992;&#27973;&#23618;&#32467;&#26500;&#65292;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20250;&#23436;&#20840;&#24573;&#30053;&#31232;&#30095;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#22120;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#20020;&#30028;&#28857;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#26377;&#25439;&#25968;&#25454;&#21387;&#32553;&#30340;&#22810;&#20010;&#23454;&#35777;&#20998;&#25903;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#27973;&#23618;&#20004;&#23618;&#35774;&#32622;&#19979;&#65292;&#22522;&#26412;&#30340;&#29702;&#35770;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#20013;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#65311;&#23545;&#20110;&#20856;&#22411;&#30340;&#31232;&#30095;&#39640;&#26031;&#25968;&#25454;&#30340;1&#20301;&#21387;&#32553;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25910;&#25947;&#20110;&#19968;&#20010;&#23436;&#20840;&#24573;&#30053;&#36755;&#20837;&#30340;&#31232;&#30095;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#21387;&#32553;&#39640;&#26031;&#28304;&#65288;&#27809;&#26377;&#31232;&#30095;&#24615;&#65289;&#30340;&#24615;&#33021;&#30456;&#21516;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#22120;&#30340;&#24418;&#29366;&#30340;&#30456;&#21464;&#29616;&#35937;&#30340;&#35777;&#25454;&#65292;&#20316;&#20026;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#20989;&#25968;&#65306;&#22312;&#20020;&#30028;&#31232;&#30095;&#27700;&#24179;&#20197;&#19979;&#65292;&#26368;&#23567;&#21270;&#22120;&#26159;&#19968;&#20010;&#38543;&#26426;&#22343;&#21248;&#36873;&#25321;&#30340;&#26059;&#36716;&#65288;&#23601;&#20687;&#38750;&#31232;&#30095;&#25968;&#25454;&#30340;&#21387;&#32553;&#19968;&#26679;&#65289;&#65307;&#22312;&#20020;&#30028;&#31232;&#30095;&#27700;&#24179;&#20197;&#19978;&#65292;&#26368;&#23567;&#21270;&#22120;&#26159;&#36523;&#20221;&#21464;&#25442;&#65288;&#38500;&#20102;&#19968;&#20010;+1&#30340;&#20559;&#31227;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05011</link><description>&lt;p&gt;
&#23548;&#33322;&#22797;&#26434;&#24615;&#65306;&#36890;&#36807;&#25193;&#23637;&#31383;&#21475;&#21305;&#37197;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;
&lt;/p&gt;
&lt;p&gt;
Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#65292;&#39318;&#27425;&#23581;&#35797;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#21407;&#22987;&#22270;&#35889;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35889;&#31934;&#31616;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#32039;&#20945;&#30340;&#22270;&#35889;&#26469;&#20943;&#23569;&#22823;&#35268;&#27169;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20026;&#20943;&#23569;&#35757;&#32451;GNNs&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#20934;&#30830;&#22797;&#21046;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#22270;&#35889;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#26080;&#25439;&#31934;&#31616;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;&#24182;&#25581;&#31034;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#22312;&#20248;&#21270;&#31934;&#31616;&#22270;&#35889;&#26102;&#25552;&#20379;&#20102;&#26469;&#33258;&#21407;&#22987;&#22270;&#35889;&#30340;&#20559;&#20506;&#21644;&#21463;&#38480;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#31934;&#31616;&#22270;&#35889;&#30340;&#35268;&#27169;&#21644;&#21151;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#36830;&#25509;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#23454;&#29616;&#26080;&#25439;&#22270;&#35889;&#31934;&#31616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05008</link><description>&lt;p&gt;
&#39640;&#25928;ViT-SAM: &#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05008
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#8212;&#8212;&#39640;&#25928;ViT-SAM&#12290;&#25105;&#20204;&#20445;&#30041;&#20102;SAM&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#29992;&#39640;&#25928;ViT&#26367;&#25442;&#20102;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#22312;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SAM-ViT-H&#22270;&#20687;&#32534;&#30721;&#22120;&#21040;&#39640;&#25928;ViT&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;SA-1B&#25968;&#25454;&#38598;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#25928;ViT&#30340;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#39640;&#25928;ViT-SAM&#22312;A100 GPU&#19978;&#30456;&#27604;SAM-ViT-H&#23454;&#29616;&#20102;48.9&#20493;&#30340;TensorRT&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#22312;https://github.com/mit-han-lab/efficientvit&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
&lt;/p&gt;</description></item><item><title>FairDebugger&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#30340;&#31995;&#32479;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#25214;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.05007</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Example-based Explanations for Random Forests using Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05007
&lt;/p&gt;
&lt;p&gt;
FairDebugger&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#30340;&#31995;&#32479;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#25214;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20363;&#22914;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#26131;&#20110;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#21463;&#21040;&#24191;&#27867;&#30340;&#27426;&#36814;&#21644;&#35748;&#21487;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20063;&#34987;&#21457;&#29616;&#20250;&#20135;&#29983;&#24847;&#22806;&#25110;&#20855;&#26377;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#37492;&#20110;&#23427;&#20204;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#26377;&#24517;&#35201;&#25214;&#20986;&#23427;&#20204;&#24847;&#22806;&#21644;&#20855;&#26377;&#27495;&#35270;&#24615;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#21644;&#35843;&#35797;&#22522;&#20110;&#26641;&#30340;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#19981;&#22810;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FairDebugger&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#36776;&#35782;&#23548;&#33268;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#26524;&#20013;&#20844;&#24179;&#24615;&#36829;&#35268;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#30340;&#26032;&#22411;&#30740;&#31350;&#25104;&#26524;&#30340;&#31995;&#32479;&#12290;FairDebugger&#29983;&#25104;&#21069;-k&#20010;&#35299;&#37322;&#65288;&#20197;&#19968;&#33268;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#30340;&#24418;&#24335;&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;FairDebugger&#39318;&#20808;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#36776;&#35782;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#20013;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#20559;&#32622;&#30028;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#31574;&#30053;&#26080;&#27861;&#24212;&#29992;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#35774;&#32622;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05002</link><description>&lt;p&gt;
&#38543;&#26426;&#20559;&#32622;&#30028;&#23545;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Randomized Confidence Bounds for Stochastic Partial Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#20013;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#20559;&#32622;&#30028;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#31574;&#30053;&#26080;&#27861;&#24212;&#29992;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#35774;&#32622;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#30417;&#25511; (PM) &#26694;&#26550;&#25552;&#20379;&#20102;&#36890;&#36807;&#19981;&#23436;&#25972;&#30340;&#21453;&#39304;&#36827;&#34892;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#34920;&#36848;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#65292;&#32780;&#29615;&#22659;&#21516;&#26102;&#36873;&#25321;&#19968;&#20010;&#32467;&#26524;&#12290;&#28982;&#21518;&#20195;&#29702;&#35266;&#23519;&#21040;&#19968;&#20010;&#20165;&#37096;&#20998;&#25552;&#20379;&#20449;&#24687;&#20851;&#20110;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#32467;&#26524;&#30340;&#21453;&#39304;&#20449;&#21495;&#12290;&#20195;&#29702;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#21453;&#39304;&#20449;&#21495;&#36873;&#25321;&#33021;&#22815;&#26368;&#23567;&#21270;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#32047;&#35745;&#25439;&#22833;&#30340;&#21160;&#20316;&#12290;&#22312;&#24773;&#22659; PM &#20013;&#65292;&#32467;&#26524;&#20381;&#36182;&#20110;&#20195;&#29702;&#22312;&#27599;&#36718;&#36873;&#25321;&#21160;&#20316;&#20043;&#21069;&#21487;&#35266;&#23519;&#21040;&#30340;&#26576;&#20123;&#38468;&#21152;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#38543;&#26426;&#32467;&#26524;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#30340; PM &#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#32622;&#20449;&#30028;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#30340;&#26032;&#31867;&#26041;&#27861;&#65292;&#23558;&#36951;&#25022;&#20445;&#35777;&#25193;&#23637;&#21040;&#29616;&#26377;&#30340;&#38543;&#26426;&#31574;&#30053;&#19981;&#36866;&#29992;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340; RandCBP &#21644; RandCBPside* &#31574;&#30053;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. On each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action on each round. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of strategies based on the randomization of deterministic confidence bounds, that extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies improve state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#65292;&#20026;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#26041;&#27861;&#22312;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04997</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#29983;&#25104;&#22411;&#27969;&#65306;&#23454;&#29616;&#22810;&#27169;&#24577;&#27969;&#24182;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#65292;&#20026;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#26041;&#27861;&#22312;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#30456;&#32467;&#21512;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#30340;&#31163;&#25955;&#25968;&#25454;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#23454;&#29616;&#12290;DFMs&#36890;&#36807;&#31616;&#21333;&#30340;&#25512;&#23548;&#21253;&#25324;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29305;&#23450;&#23454;&#20363;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19978;&#25913;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;DFMs&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#20110;&#27969;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20849;&#35774;&#35745;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#20801;&#35768;&#20351;&#29992;&#21516;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#24207;&#21015;&#25110;&#32467;&#26500;&#30340;&#28789;&#27963;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or str
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriorBoost&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#32858;&#21512;&#22238;&#24212;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24418;&#25104;&#19968;&#20123;&#36234;&#26469;&#36234;&#21516;&#36136;&#30340;&#26679;&#26412;bags&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#22312;&#20107;&#20214;&#32423;&#21035;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04987</link><description>&lt;p&gt;
PriorBoost&#65306;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#32858;&#21512;&#22238;&#24212;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriorBoost&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#32858;&#21512;&#22238;&#24212;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24418;&#25104;&#19968;&#20123;&#36234;&#26469;&#36234;&#21516;&#36136;&#30340;&#26679;&#26412;bags&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#22312;&#20107;&#20214;&#32423;&#21035;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#32858;&#21512;&#22238;&#24212;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#29992;&#20110;&#20107;&#20214;&#32423;&#25439;&#22833;&#20989;&#25968;&#30340;&#32858;&#21512;&#38598;&#21512;&#26500;&#24314;&#65288;&#22312;&#25991;&#29486;&#20013;&#31216;&#20026;bags&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#65292;&#26368;&#20248;&#30340;bagging&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#32500;&#22823;&#23567;&#21463;&#38480;&#30340;k-means&#32858;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20351;&#29992;&#31574;&#21010;&#30340;bags&#27604;&#38543;&#26426;bags&#26356;&#26377;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PriorBoost&#31639;&#27861;&#65292;&#23427;&#33258;&#36866;&#24212;&#22320;&#24418;&#25104;&#26679;&#26412;bags&#65292;&#20351;&#20854;&#22312;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#20010;&#20307;&#22238;&#24212;&#26041;&#38754;&#36234;&#26469;&#36234;&#21516;&#36136;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32858;&#21512;&#23398;&#20064;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;PriorBoost&#22312;&#20107;&#20214;&#32423;&#21035;&#39044;&#27979;&#20013;&#32463;&#24120;&#23454;&#29616;&#26368;&#20248;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SHAP&#32858;&#31867;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#26469;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#27169;&#22411;&#65292;&#24179;&#34913;&#22797;&#26434;&#24615;&#19982;&#39044;&#27979;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#24182;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33021;&#32791;&#39044;&#27979;&#21450;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04982</link><description>&lt;p&gt;
&#36229;&#36234;&#35299;&#37322;&#65306;&#22522;&#20110;SHAP&#32858;&#31867;&#30340;XAI&#33258;&#36866;&#24212;&#23398;&#20064;&#29992;&#20110;&#33021;&#32791;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;SHAP&#32858;&#31867;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#26469;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#27169;&#22411;&#65292;&#24179;&#34913;&#22797;&#26434;&#24615;&#19982;&#39044;&#27979;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#24182;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33021;&#32791;&#39044;&#27979;&#21450;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#33021;&#32791;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#21033;&#29992;SHAP&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#26469;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#27169;&#22411;&#65292;&#24179;&#34913;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#65288;1&#65289;&#33719;&#21462;SHAP&#20540;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#65288;2&#65289;&#23545;SHAP&#20540;&#36827;&#34892;&#32858;&#31867;&#20197;&#35782;&#21035;&#19981;&#21516;&#30340;&#27169;&#24335;&#21644;&#24322;&#24120;&#20540;&#65292;&#65288;3&#65289;&#26681;&#25454;&#24471;&#21040;&#30340;SHAP&#32858;&#31867;&#29305;&#24449;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#30830;&#20445;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#24314;&#31569;&#29289;&#33021;&#32791;&#35760;&#24405;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#21450;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#12289;&#22238;&#24402;&#21644;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts. Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance. We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics. Our approach mitigates overfitting and ensures robustness in handling data distribution shifts. We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#36890;&#29992;&#21270;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#22312;&#36866;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04980</link><description>&lt;p&gt;
&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of feature learning in two-layer networks after one gradient-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#21518;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#36890;&#29992;&#21270;&#35823;&#24046;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#22312;&#36866;&#24212;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#22312;&#20351;&#29992;&#21333;&#19968;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#35757;&#32451;&#21518;&#22914;&#20309;&#25913;&#36827;&#26680;&#24515;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#20511;&#21161;&#20110;&#65288;Ba et al., 2022&#65289;&#19982;&#38750;&#32447;&#24615;&#23574;&#23792;&#30697;&#38453;&#27169;&#22411;&#30340;&#20851;&#32852;&#20197;&#21450;&#23545;&#39640;&#26031;&#27867;&#21270;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;Dandi et al., 2023&#65289;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#25968;$n$&#12289;&#23485;&#24230;$p$&#21644;&#36755;&#20837;&#32500;&#24230;$d$&#25104;&#27604;&#20363;&#22686;&#38271;&#30340;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#19968;&#33268;&#24615;&#35823;&#24046;&#25551;&#36848;&#12290;&#25105;&#20204;&#20934;&#30830;&#22320;&#21051;&#30011;&#20102;&#36866;&#24212;&#25968;&#25454;&#23545;&#20110;&#32593;&#32476;&#22312;&#26799;&#24230;&#26041;&#21521;&#19978;&#39640;&#25928;&#23398;&#20064;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#65292;&#32593;&#32476;&#21482;&#33021;&#34920;&#36798;&#32447;&#24615;&#20989;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#22312;&#22823;&#23398;&#20064;&#29575;$\eta=\Theta_{d}(d)$&#30340;&#24773;&#20917;&#19979;&#29305;&#24449;&#23398;&#20064;&#23545;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#39318;&#20010;&#20934;&#30830;&#25551;&#36848;&#65292;&#36229;&#36234;&#20102;&#26680;&#24515;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04933</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20855;&#26377;&#19978;&#19979;&#25991;&#29615;&#22659;&#30340;&#19981;&#23433;&#23425;&#36172;&#21338;&#26426;&#20013;&#30340;&#24212;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04933
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23433;&#23425;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#29992;&#20110;&#24314;&#27169;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#39034;&#24207;&#36164;&#28304;&#20998;&#37197;&#12290;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#65292;&#28508;&#22312;&#30340;&#36716;&#31227;&#21160;&#24577;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RMAB&#22312;&#32447;RL&#26041;&#27861;&#26080;&#27861;&#25972;&#21512;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20844;&#20849;&#21355;&#29983;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#23646;&#24615;&#65292;&#22914;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38750;&#31283;&#24577;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19978;&#19979;&#25991;RMAB&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;BCoR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#27169;&#25311;&#21508;&#31181;&#22797;&#26434;&#30340;RMAB&#35774;&#32622;&#65292;&#22914;&#19978;&#19979;&#25991;&#21644;&#38750;&#31283;&#24577;&#30340;RMAB&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20869;&#37096;&#21644;&#21508;&#20010;&#33218;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;RMAB&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BCoR&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL). However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#22122;&#22768;&#27169;&#22411;&#21644;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#26469;&#32771;&#34385;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#26799;&#24230;&#27969;&#21160;&#21644;&#37325;&#26500;&#39057;&#35889;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.04930</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#34013;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Blue noise for diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#22122;&#22768;&#27169;&#22411;&#21644;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#26469;&#32771;&#34385;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#26799;&#24230;&#27969;&#21160;&#21644;&#37325;&#26500;&#39057;&#35889;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#65292;&#20294;&#36825;&#21487;&#33021;&#26080;&#27861;&#26368;&#20248;&#22320;&#32771;&#34385;&#21435;&#22122;&#32593;&#32476;&#37325;&#26500;&#30340;&#39057;&#35889;&#20869;&#23481;&#12290;&#23613;&#31649;&#30456;&#20851;&#22122;&#22768;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#20854;&#22312;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#31867;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#20869;&#37096;&#21644;&#36328;&#22270;&#20687;&#30340;&#30456;&#20851;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#21464;&#22122;&#22768;&#27169;&#22411;&#26469;&#23558;&#30456;&#20851;&#22122;&#22768;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#25104;&#30456;&#20851;&#22122;&#22768;&#25513;&#30721;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#30830;&#23450;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#34013;&#22122;&#22768;&#30456;&#27604;&#20165;&#20351;&#29992;&#39640;&#26031;&#30333;&#22122;&#22768;&#65288;&#38543;&#26426;&#22122;&#22768;&#65289;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20801;&#35768;&#22312;&#21333;&#20010;&#23567;&#25209;&#37327;&#20013;&#24341;&#20837;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#26799;&#24230;&#27969;&#21160;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TP&#24863;&#30693;&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25512;&#29702;&#37096;&#32626;&#26041;&#26696;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24310;&#36831;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#25968;&#25454;&#23616;&#37096;&#24615;&#21644;&#21033;&#29992;TP&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#20943;&#23569;&#20840;&#23616;&#36890;&#20449;&#65292;&#22312;&#22810;&#31181;TP&#35774;&#32622;&#19979;&#65292;&#22312;A100&#21644;H100 NVIDIA DGX&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.04925</link><description>&lt;p&gt;
TP&#24863;&#30693;&#30340;&#21435;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
TP-Aware Dequantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;TP&#24863;&#30693;&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25512;&#29702;&#37096;&#32626;&#26041;&#26696;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24310;&#36831;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#25968;&#25454;&#23616;&#37096;&#24615;&#21644;&#21033;&#29992;TP&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#20943;&#23569;&#20840;&#23616;&#36890;&#20449;&#65292;&#22312;&#22810;&#31181;TP&#35774;&#32622;&#19979;&#65292;&#22312;A100&#21644;H100 NVIDIA DGX&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#20013;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#19968;&#31181;&#20248;&#21270;&#30340;&#25512;&#29702;&#37096;&#32626;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#20869;&#26680;&#22312;&#19982;&#24352;&#37327;&#24182;&#34892;&#65288;TP&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;GPU&#20869;&#23384;&#35775;&#38382;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#65292;&#24182;&#21033;&#29992;TP&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#20943;&#23569;&#20840;&#23616;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312;A100&#21644;H100 NVIDIA DGX&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#23545;&#20110;&#21508;&#31181;TP&#35774;&#32622;&#65292;&#23545;&#20110;Llama-70B&#30340;&#36895;&#24230;&#25552;&#21319;&#39640;&#36798;1.81&#20493;&#65292;&#23545;&#20110;IBM WatsonX&#30340;Granite-20B MLP&#23618;&#38382;&#39064;&#23610;&#23544;&#30340;&#36895;&#24230;&#25552;&#21319;&#39640;&#36798;1.78&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel method that reduces model inference latency during distributed deployment of Large Language Models (LLMs). Our contribution is an optimized inference deployment scheme that address the current limitations of state-of-the-art quantization kernels when used in conjunction with Tensor Parallel (TP). Our method preserves data locality in GPU memory access patterns and exploits a priori knowledge of TP to reduce global communication. We demonstrate an up to 1.81x speedup over existing methods for Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04924</link><description>&lt;p&gt;
&#20004;&#20010;&#20132;&#26131;&#19981;&#20250;&#22256;&#25200;&#65306;&#36890;&#36807;&#26500;&#36896;&#21512;&#29702;&#30340;&#26799;&#24230;&#21305;&#37197;&#26469;&#21387;&#32553;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#35757;&#32451;&#24050;&#32463;&#22312;&#22270;&#34920;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#23436;&#25972;&#30340;&#22270;&#34920;&#21387;&#32553;&#25104;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#38598;&#12290;&#23613;&#31649;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#24378;&#35843;&#26799;&#24230;&#30340;&#21305;&#37197;&#26041;&#21521;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#36712;&#36857;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#36827;&#19968;&#27493;&#30001;&#21387;&#32553;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#24322;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#32047;&#31215;&#35823;&#24046;&#65292;&#23545;&#21387;&#32553;&#22270;&#34920;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory&#65288;\textbf{CTRL}&#65289;&#30340;&#26032;&#22411;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#24067;&#30340;&#20248;&#21270;&#36215;&#28857;&#21644;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Voronoi&#20505;&#36873;&#28857;&#36793;&#30028;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.04922</link><description>&lt;p&gt;
Voronoi Candidates&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Voronoi Candidates for Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04922
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Voronoi&#20505;&#36873;&#28857;&#36793;&#30028;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#20026;&#39640;&#25928;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#37319;&#38598;&#20934;&#21017;&#38656;&#35201;&#36827;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20869;&#37096;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#24341;&#36215;&#24456;&#22823;&#30340;&#24320;&#38144;&#12290;&#35768;&#22810;&#23454;&#38469;&#30340;BO&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#19981;&#37319;&#29992;&#23545;&#37319;&#38598;&#20989;&#25968;&#36827;&#34892;&#24418;&#24335;&#21270;&#36830;&#32493;&#20248;&#21270;&#65292;&#32780;&#26159;&#22312;&#26377;&#38480;&#30340;&#31354;&#38388;&#22635;&#20805;&#20505;&#36873;&#38598;&#19978;&#36827;&#34892;&#31163;&#25955;&#25628;&#32034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#20505;&#36873;&#28857;&#65292;&#20854;&#20301;&#20110;&#24403;&#21069;&#35774;&#35745;&#28857;&#30340;Voronoi&#38262;&#23884;&#36793;&#30028;&#19978;&#65292;&#22240;&#27492;&#23427;&#20204;&#19982;&#20004;&#20010;&#25110;&#22810;&#20010;&#35774;&#35745;&#28857;&#31561;&#36317;&#31163;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;Voronoi&#36793;&#30028;&#32780;&#19981;&#26126;&#30830;&#29983;&#25104;&#38262;&#23884;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#36866;&#24212;&#39640;&#32500;&#24230;&#20013;&#30340;&#22823;&#35774;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#26399;&#26395;&#25913;&#36827;&#26469;&#23545;&#19968;&#32452;&#27979;&#35797;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions. However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them. We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy
&lt;/p&gt;</description></item><item><title>Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.04915</link><description>&lt;p&gt;
Moco: &#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Moco: A Learnable Meta Optimizer for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04915
&lt;/p&gt;
&lt;p&gt;
Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#36890;&#24120;&#26159;NP&#38590;&#30340;&#12290;&#36807;&#21435;&#65292;&#36825;&#20123;&#38382;&#39064;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#30340;&#65292;&#20294;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#26080;&#27861;&#36827;&#19968;&#27493;&#25913;&#36827;&#24050;&#32463;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Moco&#23398;&#20064;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#20174;&#24403;&#21069;&#25628;&#32034;&#29366;&#24577;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#12290;&#36825;&#31181;&#20803;&#35757;&#32451;&#36807;&#31243;&#20197;&#25628;&#32034;&#36807;&#31243;&#20013;&#25214;&#21040;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#20026;&#30446;&#26631;&#65292;&#32473;&#23450;&#25628;&#32034;&#39044;&#31639;&#31561;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;Moco&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#12290;Moco&#26159;&#19968;&#20010;&#23436;&#20840;&#21487;&#23398;&#20064;&#30340;&#20803;&#20248;&#21270;&#22120;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#38382;&#39064;&#30340;&#23616;&#37096;&#25628;&#32034;&#25110;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#26368;&#22823;&#26368;&#23567;&#36153;&#29992;&#27969;&#38382;&#39064;&#20013;&#27979;&#35797;&#20102;Moco&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26397;&#21521;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#19988;&#31169;&#23494;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20116;&#31181;&#20195;&#34920;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04912</link><description>&lt;p&gt;
&#26397;&#21521;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#19988;&#31169;&#23494;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Biologically Plausible and Private Gene Expression Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26397;&#21521;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#19988;&#31169;&#23494;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20116;&#31181;&#20195;&#34920;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#19979;&#28216;&#24212;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24448;&#24448;&#20165;&#25253;&#21578;&#23545;&#20110;&#22522;&#26412;&#25351;&#26631;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#25454;&#20998;&#24067;&#34920;&#29616;&#20986;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#31995;&#32479;&#20998;&#26512;DP&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#33258;&#28982;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#20851;&#27880;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20116;&#31181;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;DP&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20174;&#19979;&#28216;&#25928;&#29992;&#12289;&#32479;&#35745;&#29305;&#24615;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#31561;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#27599;&#31181;DP&#29983;&#25104;&#26041;&#27861;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#20026;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#24182;&#25581;&#31034;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#26377;&#36259;&#21487;&#33021;&#24615;&#12290;&#20986;&#20154;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#27599;&#31181;DP&#29983;&#25104;&#26041;&#27861;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications. Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions. In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data. We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility. Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments. Perhaps surprisingly, our analysis re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29468;&#24819;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25945;&#23398;&#32500;&#24230;&#30340;&#32467;&#26524;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#20102;&#36229;&#31435;&#26041;&#20307;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;&#30340;&#25512;&#24191;&#12290;</title><link>https://arxiv.org/abs/2402.04907</link><description>&lt;p&gt;
&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#32452;&#21512;&#38382;&#39064;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On a Combinatorial Problem Arising in Machine Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29468;&#24819;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25945;&#23398;&#32500;&#24230;&#30340;&#32467;&#26524;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#20102;&#36229;&#31435;&#26041;&#20307;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#27169;&#22411;&#65292;&#20854;&#20013;&#25945;&#24072;&#26144;&#23556;&#26159;&#30001;&#27010;&#24565;&#21644;&#31034;&#20363;&#30340;&#22823;&#23567;&#20989;&#25968;&#26500;&#24314;&#30340;&#12290;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#20219;&#20309;&#27010;&#24565;&#25152;&#38656;&#30340;&#26368;&#23567;&#31034;&#20363;&#25968;&#37327;&#65292;&#21363;&#25152;&#35859;&#30340;&#25945;&#23398;&#32500;&#24230;&#12290;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;[7]&#29468;&#27979;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#20316;&#20026;&#27010;&#24565;&#31867;&#22823;&#23567;&#30340;&#20989;&#25968;&#26102;&#65292;&#26368;&#22351;&#24773;&#20917;&#21457;&#29983;&#22312;&#19968;&#33268;&#24615;&#30697;&#38453;&#21253;&#21547;&#20174;&#38646;&#21450;&#20197;&#19978;&#30340;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25968;&#23383;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#29468;&#24819;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#36229;&#31435;&#26041;&#20307;&#30340;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;[12]&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#30340;&#35777;&#26126;&#22522;&#20110;[10]&#30340;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a model of machine teaching where the teacher mapping is constructed from a size function on both concepts and examples. The main question in machine teaching is the minimum number of examples needed for any concept, the so-called teaching dimension. A recent paper [7] conjectured that the worst case for this model, as a function of the size of the concept class, occurs when the consistency matrix contains the binary representations of numbers from zero and up. In this paper we prove their conjecture. The result can be seen as a generalization of a theorem resolving the edge isoperimetry problem for hypercubes [12], and our proof is based on a lemma of [10].
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04906</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#24178;&#39044;&#25928;&#26524;&#65292;&#21363;&#27835;&#30103;&#25928;&#26524;&#65292;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524; (CATE) &#20272;&#35745;&#31561;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#32780;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931; (CMC) &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644; CATE &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20135;&#29983;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#22122;&#22768;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;CMC&#26694;&#26550;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#21306;&#38388;&#23485;&#24230;&#65292;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#65292;&#22312;&#36275;&#29699;&#36187;&#23395;&#20013;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#65292;&#20943;&#23569;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#21644;&#26080;&#25928;&#33457;&#36153;&#65292;&#20026;&#30495;&#23454;&#36275;&#29699;&#22242;&#38431;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#29699;&#21592;&#31119;&#21033;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04898</link><description>&lt;p&gt;
&#25104;&#21151;&#30340;&#21387;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#36275;&#29699;&#36816;&#21160;&#21592;&#21463;&#20260;&#39118;&#38505;&#21644;&#25552;&#39640;&#29699;&#38431;&#25104;&#21151;&#29575;&#30340;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#65292;&#22312;&#36275;&#29699;&#36187;&#23395;&#20013;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#65292;&#20943;&#23569;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#21644;&#26080;&#25928;&#33457;&#36153;&#65292;&#20026;&#30495;&#23454;&#36275;&#29699;&#22242;&#38431;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#29699;&#21592;&#31119;&#21033;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36275;&#29699;&#39034;&#24207;&#22242;&#38431;&#36873;&#25321;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#30495;&#23454;&#36275;&#29699;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#29699;&#21592;&#29305;&#23450;&#20449;&#24687;&#23545;&#29699;&#21592;&#21463;&#20260;&#21644;&#19981;&#21487;&#29992;&#24615;&#30340;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#34987;&#29992;&#20110;&#20026;&#28216;&#25103;&#36873;&#25321;&#22242;&#38431;&#65292;&#36890;&#36807;&#23545;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#36827;&#34892;&#25512;&#29702;&#65292;&#20248;&#21270;&#25972;&#20010;&#36187;&#23395;&#30340;&#22242;&#38431;&#32489;&#25928;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;2018/19&#33521;&#36229;&#36187;&#23395;&#30340;&#22522;&#20934;&#35299;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38477;&#20302;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#32422;13%&#21644;&#26080;&#25928;&#33457;&#36153;&#22312;&#21463;&#20260;&#29699;&#21592;&#36523;&#19978;&#30340;&#36164;&#37329;&#32422;11%&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#22522;&#20934;&#30456;&#20284;&#30340;&#36187;&#23395;&#39044;&#26399;&#31215;&#20998; - &#36825;&#34920;&#26126;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36275;&#29699;&#22242;&#38431;&#20013;&#38477;&#20302;&#25104;&#26412;&#21644;&#25913;&#21892;&#29699;&#21592;&#31119;&#21033;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27627;&#31859;&#27874;&#27874;&#26463;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04896</link><description>&lt;p&gt;
&#20174;&#26368;&#20339;&#20013;&#23398;&#20064;&#65306;&#26080;&#32447;&#36890;&#20449;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from the Best: Active Learning for Wireless Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27627;&#31859;&#27874;&#27874;&#26463;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36890;&#20449;&#20219;&#21153;&#26469;&#35828;&#65292;&#25910;&#38598;&#19968;&#20221;&#31354;&#20013;&#26080;&#32447;&#36890;&#20449;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#23545;&#31616;&#21333;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#36825;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#19987;&#23478;&#21442;&#19982;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#33021;&#28041;&#21450;&#31169;&#26377;&#30693;&#35782;&#20135;&#26435;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35745;&#31639;&#21644;&#36130;&#21153;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20027;&#21160;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20943;&#23569;&#26631;&#27880;&#24320;&#38144;&#32780;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#20851;&#38190;&#21644;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#26679;&#26412;&#65292;&#21482;&#26631;&#27880;&#36825;&#20123;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27627;&#31859;&#27874;&#27874;&#26463;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#30340;&#26631;&#27880;&#30001;&#19968;&#20010;&#22522;&#20110;&#31351;&#20030;&#25628;&#32034;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#31639;&#27861;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting an over-the-air wireless communications training dataset for deep learning-based communication tasks is relatively simple. However, labeling the dataset requires expert involvement and domain knowledge, may involve private intellectual properties, and is often computationally and financially expensive. Active learning is an emerging area of research in machine learning that aims to reduce the labeling overhead without accuracy degradation. Active learning algorithms identify the most critical and informative samples in an unlabeled dataset and label only those samples, instead of the complete set. In this paper, we introduce active learning for deep learning applications in wireless communications, and present its different categories. We present a case study of deep learning-based mmWave beam selection, where labeling is performed by a compute-intensive algorithm based on exhaustive search. We evaluate the performance of different active learning algorithms on a publicly av
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.04894</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24120;&#24120;&#34987;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#65292;&#22240;&#20026;&#23427;&#20204;&#39640;&#25928;&#19988;&#21171;&#21160;&#25104;&#26412;&#20302;&#12290;&#26426;&#22120;&#20154;&#25968;&#25454;&#37319;&#38598;&#30340;&#20851;&#38190;&#20219;&#21153;&#26159;&#22312;&#21021;&#22987;&#26410;&#30693;&#29615;&#22659;&#20013;&#35268;&#21010;&#36335;&#24452;&#65292;&#20197;&#28385;&#36275;&#24179;&#21488;&#29305;&#23450;&#30340;&#36164;&#28304;&#32422;&#26463;&#65292;&#20363;&#22914;&#26377;&#38480;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#38754;&#20020;&#30528;&#24456;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#22823;&#37327;&#26377;&#25928;&#21160;&#20316;&#30340;&#23384;&#22312;&#20197;&#21450;&#26410;&#30693;&#36974;&#25377;&#29289;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#37325;&#26032;&#35268;&#21010;&#26426;&#22120;&#20154;&#36335;&#24452;&#20197;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#26500;&#24314;&#21160;&#24577;&#22270;&#65292;&#23558;&#35268;&#21010;&#21160;&#20316;&#38480;&#21046;&#22312;&#26426;&#22120;&#20154;&#38468;&#36817;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#21709;&#24212;&#26032;&#21457;&#29616;&#30340;&#38556;&#30861;&#21644;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#23545;&#20110;&#37325;&#26032;&#35268;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24179;&#34913;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#21033;&#29992;&#22312;&#32447;&#25910;&#38598;&#30340;&#26377;&#20851;&#24863;&#20852;&#36259;&#30446;&#26631;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04892</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24418;&#24335;&#39564;&#35777;&#65288;PFV) AI&#31995;&#32479;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;/&#25110;&#23646;&#24615;&#65292;&#26041;&#27861;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31639;&#27861;&#32780;&#24050;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#65288;WMI&#65289;&#30340;AI&#31995;&#32479;PFV&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#24120;&#36890;&#29992;&#22320;&#23450;&#20041;&#38382;&#39064;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#32422;&#31616;&#21487;&#20197;&#22312;&#19981;&#20570;&#36807;&#24378;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;WMI&#27714;&#35299;&#22120;&#35299;&#20915;&#22810;&#20010;&#39564;&#35777;&#20219;&#21153;&#26469;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#19982;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#30456;&#20851;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.04888</link><description>&lt;p&gt;
RSCNet&#65306;&#20113;&#22522;WiFi&#24863;&#30693;&#30340;&#21160;&#24577;CSI&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#20174;&#32431;&#31929;&#30340;&#36890;&#20449;&#35774;&#22791;&#21457;&#23637;&#20026;&#21033;&#29992;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#25552;&#21462;&#33021;&#21147;&#30340;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#26377;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#35201;&#27714;&#23558;CSI&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#23613;&#31649;&#21487;&#34892;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#26102;&#24863;&#30693;&#21644;&#21387;&#32553;&#32593;&#32476;&#65288;RSCNet&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21387;&#32553;CSI&#26469;&#23454;&#29616;&#24863;&#30693;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#22312;&#30001;&#23569;&#37327;CSI&#24103;&#32452;&#25104;&#30340;CSI&#31383;&#21475;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#65292;&#23427;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#20174;&#20808;&#21069;&#30340;&#31383;&#21475;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20174;&#32780;&#22686;&#24378;&#24863;&#30693;&#20934;&#30830;&#24615;&#21644;CSI&#37325;&#24314;&#12290;RSCNet&#24039;&#22937;&#22320;&#24179;&#34913;&#20102;CSI&#21387;&#32553;&#21644;&#24863;&#30693;&#31934;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23454;&#26102;&#20113;&#22522;WiFi&#24863;&#30693;&#65292;&#24182;&#20943;&#23569;&#20102;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.04885</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#20998;&#25903;&#21644;&#23884;&#22871;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20013;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#36229;&#21442;&#25968;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#30452;&#25509;&#25511;&#21046;&#35757;&#32451;&#31639;&#27861;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#33719;&#24471;&#39640;&#25928;&#30340;&#35843;&#21442;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#22522;&#20110;&#19968;&#20010;&#26041;&#20415;&#20294;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#65292;&#21363;&#35843;&#21442;&#21442;&#25968;&#24444;&#27492;&#29420;&#31435;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26465;&#20214;&#20381;&#36182;&#30340;&#35843;&#21442;&#21442;&#25968;&#26159;&#24120;&#35265;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#35843;&#21442;&#21442;&#25968;&#65306;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#12290;&#23884;&#22871;&#21442;&#25968;&#25351;&#30340;&#26159;&#37027;&#20123;&#20165;&#23384;&#22312;&#20110;&#21478;&#19968;&#20010;&#35843;&#21442;&#21442;&#25968;&#29305;&#23450;&#35774;&#32622;&#20013;&#30340;&#35843;&#21442;&#21442;&#25968;&#65292;&#32780;&#20854;&#23427;&#21442;&#25968;&#22312;&#20854;&#20013;&#23884;&#22871;&#30340;&#21442;&#25968;&#31216;&#20026;&#20998;&#25903;&#21442;&#25968;&#12290;&#20026;&#20102;&#25429;&#25417;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33033;&#20914;&#27169;&#22411;LMUFormer&#65292;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04882</link><description>&lt;p&gt;
LMUFormer&#65306;&#20855;&#26377;Legendre&#35760;&#24518;&#21333;&#20803;&#30340;&#20302;&#22797;&#26434;&#24230;&#20294;&#24378;&#22823;&#30340;&#33033;&#20914;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33033;&#20914;&#27169;&#22411;LMUFormer&#65292;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22797;&#26434;&#24230;&#39640;&#19988;&#32570;&#20047;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35768;&#22810;&#36793;&#32536;&#27969;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#23558;Transformer&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#26174;&#24335;&#29366;&#24577;&#30340;RNN&#27169;&#22359;&#65292;&#26469;&#20462;&#25913;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#27169;&#22411;&#65306;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;SOTA&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21463;&#21040;&#20102;Legendre&#35760;&#24518;&#21333;&#20803;&#65288;LMU&#65289;&#22312;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;LMUFormer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.04880</link><description>&lt;p&gt;
&#32467;&#21512;&#20113;&#35745;&#31639;&#19982;&#31227;&#21160;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Combining Cloud and Mobile Computing for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#20063;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#36235;&#21183;&#32473;&#31227;&#21160;&#35774;&#22791;&#24102;&#26469;&#20102;&#38382;&#39064;&#65292;&#22914;&#20869;&#23384;&#23481;&#37327;&#21644;&#30005;&#27744;&#23551;&#21629;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#35768;&#22810;&#26381;&#21153;&#65288;&#22914;ChatGPT&#21644;Midjourney&#65289;&#22312;&#20113;&#20013;&#36816;&#34892;&#25152;&#26377;&#30340;&#25512;&#29702;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#28789;&#27963;&#24615;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20998;&#37197;&#26356;&#21487;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#20998;&#21106;&#35270;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35745;&#31639;&#20998;&#21106;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#21106;&#19981;&#20165;&#20943;&#23569;&#20102;&#29992;&#25143;&#31561;&#24453;&#26102;&#38388;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#32454;&#31890;&#24230;&#35843;&#25972;&#26469;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#24230;&#22120;&#65292;&#25910;&#38598;&#32593;&#32476;&#36136;&#37327;&#12289;&#23458;&#25143;&#31471;&#35774;&#22791;&#33021;&#21147;&#21644;&#20316;&#19994;&#35201;&#27714;&#30340;&#20449;&#24687;&#65292;&#20570;&#20986;&#20915;&#31574;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26512;&#21462;&#20986;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#21478;&#19968;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04874</link><description>&lt;p&gt;
&#36873;&#25321;&#20855;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Choosing a Classical Planner with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26512;&#21462;&#20986;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#21478;&#19968;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#26159;&#22312;&#32473;&#23450;&#35268;&#21010;&#38382;&#39064;&#30340;&#39044;&#23450;&#20041;&#35299;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#31639;&#22120;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35268;&#21010;&#35745;&#31639;&#22797;&#26434;&#65292;&#35299;&#31639;&#22120;&#22312;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#35299;&#31639;&#22120;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#23398;&#20064;&#26041;&#27861;&#34987;&#37319;&#29992;&#65292;&#20294;&#22312;&#32463;&#20856;&#30340;&#26368;&#20248;&#20195;&#20215;&#35268;&#21010;&#20013;&#65292;&#20027;&#27969;&#26041;&#27861;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32487;&#32493;&#20351;&#29992;GNN&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;GNN&#27169;&#22411;&#12289;&#22270;&#34920;&#31034;&#21644;&#33410;&#28857;&#29305;&#24449;&#20197;&#21450;&#39044;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;GNN&#33719;&#24471;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online planner selection is the task of choosing a solver out of a predefined set for a given planning problem. As planning is computationally hard, the performance of solvers varies greatly on planning problems. Thus, the ability to predict their performance on a given problem is of great importance. While a variety of learning methods have been employed, for classical cost-optimal planning the prevailing approach uses Graph Neural Networks (GNNs). In this work, we continue the line of work on using GNNs for online planner selection. We perform a thorough investigation of the impact of the chosen GNN model, graph representation and node features, as well as prediction task. Going further, we propose using the graph representation obtained by a GNN as an input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more resource-efficient yet accurate approach. We show the effectiveness of a variety of GNN-based online planner selection methods, opening up new exciting avenues
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.04870</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding Knowledge Graphs in Degenerate Clifford Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#26159;&#23454;&#25968;&#12289;&#22797;&#25968;&#21644;&#22235;&#20803;&#25968;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#32972;&#26223;&#19979;&#65292;&#21482;&#26377;&#24418;&#24335;&#20026;$Cl_{p,q}$&#65288;&#21363;&#27809;&#26377;&#38646;&#24130;&#22522;&#21521;&#37327;&#30340;&#20195;&#25968;&#65289;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#21463;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#20854;&#24130;&#25351;&#25968;&#20026;2&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#65292;&#34987;&#31216;&#20026;$Cl_{p,q,r}$&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#65288;&#26080;&#27861;&#20351;&#29992;$Cl_{p,q}$&#36827;&#34892;&#24314;&#27169;&#65289;&#24182;&#25429;&#25417;&#28304;&#20110;&#23454;&#25968;&#21644;&#22797;&#25968;&#37096;&#20998;&#38388;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#23884;&#20837;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#21442;&#25968;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#20248;&#21270;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#22522;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#36755;&#20837;&#30693;&#35782;&#22270;&#35889;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;$(p, q, r)$&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#65292;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.04869</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#23454;&#36341;&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#30340;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#65292;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#30693;&#35782;&#20316;&#20026;&#20154;&#31867;&#26234;&#33021;&#20013;&#30452;&#35266;&#35748;&#30693;&#21644;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20915;&#31574;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24110;&#21161;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#21644;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#36827;&#20837;RL&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#22240;&#26524;&#20851;&#31995;RL&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22240;&#26524;&#22270;&#27169;&#22411;&#26126;&#30830;&#22320;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#32467;&#26500;&#26356;&#26032;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#20027;&#21160;&#29615;&#22659;&#24178;&#39044;&#23398;&#20064;&#30340;RL&#20132;&#20114;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#34893;&#29983;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#20004;&#20010;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65306;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65292;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#12290;&#30001;&#20110;&#32570;&#23569;&#20844;&#20849;&#22522;&#20934;&#65292;&#29992;&#20110;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#29992;&#20110;&#35745;&#31639;&#25151;&#38388;&#20869;&#30340;&#22797;&#26434;&#22768;&#22330;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#40614;&#20811;&#39118;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#36817;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20174;&#25151;&#38388;&#20869;&#38646;&#25955;&#28857;&#27979;&#37327;&#24471;&#21040;&#30340;&#26377;&#38480;&#30340;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#26469;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#22312;&#31532;&#19968;&#20010;&#22768;&#23398;&#20849;&#25391;&#39057;&#29575;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#20026;&#20102;&#20998;&#26512;&#23558;&#22797;&#20540;&#20248;&#21270;&#24212;&#29992;&#20110;&#25152;&#32771;&#34385;&#20219;&#21153;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#19982;&#26368;&#20808;&#36827;&#30340;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
&lt;/p&gt;</description></item><item><title>CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04858</link><description>&lt;p&gt;
CodeIt&#65306;&#20855;&#26377;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04858
&lt;/p&gt;
&lt;p&gt;
CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#36890;&#24120;&#34987;&#35748;&#20026;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36890;&#29992;&#26234;&#33021;&#22522;&#20934;&#27979;&#35797;&#20363;&#22914;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#34920;&#29616;&#20173;&#28982;&#38750;&#24120;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ARC&#35270;&#20026;&#19968;&#20010;&#20197;&#32534;&#31243;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Code Iteration&#65288;CodeIt&#65289;&#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;1&#65289;&#31243;&#24207;&#25277;&#26679;&#21644;&#22238;&#39038;&#37325;&#26631;&#35760;&#20197;&#21450;2&#65289;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#32463;&#39564;&#22238;&#25918;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;episode&#30340;&#30446;&#26631;&#65288;&#21363;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#31243;&#24207;&#36755;&#20986;&#65289;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#20135;&#29983;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#26497;&#24230;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#24212;&#29992;CodeIt&#20110;ARC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#12289;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;CodeIt&#26159;&#31532;&#19968;&#20010;&#31070;&#32463;&#20803;-&#21512;&#25104;&#26426;&#21046;&#19968;&#20307;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;aLLM4TS&#26694;&#26550;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24494;&#35843;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04852</link><description>&lt;p&gt;
&#22810;&#22359;&#39044;&#27979;&#65306;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;LLMs&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;aLLM4TS&#26694;&#26550;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24494;&#35843;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;aLLM4TS&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#37325;&#26032;&#26500;&#24819;&#20026;&#19968;&#39033;&#33258;&#30417;&#30563;&#30340;&#22810;&#22359;&#39044;&#27979;&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#22359;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65306;(i) &#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22240;&#26524;&#36830;&#32493;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#19979;&#19968;&#20010;&#22359;&#39044;&#27979;&#20026;&#38170;&#28857;&#65292;&#26377;&#25928;&#22320;&#23558;LLM&#30340;&#33021;&#21147;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21516;&#27493;&#12290;(ii) &#22312;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#19978;&#36827;&#34892;&#22810;&#22359;&#39044;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#29420;&#29305;&#35201;&#32032;&#26159;&#22359;&#32423;&#35299;&#30721;&#23618;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#20381;&#36182;&#20110;&#24207;&#21015;&#32423;&#35299;&#30721;&#30340;&#26041;&#27861;&#12290;&#36825;&#26679;&#30340;&#35774;&#35745;&#30452;&#25509;&#23558;&#21333;&#20010;&#22359;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#25513;&#34109;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04845</link><description>&lt;p&gt;
AlphaFold&#36935;&#21040;Flow Matching&#29983;&#25104;&#34507;&#30333;&#36136;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
AlphaFold Meets Flow Matching for Generating Protein Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#19982;AlphaFold&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#20248;&#30340;&#32452;&#21512;&#65292;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#26500;&#35937;&#28789;&#27963;&#24615;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#29983;&#29289;&#21151;&#33021;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#24577;&#32467;&#26500;&#38598;&#21512;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#21160;&#21305;&#37197;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#39640;&#31934;&#24230;&#30340;&#21333;&#24577;&#39044;&#27979;&#22120;&#65292;&#22914;AlphaFold&#21644;ESMFold&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#27969;&#21305;&#37197;&#26694;&#26550;&#19979;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#24207;&#21015;&#26465;&#20214;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;AlphaFlow&#21644;ESMFlow&#12290;&#22312;PDB&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;AlphaFold&#21644;MSA&#23376;&#37319;&#26679;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#21512;&#12290;&#24403;&#36827;&#19968;&#27493;&#35757;&#32451;&#25152;&#26377;&#21407;&#23376;MD&#30340;&#32452;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#26410;&#35265;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#12289;&#20301;&#32622;&#20998;&#24067;&#21644;&#39640;&#38454;&#32452;&#21512;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26356;&#24555;&#30340;&#26102;&#38047;&#25910;&#25947;&#36895;&#24230;&#23558;&#38745;&#24577;PDB&#32467;&#26500;&#22810;&#26679;&#21270;&#21040;&#29305;&#23450;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#27604;&#22797;&#21046;&#30340;MD&#36712;&#36857;&#26356;&#20855;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04835</link><description>&lt;p&gt;
SARI: &#31616;&#27905;&#24179;&#22343;&#19982;&#40065;&#26834;&#24615;&#22522;&#20110;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04835
&lt;/p&gt;
&lt;p&gt;
SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (PLL) &#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#37117;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614; (&#37096;&#20998;&#26631;&#31614;) &#25104;&#23545;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30495;&#27491;&#30340;&#26631;&#31614;&#12290;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (NPLL) &#25918;&#23485;&#20102;&#36825;&#20010;&#32422;&#26463;&#65292;&#20801;&#35768;&#19968;&#20123;&#37096;&#20998;&#26631;&#31614;&#19981;&#21253;&#21547;&#30495;&#27491;&#30340;&#26631;&#31614;&#65292;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312; NPLL &#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550; SARI&#65292;&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#26368;&#36817;&#37051;&#31639;&#27861;&#23558;&#20266;&#26631;&#31614;&#20998;&#37197;&#32473;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#20266;&#26631;&#31614;&#19982;&#22270;&#20687;&#37197;&#23545;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#37319;&#29992;&#26631;&#31614;&#24179;&#28369;&#21644;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;&#32467;&#26524;&#26469;&#25913;&#36827;&#21644;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;SARI&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#24179;&#22343;&#31574;&#30053; (&#20266;&#26631;&#31614;) &#21644;&#22522;&#20110;&#35782;&#21035;&#31574;&#30053; (&#20998;&#31867;&#22120;&#35757;&#32451;)&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;SARI&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.04830</link><description>&lt;p&gt;
&#32553;&#23567;SGP4&#21644;&#39640;&#31934;&#24230;&#20256;&#25773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#36890;&#36807;&#21487;&#24494;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21270;&#30340;&#31532;&#22235;&#32423;&#25668;&#21160;(SGP4)&#36712;&#36947;&#20256;&#25773;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#29699;&#36712;&#36947;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#25913;&#36827;&#65292;SGP&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25968;&#20540;&#20256;&#25773;&#22120;&#30340;&#31934;&#24230;&#65292;&#21518;&#32773;&#30340;&#35823;&#24046;&#26174;&#33879;&#36739;&#23567;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#26032;&#22411;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#20351;SGP4&#21487;&#24494;&#21270;&#65292;dSGP4&#20415;&#20110;&#36827;&#34892;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33322;&#22825;&#22120;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#36716;&#25442;&#12289;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#35745;&#31639;&#21644;&#21327;&#26041;&#24046;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;PyTorch&#23454;&#29616;&#20801;&#35768;&#22312;&#25209;&#37327;&#30340;TLE&#65288;&#20004;&#34892;&#21442;&#25968;&#65289;&#38598;&#19978;&#36827;&#34892;&#23604;&#23596;&#30340;&#24182;&#34892;&#36712;&#36947;&#20256;&#25773;&#65292;&#21033;&#29992;CPU&#12289;GPU&#21644;&#20998;&#24067;&#24335;&#39044;&#27979;&#21355;&#26143;&#20301;&#32622;&#30340;&#39640;&#32423;&#30828;&#20214;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;&#21487;&#24494;&#24615;&#20351;&#20854;&#33021;&#19982;&#27169;&#24335;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.04825</link><description>&lt;p&gt;
&#24555;&#36895;&#23450;&#26102;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#38899;&#39057;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fast Timing-Conditioned Latent Audio Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#38271;&#31687;44.1kHz&#31435;&#20307;&#22768;&#38899;&#39057;&#21487;&#33021;&#23545;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24182;&#27809;&#26377;&#35299;&#20915;&#38899;&#20048;&#21644;&#38899;&#25928;&#22312;&#25345;&#32493;&#26102;&#38388;&#19978;&#30340;&#33258;&#28982;&#21464;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20197;&#39640;&#25928;&#26041;&#24335;&#29983;&#25104;&#38271;&#31687;&#12289;&#21487;&#21464;&#38271;&#24230;&#30340;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;Stable Audio&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#65292;&#20854;&#28508;&#22312;&#24615;&#36136;&#30001;&#19968;&#20010;&#20840;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23450;&#20041;&#12290;&#23427;&#19981;&#20165;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#36824;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#29983;&#25104;&#30340;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#20869;&#23481;&#21644;&#38271;&#24230;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#22312;A100 GPU&#19978;&#65292;Stable Audio&#33021;&#22815;&#22312;8&#31186;&#20869;&#20197;44.1kHz&#30340;&#36895;&#24230;&#28210;&#26579;&#38271;&#36798;95&#31186;&#30340;&#31435;&#20307;&#22768;&#20449;&#21495;&#12290;&#23613;&#31649;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#25512;&#29702;&#36895;&#24230;&#24555;&#65292;&#20294;&#23427;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#25991;&#26412;-&#38899;&#20048;&#21644;&#38899;&#39057;&#22522;&#20934;&#20013;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(C-DGMs)&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#36716;&#21270;&#20026;&#32422;&#26463;&#23618;(CL)&#26469;&#20445;&#35777;&#29983;&#25104;&#26679;&#26412;&#31526;&#21512;&#32473;&#23450;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;DGMs&#65292;C-DGMs&#33021;&#22815;&#26356;&#22909;&#22320;&#36981;&#23432;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.04823</link><description>&lt;p&gt;
&#20320;&#30340;&#21512;&#25104;&#25968;&#25454;&#26377;&#22810;&#30495;&#23454;&#65311;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(C-DGMs)&#65292;&#36890;&#36807;&#23558;&#32422;&#26463;&#36716;&#21270;&#20026;&#32422;&#26463;&#23618;(CL)&#26469;&#20445;&#35777;&#29983;&#25104;&#26679;&#26412;&#31526;&#21512;&#32473;&#23450;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;DGMs&#65292;C-DGMs&#33021;&#22815;&#26356;&#22909;&#22320;&#36981;&#23432;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#24050;&#34987;&#35777;&#26126;&#26159;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36234;&#26469;&#36234;&#22810;&#22320;&#25429;&#25417;&#21040;&#20854;&#29305;&#24449;&#30340;&#22797;&#26434;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35201;&#29983;&#25104;&#30495;&#23454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20165;&#20165;&#25317;&#26377;&#23545;&#20854;&#20998;&#24067;&#30340;&#33391;&#22909;&#36817;&#20284;&#36890;&#24120;&#26159;&#19981;&#22815;&#30340;&#65292;&#36824;&#38656;&#35201;&#36981;&#23432;&#32534;&#30721;&#20102;&#38382;&#39064;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;DGMs&#36716;&#21270;&#20026;&#20445;&#25345;&#32473;&#23450;&#32422;&#26463;&#30340;&#32422;&#26463;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(C-DGMs)&#12290;&#36825;&#26159;&#36890;&#36807;&#33258;&#21160;&#35299;&#26512;&#32422;&#26463;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19982;DGM&#26080;&#32541;&#38598;&#25104;&#30340;&#32422;&#26463;&#23618;(CL)&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;DGMs&#21644;&#20219;&#21153;&#65292;&#32467;&#26524;&#26174;&#31034;&#26631;&#20934;DGMs&#32463;&#24120;&#36829;&#21453;&#32422;&#26463;&#65292;&#19968;&#20123;&#36229;&#36807;&#20102;95%&#30340;&#19981;&#21512;&#35268;&#24773;&#20917;&#65292;&#32780;&#30456;&#24212;&#30340;C-DGMs&#20174;&#26410;&#36829;&#21453;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand. In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints. This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM. Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding $95\%$ non-compliance, while their corresponding C-DGMs are nev
&lt;/p&gt;</description></item><item><title>E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25193;&#23637;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#26032;&#26041;&#31243;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#24555;&#36895;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#39044;&#22788;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.04821</link><description>&lt;p&gt;
E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
E(3)-Equivariant Mesh Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04821
&lt;/p&gt;
&lt;p&gt;
E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25193;&#23637;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#26032;&#26041;&#31243;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#24555;&#36895;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#39044;&#22788;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#35282;&#32593;&#26684;&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#19977;&#32500;&#29289;&#20307;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#37117;&#33268;&#21147;&#20110;&#22312;3D&#32593;&#26684;&#19978;&#36827;&#34892;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#36825;&#20123;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#19982;&#23454;&#38469;&#24615;&#33021;&#20043;&#38388;&#24182;&#27809;&#26377;&#30452;&#25509;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#65292;&#31616;&#21333;&#30340;&#28145;&#24230;&#27169;&#22411;&#23545;&#20110;&#20960;&#20309;&#22270;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26368;&#23567;&#38480;&#24230;&#22320;&#25193;&#23637;&#20102;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;EGNNs&#65289;&#65288;Satorras&#31561;&#65292;2021&#65289;&#30340;&#26356;&#26032;&#26041;&#31243;&#65292;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#35813;&#26041;&#31243;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#26550;&#26500;&#65292;&#21363;&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#65288;EMNN&#65289;&#65292;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#31561;&#21464;&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#30340;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BOWLL&#65292;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21152;&#36895;&#20102;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04814</link><description>&lt;p&gt;
BOWLL&#65306;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
BOWLL: A Deceptively Simple Open World Lifelong Learner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BOWLL&#65292;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21152;&#36895;&#20102;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#39044;&#20808;&#30830;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26631;&#37327;&#24615;&#33021;&#25968;&#23383;&#30340;&#25913;&#36827;&#20284;&#20046;&#28145;&#28145;&#26893;&#26681;&#20110;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#24456;&#23569;&#31934;&#24515;&#31574;&#21010;&#65292;&#24212;&#29992;&#20063;&#24456;&#23569;&#20165;&#38480;&#20110;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#23454;&#38469;&#30340;&#31995;&#32479;&#26469;&#35782;&#21035;&#26032;&#27010;&#24565;&#65292;&#36991;&#20813;&#20027;&#21160;&#21253;&#25324;&#26080;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#20445;&#30041;&#20808;&#21069;&#33719;&#21462;&#30340;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#20851;&#38190;&#35201;&#32032;&#22312;&#20010;&#20307;&#19978;&#24050;&#32463;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#32467;&#21512;&#65292;&#21363;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21482;&#26159;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#20026;&#20102;&#21152;&#36895;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20854;&#39318;&#20010;&#23436;&#25972;&#19988;&#26497;&#24230;&#38656;&#35201;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#26222;&#36941;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#24378;&#35843;&#20026;&#20160;&#20040;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.04794</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#24335;&#30340;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-view Clustering via Explicit Kernel Features Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#22810;&#35270;&#35282;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#22312;&#26631;&#20934;&#26426;&#22120;&#19978;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20855;&#26377;&#25968;&#30334;&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#21508;&#31181;&#35268;&#27169;&#30340;&#22522;&#20934;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#21644;&#23646;&#24615;&#32593;&#32476;&#22810;&#35270;&#35282;&#26041;&#27861;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks. In this paper we introduce a new scalability framework for multi-view subspace clustering. An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance. The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes. We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches.
&lt;/p&gt;</description></item><item><title>Shadowheart SGD&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#20248;&#21270;&#20102;&#20808;&#21069;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#23545;&#24212;&#30340;&#21452;&#21521;&#35774;&#32622;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04785</link><description>&lt;p&gt;
Shadowheart SGD: &#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD
&lt;/p&gt;
&lt;p&gt;
Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04785
&lt;/p&gt;
&lt;p&gt;
Shadowheart SGD&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#20248;&#21270;&#20102;&#20808;&#21069;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#23545;&#24212;&#30340;&#21452;&#21521;&#35774;&#32622;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#27493;&#38598;&#20013;&#24335;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#32771;&#34385;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#24037;&#20316;&#32773;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26102;&#38388;&#19981;&#33021;&#24573;&#30053;&#65292;&#32780;&#25152;&#26377;&#24037;&#20316;&#32773;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26102;&#38388;&#21487;&#33021;&#19981;&#21516;&#12290;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;-Shadowheart SGD&#65292;&#23427;&#21487;&#35777;&#26126;&#20248;&#21270;&#20102;&#25152;&#26377;&#20808;&#21069;&#38598;&#20013;&#24335;&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Shadowheart SGD&#22312;&#21387;&#32553;&#36890;&#20449;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#26063;&#20013;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#21452;&#21521;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20174;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#32773;&#30340;&#24191;&#25773;&#19981;&#21487;&#24573;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#22312;NTK&#30340;&#35282;&#24230;&#19978;&#27604;ReLU&#28608;&#27963;&#32593;&#32476;&#26356;&#21152;&#8220;&#33391;&#22909;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04783</link><description>&lt;p&gt;
&#21608;&#26399;&#28608;&#27963;&#22352;&#26631;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04783
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#22312;NTK&#30340;&#35282;&#24230;&#19978;&#27604;ReLU&#28608;&#27963;&#32593;&#32476;&#26356;&#21152;&#8220;&#33391;&#22909;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#21608;&#26399;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#30340;ReLU&#28608;&#27963;&#32593;&#32476;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#21407;&#22240;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#20102;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#26469;&#25552;&#20379;&#23545;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#26377;&#38480;&#23485;&#24230;&#30340;&#35774;&#32622;&#19979;&#25512;&#23548;&#20986;NTK&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#19978;&#30028;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#30456;&#23545;&#36890;&#29992;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#23485;&#24230;&#33267;&#23569;&#19982;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#32447;&#24615;&#22686;&#38271;&#30340;&#23618;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20174;NTK&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#27604;ReLU&#28608;&#27963;&#32593;&#32476;&#26356;&#21152;&#8220;&#33391;&#22909;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#24212;&#29992;&#20110;&#19968;&#20010;&#26696;&#20363;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#21608;&#26399;&#24615;&#28608;&#27963;&#32593;&#32476;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks. However, there is still a limited understanding of the underlying reasons for this improved performance. In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples. Our findings indicate that periodically activated networks are \textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks. Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically. Our study offers a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#24555;&#36895;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#29109;&#30340;&#26368;&#22823;&#31062;&#20808;&#22270;&#12290;&#36890;&#36807;&#24341;&#20837;imsets&#26694;&#26550;&#21644;&#31934;&#21270;&#39532;&#23572;&#31185;&#22827;&#23646;&#24615;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;MAG&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04777</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#24555;&#36895;&#25628;&#32034;&#31639;&#27861;&#22312;&#20351;&#29992;&#29109;&#30340;&#26368;&#22823;&#31062;&#20808;&#22270;&#20013;
&lt;/p&gt;
&lt;p&gt;
A fast score-based search algorithm for maximal ancestral graphs using entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#24555;&#36895;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#29109;&#30340;&#26368;&#22823;&#31062;&#20808;&#22270;&#12290;&#36890;&#36807;&#24341;&#20837;imsets&#26694;&#26550;&#21644;&#31934;&#21270;&#39532;&#23572;&#31185;&#22827;&#23646;&#24615;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;MAG&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#31062;&#20808;&#22270;&#65288;MAGs&#65289;&#26159;&#19968;&#31867;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#22270;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#35780;&#20998;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;BIC&#35780;&#20998;&#20174;&#32463;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#26410;&#30693;MAG&#65292;&#20294;&#35813;&#26041;&#27861;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;imsets&#26694;&#26550;&#36890;&#36807;&#32463;&#39564;&#29109;&#20272;&#35745;&#21644;&#26032;&#25552;&#20986;&#30340;&#31934;&#21270;&#39532;&#23572;&#31185;&#22827;&#23646;&#24615;&#23545;MAG&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#22270;&#25628;&#32034;&#36807;&#31243;&#19982;\citet{claassen2022greedy}&#31867;&#20284;&#65292;&#20294;&#26159;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25628;&#32034;&#31639;&#27861;&#22312;&#33410;&#28857;&#25968;&#19978;&#26159;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#30340;&#65292;&#36890;&#36807;&#38480;&#21046;&#24230;&#25968;&#12289;&#26368;&#22823;&#22836;&#37096;&#22823;&#23567;&#21644;&#27495;&#35270;&#36335;&#24452;&#25968;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#30340;MAG&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Maximal ancestral graph} (MAGs) is a class of graphical model that extend the famous \emph{directed acyclic graph} in the presence of latent confounders. Most score-based approaches to learn the unknown MAG from empirical data rely on BIC score which suffers from instability and heavy computations. We propose to use the framework of imsets \citep{studeny2006probabilistic} to score MAGs using empirical entropy estimation and the newly proposed \emph{refined Markov property} \citep{hu2023towards}. Our graphical search procedure is similar to \citet{claassen2022greedy} but improved from our theoretical results. We show that our search algorithm is polynomial in number of nodes by restricting degree, maximal head size and number of discriminating paths. In simulated experiment, our algorithm shows superior performance compared to other state of art MAG learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#26469;&#25903;&#25345;&#24378;&#21270;&#23398;&#20064;(RL)&#20195;&#29702;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#20174;VLMs&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#20102;&#30452;&#25509;&#26597;&#35810;VLM&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2402.04764</link><description>&lt;p&gt;
&#20195;&#30721;&#21363;&#22870;&#21169;&#65306;&#29992;VLM&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Code as Reward: Empowering Reinforcement Learning with VLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#26469;&#25903;&#25345;&#24378;&#21270;&#23398;&#20064;(RL)&#20195;&#29702;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#20174;VLMs&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#20102;&#30452;&#25509;&#26597;&#35810;VLM&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#33021;&#22815;&#29702;&#35299;&#35270;&#35273;&#27010;&#24565;&#65292;&#25551;&#36848;&#24182;&#20998;&#35299;&#22797;&#26434;&#20219;&#21153;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#20219;&#21153;&#23436;&#25104;&#30340;&#21453;&#39304;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#26469;&#25903;&#25345;&#22686;&#24378;&#23398;&#20064;(RL)&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#21407;&#21017;&#19978;&#65292;VLMs&#38750;&#24120;&#36866;&#21512;&#36825;&#20010;&#30446;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#33258;&#28982;&#22320;&#20998;&#26512;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#23398;&#20064;&#36827;&#24230;&#30340;&#21453;&#39304;(&#22870;&#21169;)&#12290;&#28982;&#32780;&#65292;VLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#65292;&#39057;&#32321;&#26597;&#35810;&#20197;&#35745;&#31639;&#22870;&#21169;&#23558;&#26174;&#33879;&#20943;&#24930;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20195;&#30721;&#21363;&#22870;&#21169;&#8221;(VLM-CaR)&#30340;&#26694;&#26550;&#12290;VLM-CaR&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#20174;VLMs&#29983;&#25104;&#23494;&#38598;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#36731;&#20102;&#30452;&#25509;&#26597;&#35810;VLM&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#23494;&#38598;&#22870;&#21169;&#22312;&#22810;&#26679;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#29615;&#22659;&#20013;&#37117;&#38750;&#24120;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25361;&#25112;&#24615;&#29031;&#26126;&#29615;&#22659;&#19979;&#30340;&#39068;&#33394;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#39068;&#33394;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04762</link><description>&lt;p&gt;
&#25361;&#25112;&#24615;&#29031;&#26126;&#29615;&#22659;&#19979;&#30340;&#39068;&#33394;&#35782;&#21035;&#65306;&#22522;&#20110;CNN&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Color Recognition in Challenging Lighting Environments: CNN Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25361;&#25112;&#24615;&#29031;&#26126;&#29615;&#22659;&#19979;&#30340;&#39068;&#33394;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#39068;&#33394;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#32447;&#22312;&#35270;&#35273;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#26426;&#22120;&#30340;&#35270;&#35273;&#65292;&#23545;&#20110;&#21608;&#22260;&#20809;&#29031;&#26465;&#20214;&#30340;&#24863;&#30693;&#37117;&#20250;&#24433;&#21709;&#39068;&#33394;&#30340;&#35782;&#21035;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#33268;&#21147;&#20110;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#39068;&#33394;&#26816;&#27979;&#25216;&#26415;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39068;&#33394;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#21487;&#20197;&#22635;&#34917;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#39068;&#33394;&#26816;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#36793;&#32536;&#26816;&#27979;&#20998;&#21106;&#25216;&#26415;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#30830;&#23450;&#23545;&#35937;&#65292;&#28982;&#21518;&#23558;&#20998;&#21106;&#30340;&#23545;&#35937;&#36755;&#20837;&#32463;&#36807;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#23545;&#35937;&#30340;&#39068;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#19979;&#30340;&#39068;&#33394;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings. Researchers are working to enhance the color detection techniques for the application of computer vision. They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled. To address this issue, a color detection method, which is based on a Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the Convolutional Neural Network trained to detect the color of an object in different lighting conditions. It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#20013;&#21019;&#24314;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04754</link><description>&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#24067;&#23616;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#20013;&#21019;&#24314;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#24067;&#23616;&#29983;&#25104;&#26159;&#25351;&#22312;&#20855;&#26377;&#20195;&#34920;&#35774;&#35745;&#24847;&#22270;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#65288;&#20363;&#22914;&#25991;&#26723;&#21644;&#32593;&#39029;&#35774;&#35745;&#65289;&#20013;&#21019;&#24314;&#19968;&#31181;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#26368;&#36817;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#65292;&#20294;&#19982;&#20043;&#21069;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#20986;&#26356;&#26126;&#26174;&#30340;&#20559;&#31163;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#8212;&#8212;LACE&#65288;Layout Constraint Diffusion Model&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#65292;&#20363;&#22914;&#26681;&#25454;&#25351;&#23450;&#23646;&#24615;&#25490;&#21015;&#20803;&#32032;&#12289;&#23436;&#21892;&#25110;&#23436;&#25104;&#31895;&#31961;&#30340;&#24067;&#23616;&#35774;&#35745;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#36830;&#32493;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#20351;&#29992;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#35774;&#35745;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#21487;&#24494;&#30340;&#32654;&#23398;&#32422;&#26463;&#20989;&#25968;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#36890;&#36807;&#25513;&#30721;&#36755;&#20837;&#24341;&#20837;&#26465;&#20214;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LACE&#27169;&#22411;&#22312;&#19981;&#21516;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#39640;&#31232;&#30095;&#21306;&#22495;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#19982;&#20302;&#31232;&#30095;&#21306;&#22495;&#30456;&#24403;&#30340;&#27169;&#22411;&#36136;&#37327;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#26799;&#24230;&#24133;&#20540;&#20013;&#24341;&#20837;&#30340;&#22122;&#38899;&#27700;&#24179;&#25552;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#28176;&#36827;&#38480;&#21046;&#26799;&#24230;&#27969;&#21160;&#30340;&#34928;&#20943;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.04744</link><description>&lt;p&gt;
&#28176;&#36827;&#26799;&#24230;&#27969;&#22312;Transformer&#20013;&#31232;&#30095;&#35757;&#32451;&#30340;&#40065;&#26834;N:M&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#39640;&#31232;&#30095;&#21306;&#22495;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#19982;&#20302;&#31232;&#30095;&#21306;&#22495;&#30456;&#24403;&#30340;&#27169;&#22411;&#36136;&#37327;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#26799;&#24230;&#24133;&#20540;&#20013;&#24341;&#20837;&#30340;&#22122;&#38899;&#27700;&#24179;&#25552;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#28176;&#36827;&#38480;&#21046;&#26799;&#24230;&#27969;&#21160;&#30340;&#34928;&#20943;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30456;&#23545;&#36739;&#20302;&#30340;&#24320;&#38144;&#21644;&#25552;&#39640;&#30340;&#25928;&#29575;&#65292;N:M&#32467;&#26500;&#30340;&#31232;&#30095;&#24615;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#31232;&#30095;&#24615;&#24418;&#24335;&#23545;&#20110;&#38477;&#20302;&#20869;&#23384;&#21344;&#29992;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#20854;&#34920;&#31034;&#24320;&#38144;&#36739;&#23567;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#21162;&#21147;&#38024;&#23545;N:M&#32467;&#26500;&#30340;&#31232;&#30095;&#24615;&#24320;&#21457;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#20302;&#31232;&#30095;&#21306;&#22495;($\sim$50\%)&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#38754;&#23545;&#39640;&#31232;&#30095;&#21306;&#22495;($&gt;$80\%)&#26102;&#24448;&#24448;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#22312;&#39640;&#31232;&#30095;&#21306;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#19982;&#20302;&#31232;&#30095;&#21306;&#22495;&#30456;&#24403;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#36896;&#25104;&#36825;&#31181;&#24046;&#36317;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#26799;&#24230;&#24133;&#20540;&#20013;&#24341;&#20837;&#30340;&#22122;&#38899;&#27700;&#24179;&#25552;&#39640;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#37319;&#29992;&#34928;&#20943;&#26426;&#21046;&#36880;&#28176;&#38480;&#21046;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\sim$50\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($&gt;$80\%). In this work, we study the effectiveness of existing sparse training recipes at \textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20272;&#35745;&#26631;&#35760;Hawkes&#36807;&#31243;&#26465;&#20214;&#24378;&#24230;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#27169;&#22411;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#26680;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;Hawkes&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#21040;&#36798;&#26102;&#38388;&#21644;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#65292;&#33719;&#24471;&#21040;&#36798;&#24378;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04740</link><description>&lt;p&gt;
&#22810;&#32500;&#26631;&#35760;Hawkes&#36807;&#31243;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20272;&#35745;&#26631;&#35760;Hawkes&#36807;&#31243;&#26465;&#20214;&#24378;&#24230;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#27169;&#22411;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#20869;&#26680;&#21644;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;Hawkes&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#21040;&#36798;&#26102;&#38388;&#21644;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#65292;&#33719;&#24471;&#21040;&#36798;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;Hawkes&#36807;&#31243;&#26159;Hawkes&#36807;&#31243;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#20854;&#29305;&#28857;&#26159;&#27599;&#20010;&#20107;&#20214;&#30340;&#36339;&#36291;&#22823;&#23567;&#19981;&#21516;&#65292;&#19982;&#27809;&#26377;&#26631;&#35760;&#30340;Hawkes&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#24658;&#23450;&#36339;&#36291;&#22823;&#23567;&#19981;&#21516;&#12290;&#23613;&#31649;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;Hawkes&#36807;&#31243;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#19978;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#65292;&#20294;&#22312;&#26631;&#35760;Hawkes&#36807;&#31243;&#26041;&#38754;&#30340;&#25991;&#29486;&#20173;&#23384;&#22312;&#37325;&#22823;&#31354;&#30333;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20272;&#35745;&#26631;&#35760;Hawkes&#36807;&#31243;&#26465;&#20214;&#24378;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65306;&#8220;&#20855;&#26377;&#26631;&#35760;&#30340;&#27973;&#23618;&#31070;&#32463;Hawkes&#27169;&#22411;&#8221;-&#29992;&#20110;&#20855;&#26377;&#20852;&#22859;&#24615;&#20869;&#26680;&#30340;Hawkes&#36807;&#31243;&#65292;&#20197;&#21450;&#8220;&#38750;&#32447;&#24615;Hawkes&#20855;&#26377;&#26631;&#35760;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8221;-&#29992;&#20110;&#38750;&#32447;&#24615;Hawkes&#36807;&#31243;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#23558;&#36807;&#21435;&#30340;&#21040;&#36798;&#26102;&#38388;&#21450;&#20854;&#30456;&#24212;&#30340;&#26631;&#35760;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#33719;&#21462;&#21040;&#36798;&#24378;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#65292;&#20445;&#25345;&#20102;&#26631;&#35760;Hawkes&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An extension of the Hawkes process, the Marked Hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a Hawkes process without marks. While extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear Hawkes process, there remains a significant gap in the literature regarding the marked Hawkes process. In response to this, we propose a methodology for estimating the conditional intensity of the marked Hawkes process. We introduce two distinct models: \textit{Shallow Neural Hawkes with marks}- for Hawkes processes with excitatory kernels and \textit{Neural Network for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity. This approach is entirely non-parametric, preserving the interpretability associated with the marked Hawkes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#35299;&#20915;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#65292;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#32858;&#31867;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04732</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20855;&#26377;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#30340;&#22270;&#21106;
&lt;/p&gt;
&lt;p&gt;
Graph Cuts with Arbitrary Size Constraints Through Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#35299;&#20915;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#65292;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#32858;&#31867;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#24120;&#35265;&#20998;&#21106;&#26041;&#27861;&#26159;&#26368;&#23567;&#21106;&#12290;&#32463;&#20856;&#26368;&#23567;&#21106;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#23567;&#30340;&#20998;&#32452;&#65292;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#26356;&#24179;&#34913;&#30340;&#21464;&#20307;&#65292;&#22914;&#24402;&#19968;&#21270;&#21106;&#21644;&#27604;&#20363;&#21106;&#21462;&#24471;&#20102;&#26356;&#22810;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#65292;&#22914;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#65292;&#36825;&#20123;&#21464;&#20307;&#30340;&#24179;&#34913;&#32422;&#26463;&#21487;&#33021;&#36807;&#20110;&#38480;&#21046;&#65292;&#32780;&#23545;&#20110;&#23547;&#25214;&#23436;&#32654;&#24179;&#34913;&#20998;&#21306;&#26469;&#35828;&#19981;&#22815;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20135;&#29983;&#31232;&#30095;&#35299;&#65292;&#24182;&#19988;&#21482;&#27604;&#32463;&#20856;&#35889;&#32858;&#31867;&#31639;&#27861;&#22810;&#28040;&#32791;$\mathcal{O}(\log(n))$&#30340;&#38468;&#21152;&#27604;&#29575;&#65292;&#20294;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way of partitioning graphs is through minimum cuts. One drawback of classical minimum cut methods is that they tend to produce small groups, which is why more balanced variants such as normalized and ratio cuts have seen more success. However, we believe that with these variants, the balance constraints can be too restrictive for some applications like for clustering of imbalanced datasets, while not being restrictive enough for when searching for perfectly balanced partitions. Here, we propose a new graph cut algorithm for partitioning graphs under arbitrary size constraints. We formulate the graph cut problem as a regularized Gromov-Wasserstein problem. We then propose to solve it using accelerated proximal GD algorithm which has global convergence guarantees, results in sparse solutions and only incurs an additional ratio of $\mathcal{O}(\log(n))$ compared to the classical spectral clustering algorithm but was seen to be more efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#27010;&#24565;$b\textit{-&#21333;&#35843;&#36335;&#24452;}$&#21644;$B\textit{-MSNET}$&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#22312;&#26356;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#27604;&#22266;&#23450;&#20013;&#24515;&#20837;&#21475;&#28857;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#19978;&#30028;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#21644;&#38590;&#20363;&#30340;&#25361;&#25112;&#22330;&#26223;&#20013;&#12290;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#25552;&#20379;&#20102;&#20248;&#21270;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20837;&#21475;&#28857;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#30340;&#39640;&#32500;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04713</link><description>&lt;p&gt;
&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#30340;&#29702;&#35770;&#19982;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#27010;&#24565;$b\textit{-&#21333;&#35843;&#36335;&#24452;}$&#21644;$B\textit{-MSNET}$&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#22312;&#26356;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#27604;&#22266;&#23450;&#20013;&#24515;&#20837;&#21475;&#28857;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#19978;&#30028;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#21644;&#38590;&#20363;&#30340;&#25361;&#25112;&#22330;&#26223;&#20013;&#12290;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#25552;&#20379;&#20102;&#20248;&#21270;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20837;&#21475;&#28857;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#30340;&#39640;&#32500;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#27010;&#24565;&#65306;$b\textit{-&#21333;&#35843;&#36335;&#24452;}$&#21644;$B\textit{-MSNET}$&#65292;&#27604;&#29616;&#26377;&#30340;&#27010;&#24565;&#22914;MSNET&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#23454;&#38469;&#31639;&#27861;&#20013;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#65292;&#33258;&#36866;&#24212;&#20837;&#21475;&#28857;&#36873;&#25321;&#25552;&#20379;&#20102;&#27604;&#22266;&#23450;&#20013;&#24515;&#20837;&#21475;&#28857;&#26356;&#22909;&#30340;&#24615;&#33021;&#19978;&#30028;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#21644;&#38590;&#20363;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#30740;&#31350;&#28145;&#20837;&#27934;&#23519;&#20102;&#29992;&#20110;&#23454;&#38469;&#39640;&#32500;&#25968;&#25454;&#24212;&#29992;&#20013;&#30340;&#22270;&#24418;&#22522;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#20837;&#21475;&#28857;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS). We introduce novel concepts: $b\textit{-monotonic path}$ and $B\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET. We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work. Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances. Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;GNN&#26694;&#26550;&#65292;&#23558;&#26816;&#32034;&#24335;&#22240;&#26524;&#23398;&#20064;&#19982;&#22270;&#20449;&#24687;&#29942;&#39048;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#25299;&#25169;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23545;GNN&#30340;&#36879;&#26126;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04710</link><description>&lt;p&gt;
&#23558;&#26816;&#32034;&#24335;&#22240;&#26524;&#23398;&#20064;&#19982;&#20449;&#24687;&#29942;&#39048;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;GNN&#26694;&#26550;&#65292;&#23558;&#26816;&#32034;&#24335;&#22240;&#26524;&#23398;&#20064;&#19982;&#22270;&#20449;&#24687;&#29942;&#39048;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#25299;&#25169;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23545;GNN&#30340;&#36879;&#26126;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#26377;&#25928;&#22788;&#29702;&#25299;&#25169;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20107;&#21518;&#35299;&#37322;&#65292;&#20197;&#25552;&#20379;&#23545;GNN&#30340;&#36879;&#26126;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#37322;&#22797;&#26434;&#23376;&#22270;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#26080;&#27861;&#21033;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;GNN&#30340;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#36879;&#26126;&#30340;GNN&#27169;&#22411;&#26469;&#25429;&#25417;&#20851;&#38190;&#23376;&#22270;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;GNN&#30340;&#39044;&#27979;&#65292;&#20294;&#22312;&#35299;&#37322;&#26041;&#38754;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26356;&#22909;&#22320;&#32852;&#31995;GNN&#30340;&#35299;&#37322;&#19982;&#39044;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;GNN&#26694;&#26550;&#65292;&#23427;&#23558;&#26816;&#32034;&#24335;&#22240;&#26524;&#23398;&#20064;&#19982;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#29702;&#35770;&#30456;&#32467;&#21512;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#21322;&#21442;&#25968;&#22320;&#26816;&#32034;GIB&#26816;&#27979;&#21040;&#30340;&#20851;&#38190;&#23376;&#22270;&#65292;&#24182;&#21387;&#32553;&#35299;&#37322;&#20449;&#24687;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained considerable traction for their capability to effectively process topological data, yet their interpretability remains a critical concern. Current interpretation methods are dominated by post-hoc explanations to provide a transparent and intuitive understanding of GNNs. However, they have limited performance in interpreting complicated subgraphs and can't utilize the explanation to advance GNN predictions. On the other hand, transparent GNN models are proposed to capture critical subgraphs. While such methods could improve GNN predictions, they usually don't perform well on explanations. Thus, it is desired for a new strategy to better couple GNN explanation and prediction. In this study, we have developed a novel interpretable causal GNN framework that incorporates retrieval-based causal learning with Graph Information Bottleneck (GIB) theory. The framework could semi-parametrically retrieve crucial subgraphs detected by GIB and compress the ex
&lt;/p&gt;</description></item><item><title>EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.04699</link><description>&lt;p&gt;
EvoSeed&#65306;&#25581;&#31034;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#24187;&#35273;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04699
&lt;/p&gt;
&lt;p&gt;
EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#25152;&#21033;&#29992;&#65292;&#36825;&#20123;&#26679;&#26412;&#23545;&#20154;&#31867;&#24863;&#30693;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30333;&#30418;&#24615;&#36136;&#26469;&#29983;&#25104;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#65292;&#25110;&#32773;&#25913;&#21464;&#23545;&#25239;&#26679;&#26412;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#32531;&#35299;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoSeed&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;EvoSeed&#26694;&#26550;&#20351;&#29992;&#36741;&#21161;&#25193;&#25955;&#21644;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;CMA-ES&#26469;&#20248;&#21270;&#23545;&#23545;&#25239;&#31181;&#23376;&#21521;&#37327;&#30340;&#25628;&#32034;&#65292;&#35813;&#21521;&#37327;&#22312;&#32463;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22788;&#29702;&#21518;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#26080;&#38480;&#21046;&#30340;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#29983;&#25104;&#30340;&#23545;&#25239;&#22270;&#20687;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#20851;&#20998;&#37327;&#30340;&#35299;&#37322;&#26041;&#24046;&#21040;&#26080;&#27491;&#20132;&#32422;&#26463;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34913;&#37327;&#25968;&#25454;&#30697;&#38453;A&#20013;&#30001;&#30456;&#20851;&#20998;&#37327;Y = AZ&#35299;&#37322;&#30340;&#26041;&#24046;&#37096;&#20998;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;expvar(Y)&#65292;&#25918;&#26494;&#20102;&#21152;&#36733;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.04692</link><description>&lt;p&gt;
&#20174;&#30456;&#20851;&#20998;&#37327;&#30340;&#35299;&#37322;&#26041;&#24046;&#21040;&#26080;&#27491;&#20132;&#32422;&#26463;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From explained variance of correlated components to PCA without orthogonality constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#20851;&#20998;&#37327;&#30340;&#35299;&#37322;&#26041;&#24046;&#21040;&#26080;&#27491;&#20132;&#32422;&#26463;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34913;&#37327;&#25968;&#25454;&#30697;&#38453;A&#20013;&#30001;&#30456;&#20851;&#20998;&#37327;Y = AZ&#35299;&#37322;&#30340;&#26041;&#24046;&#37096;&#20998;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;expvar(Y)&#65292;&#25918;&#26494;&#20102;&#21152;&#36733;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25968;&#25454;&#30697;&#38453;A&#30340;&#22359;&#29366;&#20027;&#25104;&#20998;&#20998;&#26512;(Block PCA)&#20013;&#30340;&#21152;&#36733;Z&#30001;&#20110;&#22312;&#21333;&#20301;&#33539;&#25968;&#27491;&#20132;&#21152;&#36733;&#19978;&#26368;&#22823;&#21270;AZ&#30340;&#22256;&#38590;&#65292;&#20351;&#24471;&#20351;&#29992;1&#27491;&#21017;&#21270;&#26469;&#35774;&#35745;&#31232;&#30095;PCA&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#24456;&#38590;&#21516;&#26102;&#22788;&#29702;&#21152;&#36733;&#30340;&#27491;&#20132;&#32422;&#26463;&#21644;&#19981;&#21487;&#24494;&#30340;1&#24809;&#32602;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24341;&#20837;&#34913;&#37327;&#25968;&#25454;&#30697;&#38453;A&#20013;&#30001;&#30456;&#20851;&#20998;&#37327;Y = AZ&#35299;&#37322;&#30340;&#26041;&#24046;&#37096;&#20998;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;expvar(Y)&#26469;&#25918;&#26494;&#21152;&#36733;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20004;&#20010;&#29616;&#26377;&#23450;&#20041;Zou et al. [2006]&#21644;Shen and Huang [2008]&#30340;expvar(Y)&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#21644;&#25968;&#20540;&#23646;&#24615;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#23450;&#20041;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#21482;&#26377;&#36825;&#20004;&#20010;&#35299;&#37322;&#26041;&#24046;&#25165;&#36866;&#21512;&#20316;&#20026;&#22359;&#29366;PCA&#24418;&#24335;&#20013;&#21435;&#38500;&#27491;&#20132;&#32422;&#26463;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of AZ 2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by 1 regularization, due to the difficulty of taking care of both the orthogonality constraint on loadings and the non differentiable 1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar(Y) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04691</link><description>&lt;p&gt;
&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#31639;&#23376;&#30340;&#24369;&#21644;&#24378;&#35268;&#21017;&#26465;&#20214;&#65292;&#20197;&#25551;&#36848;&#20854;&#20869;&#22312;&#32467;&#26500;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36827;&#34892;&#20102;&#26497;&#23567;&#20540;&#19979;&#30028;&#20998;&#26512;&#65292;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#21644;&#35268;&#21017;&#26465;&#20214;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#20351;&#29992;SGD&#31639;&#27861;&#35299;&#20915;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12290;&#20540;&#24471;&#24378;&#35843;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#20272;&#35745;&#22120;&#23558;&#25910;&#25947;&#20110;&#38750;&#32447;&#24615;&#30446;&#26631;&#31639;&#23376;&#30340;&#26368;&#20339;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#24212;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#20540;&#21644;&#23454;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#20135;&#29983;&#20102;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20174;&#32780;&#23436;&#21892;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing litera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#27835;&#30103;&#25928;&#26524;&#20174;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#30340;&#35270;&#35282;&#65292;&#24182;&#27010;&#36848;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04668</link><description>&lt;p&gt;
&#20010;&#20307;&#21270;&#27835;&#30103;&#25928;&#26524;&#20174;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20010;&#20307;&#21270;&#27835;&#30103;&#25928;&#26524;&#20174;&#26102;&#38388;&#24207;&#21015;&#20581;&#24247;&#25968;&#25454;&#30340;&#35270;&#35282;&#65292;&#24182;&#27010;&#36848;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30142;&#30149;&#36127;&#25285;&#19981;&#26029;&#22686;&#21152;&#65292;&#38024;&#23545;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#20154;&#25968;&#19981;&#36275;&#30340;&#24739;&#32773;&#32676;&#20307;&#65292;&#27835;&#30103;&#25928;&#26524;&#19981;&#24179;&#31561;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#20197;&#21307;&#30103;&#27835;&#30103;&#30340;&#24179;&#22343;&#20154;&#32676;&#25928;&#24212;&#20026;&#39537;&#21160;&#21147;&#65292;&#22240;&#27492;&#37319;&#21462;&#20102;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#19981;&#19968;&#23450;&#36866;&#21512;&#27599;&#20010;&#24739;&#32773;&#12290;&#36825;&#20123;&#20107;&#23454;&#34920;&#26126;&#65292;&#24613;&#38656;&#26041;&#27861;&#23398;&#30740;&#31350;&#20010;&#20307;&#21270;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#20197;&#25512;&#21160;&#20010;&#20154;&#21270;&#27835;&#30103;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;ITE&#20272;&#35745;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#20165;&#20851;&#27880;&#20855;&#26377;&#26377;&#38480;&#22238;&#39038;&#21644;&#29702;&#35299;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20986;&#30340;&#26041;&#27861;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;ITE&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#38024;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35265;&#35299;&#12290;&#35813;&#24037;&#20316;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#24182;&#20174;&#29702;&#35770;&#20551;&#35774;&#12289;&#27835;&#30103;&#35774;&#32622;&#31867;&#22411;&#21644;&#35745;&#31639;&#26694;&#26550;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials. Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a "one-size-fits-all" approach, not necessarily what best fits each patient. These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment. Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs). To this end, this work provides an overview of ITE works for time-series data and insights into future research. The work summarizes the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks. Furthermore, th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04655</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Calibration for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (VLM) &#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#22788;&#29702;&#22270;&#20687;&#35782;&#21035;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#12289;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#24320;&#25918;&#35789;&#27719;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#25552;&#39640; VLM &#19979;&#28216;&#24615;&#33021;&#30340;&#36866;&#24212;&#26041;&#27861;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#25552;&#31034;&#23398;&#20064;&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#22823;&#22823;&#24573;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#30340; VLM &#20013;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#20250;&#22823;&#22823;&#38477;&#20302;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#25552;&#31034;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; "Distance-Aware Ca"
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#23436;&#22791;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#24230;&#20013;&#65292;&#21516;&#26102;&#35774;&#35745;&#21644;&#23398;&#20064;&#23884;&#20837;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#21453;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.04653</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21453;&#38382;&#39064;&#30340;&#36807;&#23436;&#22791;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Over Complete Deep Learning Method for Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04653
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#23436;&#22791;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#24230;&#20013;&#65292;&#21516;&#26102;&#35774;&#35745;&#21644;&#23398;&#20064;&#23884;&#20837;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#21453;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#33719;&#21462;&#21453;&#38382;&#39064;&#30340;&#26377;&#24847;&#20041;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#22522;&#20110;&#36817;&#31471;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#24403;&#24212;&#29992;&#20110;&#19968;&#20123;&#20856;&#22411;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#31867;&#20284;&#20110;&#36807;&#23436;&#22791;&#23383;&#20856;&#30340;&#20808;&#21069;&#20316;&#21697;&#65292;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#23884;&#20837;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#20013;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25105;&#20204;&#20849;&#21516;&#35774;&#35745;&#24182;&#23398;&#20064;&#23884;&#20837;&#21644;&#23884;&#20837;&#21521;&#37327;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#21644;&#24120;&#35265;&#30340;&#21453;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems.
&lt;/p&gt;</description></item><item><title>&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.04647</link><description>&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65306;&#35268;&#21010;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Plan Transformer: Planning as Latent Variable Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04647
&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#38271;&#26399;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#35268;&#21010;&#21464;&#24471;&#24517;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35268;&#21010;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32570;&#20047;&#36880;&#27493;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#26469;&#36830;&#25509;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#12290;LPT&#21487;&#20197;&#36890;&#36807;&#36712;&#36857;-&#22238;&#25253;&#23545;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#65292;&#23613;&#31649;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#65292;&#33258;&#28982;&#22320;&#32858;&#38598;&#23376;&#36712;&#36857;&#20197;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#36890;&#36807;&#39044;&#26399;&#22238;&#25253;&#23545;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#20316;&#20026;&#25512;&#26029;&#30340;&#24605;&#24819;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#25972;&#20010;&#22238;&#21512;&#20013;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#65292;&#36215;&#21040;&#19968;&#20010;&#35745;&#21010;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LPT&#21487;&#20197;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#22359;&#31232;&#30095;&#20808;&#39564;&#65292;&#29992;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04646</link><description>&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#22359;&#31232;&#30095;&#20449;&#21495;&#30340;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning with Diversification from Block Sparse Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#22359;&#31232;&#30095;&#20808;&#39564;&#65292;&#29992;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20808;&#39564;Diversified Block Sparse Prior&#65292;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#21516;&#26102;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#21033;&#29992;EM&#31639;&#27861;&#21644;&#23545;&#20598;&#19978;&#21319;&#27861;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04644</link><description>&lt;p&gt;
LEVI:&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#22312;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#24494;&#35843;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24494;&#35843;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65288;&#21363;&#65292;&#36229;&#20986;&#20998;&#24067;&#65307;OOD&#65289;&#19978;&#30340;&#27867;&#21270;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;OOB&#27867;&#21270;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#35843;&#25972;&#24494;&#35843;&#20197;&#20445;&#30041;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#38480;&#21046;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#34920;&#31034;&#21487;&#33021;&#20250;&#38459;&#30861;&#24494;&#35843;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#30340;&#37325;&#35201;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;OOB&#27867;&#21270;&#12290;&#24403;&#26032;&#20219;&#21153;&#26469;&#33258;&#20110;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#65288;&#23376;&#65289;&#39046;&#22495;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#30340;&#26041;&#24335;&#30830;&#23450;&#27169;&#22411;&#30340;&#25968;&#25454;&#39046;&#22495;&#21644;&#20855;&#20307;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35782;&#21035;&#20016;&#23500;&#32454;&#31890;&#24230;&#31867;&#21035;&#26041;&#38754;&#26356;&#21152;&#31934;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.04640</link><description>&lt;p&gt;
&#22495;&#26725;&#26753;&#65306;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#40657;&#31665;&#27169;&#22411;&#39046;&#22495;&#21462;&#35777;
&lt;/p&gt;
&lt;p&gt;
Domain Bridge: Generative model-based domain forensic for black-box models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#30340;&#26041;&#24335;&#30830;&#23450;&#27169;&#22411;&#30340;&#25968;&#25454;&#39046;&#22495;&#21644;&#20855;&#20307;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35782;&#21035;&#20016;&#23500;&#32454;&#31890;&#24230;&#31867;&#21035;&#26041;&#38754;&#26356;&#21152;&#31934;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21462;&#35777;&#35843;&#26597;&#20013;&#65292;&#30830;&#23450;&#27169;&#22411;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#25216;&#26415;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65292;&#22914;ImageNet&#26469;&#36817;&#20284;&#30446;&#26631;&#27169;&#22411;&#30340;&#39046;&#22495;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#25214;&#21040;&#24191;&#27867;&#30340;&#39046;&#22495;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#35782;&#21035;&#39046;&#22495;&#20869;&#26356;&#20855;&#32454;&#31890;&#24230;&#30340;&#31867;&#21035;&#26102;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#30830;&#23450;&#36890;&#29992;&#30340;&#25968;&#25454;&#39046;&#22495;&#65288;&#20363;&#22914;&#20154;&#33080;&#65289;&#65292;&#32780;&#19988;&#30830;&#23450;&#20102;&#20854;&#20855;&#20307;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#20329;&#25140;&#30524;&#38236;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#23884;&#20837;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#20174;&#31895;&#30053;&#30340;&#25551;&#36848;&#24320;&#22987;&#65292;&#35299;&#30721;&#22120;&#29983;&#25104;&#19968;&#32452;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#21576;&#29616;&#32473;&#26410;&#30693;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#27169;&#22411;&#25104;&#21151;&#20998;&#31867;&#23558;&#24341;&#23548;&#32534;&#30721;&#22120;&#20462;&#27491;&#25551;&#36848;&#65292;&#28982;&#21518;&#29992;&#20110;&#29983;&#25104;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#26356;&#20855;&#20307;&#30340;&#22270;&#20687;&#38598;&#21512;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#25913;&#36827;&#32553;&#23567;&#20102;&#39046;&#22495;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In forensic investigations of machine learning models, techniques that determine a model's data domain play an essential role, with prior work relying on large-scale corpora like ImageNet to approximate the target model's domain. Although such methods are effective in finding broad domains, they often struggle in identifying finer-grained classes within those domains. In this paper, we introduce an enhanced approach to determine not just the general data domain (e.g., human face) but also its specific attributes (e.g., wearing glasses). Our approach uses an image embedding model as the encoder and a generative model as the decoder. Beginning with a coarse-grained description, the decoder generates a set of images, which are then presented to the unknown target model. Successful classifications by the model guide the encoder to refine the description, which in turn, are used to produce a more specific set of images in the subsequent iteration. This iterative refinement narrows down the 
&lt;/p&gt;</description></item><item><title>A-X&#20381;&#36182;&#20851;&#31995;&#26159;&#24433;&#21709;&#22270;&#21367;&#31215;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#29305;&#24449;&#37325;&#25490;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04621</link><description>&lt;p&gt;
&#22270;&#25299;&#25169;&#32467;&#26500;&#19978;&#30340;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#20102;&#22270;&#21367;&#31215;&#30340;&#25928;&#26524;&#65306;&#21516;&#36136;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04621
&lt;/p&gt;
&lt;p&gt;
A-X&#20381;&#36182;&#20851;&#31995;&#26159;&#24433;&#21709;&#22270;&#21367;&#31215;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#29305;&#24449;&#37325;&#25490;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#37325;&#25490;&#21516;&#19968;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#29305;&#24449;&#21521;&#37327;&#22914;&#20309;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65311;&#30452;&#35266;&#22320;&#35828;&#65292;&#29305;&#24449;&#37325;&#25490;&#25200;&#20081;&#20102;GNNs&#20174;&#22270;&#25299;&#25169;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65288;A-X&#20381;&#36182;&#20851;&#31995;&#65289;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;GNNs&#30340;&#23398;&#20064;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#29305;&#24449;&#37325;&#25490;&#20043;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#12290;&#30001;&#20110;&#24573;&#35270;&#20102;A-X&#20381;&#36182;&#20851;&#31995;&#23545;GNNs&#30340;&#24433;&#21709;&#65292;&#20808;&#21069;&#30340;&#25991;&#29486;&#27809;&#26377;&#32473;&#20986;&#23545;&#35813;&#29616;&#35937;&#30340;&#28385;&#24847;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22914;&#20309;&#22312;&#25511;&#21046;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#24230;&#37327;A-X&#20381;&#36182;&#20851;&#31995;&#65311;&#20854;&#27425;&#65292;A-X&#20381;&#36182;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;GNNs&#65311;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24230;&#37327;A-X&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#25511;&#21046;A-X&#20381;&#36182;&#20851;&#31995;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#65288;iii&#65289;&#24314;&#31435;&#20102;A-X&#20381;&#36182;&#20851;&#31995;&#19982;&#22270;&#21367;&#31215;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#23545;&#23454;&#38469;&#22270;&#36827;&#34892;&#20102;&#19982;&#29702;&#35770;&#19968;&#33268;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;A-X&#20381;&#36182;&#20851;&#31995;&#23545;GNNs&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that aligns with the theory. We conclude that A-X depe
&lt;/p&gt;</description></item><item><title>CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04620</link><description>&lt;p&gt;
CataractBot&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04620
&lt;/p&gt;
&lt;p&gt;
CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#34892;&#19994;&#30340;&#21457;&#23637;&#65292;&#24739;&#32773;&#36234;&#26469;&#36234;&#36861;&#27714;&#26356;&#21487;&#38752;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20449;&#24687;&#26469;&#28304;&#65292;&#20294;&#25968;&#23383;&#26102;&#20195;&#21364;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#36807;&#22810;&#19988;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#24739;&#32773;&#20027;&#35201;&#20449;&#20219;&#21307;&#29983;&#21644;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#65292;&#31361;&#26174;&#20102;&#19987;&#23478;&#35748;&#21487;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19987;&#23478;&#38754;&#20020;&#30340;&#21387;&#21147;&#23548;&#33268;&#20102;&#27807;&#36890;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CataractBot&#65292;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#19982;&#21360;&#24230;&#19968;&#23478;&#19977;&#32423;&#30524;&#31185;&#21307;&#38498;&#21512;&#20316;&#24320;&#21457;&#30340;CataractBot&#36890;&#36807;&#26597;&#35810;&#31574;&#21010;&#30340;&#30693;&#35782;&#24211;&#65292;&#21363;&#26102;&#22238;&#31572;&#30333;&#20869;&#38556;&#25163;&#26415;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#24322;&#27493;&#25552;&#20379;&#19987;&#23478;&#39564;&#35777;&#30340;&#31572;&#22797;&#12290;CataractBot&#20855;&#22791;&#22810;&#27169;&#24335;&#25903;&#25345;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#19982;49&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#65292;CataractBot&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.04613</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;Moreau&#21253;&#32476;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27979;&#24230;f-&#24046;&#24322;&#65292;&#20363;&#22914;Kullback-Leibler&#24046;&#24322;&#65292;&#23545;&#20110;&#25152;&#28041;&#21450;&#30340;&#27979;&#24230;&#30340;&#25903;&#25345;&#23384;&#22312;&#38480;&#21046;&#12290;&#35299;&#20915;&#21150;&#27861;&#26159;&#36890;&#36807;&#19982;&#29305;&#24449;&#26680;K&#30456;&#20851;&#30340;&#24179;&#26041;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#23545;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#26469;&#26174;&#31034;&#30456;&#24212;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#37325;&#20889;&#20026;&#19982;K&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#26576;&#20123;&#20989;&#25968;&#30340;Moreau&#21253;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20851;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;Moreau&#21253;&#32476;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#21450;&#20854;&#26799;&#24230;&#30340;&#23646;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#20998;&#26512;&#21463;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32463;&#39564;&#27979;&#24230;&#24320;&#22987;&#30340;Wasserstein&#26799;&#24230;&#27969;&#65292;&#24182;&#25552;&#20379;&#20351;&#29992;Tsallis-$\alpha$&#24046;&#24322;&#30340;&#27010;&#24565;&#24615;&#25968;&#20540;&#31034;&#20363;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
&lt;/p&gt;</description></item><item><title>JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04599</link><description>&lt;p&gt;
&#35265; JEANIE&#65306;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30456;&#20284;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04599
&lt;/p&gt;
&lt;p&gt;
JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24207;&#21015;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24178;&#25200;&#24615;&#21464;&#21270;&#65292;&#21253;&#25324;&#21160;&#20316;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#20027;&#20307;&#23039;&#21183;&#65292;&#23548;&#33268;&#22312;&#27604;&#36739;&#20004;&#32452;&#24103;&#25110;&#35780;&#20272;&#20004;&#20010;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#26102;&#20135;&#29983;&#26102;&#38388;-&#35270;&#35282;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#23545;&#27604;&#30340;&#32852;&#21512;&#26102;&#38388;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#23545;&#40784;&#26041;&#27861;&#65288;JEANIE&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#33021;&#22815;&#22312;&#19977;&#32500;&#20013;&#36731;&#26494;&#25805;&#20316;&#25668;&#20687;&#26426;&#21644;&#20027;&#20307;&#23039;&#21183;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#65288;FSAR&#65289;&#19978;&#35780;&#20272;&#20102;JEANIE&#65292;&#20854;&#20013;&#30001;&#20110;&#26032;&#31867;&#21035;&#26679;&#26412;&#26377;&#38480;&#65292;&#36890;&#36807;&#21305;&#37197;&#22909;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#65288;&#32452;&#25104;&#24207;&#21015;&#30340;&#26102;&#38388;&#22359;&#65289;&#26469;&#25490;&#38500;&#24178;&#25200;&#21464;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#26597;&#35810;&#24207;&#21015;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#25668;&#20687;&#26426;&#20301;&#32622;&#21019;&#24314;&#22810;&#20010;&#35270;&#35282;&#12290;&#23545;&#20110;&#25903;&#25345;&#24207;&#21015;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27169;&#25311;&#20986;&#30340;&#26597;&#35810;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#25903;&#25345;&#26102;&#38388;&#22359;&#21487;&#20197;&#19982;&#35270;&#35282;&#27169;&#25311;&#30340;&#26597;&#35810;&#24207;&#21015;&#21305;&#37197;&#65292;&#22914;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#35299;&#20915;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04596</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#30340;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#25913;&#36827;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#35299;&#20915;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20856;&#22411;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#21482;&#33021;&#20174;&#22266;&#23450;&#30340;&#26679;&#26412;&#21644;&#26631;&#31614;&#38598;&#20013;&#23398;&#20064;&#65292;&#36825;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#20013;&#25968;&#25454;&#20197;&#26679;&#26412;&#27969;&#30340;&#24418;&#24335;&#21040;&#36798;&#65292;&#24182;&#19988;&#24448;&#24448;&#38543;&#26102;&#38388;&#20851;&#32852;&#30528;&#22810;&#20010;&#26631;&#31614;&#12290;&#36825;&#20419;&#20351;&#30740;&#31350;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20855;&#26377;&#36739;&#22823;&#30340;&#35745;&#31639;&#37327;&#12290;&#34429;&#28982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#23578;&#26410;&#23558;&#20854;&#29992;&#20110;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#20934;&#30830;&#22320;&#30830;&#23450;SNNs&#30340;&#22810;&#20010;&#26631;&#31614;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#30740;&#31350;&#24046;&#36317;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04580</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#34028;&#21187;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#20174;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36275;&#22815;&#30340;&#26080;&#20559;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#37319;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#29615;&#22659;&#65289;&#65292;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#24555;&#36895;&#27169;&#22411;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28304;&#39046;&#22495;&#30340;&#29615;&#22659;&#21644;&#20855;&#36523;&#26041;&#24335;&#21487;&#33021;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#29305;&#24449;&#30456;&#24046;&#24456;&#22823;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#24046;&#36317;&#30340;&#31934;&#32454;&#20998;&#31867;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#20010;&#38382;&#39064;&#35774;&#32622;&#30340;&#24635;&#20307;&#35265;&#35299;&#21644;&#35774;&#35745;&#32771;&#34385;&#12290;&#25105;&#20204;&#36824;&#23601;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#27861;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#65292;&#35299;&#20915;&#20102;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2402.04579</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Collective Counterfactual Explanations via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#65292;&#35299;&#20915;&#20102;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#25552;&#20379;&#20010;&#20307;&#30340;&#25104;&#26412;&#26368;&#20248;&#34892;&#21160;&#65292;&#20197;&#25913;&#21464;&#26631;&#31614;&#20026;&#25152;&#38656;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22823;&#37327;&#23454;&#20363;&#23547;&#27714;&#29366;&#24577;&#20462;&#25913;&#65292;&#36825;&#31181;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25512;&#33616;&#24573;&#35270;&#20102;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#24314;&#35758;&#29992;&#25143;&#35748;&#20026;&#26159;&#24322;&#24120;&#20540;&#30340;&#34892;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#37325;&#28857;&#26159;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;&#20511;&#37492;&#26368;&#20248;&#20256;&#36755;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#31181;&#38598;&#20307;&#26041;&#27861;&#22914;&#20309;&#25913;&#36827;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#25903;&#25345;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide individuals with cost-optimal actions that can alter their labels to desired classes. However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs. Furthermore, these recommendations, disregarding the underlying data distribution, may suggest actions that users perceive as outliers. To address these issues, our work proposes a collective approach for formulating counterfactual explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions. Our problem naturally casts as an optimal transport problem. Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical counterfactual explanations. We support our proposal with numerical simulations, illustrating the effectiveness of the proposed approach and its relation to classic methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#34892;&#20026;&#29305;&#24449;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.04567</link><description>&lt;p&gt;
OIL-AD: &#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#34892;&#20026;&#29305;&#24449;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#27491;&#24120;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#21644;&#20219;&#21153;&#30340;&#39034;&#24207;&#24615;&#36136;&#12290;&#22823;&#37096;&#20998;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#23545;&#29615;&#22659;&#21160;&#24577;&#12289;&#22870;&#21169;&#20449;&#21495;&#21644;&#19982;&#29615;&#22659;&#30340;&#22312;&#32447;&#20132;&#20114;&#31561;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#65288;OIL-AD&#65289;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#25552;&#21462;&#30340;&#34892;&#20026;&#29305;&#24449;&#65288;&#21160;&#20316;&#20248;&#21270;&#21644;&#39034;&#24207;&#20851;&#32852;&#65289;&#26469;&#26816;&#27979;&#20915;&#31574;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#12290;&#25105;&#20204;&#30340;&#31163;&#32447;&#23398;&#20064;&#27169;&#22411;&#26159;&#22522;&#20110;&#21464;&#21387;&#22120;&#31574;&#30053;&#32593;&#32476;&#30340;&#34892;&#20026;&#20811;&#38534;&#30340;&#36866;&#24212;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#27491;&#24120;&#36712;&#36857;&#20013;&#23398;&#20064;Q&#20989;&#25968;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;Q&#20989;&#25968;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#65292;&#23558;&#20652;&#21270;&#21058;&#21512;&#25104;&#39046;&#22495;&#30340;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#21442;&#25968;&#65292;&#20197;&#21152;&#24555;&#21644;&#22686;&#24378;&#20652;&#21270;&#21058;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04557</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20652;&#21270;&#21058;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
An Artificial Intelligence (AI) workflow for catalyst design and optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#65292;&#23558;&#20652;&#21270;&#21058;&#21512;&#25104;&#39046;&#22495;&#30340;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#21442;&#25968;&#65292;&#20197;&#21152;&#24555;&#21644;&#22686;&#24378;&#20652;&#21270;&#21058;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#32039;&#36843;&#30340;&#29615;&#22659;&#38382;&#39064;&#21644;&#33021;&#28304;&#38656;&#27714;&#30340;&#36807;&#31243;&#20013;&#65292;&#24120;&#35268;&#30340;&#20652;&#21270;&#21058;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#24448;&#24448;&#22240;&#20652;&#21270;&#21058;&#21442;&#25968;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#21644;&#24191;&#27867;&#24615;&#32780;&#19981;&#36275;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20986;&#29616;&#20026;&#20652;&#21270;&#21058;&#20248;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#20256;&#32479;&#25216;&#26415;&#32570;&#38519;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#26085;&#30410;&#22686;&#38271;&#30340;&#20652;&#21270;&#21058;&#21512;&#25104;&#31185;&#23398;&#25991;&#29486;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#24555;&#21644;&#22686;&#24378;&#20652;&#21270;&#21058;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20808;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#19982;&#24378;&#22823;&#30340;&#20248;&#21270;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#23558;&#20174;&#19981;&#21516;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space. The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques. However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis. To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization. Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#20248;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26080;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#21644;&#20302;&#31209;&#36817;&#20284;&#39044;&#22788;&#29702;&#22120;&#65292;&#24182;&#22312;&#32447;&#26356;&#26032;&#36825;&#20004;&#31181;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#23545;&#31216;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#38459;&#23612;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#23398;&#20064;&#29575;&#21644;&#27493;&#38271;&#21487;&#20197;&#33258;&#28982;&#24402;&#19968;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#40664;&#35748;&#20540;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04553</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#26446;&#32676;&#39044;&#22788;&#29702;&#22120;&#30340;&#26354;&#29575;&#20449;&#24687;&#20248;&#21270;SGD&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Curvature-Informed SGD via General Purpose Lie-Group Preconditioners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#20248;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26080;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#21644;&#20302;&#31209;&#36817;&#20284;&#39044;&#22788;&#29702;&#22120;&#65292;&#24182;&#22312;&#32447;&#26356;&#26032;&#36825;&#20004;&#31181;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#23545;&#31216;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#38459;&#23612;&#30340;&#38656;&#27714;&#65292;&#20351;&#24471;&#23398;&#20064;&#29575;&#21644;&#27493;&#38271;&#21487;&#20197;&#33258;&#28982;&#24402;&#19968;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#40664;&#35748;&#20540;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20174;&#40657;&#22622;&#30697;&#38453;-&#21521;&#37327;&#20056;&#31215;&#25110;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#26377;&#38480;&#24046;&#20998;&#20013;&#33719;&#24471;&#30340;&#26354;&#29575;&#20449;&#24687;&#65292;&#31867;&#20284;&#20110;BFGS&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#20010;&#39044;&#22788;&#29702;&#22120;&#65306;&#19968;&#20010;&#26080;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#21644;&#19968;&#20010;&#20302;&#31209;&#36817;&#20284;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#23545;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#19981;&#38656;&#35201;&#32447;&#24615;&#25628;&#32034;&#25110;&#38459;&#23612;&#30340;&#20934;&#21017;&#22312;&#32447;&#26356;&#26032;&#20004;&#20010;&#39044;&#22788;&#29702;&#22120;&#12290;&#20026;&#20102;&#20445;&#25345;&#30456;&#24212;&#30340;&#23545;&#31216;&#24615;&#25110;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#39044;&#22788;&#29702;&#22120;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#36830;&#36890;&#26446;&#32676;&#12290;&#26446;&#32676;&#30340;&#31561;&#21464;&#24615;&#36136;&#31616;&#21270;&#20102;&#39044;&#22788;&#29702;&#22120;&#30340;&#25311;&#21512;&#36807;&#31243;&#65292;&#32780;&#23427;&#30340;&#19981;&#21464;&#24615;&#36136;&#28040;&#38500;&#20102;&#22312;&#20108;&#38454;&#20248;&#21270;&#22120;&#20013;&#26222;&#36941;&#38656;&#35201;&#30340;&#38459;&#23612;&#65292;&#20174;&#32780;&#20351;&#21442;&#25968;&#26356;&#26032;&#30340;&#23398;&#20064;&#29575;&#21644;&#39044;&#22788;&#29702;&#22120;&#25311;&#21512;&#30340;&#27493;&#38271;&#33258;&#28982;&#22320;&#34987;&#24402;&#19968;&#21270;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#40664;&#35748;&#20540;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04550</link><description>&lt;p&gt;
Riemann-Lebesgue Forest&#22238;&#24402;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemann-Lebesgue Forest for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;Riemann-Lebesgue Forest (RLF)&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#21010;&#20998;&#20989;&#25968;&#30340;&#20540;&#22495;&#20026;&#22810;&#20010;&#21306;&#38388;&#26469;&#36924;&#36817;&#21487;&#27979;&#20989;&#25968;&#30340;&#24605;&#24819;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;Riemann-Lebesgue Tree&#12290;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#25512;&#23548;&#20102;RLF&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Forest (RLF)&#12290;RLF&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#20989;&#25968;&#30340;&#20540;&#22495;&#21010;&#20998;&#20026;&#20960;&#20010;&#21306;&#38388;&#26469;&#27169;&#25311;&#21487;&#27979;&#20989;&#25968;&#30340;&#36924;&#36817;&#26041;&#24335;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Riemann-Lebesgue Tree&#65292;&#23427;&#22312;&#27599;&#20010;&#38750;&#21494;&#33410;&#28857;&#19978;&#26377;&#26426;&#20250;&#20174;&#21709;&#24212;Y&#25110;&#29305;&#24449;&#31354;&#38388;X&#20013;&#30340;&#26041;&#21521;&#36827;&#34892;&#20999;&#21106;&#12290;&#25105;&#20204;&#36890;&#36807;Hoeffding&#20998;&#35299;&#21644;Stein&#26041;&#27861;&#26469;&#25512;&#23548;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;RLF&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#24403;&#24213;&#23618;&#20989;&#25968;Y=f(X)&#36981;&#24490;&#21152;&#27861;&#22238;&#24402;&#27169;&#22411;&#26102;&#65292;RLF&#19982;Scornet&#31561;&#20154;&#30340;&#35770;&#35777;&#65288;2014&#24180;&#65289;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;RLF&#19982;&#21407;&#22987;&#38543;&#26426;&#26862;&#26519;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#20316;&#20026;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#65292;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#21516;&#26102;&#20173;&#20445;&#25345;</title><link>https://arxiv.org/abs/2402.04539</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#20214;&#33258;&#29983;&#25104;&#30340;&#24341;&#23548;&#23398;&#20064;&#22810;&#26679;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Policies with Soft Self-Generated Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#20316;&#20026;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#65292;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#21516;&#26102;&#20173;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#21644;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#22870;&#21169;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#20960;&#20046;&#24456;&#23569;&#33021;&#22815;&#33719;&#24471;&#38750;&#38646;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#35745;&#31639;&#30340;&#26799;&#24230;&#21487;&#33021;&#26159;&#38543;&#26426;&#30340;&#19988;&#32570;&#20047;&#26377;&#25928;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#21487;&#20197;&#20351;&#23398;&#20064;&#36807;&#31243;&#26356;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#36825;&#20123;&#32463;&#39564;&#24517;&#39035;&#25104;&#21151;&#65292;&#24182;&#21487;&#33021;&#36807;&#24230;&#21033;&#29992;&#23427;&#20204;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#37319;&#21462;&#27425;&#20248;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#36827;&#34892;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#31574;&#30053;&#25913;&#36827;&#27493;&#39588;&#21644;&#20351;&#29992;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#30340;&#39069;&#22806;&#25506;&#32034;&#27493;&#39588;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65292;&#23558;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#65292;&#36890;&#36807;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#23454;&#29616;&#20102;&#22270;&#20013;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;&#36890;&#36807;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#24182;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#20063;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04538</link><description>&lt;p&gt;
&#19977;&#20803;&#20132;&#20114;&#25913;&#36827;&#22270;&#36716;&#25442;&#22120;&#65306;&#36890;&#36807;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#23376;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#65292;&#36890;&#36807;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#23454;&#29616;&#20102;&#22270;&#20013;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;&#36890;&#36807;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#24182;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#20063;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#36890;&#24120;&#32570;&#20047;&#30452;&#25509;&#30340;&#23545;&#31561;&#36890;&#20449;&#65292;&#32780;&#26159;&#36890;&#36807;&#20849;&#21516;&#33410;&#28857;&#24378;&#21046;&#30456;&#37051;&#23545;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22270;&#36716;&#25442;&#22120;&#65288;TGT&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26032;&#39062;&#30340;&#19977;&#20803;&#27880;&#24847;&#21147;&#21644;&#32858;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22270;&#20013;&#20004;&#20010;&#30456;&#37051;&#23545;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#12290;TGT&#39318;&#20808;&#20174;2D&#22270;&#20013;&#39044;&#27979;&#21407;&#23376;&#38388;&#36317;&#31163;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#36317;&#31163;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#21644;&#38543;&#26426;&#25512;&#26029;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24320;&#25918;&#25361;&#25112;&#22522;&#20934;PCQM4Mv2&#21644;OC20 IS2RE&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#36824;&#22312;QM9&#12289;MOLPCBA&#21644;LIT-PCBA&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#23637;&#31034;&#20102;TGT&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node. We propose the Triplet Graph Transformer (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP).
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.04536</link><description>&lt;p&gt;
&#22522;&#20110;&#35302;&#35273;&#30340;&#20174;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#29289;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tactile-based Object Retrieval From Granular Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GEOTACT&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#39063;&#31890;&#20171;&#36136;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;&#20165;&#20381;&#38752;&#35302;&#35273;&#21453;&#39304;&#26469;&#23436;&#25104;&#65292;&#22240;&#20026;&#19968;&#20010;&#22475;&#34255;&#30340;&#29289;&#20307;&#21487;&#33021;&#23436;&#20840;&#34987;&#35270;&#35273;&#38544;&#34255;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#35302;&#35273;&#21453;&#39304;&#26412;&#36523;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#21608;&#22260;&#20171;&#36136;&#36827;&#34892;&#26222;&#36941;&#25509;&#35302;&#65292;&#24182;&#19988;&#30001;&#35302;&#35273;&#35835;&#25968;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#34920;&#36848;&#23548;&#33268;&#20102;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#25805;&#20316;&#22120;&#20351;&#29992;&#36825;&#20123;&#34892;&#20026;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#29289;&#20307;&#24341;&#23548;&#21040;&#31283;&#23450;&#30340;&#25235;&#21462;&#20301;&#32622;&#65292;&#23613;&#31649;&#23384;&#22312;&#20551;&#30340;&#21644;&#22122;&#22768;&#30340;&#35302;&#35273;&#35835;&#25968;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#36825;&#20123;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GEOTACT&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SumRec&#65292;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#20013;&#25512;&#33616;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#35805;&#20013;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;SumRec&#26694;&#26550;&#27604;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.04523</link><description>&lt;p&gt;
SumRec: &#20351;&#29992;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#36827;&#34892;&#25512;&#33616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SumRec: A Framework for Recommendation using Open-Domain Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SumRec&#65292;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#20013;&#25512;&#33616;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#35805;&#20013;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;SumRec&#26694;&#26550;&#27604;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#23545;&#35805;&#21253;&#21547;&#30528;&#20851;&#20110;&#35828;&#35805;&#32773;&#20852;&#36259;&#12289;&#20559;&#22909;&#21644;&#32463;&#39564;&#30340;&#22823;&#37327;&#26377;&#29992;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#23545;&#35805;&#20013;&#30340;&#30693;&#35782;&#26469;&#20010;&#24615;&#21270;&#21508;&#31181;&#31995;&#32479;&#24182;&#20026;&#39640;&#32423;&#20449;&#24687;&#25552;&#20379;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#23545;&#35805;&#20013;&#25512;&#33616;&#20449;&#24687;&#30340;&#26032;&#39062;&#26694;&#26550;SumRec&#12290;&#35813;&#30740;&#31350;&#36824;&#20351;&#29992;ChatRec&#36825;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#26816;&#39564;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#21462;&#35828;&#35805;&#32773;&#21644;&#29289;&#21697;&#30340;&#29305;&#24449;&#65292;SumRec&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#35828;&#35805;&#32773;&#21644;&#29289;&#21697;&#20449;&#24687;&#36755;&#20837;&#21040;&#35780;&#20998;&#20272;&#35745;&#27169;&#22411;&#20013;&#65292;&#29983;&#25104;&#25512;&#33616;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SumRec&#26694;&#26550;&#27604;&#20351;&#29992;&#23545;&#35805;&#21644;&#21407;&#22987;&#29289;&#21697;&#25551;&#36848;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a recommendation score.Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#20960;&#20309;&#32467;&#26500;&#30340;&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#30340;&#27010;&#24565;&#65292;&#20026;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2402.04516</link><description>&lt;p&gt;
&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#30340;&#24191;&#20041; Sobolev &#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Generalized Sobolev Transport for Probability Measures on a Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04516
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#20960;&#20309;&#32467;&#26500;&#30340;&#22270;&#19978;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#30340;&#27010;&#24565;&#65292;&#20026;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#22312;&#22270;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;Le &#31561;&#20154;&#65288;2022&#65289;&#21033;&#29992;&#22270;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181; OT &#30340;&#21464;&#20307;&#65292;&#31216;&#20026; Sobolev &#20256;&#36755;&#65288;ST&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#38381;&#24335;&#34920;&#36798;&#24335;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;ST &#30340;&#23450;&#20041;&#20013;&#23454;&#36136;&#19978;&#19982; $L^p$ &#20960;&#20309;&#32467;&#26500;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#36825;&#20351;&#24471;&#22312;&#20854;&#20182;&#20808;&#39564;&#32467;&#26500;&#20013;&#21033;&#29992; ST &#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#32463;&#20856;&#30340; OT &#36890;&#36807;&#20462;&#25913;&#24213;&#23618;&#25104;&#26412;&#20989;&#25968;&#20855;&#26377;&#36866;&#24212;&#21508;&#31181;&#20960;&#20309;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#36229;&#21147; Wassestein&#65288;OW&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;\emph{Orlicz &#20960;&#20309;&#32467;&#26500;}&#36229;&#36234;&#20102; $L^p$ &#32467;&#26500;&#12290;&#19982;&#20351;&#29992;&#26631;&#20934; $p$-&#38454; Wassestein &#30456;&#27604;&#65292;OW &#26174;&#33879;&#25552;&#39640;&#20102;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20004;&#23618;&#20248;&#21270; formulation&#65292;OW &#22312;&#20854;&#35745;&#31639;&#19978;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31867;&#29305;&#23450;&#30340;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex funct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#36319;&#36394;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PKF&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.04498</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#24577;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#30340;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#36319;&#36394;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PKF&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#26159;&#19968;&#31181;&#26368;&#20248;&#32447;&#24615;&#29366;&#24577;&#39044;&#27979;&#31639;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#23398;&#12289;&#32463;&#27982;&#23398;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#22826;&#31354;&#25506;&#32034;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;KF&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#21160;&#24577;&#36319;&#36394;&#19982;&#24213;&#23618;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#24212;&#29992;&#26159;&#33258;&#21160;&#26816;&#27979;&#20869;&#37096;&#26426;&#21046;&#27169;&#22411;&#19982;&#25968;&#25454;&#22312;&#26102;&#38388;&#19978;&#21457;&#29983;&#21464;&#21270;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25551;&#36848;PKF&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;PKF&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration. Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty. An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner. First, we provide theorems characterizing the convergence of the PKF algorithm. Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude. Finally, we apply this method to biological time-course dataset i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26799;&#24230;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20869;&#21487;&#20197;&#20197;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#36827;&#34892;&#21069;&#21521;&#35745;&#31639;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#38656;&#35201;&#36229;&#36807;&#20108;&#27425;&#26102;&#38388;&#65292;&#36825;&#23545;&#20110;LLM&#35757;&#32451;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04497</link><description>&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#30340;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Fine-Grained Complexity of Gradient Computation for Training Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26799;&#24230;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20869;&#21487;&#20197;&#20197;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#36827;&#34892;&#21069;&#21521;&#35745;&#31639;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#38656;&#35201;&#36229;&#36807;&#20108;&#27425;&#26102;&#38388;&#65292;&#36825;&#23545;&#20110;LLM&#35757;&#32451;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#20316;&#20986;&#20102;&#22522;&#26412;&#36129;&#29486;&#12290;&#35201;&#35757;&#32451;&#19968;&#20010;LLM&#65292;&#20154;&#20204;&#38656;&#35201;&#20132;&#26367;&#36816;&#34892;&#8220;&#21069;&#21521;&#35745;&#31639;&#8221;&#21644;&#8220;&#21453;&#21521;&#35745;&#31639;&#8221;&#12290;&#21069;&#21521;&#35745;&#31639;&#21487;&#20197;&#35270;&#20026;&#27880;&#24847;&#21147;&#20989;&#25968;&#30340;&#35780;&#20272;&#65292;&#32780;&#21518;&#21521;&#35745;&#31639;&#21487;&#20197;&#35270;&#20026;&#26799;&#24230;&#35745;&#31639;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;[Alman&#21644;Song&#65292;NeurIPS 2023]&#35777;&#26126;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20013;&#21069;&#21521;&#27493;&#39588;&#21487;&#20197;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#25191;&#34892;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#65292;&#38500;&#38750;&#27969;&#34892;&#30340;&#20551;&#35774;SETH&#19981;&#25104;&#31435;&#65292;&#21542;&#21017;&#27809;&#26377;&#30495;&#27491;&#30340;&#20122;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35745;&#31639;&#21333;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#20197;&#21450;LLM&#35757;&#32451;&#30340;&#25972;&#20010;&#36807;&#31243;&#20013;&#20284;&#20046;&#26356;&#38590;&#30340;&#38382;&#39064;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#36825;&#23436;&#20840;&#21051;&#30011;&#20102;LLM&#35757;&#32451;&#27599;&#20010;&#27493;&#39588;&#30340;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.04494</link><description>&lt;p&gt;
&#19981;&#38656;&#25628;&#32034;&#21363;&#21487;&#23454;&#29616;&#22823;&#24072;&#32423;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Grandmaster-Level Chess Without Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#35268;&#27169;&#21270;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#21644;&#31354;&#21069;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22269;&#38469;&#35937;&#26827;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#20381;&#36182;&#22797;&#26434;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#26174;&#24335;&#25628;&#32034;&#25110;&#20108;&#32773;&#32467;&#21512;&#30340;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;1000&#19975;&#23616;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;2.7&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#29992;&#24378;&#22823;&#30340;Stockfish 16&#24341;&#25806;&#25552;&#20379;&#30340;&#21160;&#20316;&#20540;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26827;&#23616;&#65292;&#20135;&#29983;&#22823;&#32422;150&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;Elo&#19978;&#36798;&#21040;&#20102;2895&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#39046;&#22495;&#30340;&#35843;&#25972;&#25110;&#26174;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;AlphaZero&#30340;&#31574;&#30053;&#21644;&#20215;&#20540;&#32593;&#32476;&#65288;&#26080;MCTS&#65289;&#20197;&#21450;GPT-3.5-turbo-instruct&#12290;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#24182;&#36798;&#21040;&#20102;$O(\epsilon^{-2})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.04493</link><description>&lt;p&gt;
&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#22522;&#26412;&#23545;&#20598;&#31639;&#27861;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#31209;MDPs&#19978;&#30340;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#24182;&#36798;&#21040;&#20102;$O(\epsilon^{-2})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#31181;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#20302;&#31209;MDPs&#25110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#29616;&#26377;&#31639;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#20248;&#21270;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(\epsilon^{-2})$&#26102;&#65292;&#35201;&#20040;&#38656;&#35201;&#22343;&#21248;&#30340;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#65292;&#35201;&#20040;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25240;&#25187;&#26080;&#31351;&#26102;&#27573;&#35774;&#32622;&#19979;&#65292;&#29992;&#20110;&#20302;&#31209;MDPs&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#26412;&#23545;&#20598;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#37096;&#20998;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#35813;&#35774;&#32622;&#20013;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#36798;&#21040;$O(\epsilon^{-2})$&#30340;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20248;&#20110;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#65292;&#20854;&#38656;&#35201;$O(\epsilon^{-4})$&#20010;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#25903;&#25345;&#39069;&#22806;&#22870;&#21169;&#20449;&#21495;&#30340;&#32422;&#26463;&#65292;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#21040;&#31163;&#32447;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#25918;&#22823;&#20102;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25239;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#35777;&#26126;&#20102;DP&#21487;&#20197;&#20943;&#23569;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;DP&#21644;CDA&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04489</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#22312;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#28040;&#38500;&#20559;&#35265;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
De-amplifying Bias from Differential Privacy in Language Model Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#25918;&#22823;&#20102;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25239;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#35777;&#26126;&#20102;DP&#21487;&#20197;&#20943;&#23569;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;DP&#21644;CDA&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20174;&#19994;&#32773;&#32463;&#24120;&#22312;&#27169;&#22411;&#20013;&#36861;&#27714;&#30340;&#20004;&#20010;&#37325;&#35201;&#20215;&#20540;&#12290;&#20844;&#24179;&#24615;&#26088;&#22312;&#20943;&#23569;&#23545;&#31038;&#20250;/&#20154;&#21475;&#20122;&#32452;&#30340;&#27169;&#22411;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#36890;&#36807;&#38480;&#21046;&#20219;&#20309;&#20010;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#24433;&#21709;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#21487;&#38752;&#30340;ML&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#23545;&#20110;&#37027;&#20123;&#24076;&#26395;&#35299;&#20915;&#20004;&#32773;&#30340;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;DP&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#26102;&#25918;&#22823;&#20102;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#65292;&#20135;&#29983;&#20102;&#27604;&#27809;&#26377;DP&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#21152;&#20559;&#35265;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#25918;&#22823;&#30340;&#21407;&#22240;&#26159;&#26799;&#24230;&#22312;&#23376;&#32452;&#20043;&#38388;&#30340;&#25910;&#25947;&#19981;&#24179;&#34913;&#12290;&#36890;&#36807;&#20108;&#20803;&#24615;&#21035;&#20559;&#35265;&#30340;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#25239;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#20063;&#33021;&#36890;&#36807;DP&#20943;&#23569;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22240;&#27492;&#65292;DP&#21644;CDA&#21487;&#20197;&#19968;&#36215;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20004;&#32773;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#32852;&#37030;&#36172;&#21338;&#23398;&#20064;&#30340;&#28608;&#21169;&#26426;&#21046;&#23384;&#22312;&#34987;&#31574;&#30053;&#24615;&#23458;&#25143;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Truth-FedBan&#30340;&#28608;&#21169;&#20860;&#23481;&#36890;&#20449;&#21327;&#35758;&#65292;&#20351;&#24471;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#19982;&#20854;&#33258;&#25253;&#25104;&#26412;&#26080;&#20851;&#65292;&#21482;&#26377;&#25253;&#21578;&#30495;&#23454;&#25104;&#26412;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#25928;&#29992;&#65292;&#24182;&#19988;&#20173;&#33021;&#20445;&#35777;&#20122;&#32447;&#24615;&#30340;&#36951;&#25022;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04485</link><description>&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#36172;&#21338;&#23398;&#20064;&#30340;&#28608;&#21169;&#30495;&#23454;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Incentivized Truthful Communication for Federated Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04485
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#36172;&#21338;&#23398;&#20064;&#30340;&#28608;&#21169;&#26426;&#21046;&#23384;&#22312;&#34987;&#31574;&#30053;&#24615;&#23458;&#25143;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Truth-FedBan&#30340;&#28608;&#21169;&#20860;&#23481;&#36890;&#20449;&#21327;&#35758;&#65292;&#20351;&#24471;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#19982;&#20854;&#33258;&#25253;&#25104;&#26412;&#26080;&#20851;&#65292;&#21482;&#26377;&#25253;&#21578;&#30495;&#23454;&#25104;&#26412;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#25928;&#29992;&#65292;&#24182;&#19988;&#20173;&#33021;&#20445;&#35777;&#20122;&#32447;&#24615;&#30340;&#36951;&#25022;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#32852;&#37030;&#36172;&#21338;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#28608;&#21169;&#26426;&#21046;&#26469;&#20419;&#20351;&#23458;&#25143;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#20854;&#20013;&#19968;&#20010;&#23458;&#25143;&#21482;&#26377;&#22312;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;&#28608;&#21169;&#36229;&#36807;&#20854;&#21442;&#19982;&#25104;&#26412;&#26102;&#25165;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28608;&#21169;&#26426;&#21046;&#22825;&#30495;&#22320;&#20551;&#35774;&#23458;&#25143;&#26159;&#30495;&#23454;&#30340;&#65292;&#21363;&#20182;&#20204;&#37117;&#25253;&#21578;&#33258;&#24049;&#30340;&#30495;&#23454;&#25104;&#26412;&#65292;&#22240;&#27492;&#21442;&#19982;&#25104;&#26412;&#26356;&#39640;&#30340;&#23458;&#25143;&#22768;&#31216;&#30340;&#36234;&#39640;&#65292;&#26381;&#21153;&#22120;&#24517;&#39035;&#25903;&#20184;&#30340;&#36234;&#22810;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26426;&#21046;&#23481;&#26131;&#21463;&#21040;&#24076;&#26395;&#36890;&#36807;&#38169;&#35823;&#25253;&#21578;&#26469;&#20248;&#21270;&#33258;&#24049;&#25928;&#29992;&#30340;&#31574;&#30053;&#24615;&#23458;&#25143;&#30340;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#21169;&#20860;&#23481;&#65288;&#21363;&#30495;&#23454;&#65289;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#21517;&#20026;Truth-FedBan&#65292;&#20854;&#20013;&#27599;&#20010;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#19982;&#20854;&#33258;&#25253;&#25104;&#26412;&#26080;&#20851;&#65292;&#32780;&#25253;&#21578;&#30495;&#23454;&#25104;&#26412;&#26159;&#23454;&#29616;&#26368;&#20339;&#25928;&#29992;&#30340;&#21807;&#19968;&#26041;&#24335;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;Truth-FedBan&#20173;&#33021;&#20445;&#35777;&#20122;&#32447;&#24615;&#30340;&#36951;&#25022;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#24320;&#38144;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#26680;&#24515;&#35299;&#20915;&#20102;&#23458;&#25143;&#20043;&#38388;&#30340;&#28608;&#21169;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32852;&#37030;&#36172;&#21338;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the efficiency and practicality of federated bandit learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost. However, existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting. To address this issue, we propose an incentive compatible (i.e., truthful) communication protocol, named Truth-FedBan, where the incentive for each participant is independent of its self-reported cost, and reporting the true cost is the only way to achieve the best utility. More importantly, Truth-FedBan still guarantees the sub-linear regret and communication cost without any overheads. In other words, the core 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;KDD Cup 99&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#36229;&#36807;98&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04469</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
IoT Network Traffic Analysis with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;KDD Cup 99&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#36229;&#36807;98&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#24182;&#20135;&#29983;&#22823;&#37327;&#21160;&#24577;&#25968;&#25454;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#30417;&#27979;&#21644;&#26816;&#27979;&#24322;&#24120;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22788;&#29702;&#21644;&#23398;&#20064;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#19981;&#38656;&#35201;&#24102;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#20351;&#24471;&#21487;&#33021;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#34987;&#21457;&#29616;&#30340;&#26032;&#22411;&#21644;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#21270;&#21644;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#22312;&#21518;&#21488;&#36830;&#32493;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#30417;&#25511;&#22823;&#22411;&#29289;&#32852;&#32593;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#22312;KDD Cup 99&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#38598;&#25104;&#25216;&#26415;&#23454;&#29616;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;98&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As IoT networks become more complex and generate massive amounts of dynamic data, it is difficult to monitor and detect anomalies using traditional statistical methods and machine learning methods. Deep learning algorithms can process and learn from large amounts of data and can also be trained using unsupervised learning techniques, meaning they don't require labelled data to detect anomalies. This makes it possible to detect new and unknown anomalies that may not have been detected before. Also, deep learning algorithms can be automated and highly scalable; thereby, they can run continuously in the backend and make it achievable to monitor large IoT networks instantly. In this work, we conduct a literature review on the most recent works using deep learning techniques and implement a model using ensemble techniques on the KDD Cup 99 dataset. The experimental results showcase the impressive performance of our deep anomaly detection model, achieving an accuracy of over 98\%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#27788;&#31995;&#32479;&#30340;&#19981;&#21464;&#27979;&#24230;&#21644;&#21160;&#24577;&#26469;&#35299;&#20915;&#23398;&#20064;&#21160;&#24577;&#30340;&#22256;&#38590;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26694;&#26550;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04467</link><description>&lt;p&gt;
DySLIM: &#21033;&#29992;&#19981;&#21464;&#27979;&#24230;&#23454;&#29616;&#28151;&#27788;&#31995;&#32479;&#30340;&#21160;&#24577;&#31283;&#23450;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#27788;&#31995;&#32479;&#30340;&#19981;&#21464;&#27979;&#24230;&#21644;&#21160;&#24577;&#26469;&#35299;&#20915;&#23398;&#20064;&#21160;&#24577;&#30340;&#22256;&#38590;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26694;&#26550;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32791;&#25955;&#24615;&#28151;&#27788;&#31995;&#32479;&#20013;&#23398;&#20064;&#21160;&#24577;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#23548;&#33268;&#23398;&#20064;&#21160;&#24577;&#30340;&#35823;&#24046;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#36941;&#21382;&#24615;&#21644;&#21560;&#24341;&#23376;&#65306;&#19968;&#20010;&#32039;&#20945;&#32780;&#39640;&#24230;&#22797;&#26434;&#30340;&#27969;&#24418;&#65292;&#36712;&#36857;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#35813;&#27969;&#24418;&#65292;&#24182;&#25903;&#25345;&#19968;&#20010;&#19981;&#21464;&#27979;&#24230;&#65292;&#21363;&#19968;&#20010;&#22312;&#21160;&#24577;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#20915;&#23450;&#20102;&#31995;&#32479;&#30340;&#38271;&#26399;&#32479;&#35745;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19981;&#21464;&#27979;&#24230;&#20197;&#21450;&#21160;&#24577;&#65292;&#19982;&#36890;&#24120;&#21482;&#38024;&#23545;&#36712;&#36857;&#20043;&#38388;&#30340;&#35823;&#24046;&#30340;&#20856;&#22411;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#24448;&#24448;&#20250;&#21457;&#25955;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#22788;&#29702;&#19988;&#26679;&#26412;&#39640;&#25928;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#30446;&#26631;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics St
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;NVIDIA Holoscan&#24179;&#21488;&#20013;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#30340;&#24310;&#36831;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.04466</link><description>&lt;p&gt;
NVIDIA Holoscan&#20013;&#38754;&#21521;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;NVIDIA Holoscan&#24179;&#21488;&#20013;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#30340;&#24310;&#36831;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;ML&#25216;&#26415;&#30340;&#24341;&#20837;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21307;&#30103;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#21307;&#30103;&#35774;&#22791;&#21046;&#36896;&#21830;&#28212;&#26395;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;AI&#21644;ML&#30340;&#20248;&#21183;&#65292;&#23558;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#21040;&#19968;&#20010;&#24179;&#21488;&#19978;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#65292;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#37117;&#26377;&#33258;&#24049;&#30340;&#21487;&#35270;&#21270;&#32452;&#20214;&#65292;&#20250;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#36164;&#28304;&#20105;&#29992;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21046;&#36896;&#21830;&#36890;&#24120;&#20250;&#20026;&#19981;&#21516;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#21333;&#29420;&#30340;&#24037;&#20316;&#31449;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#36130;&#21153;&#12289;&#33021;&#28304;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;NVIDIA&#30340;Holoscan&#24179;&#21488;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;Holoscan&#26159;&#19968;&#20010;&#29992;&#20110;&#27969;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22270;&#20687;&#30340;&#23454;&#26102;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#20248;&#21270;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#22270;&#24418;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#21033;&#29992;CUDA MPS&#23545;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20998;&#38548;&#35745;&#31639;&#21644;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments. Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform. However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions. To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs. This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images. We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks. Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics proc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#30446;&#21069;&#30456;&#23545;&#23569;&#26377;&#30340;&#20851;&#27880;&#21644;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.04453</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Potential of AutoML for Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#30446;&#21069;&#30456;&#23545;&#23569;&#26377;&#30340;&#20851;&#27880;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#24050;&#32463;&#22312;&#21253;&#25324;&#27169;&#22411;&#21387;&#32553;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#22823;&#22823;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24212;&#29992;&#12290;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ML&#30340;&#19968;&#20010;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;AutoML&#22312;RecSys&#31038;&#21306;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;RecSys&#20063;&#27809;&#26377;&#22312;AutoML&#31038;&#21306;&#20013;&#24341;&#36215;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#33258;&#21160;&#21270;&#25512;&#33616;&#31995;&#32479;&#65288;AutoRecSys&#65289;&#24211;&#37319;&#29992;&#20102;AutoML&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24211;&#37117;&#26159;&#22522;&#20110;&#23398;&#29983;&#39033;&#30446;&#24320;&#21457;&#30340;&#65292;&#24182;&#19988;&#27809;&#26377;&#25552;&#20379;AutoML&#24211;&#30340;&#21151;&#33021;&#21644;&#23436;&#21892;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#22312;&#19968;&#20010;&#27809;&#26377;&#32463;&#39564;&#30340;&#29992;&#25143;&#24819;&#35201;&#23454;&#29616;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#30340;&#22330;&#26223;&#20013;&#65292;AutoML&#24211;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26469;&#33258;15&#20010;&#24211;&#30340;60&#20010;AutoML&#12289;AutoRecSys&#12289;ML&#21644;RecSys&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#22343;&#20540;&#39044;&#27979;&#22522;&#20934;&#27169;&#22411;&#22312;14&#20010;&#26174;&#24335;&#21453;&#39304;&#30340;RecSys&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning (AutoML) has greatly advanced applications of Machine Learning (ML) including model compression, machine translation, and computer vision. Recommender Systems (RecSys) can be seen as an application of ML. Yet, AutoML has found little attention in the RecSys community; nor has RecSys found notable attention in the AutoML community. Only few and relatively simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt AutoML techniques. However, these libraries are based on student projects and do not offer the features and thorough development of AutoML libraries. We set out to determine how AutoML libraries perform in the scenario of an inexperienced user who wants to implement a recommender system. We compared the predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from 15 libraries, including a mean predictor baseline, on 14 explicit feedback RecSys datasets. To simulate the perspective of an inexperienced user, the algo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#32654;&#26631;&#31614;&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20174;&#23436;&#20840;&#27880;&#37322;&#30340;&#26631;&#31614;&#20013;&#21024;&#38500;50%&#30340;&#32454;&#32990;&#27880;&#37322;&#20165;&#23545;DSC&#24471;&#20998;&#36896;&#25104;&#36731;&#24494;&#38477;&#20302;&#65292;&#24182;&#19988;&#21333;&#32452;&#32455;&#27169;&#22411;&#22312;&#26410;&#30693;&#32452;&#32455;&#31867;&#22411;&#19978;&#30340;&#34920;&#29616;&#19982;&#22810;&#32452;&#32455;&#27169;&#22411;&#30456;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.04446</link><description>&lt;p&gt;
&#23558;&#32454;&#32990;&#20998;&#21106;&#27169;&#22411;&#30340;&#26497;&#38480;&#25512;&#33267;&#26497;&#38480;&#20197;&#36827;&#34892;&#25104;&#20687;&#36136;&#35889;&#32454;&#32990;&#26415;
&lt;/p&gt;
&lt;p&gt;
Pushing the limits of cell segmentation models for imaging mass cytometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#32654;&#26631;&#31614;&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20174;&#23436;&#20840;&#27880;&#37322;&#30340;&#26631;&#31614;&#20013;&#21024;&#38500;50%&#30340;&#32454;&#32990;&#27880;&#37322;&#20165;&#23545;DSC&#24471;&#20998;&#36896;&#25104;&#36731;&#24494;&#38477;&#20302;&#65292;&#24182;&#19988;&#21333;&#32452;&#32455;&#27169;&#22411;&#22312;&#26410;&#30693;&#32452;&#32455;&#31867;&#22411;&#19978;&#30340;&#34920;&#29616;&#19982;&#22810;&#32452;&#32455;&#27169;&#22411;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#20687;&#36136;&#35889;&#32454;&#32990;&#26415;&#26159;&#19968;&#31181;&#30456;&#23545;&#26032;&#30340;&#22312;&#20122;&#32454;&#32990;&#20998;&#36776;&#29575;&#19979;&#25104;&#20687;&#29983;&#29289;&#32452;&#32455;&#30340;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#26041;&#27861;&#20351;&#24471;&#23545;&#32454;&#32990;&#31867;&#22411;&#21644;&#24418;&#24577;&#30340;&#31934;&#30830;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#20855;&#26377;&#23436;&#20840;&#27880;&#37322;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#32654;&#26631;&#31614;&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#23436;&#20840;&#27880;&#37322;&#30340;&#26631;&#31614;&#25513;&#27169;&#20013;&#21024;&#38500;50%&#30340;&#32454;&#32990;&#27880;&#37322;&#20165;&#23558;Dice&#30456;&#20284;&#24615;&#31995;&#25968;&#65288;DSC&#65289;&#35780;&#20998;&#38477;&#20302;&#21040;0.874&#65288;&#19982;&#23436;&#20840;&#27880;&#37322;GT&#25513;&#27169;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;0.889&#30456;&#27604;&#65289;&#12290;&#36825;&#24847;&#21619;&#30528;&#27880;&#37322;&#26102;&#38388;&#23454;&#38469;&#19978;&#21487;&#20197;&#20943;&#23569;&#33267;&#23569;&#19968;&#21322;&#32780;&#19981;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23558;&#25105;&#20204;&#30340;&#21333;&#32452;&#32455;&#27169;&#22411;&#35757;&#32451;&#20110;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#19978;&#20165;&#23558;DSC&#38477;&#20302;&#20102;0.031&#65292;&#19982;&#20854;&#22810;&#32452;&#32455;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26410;&#30693;&#32452;&#32455;&#31867;&#22411;&#19978;&#20960;&#20046;&#27809;&#26377;&#36136;&#37327;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imaging mass cytometry (IMC) is a relatively new technique for imaging biological tissue at subcellular resolution. In recent years, learning-based segmentation methods have enabled precise quantification of cell type and morphology, but typically rely on large datasets with fully annotated ground truth (GT) labels. This paper explores the effects of imperfect labels on learning-based segmentation models and evaluates the generalisability of these models to different tissue types. Our results show that removing 50% of cell annotations from GT masks only reduces the dice similarity coefficient (DSC) score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT masks). This implies that annotation time can in fact be reduced by at least half without detrimentally affecting performance. Furthermore, training our single-tissue model on imperfect labels only decreases DSC by 0.031 on an unseen tissue type compared to its multi-tissue counterpart, with negligible qualitative d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#30456;&#20851;&#24615;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#20351;&#29992;&#24635;&#30456;&#20851;&#24615;&#26469;&#25429;&#33719;&#39640;&#38454;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#20851;&#20110;&#25968;&#25454;&#32467;&#26500;&#30340;&#38544;&#34255;&#27934;&#23519;&#21147;&#65292;&#24182;&#29992;&#20110;&#25506;&#32034;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.04440</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#24635;&#30456;&#20851;&#24615;&#30340;&#39640;&#38454;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Exploring higher-order neural network node interactions with total correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#30456;&#20851;&#24615;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#20351;&#29992;&#24635;&#30456;&#20851;&#24615;&#26469;&#25429;&#33719;&#39640;&#38454;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#20851;&#20110;&#25968;&#25454;&#32467;&#26500;&#30340;&#38544;&#34255;&#27934;&#23519;&#21147;&#65292;&#24182;&#29992;&#20110;&#25506;&#32034;&#21644;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#24577;&#31995;&#32479;&#12289;&#21327;&#20316;&#21644;&#20154;&#33041;&#31561;&#39046;&#22495;&#20013;&#65292;&#21464;&#37327;&#20197;&#22797;&#26434;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#22320;&#25551;&#36848;&#39640;&#38454;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#20114;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#20123;&#20132;&#20114;&#38543;&#30528;&#25968;&#25454;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#30456;&#20851;&#24615;&#35299;&#37322;&#65288;CorEx&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#22522;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#26469;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#22312;&#23616;&#37096;&#23610;&#24230;&#19978;&#25429;&#33719;&#39640;&#38454;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#24635;&#30456;&#20851;&#24615;&#30340;&#22810;&#20803;&#20114;&#20449;&#24687;&#30340;&#21464;&#20307;&#26469;&#26500;&#24314;&#25968;&#25454;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#30340;&#28508;&#22312;&#22240;&#23376;&#34920;&#31034;&#65292;&#20197;&#23398;&#20064;&#23616;&#37096;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#30456;&#20851;&#24615;&#35299;&#37322;&#26469;&#25506;&#32034;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#20132;&#20114;&#65292;&#20197;&#25552;&#21462;&#20851;&#20110;&#25968;&#25454;&#32467;&#26500;&#30340;&#38544;&#34255;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#30456;&#20851;&#24615;&#35299;&#37322;&#30340;&#36866;&#29992;&#24615;&#65292;&#29992;&#20110;&#25506;&#32034;&#21644;&#35299;&#37322;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways. Yet accurately characterizing higher-order variable interactions (HOIs) is a difficult problem that is further exacerbated when the HOIs change across the data. To solve this problem we propose a new method called Local Correlation Explanation (CorEx) to capture HOIs at a local scale by first clustering data points based on their proximity on the data manifold. We then use a multivariate version of the mutual information called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local HOIs. We use Local CorEx to explore HOIs in synthetic and real world data to extract hidden insights about the data structure. Lastly, we demonstrate Local CorEx's suitability to explore and interpret the inner workings of trained neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;&#26159;&#20851;&#20110;&#23558;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23545;&#35937;&#38598;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.04436</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;
&lt;/p&gt;
&lt;p&gt;
Continuous Multidimensional Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04436
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#32500;&#26631;&#24230;&#26159;&#20851;&#20110;&#23558;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23545;&#35937;&#38598;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#26631;&#24230;(MDS)&#26159;&#23558;&#20851;&#20110;&#19968;&#32452;$n$&#20010;&#23545;&#35937;&#30340;&#36317;&#31163;&#20449;&#24687;&#23884;&#20837;&#21040;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#12290;&#26368;&#21021;&#30001;&#24515;&#29702;&#27979;&#37327;&#23398;&#30028;&#26500;&#24605;&#65292;MDS&#20851;&#27880;&#30340;&#26159;&#23884;&#20837;&#21040;&#19968;&#32452;&#22266;&#23450;&#23545;&#35937;&#19978;&#30340;&#19968;&#32452;&#22266;&#23450;&#36317;&#31163;&#12290;&#29616;&#20195;&#20851;&#27880;&#30340;&#38382;&#39064;&#26356;&#24120;&#28041;&#21450;&#21040;&#30740;&#31350;&#19982;&#19968;&#32452;&#19981;&#26029;&#22686;&#21152;&#30340;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#19968;&#31995;&#21015;&#36317;&#31163;&#30340;&#26497;&#38480;&#34892;&#20026;&#65292;&#22914;&#22312;&#38543;&#26426;&#22270;&#30340;&#32479;&#35745;&#25512;&#26029;&#30340;&#28176;&#36817;&#29702;&#35770;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#28857;&#21040;&#38598;&#21512;&#26144;&#23556;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#33509;$n$&#22266;&#23450;&#65292;&#21017;&#23884;&#20837;&#32467;&#26500;&#30340;&#26497;&#38480;&#26159;&#26497;&#38480;&#36317;&#31163;&#30340;&#23884;&#20837;&#32467;&#26500;&#12290;&#20294;&#22914;&#26524;$n$&#22686;&#21152;&#24590;&#20040;&#21150;&#21602;&#65311;&#37027;&#20040;&#23601;&#38656;&#35201;&#37325;&#26032;&#21046;&#23450;MDS&#65292;&#20197;&#20415;&#23558;&#25972;&#20010;&#23884;&#20837;&#38382;&#39064;&#24207;&#21015;&#35270;&#20026;&#19968;&#20010;&#22266;&#23450;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#19968;&#31181;&#37325;&#26032;&#21046;&#23450;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20123;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.
&lt;/p&gt;</description></item><item><title>PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.04435</link><description>&lt;p&gt;
PreGIP: &#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04435
&lt;/p&gt;
&lt;p&gt;
PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#30340;GNNs&#25104;&#20026;&#21512;&#27861;&#25317;&#26377;&#32773;&#30340;&#39640;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#25163;&#21487;&#33021;&#20250;&#38750;&#27861;&#22797;&#21046;&#21644;&#37096;&#32626;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#29992;&#20110;&#20854;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#22987;&#23581;&#35797;&#20026;IP&#20445;&#25252;&#28155;&#21152;GNN&#20998;&#31867;&#22120;&#30340;&#27700;&#21360;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#25165;&#33021;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;GNN&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;PreGIP&#65292;&#29992;&#20110;&#22312;&#20445;&#25345;&#23884;&#20837;&#31354;&#38388;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#32473;GNN&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#28155;&#21152;&#27700;&#21360;&#20197;&#36827;&#34892;IP&#20445;&#25252;&#12290;PreGIP&#24341;&#20837;&#20102;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#12290;&#21516;&#26102;&#37319;&#29992;&#20102;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#25193;&#23637;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and exte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#23545;&#33016;&#37096;CT&#20998;&#31867;&#24615;&#33021;&#30340;&#38480;&#21046;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#38169;&#35823;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21487;&#20197;&#24525;&#21463;&#39640;&#36798;10%&#30340;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#25152;&#26377;&#30142;&#30149;&#30340;&#20998;&#31867;&#24615;&#33021;&#31283;&#27493;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.04419</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22312;&#33016;&#37096;CT&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What limits performance of weakly supervised deep learning for chest CT classification?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#23545;&#33016;&#37096;CT&#20998;&#31867;&#24615;&#33021;&#30340;&#38480;&#21046;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#38169;&#35823;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#21487;&#20197;&#24525;&#21463;&#39640;&#36798;10%&#30340;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#25152;&#26377;&#30142;&#30149;&#30340;&#20998;&#31867;&#24615;&#33021;&#31283;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#30142;&#30149;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#65292;&#24369;&#30417;&#30563;&#23398;&#20064;&#19982;&#22024;&#26434;&#25968;&#25454;&#25104;&#20026;&#21307;&#23398;&#24433;&#20687;&#23398;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#38480;&#21046;&#20197;&#21450;&#36825;&#20123;&#32422;&#26463;&#23545;&#30142;&#30149;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#26465;&#20214;&#26469;&#27979;&#35797;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36880;&#28176;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#38169;&#35823;&#26469;&#26816;&#26597;&#27169;&#22411;&#23545;&#22024;&#26434;&#25968;&#25454;&#30340;&#23481;&#24525;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20108;&#20803;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#28155;&#21152;&#20102;10%&#30340;&#26631;&#31614;&#38169;&#35823;&#20043;&#21069;&#24525;&#21463;&#65292;&#32780;&#30142;&#30149;&#20998;&#31867;&#24615;&#33021;&#20250;&#20986;&#29616;&#19979;&#38477;&#12290;&#23545;&#20110;&#25152;&#26377;&#30142;&#30149;&#31867;&#21035;&#65292;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#30142;&#30149;&#20998;&#31867;&#24615;&#33021;&#31283;&#27493;&#25552;&#21319;&#65292;&#28982;&#21518;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance. In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04417</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#21306;&#22359;&#38142;&#30340;&#31283;&#20581;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#25110;&#21442;&#19982;&#32773;&#20998;&#24067;&#22312;&#19968;&#20010;&#23436;&#20840;&#20998;&#25955;&#30340;&#21306;&#22359;&#38142;&#19978;&#65292;&#20854;&#20013;&#19968;&#20123;&#21487;&#33021;&#26159;&#24694;&#24847;&#30340;&#12290;&#33218;&#30340;&#22870;&#21169;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#26159;&#22343;&#21248;&#30340;&#65292;&#36981;&#24490;&#26102;&#38388;&#19981;&#21464;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#21482;&#26377;&#22312;&#31995;&#32479;&#36275;&#22815;&#23433;&#20840;&#26102;&#25165;&#21521;&#21442;&#19982;&#32773;&#36879;&#38706;&#12290;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#26377;&#25928;&#22320;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#21306;&#22359;&#38142;&#30340;&#20808;&#36827;&#25216;&#26415;&#21644;&#26032;&#39062;&#30340;&#26426;&#21046;&#32467;&#21512;&#21040;&#31995;&#32479;&#20013;&#65292;&#20026;&#35802;&#23454;&#21442;&#19982;&#32773;&#35774;&#35745;&#26368;&#20339;&#31574;&#30053;&#12290;&#36825;&#26679;&#21487;&#20197;&#24212;&#23545;&#21508;&#31181;&#24694;&#24847;&#34892;&#20026;&#24182;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#32452;&#21487;&#20197;&#35775;&#38382;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#39564;&#35777;&#32773;&#27744;&#65292;&#20026;&#36825;&#20123;&#39564;&#35777;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#31614;&#21517;&#30340;&#20840;&#26032;&#20849;&#35782;&#26426;&#21046;&#65292;&#24182;&#21457;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04416</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#39046;&#22495;&#22312;&#20849;&#20139;&#26631;&#31614;&#31354;&#38388;&#30340;&#20551;&#35774;&#19979;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DG&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20016;&#23500;&#30340;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#28304;&#25968;&#25454;&#65292;&#36825;&#20010;&#35201;&#27714;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#20026;&#33719;&#21462;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#36153;&#29992;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;(UDG)&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#29256;&#26412;&#65292;&#35813;&#38382;&#39064;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;LAION-2B&#22312;&#24494;&#35843;&#26399;&#38388;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26174;&#24335;&#22320;&#20551;&#35774;&#28304;&#25968;&#25454;&#38598;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20219;&#20309;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#28304;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#31354;&#38388;&#20013;&#39640;&#25928;&#25628;&#32034;&#30340;&#21069;&#25552;&#12290;&#38024;&#23545;&#36825;&#31181;&#22810;&#27169;&#24577;UDG&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#65288;&#23567;&#20110;100K&#65289;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($&lt;$100K) subset of the source data in th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.04412</link><description>&lt;p&gt;
VampPrior&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The VampPrior Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;DLVMs&#65289;&#30340;&#32858;&#31867;&#20808;&#39564;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36739;&#24046;&#30340;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#38598;&#25104;&#21644;&#32858;&#31867;&#30340;&#26041;&#24335;&#26497;&#22823;&#22320;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;scRNA-seq&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;VampPrior&#65288;Tomczak&#21644;Welling&#65292;2018&#65289;&#35843;&#25972;&#20026;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24471;&#21040;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#65292;&#20132;&#26367;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#65292;&#20197;&#28165;&#26970;&#22320;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#23558;VMM&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;scRNA-seq&#38598;&#25104;&#26041;&#27861;scVI&#65288;Lopez&#31561;&#65292;2018&#65289;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04411</link><description>&lt;p&gt;
Chatbot&#36935;&#35265;&#31649;&#36947;&#65306;&#21033;&#29992;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;DFA-LLM&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#21319;&#23545;&#35805;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;LLM&#22312;&#29305;&#23450;&#24773;&#26223;&#65288;&#22914;&#24773;&#24863;&#25903;&#25345;&#21644;&#23458;&#25143;&#26381;&#21153;&#65289;&#20013;&#29983;&#25104;&#35268;&#33539;&#21512;&#35268;&#30340;&#22238;&#22797;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#20174;&#35757;&#32451;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#23884;&#20837;&#21040;LLM&#20013;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;LLM&#33021;&#22815;&#25353;&#29031;DFA&#25351;&#23548;&#30340;&#30830;&#23450;&#24615;&#22238;&#24212;&#36335;&#24452;&#26469;&#22238;&#24212;&#12290;DFA-LLM&#30340;&#20248;&#21183;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#32467;&#26500;&#65292;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#22238;&#22797;&#26816;&#32034;&#20197;&#21450;&#19982;&#29616;&#26377;LLM&#30340;&#21363;&#25554;&#21363;&#29992;&#20860;&#23481;&#24615;&#12290;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#39564;&#35777;&#20102;DFA-LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#35805;&#20195;&#29702;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRECA&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;FedTruth&#26694;&#26550;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#25490;&#38500;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;FRECA&#23545;&#20110;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04409</link><description>&lt;p&gt;
&#36827;&#19968;&#27493;&#23454;&#29616;&#20844;&#24179;&#12289;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRECA&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;FedTruth&#26694;&#26550;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#25490;&#38500;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;FRECA&#23545;&#20110;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#23458;&#25143;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#35780;&#20272;&#27599;&#20010;&#23458;&#25143;&#30340;&#36129;&#29486;&#23545;&#20110;&#23458;&#25143;&#36873;&#25321;&#21644;&#34917;&#20607;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#36890;&#24120;&#20855;&#26377;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-iid&#65289;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#25110;&#21457;&#25955;&#30340;&#26356;&#26032;&#65292;&#22240;&#27492;&#35780;&#20272;&#23458;&#25143;&#36129;&#29486;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#26080;&#27861;&#35775;&#38382;&#23458;&#25143;&#30340;&#26412;&#22320;&#25968;&#25454;&#25110;&#22522;&#20934;&#26681;&#25968;&#25454;&#38598;&#26102;&#65292;&#24694;&#24847;&#23458;&#25143;&#30340;&#39118;&#38505;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20844;&#24179;&#12289;&#40065;&#26834;&#21644;&#39640;&#25928;&#23458;&#25143;&#35780;&#20272;&#65288;FRECA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;FL&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;FRECA&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;FedTruth&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#36807;&#28388;&#20986;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32858;&#21512;&#31639;&#27861;&#12290;FRECA&#36824;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#20165;&#20165;&#22312;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#19978;&#25805;&#20316;&#65292;&#19988;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20840;&#23616;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#24182;&#34892;&#22270;&#32534;&#30721;&#23884;&#20837;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#24182;&#34892;&#21270;&#23454;&#29616;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#21462;&#24471;&#20102;500&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04403</link><description>&lt;p&gt;
&#36793;&#24182;&#34892;&#22270;&#32534;&#30721;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Edge-Parallel Graph Encoder Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04403
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#24182;&#34892;&#22270;&#32534;&#30721;&#23884;&#20837;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#24182;&#34892;&#21270;&#23454;&#29616;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#21462;&#24471;&#20102;500&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#23884;&#20837;&#22270;&#30340;&#26032;&#31639;&#27861;&#24050;&#32463;&#38477;&#20302;&#20102;&#23547;&#25214;&#20302;&#32500;&#34920;&#31034;&#30340;&#28176;&#36817;&#22797;&#26434;&#24230;&#12290;One-Hot Graph Encoder Embedding (GEE) &#22312;&#36793;&#30340;&#32447;&#24615;&#36880;&#20010;&#36890;&#36807;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#23884;&#20837;&#25910;&#25947;&#21040;&#35889;&#23884;&#20837;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#21463;&#21040;&#35299;&#37322;&#22411;&#35821;&#35328;&#20018;&#34892;&#23454;&#29616;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;GEE&#37325;&#26500;&#20026;&#22312;Ligra&#22270;&#24341;&#25806;&#20013;&#24182;&#34892;&#31243;&#24207;&#65292;&#23558;&#20989;&#25968;&#26144;&#23556;&#21040;&#22270;&#30340;&#36793;&#19978;&#65292;&#24182;&#20351;&#29992;&#26080;&#38145;&#21407;&#23376;&#25351;&#20196;&#38450;&#27490;&#25968;&#25454;&#31454;&#20105;&#12290;&#22312;&#19968;&#20010;&#26377;18&#20159;&#26465;&#36793;&#30340;&#22270;&#19978;&#65292;&#19982;&#21407;&#22987;&#23454;&#29616;&#30456;&#27604;&#65292;&#36825;&#23558;&#23548;&#33268;500&#20493;&#30340;&#21152;&#36895;&#21644;&#32534;&#35793;&#21363;&#26102;&#29256;&#26412;&#30340;17&#20493;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
New algorithms for embedding graphs have reduced the asymptotic complexity of finding low-dimensional representations. One-Hot Graph Encoder Embedding (GEE) uses a single, linear pass over edges and produces an embedding that converges asymptotically to the spectral embedding. The scaling and performance benefits of this approach have been limited by a serial implementation in an interpreted language. We refactor GEE into a parallel program in the Ligra graph engine that maps functions over the edges of the graph and uses lock-free atomic instrutions to prevent data races. On a graph with 1.8B edges, this results in a 500 times speedup over the original implementation and a 17 times speedup over a just-in-time compiled version.
&lt;/p&gt;</description></item><item><title>CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04400</link><description>&lt;p&gt;
&#29983;&#25104;&#24102;&#26377;&#30149;&#20154;&#26102;&#38388;&#36724;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;CEHR-GPT
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04400
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#25512;&#36827;&#21307;&#30103;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#21307;&#30103;&#25968;&#25454;&#30340;&#30740;&#31350;&#20154;&#21592;&#32780;&#35328;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#34920;&#26684;&#26684;&#24335;&#65292;&#24573;&#30053;&#20102;&#30149;&#20154;&#21382;&#21490;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#22797;&#21046;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#12289;&#20154;&#21475;&#20272;&#35745;&#12289;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#28304;&#33258;CEHR-BERT&#30340;&#29305;&#23450;&#30149;&#20154;&#34920;&#31034;&#35757;&#32451;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#21487;&#26080;&#32541;&#36716;&#25442;&#30340;&#30149;&#20154;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04398</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Learning from Time Series under Temporal Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20998;&#31867;&#20219;&#21153;&#21463;&#21040;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#22122;&#22768;&#21487;&#33021;&#20250;&#23548;&#33268;&#26631;&#31614;&#36136;&#37327;&#38543;&#26102;&#38388;&#25913;&#21892;&#12289;&#24694;&#21270;&#25110;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#21644;&#31995;&#32479;&#21270;&#20102;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#20998;&#31867;&#30340;&#19968;&#20010;&#26410;&#32463;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#22810;&#20010;&#26631;&#31614;&#36830;&#32493;&#35760;&#24405;&#65292;&#21516;&#26102;&#21463;&#21040;&#19968;&#20010;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#20989;&#25968;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24314;&#27169;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#30340;&#25345;&#32493;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#23545;&#22122;&#22768;&#20855;&#26377;&#23481;&#24525;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#65292;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;&#20171;&#32461;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20174;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04384</link><description>&lt;p&gt;
&#22312;&#20845;&#20010;&#31616;&#21333;&#30340;&#27493;&#39588;&#20013;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models in Six Simple Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;&#20171;&#32461;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20174;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26159;&#19968;&#31867;&#38750;&#24120;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#12289;&#34507;&#30333;&#36136;&#21644;&#26448;&#26009;&#21512;&#25104;&#12289;&#22825;&#27668;&#39044;&#27979;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#26367;&#20195;&#31561;&#22810;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#20854;&#26222;&#21450;&#24230;&#24456;&#39640;&#65292;&#20294;&#24456;&#38590;&#25214;&#21040;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;DDPM&#20171;&#32461;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#24517;&#35201;&#30340;&#31616;&#27905;&#35299;&#37322;&#26080;&#27861;&#38416;&#26126;&#21046;&#23450;DDPM&#25152;&#37319;&#21462;&#30340;&#19981;&#21516;&#35774;&#35745;&#27493;&#39588;&#20197;&#21450;&#30465;&#30053;&#20102;&#27493;&#39588;&#30340;&#29702;&#30001;&#20197;&#33410;&#30465;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35770;&#36848;&#36890;&#24120;&#20174;&#21464;&#20998;&#19979;&#30028;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#36825;&#26159;&#19981;&#24517;&#35201;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#65292;&#22240;&#20026;&#23427;&#28151;&#28102;&#20102;&#26041;&#27861;&#22863;&#25928;&#30340;&#21407;&#22240;&#24182;&#26263;&#31034;&#20102;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#29992;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#26159;&#32654;&#20029;&#19988;&#26222;&#36941;&#30340;&#65292;&#20294;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#20998;&#26512;&#21644;&#20943;&#36731;&#30495;&#23454;&#21644;&#21512;&#25104;&#22270;&#20013;&#30340;&#32467;&#26500;&#20559;&#24046;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#26469;&#32531;&#35299;&#20559;&#24046;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04383</link><description>&lt;p&gt;
&#20844;&#24179;&#22270;&#29983;&#25104;&#65306;&#20844;&#24179;&#24615;&#30340;&#22270;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FairWire: Fair Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#20998;&#26512;&#21644;&#20943;&#36731;&#30495;&#23454;&#21644;&#21512;&#25104;&#22270;&#20013;&#30340;&#32467;&#26500;&#20559;&#24046;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#26469;&#32531;&#35299;&#20559;&#24046;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#22240;&#20854;&#20998;&#26512;&#21644;&#23398;&#20064;&#22797;&#26434;&#20851;&#32852;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#26377;&#20559;&#22270;&#32467;&#26500;&#25918;&#22823;&#20102;&#19981;&#20844;&#24179;&#24433;&#21709;&#65292;&#22312;&#23454;&#38469;&#20915;&#31574;&#31995;&#32479;&#30340;&#37096;&#32626;&#20013;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#21512;&#25104;&#22270;&#29983;&#25104;&#23545;&#20110;&#38544;&#31169;&#21644;&#21487;&#25193;&#23637;&#24615;&#32771;&#34385;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29983;&#25104;&#24335;&#23398;&#20064;&#31639;&#27861;&#23545;&#32467;&#26500;&#20559;&#24046;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#35843;&#26597;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20998;&#26512;&#21644;&#20943;&#36731;&#30495;&#23454;&#21644;&#21512;&#25104;&#22270;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#20998;&#26512;&#36896;&#25104;&#20108;&#20803;&#20851;&#31995;&#39044;&#27979;&#19981;&#24179;&#31561;&#30340;&#32467;&#26500;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#20026;&#20102;&#20943;&#36731;&#25152;&#21457;&#29616;&#30340;&#20559;&#24046;&#22240;&#32032;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#65292;&#20855;&#26377;&#22810;&#31181;&#29992;&#36884;&#12290;&#37492;&#20110;&#22270;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#25918;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for the deployment of them in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on the structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generatio
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#26448;&#26009;&#65292;&#20855;&#26377;&#21487;&#38752;&#24615;&#39640;&#21644;&#28789;&#27963;&#24615;&#24378;&#30340;&#20248;&#21183;&#65292;&#33021;&#20197;&#36739;&#39640;&#30340;&#36895;&#29575;&#29983;&#25104;&#34987;&#39044;&#27979;&#20026;&#20122;&#31283;&#24577;&#30340;&#26448;&#26009;&#12290;</title><link>https://arxiv.org/abs/2402.04379</link><description>&lt;p&gt;
&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31283;&#23450;&#30340;&#26080;&#26426;&#26448;&#26009;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned Language Models Generate Stable Inorganic Materials as Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04379
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#26448;&#26009;&#65292;&#20855;&#26377;&#21487;&#38752;&#24615;&#39640;&#21644;&#28789;&#27963;&#24615;&#24378;&#30340;&#20248;&#21183;&#65292;&#33021;&#20197;&#36739;&#39640;&#30340;&#36895;&#29575;&#29983;&#25104;&#34987;&#39044;&#27979;&#20026;&#20122;&#31283;&#24577;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65292;&#20197;&#29983;&#25104;&#31283;&#23450;&#26448;&#26009;&#12290;&#34429;&#28982;&#38750;&#20256;&#32479;&#65292;&#20294;&#22312;&#25991;&#26412;&#32534;&#30721;&#30340;&#21407;&#23376;&#25968;&#25454;&#19978;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#31616;&#21333;&#26131;&#34892;&#65292;&#21516;&#26102;&#21487;&#38752;&#24615;&#39640;&#65292;&#32422;90%&#30340;&#37319;&#26679;&#32467;&#26500;&#36981;&#23432;&#21407;&#23376;&#20301;&#32622;&#21644;&#30005;&#33655;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#36890;&#36807;&#26469;&#33258;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#21644;&#37329;&#26631;&#20934;DFT&#35745;&#31639;&#30340;&#33021;&#37327;&#20197;&#19978;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26368;&#24378;&#27169;&#22411;&#65288;&#32454;&#35843;LLaMA-2 70B&#65289;&#21487;&#20197;&#20197;CDVAE&#31454;&#20105;&#25193;&#25955;&#27169;&#22411;&#30340;&#32422;&#20004;&#20493;&#36895;&#29575;&#65288;49% vs 28%&#65289;&#29983;&#25104;&#34987;&#39044;&#27979;&#20026;&#20122;&#31283;&#24577;&#30340;&#26448;&#26009;&#12290;&#30001;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#22266;&#26377;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#31283;&#23450;&#26448;&#26009;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#12289;&#37096;&#20998;&#32467;&#26500;&#30340;&#22635;&#20805;&#21644;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#26230;&#20307;&#32467;&#26500;&#30340;&#20851;&#38190;&#23545;&#31216;&#24615;&#30340;&#33021;&#21147;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#25913;&#21892;&#65292;&#36825;&#34920;&#26126;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#20559;&#24046;&#20986;&#22855;&#22320;&#36866;&#21512;&#21407;&#23376;&#24615;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic
&lt;/p&gt;</description></item><item><title>NeRCC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#65292;&#21253;&#25324;&#22238;&#24402;&#32534;&#30721;&#12289;&#35745;&#31639;&#21644;&#22238;&#24402;&#35299;&#30721;&#19977;&#20010;&#23618;&#27425;&#65292;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04377</link><description>&lt;p&gt;
$\texttt{NeRCC}$: &#20869;&#23884;&#22238;&#24402;&#32534;&#30721;&#35745;&#31639;&#29992;&#20110;&#20855;&#26377;&#24377;&#24615;&#30340;&#20998;&#24067;&#24335;&#39044;&#27979;&#26381;&#21153;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
$\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04377
&lt;/p&gt;
&lt;p&gt;
NeRCC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#65292;&#21253;&#25324;&#22238;&#24402;&#32534;&#30721;&#12289;&#35745;&#31639;&#21644;&#22238;&#24402;&#35299;&#30721;&#19977;&#20010;&#23618;&#27425;&#65292;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25302;&#23614;&#33410;&#28857;(stragglers)&#26159;&#39044;&#27979;&#26381;&#21153;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#65292;&#20219;&#21153;&#26159;&#22312;&#39044;&#20808;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeRCC&#30340;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#12290;NeRCC&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#65306;(1)&#22238;&#24402;&#32534;&#30721;&#21644;&#25277;&#26679;&#65292;&#29983;&#25104;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#28857;&#30340;&#32452;&#21512;&#65307;(2)&#35745;&#31639;&#65292;&#20854;&#20013;&#19968;&#20010;&#24037;&#20316;&#38598;&#32676;&#22312;&#32534;&#30721;&#25968;&#25454;&#28857;&#19978;&#36816;&#34892;&#25512;&#29702;&#65307;(3)&#22238;&#24402;&#35299;&#30721;&#21644;&#25277;&#26679;&#65292;&#20174;&#32534;&#30721;&#25968;&#25454;&#28857;&#30340;&#21487;&#29992;&#39044;&#27979;&#20013;&#36817;&#20284;&#24674;&#22797;&#20986;&#21407;&#22987;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#26694;&#26550;&#30340;&#24635;&#20307;&#30446;&#26631;&#25581;&#31034;&#20102;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#20013;&#20004;&#20010;&#22238;&#24402;&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24635;&#32467;&#23427;&#20204;&#23545;&#20004;&#20010;&#32852;&#21512;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.04376</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for learning with real and surrogate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#25104;&#26412;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#33539;&#22260;&#20869;, &#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#30456;&#21453;&#22320;, &#21487;&#20197;&#23558;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#25110;&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;, &#20316;&#20026;&#26367;&#20195;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#23558;&#26367;&#20195;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;, &#24182;&#20351;&#29992;&#29702;&#35770;&#27169;&#22411;&#21644;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(i) &#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21407;&#22987;&#20998;&#24067;&#30340;&#27979;&#35797;&#35823;&#24046;&#65307;(ii) &#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#25928;&#30410;, &#20351;&#29992;&#26368;&#20248;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38750;&#24120;&#20851;&#38190;&#65307;(iii) &#22312;&#28151;&#21512;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#12290;&#36825;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.04375</link><description>&lt;p&gt;
&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#27169;&#22411;&#21487;&#33021;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#20013;&#20010;&#20307;&#30340;&#31169;&#23494;&#20449;&#24687;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#38450;&#27490;&#25935;&#24863;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#32780;&#19981;&#26159;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21512;&#25104;&#25968;&#25454;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#28857;&#26159;&#33021;&#22815;&#20445;&#25345;&#21407;&#22987;&#20998;&#24067;&#30340;&#20302;&#38454;&#36793;&#32536;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38024;&#23545;&#22312;&#36825;&#31181;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#38024;&#23545;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#32467;&#26524;&#20043;&#22806;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04362</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36880;&#28176;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Networks Learn Statistics of Increasing Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#20551;&#35774;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#23398;&#20064;&#20302;&#38454;&#30697;&#65292;&#28982;&#21518;&#20877;&#36716;&#21521;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32593;&#32476;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#22312;&#26368;&#22823;&#29109;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#30340;&#26377;&#21147;&#26032;&#35777;&#25454;&#65292;&#32473;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#26032;&#35777;&#25454;&#26469;&#25903;&#25345;DSB&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35777;&#26126;&#20196;&#29260;$n$-gram&#39057;&#29575;&#19982;&#23884;&#20837;&#21521;&#37327;&#30340;&#30697;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#22312;LLM&#20013;&#25214;&#21040;&#20542;&#21521;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#23558;DSB&#25193;&#23637;&#21040;&#31163;&#25955;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#23558;&#19968;&#31867;&#30340;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#25163;&#26415;&#24615;&#22320;&#32534;&#36753;&#25104;&#19982;&#21478;&#19968;&#31867;&#30456;&#21305;&#37197;&#65292;&#28982;&#21518;&#23637;&#31034;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#26469;&#33258;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/EleutherAI/features-across-time &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25512;&#29702;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;10-100&#20493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#22312;&#20248;&#21270;&#36873;&#25321;&#21644;&#35774;&#35745;&#33258;&#36866;&#24212;&#25512;&#29702;&#29366;&#24577;&#31354;&#38388;&#26041;&#38754;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04359</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25512;&#29702;: &#29702;&#35770;&#26497;&#38480;&#19982;&#26410;&#24320;&#21457;&#30340;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Adaptive Inference: Theoretical Limits and Unexplored Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25512;&#29702;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#35777;&#26126;&#21487;&#20197;&#23454;&#29616;10-100&#20493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#22312;&#20248;&#21270;&#36873;&#25321;&#21644;&#35774;&#35745;&#33258;&#36866;&#24212;&#25512;&#29702;&#29366;&#24577;&#31354;&#38388;&#26041;&#38754;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#33258;&#36866;&#24212;&#25512;&#29702;&#31639;&#27861;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#26426;&#20250;&#22823;&#23567;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#23454;&#29616;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#30340;&#26032;&#36817;&#20284;&#21644;&#31934;&#30830;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35777;&#25454;&#35777;&#26126;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;10-100&#20493;&#30340;&#25928;&#29575;&#25552;&#21319;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#24615;&#33021;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#21644;&#35774;&#35745;&#33258;&#36866;&#24212;&#25512;&#29702;&#29366;&#24577;&#31354;&#38388;&#26469;&#25913;&#36827;&#21487;&#36798;&#21040;&#30340;&#25928;&#29575;&#25552;&#21319;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms. We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties. Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces.
&lt;/p&gt;</description></item><item><title>PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04355</link><description>&lt;p&gt;
PQMass: &#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#27010;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04355
&lt;/p&gt;
&lt;p&gt;
PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20272;&#35745;&#20004;&#20010;&#26679;&#26412;&#38598;&#21512;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#27010;&#29575;&#65292;&#20026;&#35780;&#20272;&#21333;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#25110;&#27604;&#36739;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#19978;&#20005;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#27604;&#36739;&#21487;&#20197;&#36890;&#36807;&#23558;&#31354;&#38388;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#30340;&#21306;&#22495;&#24182;&#27604;&#36739;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#26469;&#36827;&#34892;&#12290;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#29983;&#25104;&#27169;&#22411;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#23427;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#26080;&#38656;&#38477;&#32500;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#30495;&#23454;&#20998;&#24067;&#23494;&#24230;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#25110;&#25311;&#21512;&#20219;&#20309;&#36741;&#21161;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#23427;&#30528;&#37325;&#20110;&#36817;&#20284;&#35745;&#31639;&#23494;&#24230;&#30340;&#31215;&#20998;&#65288;&#27010;&#29575;&#36136;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) acros
&lt;/p&gt;</description></item><item><title>Hedgehog&#26159;&#19968;&#31181;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#36890;&#36807;&#20445;&#25345;&#23574;&#38160;&#21644;&#21333;&#35843;&#24615;&#26469;&#24357;&#34917;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.04347</link><description>&lt;p&gt;
&#21050;&#29484;&#19982;&#35946;&#29482;&#65306;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#34920;&#36798;&#24615;&#32447;&#24615;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Hedgehog &amp; the Porcupine: Expressive Linear Attentions with Softmax Mimicry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04347
&lt;/p&gt;
&lt;p&gt;
Hedgehog&#26159;&#19968;&#31181;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#36890;&#36807;&#20445;&#25345;&#23574;&#38160;&#21644;&#21333;&#35843;&#24615;&#26469;&#24357;&#34917;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24050;&#32463;&#26174;&#31034;&#20986;&#25552;&#39640;Transformer&#25928;&#29575;&#30340;&#28508;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#20855;&#26377;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#65306;&#65288;1&#65289;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32447;&#24615;Transformer&#65292;&#65288;2&#65289;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;Transformer&#36827;&#34892;&#8220;&#24494;&#35843;-&#36716;&#25442;&#8221;&#20026;&#32447;&#24615;&#29256;&#26412;&#65292;&#24182;&#24674;&#22797;&#20219;&#21153;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23558;&#35832;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;Transformer&#36827;&#34892;&#8220;&#39044;&#35757;&#32451;-&#36716;&#25442;&#8221;&#65292;&#20197;&#23454;&#29616;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#24120;&#24120;&#19981;&#22914;&#26631;&#20934;&#30340;softmax&#27880;&#24847;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#32570;&#20047;&#19982;&#33391;&#22909;&#24615;&#33021;&#30456;&#20851;&#30340;softmax&#27880;&#24847;&#21147;&#30340;&#20851;&#38190;&#23646;&#24615;&#65306;&#20302;&#29109;&#65288;&#25110;&#8220;&#23574;&#23792;&#8221;&#65289;&#26435;&#37325;&#21644;&#28857;&#31215;&#21333;&#35843;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#29305;&#24449;&#26144;&#23556;&#65292;&#20445;&#30041;&#20102;&#36825;&#20123;&#23646;&#24615;&#65292;&#24182;&#19982;softmax&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#32447;&#24615;&#27880;&#24847;&#21147;&#20013;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hedgehog&#65292;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20445;&#25345;&#20102;&#23574;&#23792;&#21644;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04344</link><description>&lt;p&gt;
&#21435;&#26657;&#20934;&#26159;&#21542;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Confidence Calibration Help Conformal Prediction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26500;&#24314;&#20102;&#19968;&#32452;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20551;&#35774;&#20449;&#24515;&#26657;&#20934;&#21487;&#20197;&#20026;&#19968;&#33268;&#24615;&#39044;&#27979;&#24102;&#26469;&#22909;&#22788;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#20250;&#24847;&#22806;&#22320;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#25913;&#21892;&#20102;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#19988;&#28201;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#21017;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#32622;&#20449;&#24230;&#20250;&#38477;&#20302;&#22312;&#39044;&#27979;&#38598;&#20013;&#28155;&#21152;&#26032;&#31867;&#30340;&#27010;&#29575;&#12290;&#21463;&#21040;&#36825;&#19968;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;$\textbf{&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;}$ (ConfTS)&#65292;&#36890;&#36807;&#38408;&#20540;&#19982;&#30495;&#23454;&#26631;&#31614;&#30340;&#38750;&#19968;&#33268;&#24615;&#20998;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#20462;&#27491;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ConfTS&#30340;&#26032;&#30446;&#26631;&#23558;&#20351;&#28201;&#24230;&#20540;&#26397;&#30528;&#20248;&#21270;&#38598;&#30340;&#26041;&#21521;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability. Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead. Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set. Inspired by the analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label. In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.04335</link><description>&lt;p&gt;
LegalLens: &#21033;&#29992;LLMs&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#31532;&#19968;&#20010;&#26159;&#26816;&#27979;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#19982;&#21487;&#33021;&#21463;&#24433;&#21709;&#30340;&#20010;&#20154;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#30001;&#39046;&#22495;&#19987;&#23478;&#27880;&#37322;&#36827;&#34892;&#39564;&#35777;&#12290;&#20004;&#20010;&#20219;&#21153;&#37117;&#26159;&#20026;&#38598;&#20307;&#35785;&#35772;&#26696;&#24773;&#22659;&#29305;&#21035;&#35774;&#35745;&#30340;&#12290;&#23454;&#39564;&#35774;&#35745;&#37319;&#29992;&#20102;&#26469;&#33258;BERT&#31995;&#21015;&#21644;&#24320;&#28304;LLMs&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#21487;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#20854;&#36829;&#35268;&#34892;&#20026;&#35782;&#21035;&#30340;F1&#20998;&#25968;&#20026;62.69&#65285;&#65292;&#19982;&#21463;&#23475;&#32773;&#30456;&#20851;&#30340;&#20998;&#25968;&#20026;81.02&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#29992;&#20110;&#23454;&#39564;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#24515;&#30005;&#22270;&#35889;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#23454;&#29616;&#20102;&#23545;&#20154;&#26684;&#29305;&#24449;&#30340;&#35782;&#21035;&#12290;&#30740;&#31350;&#20351;&#29992;&#24515;&#30005;&#22270;&#35889;&#20316;&#20026;&#29305;&#24449;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#35270;&#21270;&#21464;&#25442;&#22120;&#30340;&#24341;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20154;&#26684;&#29305;&#24449;&#20998;&#31867;&#12290;&#36825;&#23545;&#20154;&#26684;&#20998;&#26512;&#21644;&#24515;&#29702;&#30740;&#31350;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.04326</link><description>&lt;p&gt;
&#20351;&#29992;&#24515;&#30005;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;&#20154;&#26684;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Personality Trait Recognition using ECG Spectrograms and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#24515;&#30005;&#22270;&#35889;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#23454;&#29616;&#20102;&#23545;&#20154;&#26684;&#29305;&#24449;&#30340;&#35782;&#21035;&#12290;&#30740;&#31350;&#20351;&#29992;&#24515;&#30005;&#22270;&#35889;&#20316;&#20026;&#29305;&#24449;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#35270;&#21270;&#21464;&#25442;&#22120;&#30340;&#24341;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20154;&#26684;&#29305;&#24449;&#20998;&#31867;&#12290;&#36825;&#23545;&#20154;&#26684;&#20998;&#26512;&#21644;&#24515;&#29702;&#30740;&#31350;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#26469;&#35782;&#21035;&#20154;&#26684;&#29305;&#24449;&#12290;&#22312;&#22823;&#20116;&#20154;&#26684;&#29305;&#24449;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#65292;&#21253;&#25324;&#22806;&#21521;&#24615;&#12289;&#31070;&#32463;&#36136;&#12289;&#23452;&#20154;&#24615;&#12289;&#36131;&#20219;&#24515;&#21644;&#24320;&#25918;&#24615;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#24515;&#30005;&#22270;&#35889;&#20316;&#20026;&#20449;&#24687;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;&#30830;&#23450;&#20102;&#29983;&#25104;&#24515;&#30005;&#22270;&#35889;&#30340;&#26368;&#20339;&#31383;&#21475;&#22823;&#23567;&#65292;&#24182;&#37319;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#20855;&#20307;&#20026;Resnet-18&#21644;&#21487;&#35270;&#21270;&#21464;&#25442;&#22120;(ViT)&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20154;&#26684;&#29305;&#24449;&#20998;&#31867;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#24471;&#30340;ASCE RTAIN &#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;58&#20010;&#21442;&#19982;&#32773;&#30340;&#21508;&#31181;&#29983;&#29702;&#20449;&#21495;&#65292;&#21253;&#25324;&#24515;&#30005;&#22270;&#35760;&#24405;&#65292;&#22312;&#21576;&#29616;&#34987;&#27426;&#24841;&#21644;&#21796;&#36215;&#27700;&#24179;&#20998;&#31867;&#30340;&#35270;&#39057;&#21050;&#28608;&#26399;&#38388;&#25910;&#38598;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#22312;&#20154;&#26684;&#29305;&#24449;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#22987;&#32456;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to recognizing personality traits using deep learning (DL) methods applied to electrocardiogram (ECG) signals. Within the framework of detecting the big five personality traits model encompassing extra-version, neuroticism, agreeableness, conscientiousness, and openness, the research explores the potential of ECG-derived spectrograms as informative features. Optimal window sizes for spectrogram generation are determined, and a convolutional neural network (CNN), specifically Resnet-18, and visual transformer (ViT) are employed for feature extraction and personality trait classification. The study utilizes the publicly available ASCERTAIN dataset, which comprises various physiological signals, including ECG recordings, collected from 58 participants during the presentation of video stimuli categorized by valence and arousal levels. The outcomes of this study demonstrate noteworthy performance in personality trait classification, consistently ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#26469;&#22686;&#24378;DNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;DNN&#23618;&#19978;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#26469;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04325</link><description>&lt;p&gt;
&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#22686;&#24378;DNN&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#26469;&#22686;&#24378;DNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;DNN&#23618;&#19978;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#26469;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#22312;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#26041;&#38754;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#65292;&#20174;&#21307;&#30103;&#21644;&#37329;&#34701;&#21040;&#27773;&#36710;&#12290;&#23613;&#31649;&#20854;&#20855;&#26377;&#30340;&#36716;&#22411;&#24433;&#21709;&#65292;DNN&#38754;&#20020;&#30528;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23545;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19982;&#26356;&#22797;&#26434;&#21644;&#26356;&#22823;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22686;&#24378;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#36890;&#36807;&#22343;&#21248;&#27880;&#20837;&#22122;&#38899;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#22122;&#38899;&#27880;&#20837;&#31639;&#27861;&#65292; strategically applied at&#27599;&#19968;&#20010;DNN layer&#65292;&#20197;&#24178;&#25200;&#25915;&#20987;&#20013;&#24341;&#20837;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#36890;&#36807;&#37319;&#29992;&#36817;&#20284;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35782;&#21035;&#24182;&#20445;&#25252;&#20851;&#38190;&#31070;&#32463;&#20803;&#65292;&#21516;&#26102;&#23545;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robus
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#20809;&#23376;&#35745;&#25968;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;PCCT&#65289;&#26159;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#30340;&#25361;&#25112;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#20855;&#26377;&#25552;&#39640;&#20083;&#33146;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#30103;&#25928;&#21644;&#32454;&#33410;&#27700;&#24179;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;PCCT&#36824;&#38598;&#25104;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#24433;&#20687;&#32452;&#23398;&#29305;&#24449;&#30340;&#30740;&#31350;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#12290;PCCT&#38754;&#20020;&#25361;&#25112;&#24182;&#38656;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.04301</link><description>&lt;p&gt;
&#28145;&#24230;&#20809;&#23376;&#35745;&#25968;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep PCCT: Photon Counting Computed Tomography Deep Learning Applications Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04301
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20809;&#23376;&#35745;&#25968;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;PCCT&#65289;&#26159;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#30340;&#25361;&#25112;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#20855;&#26377;&#25552;&#39640;&#20083;&#33146;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#30103;&#25928;&#21644;&#32454;&#33410;&#27700;&#24179;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;PCCT&#36824;&#38598;&#25104;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#24433;&#20687;&#32452;&#23398;&#29305;&#24449;&#30340;&#30740;&#31350;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#12290;PCCT&#38754;&#20020;&#25361;&#25112;&#24182;&#38656;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#38754;&#20020;&#31354;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#12289;&#30005;&#23376;&#22122;&#22768;&#24178;&#25200;&#21644;&#23545;&#27604;&#24230;&#22122;&#22768;&#27604;&#24046;&#30340;&#25361;&#25112;&#12290;&#20809;&#23376;&#35745;&#25968;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;PCCT&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;PCCT&#22312;&#20020;&#24202;&#21069;&#30740;&#31350;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#24378;&#35843;&#20854;&#20811;&#26381;&#20256;&#32479;&#25104;&#20687;&#38480;&#21046;&#30340;&#28508;&#21147;&#12290;&#20363;&#22914;&#65292;PCCT&#22312;&#25913;&#21892;&#20083;&#33146;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#30103;&#25928;&#65292;&#25552;&#20379;&#20102;&#20197;&#21069;&#38590;&#20197;&#36798;&#21040;&#30340;&#32454;&#33410;&#27700;&#24179;&#12290;&#36890;&#36807;&#20998;&#26512;&#24403;&#21069;&#20851;&#20110;PCCT&#30340;&#25991;&#29486;&#65292;&#32508;&#36848;&#21576;&#29616;&#20102;&#35813;&#25216;&#26415;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#25195;&#25551;&#20202;&#30340;&#20027;&#35201;&#29305;&#28857;&#21450;&#20854;&#21508;&#31181;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;PCCT&#30340;&#38598;&#25104;&#20197;&#21450;&#24433;&#20687;&#32452;&#23398;&#29305;&#24449;&#30340;&#30740;&#31350;&#65292;&#24182;&#21576;&#29616;&#20102;&#22312;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#25506;&#35752;&#20102;PCCT&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging faces challenges such as limited spatial resolution, interference from electronic noise and poor contrast-to-noise ratios. Photon Counting Computed Tomography (PCCT) has emerged as a solution, addressing these issues with its innovative technology. This review delves into the recent developments and applications of PCCT in pre-clinical research, emphasizing its potential to overcome traditional imaging limitations. For example PCCT has demonstrated remarkable efficacy in improving the detection of subtle abnormalities in breast, providing a level of detail previously unattainable. Examining the current literature on PCCT, it presents a comprehensive analysis of the technology, highlighting the main features of scanners and their varied applications. In addition, it explores the integration of deep learning into PCCT, along with the study of radiomic features, presenting successful applications in data processing. While acknowledging these advances, it also discusses the
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04298</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-View Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04298
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;(SR)&#25628;&#32034;&#34920;&#31034;&#35299;&#37322;&#21464;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#30446;&#21069;&#30340;SR&#26041;&#27861;&#20551;&#35774;&#20174;&#21333;&#20010;&#23454;&#39564;&#20013;&#25552;&#21462;&#30340;&#21333;&#20010;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#19981;&#21516;&#35774;&#32622;&#30340;&#22810;&#20010;&#23454;&#39564;&#32467;&#26524;&#38598;&#12290;&#20256;&#32479;&#30340;SR&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#28508;&#22312;&#30340;&#34920;&#36798;&#24335;&#65292;&#22240;&#20026;&#27599;&#20010;&#23454;&#39564;&#30340;&#21442;&#25968;&#21487;&#33021;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#23454;&#39564;&#29615;&#22659;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#21442;&#25968;&#21270;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35780;&#20272;&#30340;&#34920;&#36798;&#24335;&#36866;&#24212;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#36820;&#22238;&#33021;&#22815;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#21442;&#25968;&#20989;&#25968;&#26063;f(x; \theta)&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#24050;&#30693;&#34920;&#36798;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26469;&#23637;&#31034;MvSR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;HGNN&#26041;&#27861;&#65292;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#36716;&#21270;&#20026;Multi-Layer Perceptron (MLPs)&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#65292;&#32780;LightHGNN$^+$&#21017;&#27880;&#20837;&#20102;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04296</link><description>&lt;p&gt;
&#20809;HGNN&#65306;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#33976;&#39311;&#25104;MLPs&#65292;&#25512;&#26029;&#36895;&#24230;&#25552;&#21319;100&#20493;
&lt;/p&gt;
&lt;p&gt;
LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;HGNN&#26041;&#27861;&#65292;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#36716;&#21270;&#20026;Multi-Layer Perceptron (MLPs)&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#65292;&#32780;LightHGNN$^+$&#21017;&#27880;&#20837;&#20102;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#39640;&#38454;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#23637;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36229;&#22270;&#30340;&#39640;&#38454;&#24314;&#27169;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#22686;&#21152;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#22312;&#23454;&#38469;&#24037;&#19994;&#37096;&#32626;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;HGNNs&#39640;&#38454;&#32467;&#26500;&#20381;&#36182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26159;&#39640;&#25928;&#37096;&#32626;&#30340;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;HGNNs&#21644;&#39640;&#25928;&#25512;&#26029;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#28040;&#38500;HGNNs&#30340;&#36229;&#22270;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25913;&#21892;&#25512;&#26029;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LightHGNN&#21644;LightHGNN$^+$&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#20302;&#22797;&#26434;&#24615;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#30452;&#25509;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#20013;&#65292;&#32780;LightHGNN$^+$&#21017;&#36827;&#19968;&#27493;&#26174;&#24335;&#22320;&#23558;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#27880;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into 
&lt;/p&gt;</description></item><item><title>AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04292</link><description>&lt;p&gt;
AdaFlow: &#21464;&#24322;&#33258;&#36866;&#24212;&#27969;&#31574;&#30053;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04292
&lt;/p&gt;
&lt;p&gt;
AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20915;&#31574;&#20013;&#25913;&#36827;&#20102;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#65292;&#20294;&#30001;&#20110;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#36882;&#24402;&#32780;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22810;&#26679;&#21270;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;AdaFlow&#20351;&#29992;&#29366;&#24577;&#26465;&#20214;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34920;&#31034;&#31574;&#30053;&#65292;&#36825;&#34987;&#31216;&#20026;&#27010;&#29575;&#27969;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#35757;&#32451;&#25439;&#22833;&#30340;&#26465;&#20214;&#26041;&#24046;&#19982;ODE&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#20351;AdaFlow&#25104;&#20026;&#19968;&#20010;&#33258;&#36866;&#24212;&#20915;&#31574;&#32773;&#65292;&#33021;&#22815;&#24555;&#36895;&#25512;&#29702;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21160;&#20316;&#20998;&#24067;&#34987;&#38477;&#20302;&#21040;&#19968;&#27493;&#29983;&#25104;&#22120;&#26102;&#65292;&#23427;&#33258;&#21160;&#36864;&#21270;&#21040;&#19968;&#20010;&#19968;&#27493;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
&lt;/p&gt;</description></item><item><title>BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04291</link><description>&lt;p&gt;
BiLLM: &#25512;&#21160;LLMs&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04291
&lt;/p&gt;
&lt;p&gt;
BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#36890;&#29992;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20108;&#20540;&#21270;&#21487;&#20197;&#23558;&#27169;&#22411;&#26435;&#37325;&#26497;&#22823;&#22320;&#20943;&#23569;&#21040;&#20165;1&#20301;&#65292;&#38477;&#20302;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#25216;&#26415;&#22312;&#36229;&#20302;&#20301;&#23485;&#19979;&#26080;&#27861;&#20445;&#25345;LLM&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLM&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#12290;&#22522;&#20110;LLMs&#30340;&#26435;&#37325;&#20998;&#24067;&#65292;BiLLM&#39318;&#20808;&#35782;&#21035;&#21644;&#32467;&#26500;&#36873;&#25321;&#37325;&#35201;&#30340;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#20108;&#20540;&#21270;&#27531;&#24046;&#36924;&#36817;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#21387;&#32553;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#38750;&#37325;&#35201;&#26435;&#37325;&#30340;&#38047;&#24418;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#20998;&#21106;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#20998;&#32452;&#21644;&#20108;&#20540;&#21270;&#12290;BiLLM&#39318;&#27425;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
&lt;/p&gt;</description></item><item><title>CasCast&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#29087;&#32451;&#30340;&#32423;&#32852;&#24314;&#27169;&#35299;&#20915;&#20102;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21270;&#21644;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04290</link><description>&lt;p&gt;
CasCast: &#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#30340;&#29087;&#32451;&#32423;&#32852;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04290
&lt;/p&gt;
&lt;p&gt;
CasCast&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#29087;&#32451;&#30340;&#32423;&#32852;&#24314;&#27169;&#35299;&#20915;&#20102;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21270;&#21644;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38647;&#36798;&#25968;&#25454;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#22312;&#26497;&#31471;&#22825;&#27668;&#39044;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#35299;&#20915;&#65306;&#65288;i&#65289;&#23545;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21464;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23545;&#26497;&#31471;&#38477;&#27700;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CasCast&#65292;&#19968;&#20010;&#30001;&#30830;&#23450;&#24615;&#37096;&#20998;&#21644;&#27010;&#29575;&#24615;&#37096;&#20998;&#32452;&#25104;&#30340;&#32423;&#32852;&#26694;&#26550;&#65292;&#23558;&#20013;&#23610;&#24230;&#38477;&#27700;&#20998;&#24067;&#21644;&#23567;&#23610;&#24230;&#27169;&#24335;&#30340;&#39044;&#27979;&#35299;&#32806;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#35757;&#32451;&#32423;&#32852;&#26694;&#26550;&#65292;&#24182;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#27010;&#29575;&#24615;&#24314;&#27169;&#65292;&#21033;&#29992;&#38754;&#21521;&#24103;&#30340;&#24341;&#23548;&#25193;&#25955;&#21464;&#25442;&#22120;&#22686;&#24378;&#26497;&#31471;&#20107;&#20214;&#30340;&#20248;&#21270;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#38647;&#36798;&#38477;&#27700;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;CasCast&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competi
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#21644;&#25968;&#25454;&#22122;&#22768;&#31561;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#25104;&#26524;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#26102;&#20195;&#12290;</title><link>https://arxiv.org/abs/2402.04286</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Progress and Opportunities of Foundation Models in Bioinformatics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04286
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#21644;&#25968;&#25454;&#22122;&#22768;&#31561;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#25104;&#26524;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26085;&#30410;&#25972;&#21512;&#19979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#37319;&#29992;&#65292;&#32463;&#21382;&#20102;&#19968;&#20010;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#20123;AI&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#21382;&#21490;&#24615;&#30340;&#25361;&#25112;&#65292;&#22914;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#22122;&#22768;&#30340;&#23384;&#22312;&#12290;FMs&#29305;&#21035;&#25797;&#38271;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#29983;&#29289;&#23398;&#32972;&#26223;&#19979;&#36825;&#26159;&#24120;&#35265;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#23454;&#39564;&#30830;&#23450;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#31243;&#36153;&#26102;&#36153;&#21147;&#12290;&#36825;&#19968;&#29305;&#24615;&#20351;FMs&#22312;&#21508;&#31181;&#19979;&#28216;&#39564;&#35777;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#20986;&#30340;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#23427;&#20204;&#26377;&#25928;&#22320;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;FMs&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24320;&#21551;&#20102;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#26412;&#32508;&#36848;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;FMs&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#36861;&#28335;&#20854;&#28436;&#21464;&#21644;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bioinformatics has witnessed a paradigm shift with the increasing integration of artificial intelligence (AI), particularly through the adoption of foundation models (FMs). These AI techniques have rapidly advanced, addressing historical challenges in bioinformatics such as the scarcity of annotated data and the presence of data noise. FMs are particularly adept at handling large-scale, unlabeled data, a common scenario in biological contexts due to the time-consuming and costly nature of experimentally determining labeled data. This characteristic has allowed FMs to excel and achieve notable results in various downstream validation tasks, demonstrating their ability to represent diverse biological entities effectively. Undoubtedly, FMs have ushered in a new era in computational biology, especially in the realm of deep learning. The primary goal of this survey is to conduct a systematic investigation and summary of FMs in bioinformatics, tracing their evolution, current research status
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;MDGNNs&#22312;&#22788;&#29702;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04284</link><description>&lt;p&gt;
PRES: &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35299;&#20915;&#20102;&#35757;&#32451;&#20013;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;MDGNNs&#22312;&#22788;&#29702;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#26102;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23384;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNNs&#65289;&#26159;&#19968;&#31867;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#25552;&#21462;&#12289;&#25552;&#28860;&#21644;&#35760;&#24518;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#26080;&#20869;&#23384;&#30340;&#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;MDGNNs&#38754;&#20020;&#30528;&#22788;&#29702;&#32416;&#32467;&#30340;&#26102;&#38388;&#21644;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#24207;&#21015;&#36827;&#34892;&#39034;&#24207;&#21644;&#26102;&#38388;&#39034;&#24207;&#30340;&#22788;&#29702;&#65292;&#20197;&#25429;&#25417;&#20934;&#30830;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#25209;&#37327;&#35757;&#32451;&#20013;&#65292;&#21516;&#19968;&#25209;&#27425;&#20869;&#30340;&#26102;&#38388;&#25968;&#25454;&#28857;&#23558;&#34987;&#24182;&#34892;&#22788;&#29702;&#65292;&#32780;&#23427;&#20204;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#23558;&#34987;&#24573;&#35270;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26102;&#38388;&#38388;&#26029;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#30340;&#26102;&#38388;&#25209;&#37327;&#22823;&#23567;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;MDGNNs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;MDGNNs&#30340;&#22823;&#35268;&#27169;&#39640;&#25928;&#35757;&#32451;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#22823;&#30340;&#26102;&#38388;&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;MDGNNs&#26102;&#30340;&#26102;&#38388;&#38388;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#29702;&#35770;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04278</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#23376;&#23494;&#24230;&#20272;&#35745;&#30340;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Gaussian Plane-Wave Neural Operator for Electron Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#30005;&#23376;&#23494;&#24230;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21270;&#23398;&#31995;&#32479;&#21644;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;(DFT)&#27169;&#25311;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#26031;&#24179;&#38754;&#27874;&#31070;&#32463;&#31639;&#23376;(GPWNO)&#65292;&#23427;&#22312;&#26080;&#38480;&#32500;&#21151;&#33021;&#31354;&#38388;&#20013;&#20351;&#29992;&#24179;&#38754;&#27874;&#21644;&#39640;&#26031;&#22411;&#36712;&#36947;&#22522;&#24213;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;DFT&#30340;&#32972;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#12290;&#29305;&#21035;&#22320;&#65292;&#30001;&#20110;&#20004;&#31181;&#22522;&#24213;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#23494;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#25104;&#20998;&#37117;&#21487;&#20197;&#34987;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;&#23545;QM9&#12289;MD&#21644;&#26448;&#26009;&#39033;&#30446;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;GPWNO&#30456;&#27604;&#20854;&#20182;&#21313;&#31181;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies machine learning for electron density prediction, which is fundamental for understanding chemical systems and density functional theory (DFT) simulations. To this end, we introduce the Gaussian plane-wave neural operator (GPWNO), which operates in the infinite-dimensional functional space using the plane-wave and Gaussian-type orbital bases, widely recognized in the context of DFT. In particular, both high- and low-frequency components of the density can be effectively represented due to the complementary nature of the two bases. Extensive experiments on QM9, MD, and material project datasets demonstrate GPWNO's superior performance over ten baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;LFADS&#27169;&#22411;&#26377;&#25928;&#22320;&#37096;&#32626;&#21040;FPGA&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#25968;&#25454;&#30340;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#20026;&#23454;&#26102;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.04274</link><description>&lt;p&gt;
&#29992;&#20110;&#23454;&#26102;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#30340;LFADS&#30340;FPGA&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
FPGA Deployment of LFADS for Real-time Neuroscience Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;LFADS&#27169;&#22411;&#26377;&#25928;&#22320;&#37096;&#32626;&#21040;FPGA&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#25968;&#25454;&#30340;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#20026;&#23454;&#26102;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35760;&#24405;&#31070;&#32463;&#27963;&#21160;&#20026;&#30740;&#31350;&#31070;&#32463;&#20154;&#32676;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#20998;&#26512;&#36825;&#31181;&#39640;&#32500;&#24230;&#27979;&#37327;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#37096;&#32626;&#23398;&#20064;&#20302;&#32500;&#24230;&#28508;&#22312;&#21160;&#24577;&#30340;&#31639;&#27861;&#12290;LFADS&#65288;&#36890;&#36807;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#28508;&#22312;&#22240;&#32032;&#20998;&#26512;&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#20174;&#21516;&#26102;&#35760;&#24405;&#30340;&#39640;&#32500;&#31070;&#32463;&#23574;&#23792;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#21160;&#24577;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#22797;&#26434;&#30340;&#33041;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25512;&#26029;&#24310;&#36831;&#20026;&#27627;&#31186;&#32423;&#12290;&#38543;&#30528;&#21516;&#26102;&#35760;&#24405;&#35768;&#22810;&#31070;&#32463;&#20803;&#30340;&#33021;&#21147;&#20197;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#26500;&#24314;&#20302;&#24310;&#36831;&#25512;&#26029;&#35745;&#31639;&#31639;&#27861;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#25552;&#39640;LFADS&#30340;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;LFADS&#27169;&#22411;&#26377;&#25928;&#22320;&#23454;&#29616;&#21040;&#20102;&#21487;&#32534;&#31243;&#36923;&#36753;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#19978;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#26174;&#31034;&#20986;41.97 $\mu$s&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale recordings of neural activity are providing new opportunities to study neural population dynamics. A powerful method for analyzing such high-dimensional measurements is to deploy an algorithm to learn the low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical Systems) is a deep learning method for inferring latent dynamics from high-dimensional neural spiking data recorded simultaneously in single trials. This method has shown a remarkable performance in modeling complex brain signals with an average inference latency in milliseconds. As our capacity of simultaneously recording many neurons is increasing exponentially, it is becoming crucial to build capacity for deploying low-latency inference of the computing algorithms. To improve the real-time processing ability of LFADS, we introduce an efficient implementation of the LFADS models onto Field Programmable Gate Arrays (FPGA). Our implementation shows an inference latency of 41.97 $\mu$s for processi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDA&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#23398;&#20064;&#26469;&#25171;&#30772;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04273</link><description>&lt;p&gt;
&#25171;&#30772;&#25968;&#25454;&#23396;&#23707;&#65306;&#36328;&#39046;&#22495;&#23398;&#20064;&#23454;&#29616;&#29420;&#31435;&#31169;&#26377;&#28304;&#30340;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDA&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#23398;&#20064;&#26469;&#25171;&#30772;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#19981;&#21516;&#26234;&#33021;&#20307;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#20844;&#21496;&#12290;&#27599;&#20010;&#20844;&#21496;&#21487;&#33021;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21508;&#20010;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#28304;&#22312;&#27599;&#20010;&#20844;&#21496;&#20013;&#26159;&#30456;&#20114;&#29420;&#31435;&#21644;&#31169;&#26377;&#30340;&#65292;&#23548;&#33268;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#20013;&#35757;&#32451;&#19981;&#21516;&#26234;&#33021;&#20307;&#30340;&#19981;&#21516;&#31169;&#26377;&#25968;&#25454;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#19978;&#36848;&#20998;&#24067;&#24046;&#24322;&#36896;&#25104;&#30340;&#25968;&#25454;&#23396;&#23707;&#21487;&#33021;&#23548;&#33268;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20998;&#24067;&#24046;&#24322;&#23545;&#29616;&#26377;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25171;&#30772;&#25968;&#25454;&#23396;&#23707;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#36328;&#39046;&#22495;&#23398;&#20064;&#30340;&#29305;&#24449;&#20998;&#24067;&#24863;&#30693;&#32858;&#21512;&#65288;FDA&#65289;&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;FDA&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#26088;&#22312;&#21152;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing interme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;Tigrigna&#35821;&#35328;&#30340;&#22823;&#35789;&#27719;&#37327;&#33258;&#21457;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35774;&#35745;&#19982;&#24320;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;Sphinx&#24037;&#20855;&#24320;&#21457;&#22768;&#23398;&#27169;&#22411;&#21644;&#20351;&#29992;SRIM&#24037;&#20855;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#23454;&#29616;&#20102;&#23545;Tigrigna&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04254</link><description>&lt;p&gt;
Tigrigna&#22823;&#35789;&#27719;&#37327;&#33258;&#21457;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Large Vocabulary Spontaneous Speech Recognition for Tigrigna
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;Tigrigna&#35821;&#35328;&#30340;&#22823;&#35789;&#27719;&#37327;&#33258;&#21457;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35774;&#35745;&#19982;&#24320;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;Sphinx&#24037;&#20855;&#24320;&#21457;&#22768;&#23398;&#27169;&#22411;&#21644;&#20351;&#29992;SRIM&#24037;&#20855;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#23454;&#29616;&#20102;&#23545;Tigrigna&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#26684;&#21033;&#23612;&#20122;&#35821;&#20013;&#29420;&#31435;&#20110;&#21457;&#35328;&#32773;&#30340;&#33258;&#21457;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#30740;&#31350;&#23581;&#35797;&#12290;&#35813;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#22768;&#23398;&#27169;&#22411;&#37319;&#29992;&#21345;&#20869;&#22522;&#26757;&#38534;&#22823;&#23398;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24320;&#21457;&#24037;&#20855;&#65288;Sphinx&#65289;&#24320;&#21457;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21017;&#37319;&#29992;SRIM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis proposes and describes a research attempt at designing and developing a speaker independent spontaneous automatic speech recognition system for Tigrigna The acoustic model of the Speech Recognition System is developed using Carnegie Mellon University Automatic Speech Recognition development tool (Sphinx) while the SRIM tool is used for the development of the language model.   Keywords Automatic Speech Recognition Tigrigna language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04193</link><description>&lt;p&gt;
&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Gradient Coding in Decentralized Learning for Evading Stragglers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#23613;&#31649;&#26799;&#24230;&#32534;&#30721;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20197;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#65292;&#21363;&#35774;&#22791;&#20351;&#29992;&#20887;&#20313;&#35757;&#32451;&#25968;&#25454;&#21457;&#36865;&#32534;&#30721;&#26799;&#24230;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#25955;&#24335;&#23398;&#20064;&#22330;&#26223;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21442;&#25968;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#32534;&#30721;&#26694;&#26550;&#30340;&#32534;&#30721;&#26799;&#24230;&#36827;&#34892;&#26412;&#22320;&#26356;&#26032;&#65292;&#28982;&#21518;&#20197;&#20843;&#21350;&#26041;&#24335;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GOCO&#22312;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory for Kernel Packets: from state space model to compactly supported basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#65288;SS&#65289;&#27169;&#22411;&#20844;&#24335;&#21487;&#20197;&#23558;&#20854;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#38477;&#20302;&#21040;O&#65288;n&#65289;&#65288;n&#20026;&#25968;&#25454;&#28857;&#20010;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;m&#32500;&#30340;GP&#30340;SS&#27169;&#22411;&#20844;&#24335;&#31561;&#20215;&#20110;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#31216;&#20026;&#36890;&#29992;&#21491;&#26680;&#20998;&#32452;&#65288;KP&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;GP&#21327;&#26041;&#24046;&#20989;&#25968;K&#30340;&#21464;&#25442;&#65292;&#20351;&#24471;&#23545;&#20110;&#20219;&#24847;$t \leq t_1$&#65292;$0 \leq j \leq m-1$&#21644;$m+1$&#20010;&#36830;&#32493;&#28857;$t_i$&#65292;&#37117;&#28385;&#36275;$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$&#65292;&#20854;&#20013;${D}_t^{(j)}f(t)$&#34920;&#31034;&#22312;$t$&#19978;&#20316;&#29992;&#30340;&#31532;j&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;GP&#30340;&#21521;&#21518;SS&#27169;&#22411;&#20844;&#24335;&#65292;&#24471;&#21040;&#20102;&#19979;&#19968;&#20010;$m$&#20010;&#36830;&#32493;&#28857;&#30340;&#24038;&#26680;&#20998;&#32452;&#30340;&#27010;&#24565;&#65306;$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$&#65292;&#23545;&#20110;&#20219;&#24847;$t\geq t_{2m}$&#12290;&#36890;&#36807;&#32467;&#21512;&#24038;&#21491;&#26680;&#20998;&#32452;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#20123;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36866;&#24403;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#65306;&#23545;&#20110;&#20219;&#24847;$t\not\in(t_0,t_{2m})$&#21644;$j=0,\cdots,m-1$&#65292;$\phi^{(j)}(t)=0$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#19979;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#34920;&#29616;&#20986;&#21152;&#24378;&#30340;NC2&#65292;&#24182;&#21487;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.03979</link><description>&lt;p&gt;
&#20132;&#21449;&#29109;&#19982;&#26631;&#31614;&#24179;&#28369;&#65306;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#19979;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#34920;&#29616;&#20986;&#21152;&#24378;&#30340;NC2&#65292;&#24182;&#21487;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#20174;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#21152;&#24378;&#30340;NC2&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#29702;&#35299;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#24615;&#33021;&#20248;&#21183;&#21644;&#22686;&#24378;&#30340;&#27169;&#22411;&#26657;&#20934;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#25512;&#23548;&#20986;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26631;&#31614;&#24179;&#28369;&#19979;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#26465;&#20214;&#25968;&#65292;&#22240;&#27492;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32508;&#21512;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#26631;&#31614;&#24179;&#28369;&#30340;&#25928;&#26524;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri
&lt;/p&gt;</description></item><item><title>AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03784</link><description>&lt;p&gt;
AirPhyNet: &#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03784
&lt;/p&gt;
&lt;p&gt;
AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#21644;&#24314;&#27169;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#29615;&#22659;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24110;&#21161;&#20010;&#20154;&#21644;&#24403;&#23616;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#31934;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#32570;&#20047;&#22362;&#23454;&#29289;&#29702;&#22522;&#30784;&#30340;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Physics guided Neural Network for Air Quality Prediction&#65288;AirPhyNet&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#20004;&#20010;&#25104;&#29087;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#25193;&#25955;&#21644;&#24179;&#27969;&#65289;&#23558;&#20854;&#34920;&#31034;&#20026;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#23558;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#25429;&#25417;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03583</link><description>&lt;p&gt;
MQuinE:&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20013;&#8220;Z-&#24726;&#35770;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03583
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;KGE&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#27969;&#34892;&#30340;&#29616;&#26377;KGE&#27169;&#22411;&#23384;&#22312;&#34920;&#36798;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;Z-&#24726;&#35770;&#8221;&#12290;&#21463;&#21040;Z-&#24726;&#35770;&#30340;&#23384;&#22312;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KGE&#27169;&#22411;&#65292;&#31216;&#20026;MQuinE&#65292;&#22312;&#19981;&#21463;Z-&#24726;&#35770;&#30340;&#22256;&#25200;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;/&#38750;&#23545;&#31216;&#65292;&#36870;&#21521;&#65292;1-N/N-1/N-N&#21644;&#32452;&#21512;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#23545;&#23454;&#38469;&#30693;&#35782;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Z-&#24726;&#35770;&#30830;&#23454;&#38477;&#20302;&#20102;&#29616;&#26377;KGE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#36229;&#36807;20&#65285;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;MQuinE&#21487;&#20197;&#20943;&#36731;Z-&#24726;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20197;&#26126;&#26174;&#20248;&#21183;&#36229;&#36234;&#29616;&#26377;&#30340;KGE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#20013;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03363</link><description>&lt;p&gt;
&#25506;&#32034;&#36136;&#25968;&#20998;&#31867;&#65306;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03363
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#20013;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#35770;&#65292;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#19978;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26680;&#24515;&#26159;&#24320;&#21457;&#19968;&#31181;&#39640;&#24230;&#31232;&#30095;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32452;&#21512;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#22312;&#35782;&#21035;&#36136;&#25968;&#26102;&#36798;&#21040;&#20102;&#36229;&#36807;99\%&#30340;&#21484;&#22238;&#29575;&#65292;&#22312;&#35782;&#21035;&#38750;&#36136;&#25968;&#26102;&#36798;&#21040;&#20102;79\%&#30340;&#21484;&#22238;&#29575;&#65292;&#36825;&#20123;&#25968;&#23383;&#26159;&#20174;&#26412;&#36136;&#19978;&#19981;&#24179;&#34913;&#30340;&#39034;&#24207;&#25972;&#25968;&#24207;&#21015;&#20013;&#24471;&#20986;&#30340;&#65292;&#24182;&#19988;&#22312;&#23436;&#25104;&#21333;&#20010;&#35757;&#32451;&#21608;&#26399;&#20043;&#21069;&#36805;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#20351;&#29992; $10^6$ &#20010;&#25972;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25351;&#23450;&#30340;&#25972;&#25968;&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340; $2 \times 10^6$ &#20010;&#25972;&#25968;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#33539;&#22260;&#20174; $10^6$ &#21040; $3 \times 10^6$&#65292;&#20559;&#31227;&#37327;&#30456;&#21516;&#12290;&#23613;&#31649;&#21463;&#38480;&#20110;&#36164;&#28304;&#30340;&#20869;&#23384;&#23481;&#37327;&#65292;&#38480;&#21046;&#25105;&#20204;&#30340;&#20998;&#26512;&#36328;&#36234;&#20102; $3\times10^6$&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;......&#30340;&#24212;&#29992;&#20570;&#20986;&#20102;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99\% in identifying prime numbers and 79\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using $10^6$ integers starting from a specified integer and tested on a different range of $2 \times 10^6$ integers extending from $10^6$ to $3 \times 10^6$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of $3\times10^6$, we believe that our study contribute to the application of machine learning in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03292</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#20316;&#20026;&#40657;&#30418;&#20113;&#26381;&#21153;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#65292;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#38646;&#26679;&#26412;&#31163;&#32676;&#25968;&#25454;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19981;&#23646;&#20110;&#20998;&#31867;&#22120;&#26631;&#31614;&#38598;&#20294;&#34987;&#38169;&#35823;&#22320;&#24402;&#31867;&#20026;&#20837;&#22495;&#65288;ID&#65289;&#23545;&#35937;&#30340;OOD&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;RONIN&#20351;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29992;&#20462;&#22797;&#26367;&#25442;&#25481;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#12290;RONIN&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#36755;&#20837;&#23545;&#35937;&#25509;&#36817;&#20837;&#22495;&#22495;&#12290;&#32467;&#26524;&#26159;&#65292;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;ID&#24773;&#20917;&#19979;&#38750;&#24120;&#25509;&#36817;&#21407;&#22987;&#23545;&#35937;&#65292;&#22312;OOD&#24773;&#20917;&#19979;&#21017;&#30456;&#24046;&#36739;&#36828;&#65292;&#20351;&#24471;RONIN&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RONIN&#22312;&#38646;&#26679;&#26412;&#21644;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03019</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#30340;Taylor&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Taylor Videos for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03019
&lt;/p&gt;
&lt;p&gt;
Taylor&#35270;&#39057;&#26159;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#29992;&#20110;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#21160;&#20316;&#25552;&#21462;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#36817;&#20284;&#35745;&#31639;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#20316;&#25552;&#21462;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#21160;&#20316;&#26159;&#21160;&#20316;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21160;&#20316;(i)&#27809;&#26377;&#26126;&#30830;&#30340;&#24418;&#24335;&#65292;(ii)&#25317;&#26377;&#35832;&#22914;&#20301;&#31227;&#12289;&#36895;&#24230;&#21644;&#21152;&#36895;&#24230;&#31561;&#21508;&#31181;&#27010;&#24565;&#65292;(iii)&#36890;&#24120;&#20250;&#21463;&#21040;&#19981;&#31283;&#23450;&#20687;&#32032;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taylor&#35270;&#39057;&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#26684;&#24335;&#65292;&#23427;&#31361;&#20986;&#26174;&#31034;&#20102;&#27599;&#20010;&#24103;&#20013;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#25381;&#25163;&#65289;&#34987;&#31216;&#20026;Taylor&#24103;&#12290;Taylor&#35270;&#39057;&#30340;&#21629;&#21517;&#26469;&#28304;&#20110;Taylor&#32423;&#25968;&#65292;&#23427;&#20351;&#29992;&#37325;&#35201;&#30340;&#39033;&#26469;&#36817;&#20284;&#32473;&#23450;&#28857;&#19978;&#30340;&#20989;&#25968;&#12290;&#22312;&#35270;&#39057;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#20316;&#25552;&#21462;&#20989;&#25968;&#65292;&#26088;&#22312;&#20174;&#35270;&#39057;&#26102;&#38388;&#22359;&#20013;&#25552;&#21462;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#22359;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24103;&#12289;&#24046;&#20998;&#24103;&#21644;&#39640;&#38454;&#24046;&#20998;&#24103;&#36827;&#34892;Taylor&#23637;&#24320;&#65292;&#20197;&#36817;&#20284;&#35745;&#31639;&#36215;&#22987;&#24103;&#19978;&#30340;&#36825;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Taylor&#32423;&#25968;&#20013;&#39640;&#38454;&#39033;&#30340;&#27714;&#21644;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02910</link><description>&lt;p&gt;
DS-MS-TCN: &#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Otago&#36816;&#21160;&#35745;&#21010;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#30340;&#37325;&#35201;&#24247;&#22797;&#20030;&#25514;&#65292;&#26088;&#22312;&#22686;&#24378;&#24179;&#34913;&#21644;&#21147;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#65292;&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#35782;&#21035;Otago&#20307;&#25805;&#21160;&#20316;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#22312;&#23454;&#39564;&#23460;&#35774;&#32622;&#20013;&#25307;&#21215;&#20102;36&#21517;&#32769;&#24180;&#20154;&#65292;&#24182;&#23545;&#39069;&#22806;&#25307;&#21215;&#30340;7&#21517;&#32769;&#24180;&#20154;&#36827;&#34892;&#20102;&#23478;&#24237;&#35780;&#20272;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;(DS-MS-TCN)&#65292;&#29992;&#20110;&#20004;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#65292;&#23558;&#20854;&#32435;&#20837;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27169;&#22411;&#19987;&#27880;&#20110;&#35782;&#21035;&#27599;&#20010;&#20307;&#25805;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;(&#24494;&#26631;&#31614;)&#12290;&#38543;&#21518;&#30340;&#38454;&#27573;&#25193;&#23637;&#20102;&#35782;&#21035;&#33539;&#22260;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LG-GNN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#35745;&#31639;&#36793;&#32536;&#27010;&#29575;&#26469;&#39044;&#27979;&#22270;&#20013;&#30340;&#38142;&#25509;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#32479;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#26550;&#26500;&#23545;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#37117;&#36866;&#29992;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02692</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees for Link Prediction using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LG-GNN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#35745;&#31639;&#36793;&#32536;&#27010;&#29575;&#26469;&#39044;&#27979;&#22270;&#20013;&#30340;&#38142;&#25509;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#32479;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#26550;&#26500;&#23545;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#37117;&#36866;&#29992;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30001;&#22270;&#19978;&#29983;&#25104;&#30340;&#22270;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#25512;&#23548;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;GNN&#26550;&#26500;&#65288;LG-GNN&#65289;&#65292;&#21487;&#20197;&#20135;&#29983;&#23545;&#28508;&#22312;&#36793;&#32536;&#27010;&#29575;&#30340;&#19968;&#33268;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#22343;&#26041;&#35823;&#24046;&#36827;&#34892;&#20102;&#30028;&#23450;&#65292;&#24182;&#23545;LG-GNN&#22312;&#26816;&#27979;&#39640;&#27010;&#29575;&#36793;&#32536;&#30340;&#33021;&#21147;&#32473;&#20986;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#36866;&#29992;&#20110;&#31232;&#30095;&#21644;&#31264;&#23494;&#22270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#20856;GCN&#26550;&#26500;&#30340;&#19968;&#20123;&#32570;&#28857;&#65292;&#24182;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#33258;&#21160;&#21457;&#29616;&#40657;&#30418;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#22312;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#32447;&#30340;&#21516;&#26102;&#65292;&#36824;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02355</link><description>&lt;p&gt;
Symbol:&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#29983;&#25104;&#28789;&#27963;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#33258;&#21160;&#21457;&#29616;&#40657;&#30418;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#22312;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#32447;&#30340;&#21516;&#26102;&#65292;&#36824;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;MetaBBO&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20803;&#23398;&#20064;&#20256;&#32479;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#39044;&#23450;&#20041;&#25163;&#24037;&#20248;&#21270;&#22120;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{Symbol}&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31526;&#21495;&#26041;&#31243;&#23398;&#20064;&#26469;&#20419;&#36827;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#33258;&#21160;&#21457;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21495;&#26041;&#31243;&#29983;&#25104;&#22120;(SEG)&#65292;&#20801;&#35768;&#20026;&#29305;&#23450;&#20219;&#21153;&#21644;&#20248;&#21270;&#27493;&#39588;&#21160;&#24577;&#29983;&#25104;&#38381;&#24335;&#20248;&#21270;&#35268;&#21017;&#12290;&#22312;\textsc{Symbol}&#20869;&#37096;&#65292;&#25105;&#20204;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#20803;&#23398;&#20064;SEG&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;\textsc{Symbol}&#29983;&#25104;&#30340;&#20248;&#21270;&#22120;&#19981;&#20165;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;BBO&#21644;MetaBBO&#22522;&#20934;&#32447;&#65292;&#32780;&#19988;&#22312;&#23436;&#20840;&#19981;&#21516;&#38382;&#39064;&#30340;&#20840;&#26032;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present \textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within \textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by \textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.02347</link><description>&lt;p&gt;
Riemannian Preconditioned LoRA&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#65292;&#22686;&#24378;&#20102;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#20351;&#29992;&#35813;&#39044;&#26465;&#20214;&#22120;&#24494;&#35843;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LoRA&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;Riemannian&#39044;&#26465;&#20214;&#22120;&#26469;&#25552;&#21319;&#20854;&#20248;&#21270;&#27493;&#39588;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;$r\times r$&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;&#20854;&#20013;$r$&#26159;LoRA&#30340;&#31209;&#12290;&#36825;&#20010;&#39044;&#26465;&#20214;&#22120;&#23545;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20195;&#30721;&#21482;&#38656;&#35201;&#20570;&#20986;&#24456;&#23567;&#30340;&#25913;&#21464;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#23384;&#20648;&#21644;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#65292;SGD&#21644;AdamW&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#38752;&#24615;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#23545;&#20110;&#23398;&#20064;&#29575;&#31561;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#39044;&#26465;&#20214;&#22120;&#22312;&#20984;&#21442;&#25968;&#21270;&#19979;&#24494;&#35843;&#20004;&#23618;ReLU&#32593;&#32476;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#26080;&#20851;&#12290;&#36825;&#20010;&#26032;&#30340;Riemannian&#39044;&#26465;&#20214;&#22120;&#22312;&#32463;&#20856;&#30340;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24050;&#32463;&#26377;&#36807;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02304</link><description>&lt;p&gt;
&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20174;&#22320;&#38663;&#24314;&#27169;&#21040;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#39640;&#39057;&#27874;&#20256;&#25773;&#30340;&#39640;&#20445;&#30495;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#27874;&#20256;&#25773;&#27169;&#22411;&#20013;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#36275;&#22815;&#20934;&#30830;&#30340;&#32454;&#27714;&#35299;&#22120;&#36755;&#20986;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#31895;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#31283;&#23450;&#19988;&#24555;&#36895;&#30340;&#27714;&#35299;&#22120;&#36824;&#20801;&#35768;&#20351;&#29992;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;Parareal&#26469;&#25552;&#21462;&#21644;&#32416;&#27491;&#39640;&#39057;&#27874;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Nguyen&#21644;Tsai&#65288;2023&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#31995;&#32479;&#65292;&#23558;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#26694;&#26550;&#20013;&#12290;&#22312;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#21644;Parareal&#26041;&#26696;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#35843;&#30340;&#32467;&#26500;&#22312;&#19981;&#29306;&#29298;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
&lt;/p&gt;</description></item><item><title>&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.02216</link><description>&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02216
&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;Graph Foundation Model&#65292;GFM&#65289;&#26159;&#22270;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#36328;&#19981;&#21516;&#22270;&#21644;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#30340;&#22270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;GFM&#12290;&#26500;&#24314;GFM&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#33021;&#22312;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#30340;&#22270;&#20043;&#38388;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; GFM &#21457;&#23637;&#35270;&#35282;&#65292;&#36890;&#36807;&#20513;&#23548;&#8220;&#22270;&#35789;&#27719;&#34920;&#8221;&#65292;&#21363;&#28508;&#34255;&#20110;&#22270;&#20013;&#30340;&#22522;&#26412;&#21487;&#36801;&#31227;&#21333;&#20803;&#26469;&#32534;&#30721;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20174;&#32593;&#32476;&#20998;&#26512;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#31283;&#23450;&#24615;&#31561;&#37325;&#35201;&#26041;&#38754;&#26469;&#24314;&#31435;&#22270;&#35789;&#27719;&#34920;&#12290;&#36825;&#31181;&#35789;&#27719;&#34920;&#30340;&#35270;&#35282;&#26377;&#21161;&#20110;&#25353;&#29031;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#25512;&#36827;&#26410;&#26469;&#30340;GFM&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;</title><link>https://arxiv.org/abs/2402.02018</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#36229;&#32423;&#35745;&#31639;&#30740;&#31350;&#21644;LLMs&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Position Paper: The Landscape and Challenges of HPC Research and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02018
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22343;&#23637;&#29616;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#20219;&#21153;&#20013;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#26426;&#26500;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#36164;&#28304;&#65292;&#36798;&#21040;&#25110;&#31361;&#30772;&#20102;&#36229;&#32423;&#35745;&#31639;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#23558;&#36825;&#20123;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#35843;&#25972;&#21644;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#23558;&#20250;&#38750;&#24120;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#25105;&#20204;&#19978;&#36848;&#35266;&#28857;&#30340;&#29702;&#30001;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#24819;&#27861;&#22312;HPC&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01995</link><description>&lt;p&gt;
&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#65306;&#31532;&#19968;&#27425;&#36817;&#20284;&#31639;&#27861;&#65292;&#20855;&#26377;&#20840;&#32622;&#20449;&#21306;&#38388;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#65292;&#23558;&#26377;&#38480;&#30340;&#27835;&#30103;&#39044;&#31639;&#20998;&#37197;&#21040;&#21487;&#29992;&#30340;&#39118;&#38505;&#26102;&#38388;&#19978;&#26159;&#20943;&#23569;&#29992;&#25143;&#30130;&#21171;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#23454;&#38469;&#39118;&#38505;&#26102;&#38388;&#25968;&#37327;&#65292;&#36825;&#19968;&#31574;&#30053;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#24341;&#20837;&#36817;&#20284;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#31454;&#20105;&#27604;&#20998;&#26512;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;HeartSteps&#31227;&#21160;&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01734</link><description>&lt;p&gt;
CFTM: &#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFTM: Continuous time fractional topic model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01734
&lt;/p&gt;
&lt;p&gt;
CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;&#65288;cFTM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#26377;&#25928;&#22320;&#35782;&#21035;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;cFTM&#21487;&#20197;&#25429;&#25417;&#21040;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#20013;&#30340;&#36825;&#20123;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#65292;&#21453;&#26144;&#20102;fBm&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;cFTM&#30340;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#19982;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;LDA&#30340;&#30456;&#24403;&#12290;&#20026;&#20102;&#35777;&#26126;cFTM&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#27982;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#20123;&#27979;&#35797;&#30340;&#32467;&#26524;&#25903;&#25345;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#38543;&#26102;&#38388;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2401.18079</link><description>&lt;p&gt;
KVQuant: &#20197;KV&#32531;&#23384;&#37327;&#21270;&#23454;&#29616;1000&#19975;&#19978;&#19979;&#25991;&#38271;&#24230;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18079
&lt;/p&gt;
&lt;p&gt;
KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25688;&#35201;&#31561;&#38656;&#35201;&#22823;&#31383;&#21475;&#19978;&#19979;&#25991;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;KV&#32531;&#23384;&#28608;&#27963;&#25104;&#20026;&#35760;&#24518;&#28040;&#32791;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21387;&#32553;KV&#32531;&#23384;&#28608;&#27963;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#65288;&#22914;&#20302;&#20110;4&#20301;&#65289;&#30340;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KVQuant&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26041;&#27861;&#37327;&#21270;&#32531;&#23384;&#30340;KV&#28608;&#27963;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(i)&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#65292;&#22312;&#37327;&#21270;&#38190;&#28608;&#27963;&#26102;&#35843;&#25972;&#32500;&#24230;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20998;&#24067;&#65307;(ii)RoPE&#21069;&#37327;&#21270;&#38190;&#65292;&#22312;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#20043;&#21069;&#37327;&#21270;&#38190;&#28608;&#27963;&#20197;&#20943;&#36731;&#20854;&#23545;&#37327;&#21270;&#30340;&#24433;&#21709;&#65307;(iii)&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#22312;&#27599;&#23618;&#25512;&#23548;&#20986;&#26435;&#37325;&#24863;&#30693;&#30340;&#38750;&#22343;&#21248;&#25968;&#25454;&#31867;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#39640;&#20110;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#26102;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17760</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#39640;&#20110;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#26102;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#31867;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#20998;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#30149;&#24577;&#26465;&#20214;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#39640;&#20110;&#25110;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#32447;&#24615;&#20272;&#35745;&#22120;&#30340;&#27491;&#21017;&#21270;LDA&#65288;RLDA&#65289;&#26041;&#27861;&#12290;RLDA&#26041;&#27861;&#30340;&#24615;&#33021;&#24050;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#24182;&#24050;&#25552;&#20986;&#20102;&#26368;&#20248;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#38750;&#32447;&#24615;&#65288;NL&#65289;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30456;&#19968;&#33268;&#30340;&#27491;&#21322;&#23450; Ridge &#22411;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#21033;&#29992;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#24471;&#21040;&#20102;&#35813;&#20272;&#35745;&#22120;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#25152;&#25552;&#20986;&#30340;NL-RLDA&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear discriminant analysis (LDA) is a widely used technique for data classification. The method offers adequate performance in many classification problems, but it becomes inefficient when the data covariance matrix is ill-conditioned. This often occurs when the feature space's dimensionality is higher than or comparable to the training data size. Regularized LDA (RLDA) methods based on regularized linear estimators of the data covariance matrix have been proposed to cope with such a situation. The performance of RLDA methods is well studied, with optimal regularization schemes already proposed. In this paper, we investigate the capability of a positive semidefinite ridge-type estimator of the inverse covariance matrix that coincides with a nonlinear (NL) covariance matrix estimator. The estimator is derived by reformulating the score function of the optimal classifier utilizing linear estimation methods, which eventually results in the proposed NL-RLDA classifier. We derive asymptot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2401.16123</link><description>&lt;p&gt;
&#23547;&#27714;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#65311;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#22686;&#37327;&#23398;&#20064;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#36710;&#34892;&#19994;&#36805;&#36895;&#21521;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#36710;&#36742;&#21457;&#23637;&#65292;&#20256;&#32479;&#30340;&#36710;&#36742;&#20132;&#20114;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#35302;&#25720;&#21644;&#35821;&#38899;&#21629;&#20196;&#30340;&#31995;&#32479;&#65289;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#38750;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#24341;&#29992;&#36710;&#36742;&#22806;&#37096;&#29289;&#20307;&#65289;&#20013;&#24050;&#32463;&#21464;&#24471;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36716;&#21521;&#23039;&#21183;&#36755;&#20837;&#65288;&#22914;&#25163;&#21183;&#12289;&#35270;&#32447;&#21644;&#22836;&#37096;&#23039;&#21183;&#25163;&#21183;&#65289;&#20316;&#20026;&#39550;&#39542;&#36807;&#31243;&#20013;&#26356;&#21512;&#36866;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39550;&#39542;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#65292;&#39550;&#39542;&#21592;&#30340;&#23039;&#21183;&#36755;&#20837;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#34429;&#28982;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#22266;&#26377;&#30340;&#21487;&#21464;&#24615;&#21487;&#20197;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#65292;&#20294;&#26222;&#36941;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#38024;&#23545;&#30446;&#26631;&#24341;&#29992;&#20351;&#29992;&#32422;&#26463;&#30340;&#21333;&#23454;&#20363;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#25345;&#32493;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#25361;&#25112;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#65292;&#26377;&#25928;&#23454;&#29616;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.15753</link><description>&lt;p&gt;
&#36890;&#36807;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#23545;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#25361;&#25112;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#65292;&#26377;&#25928;&#23454;&#29616;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#30340;&#22686;&#24378;&#29616;&#23454;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#27169;&#24335;&#65292;&#23427;&#20801;&#35768;&#22806;&#31185;&#21307;&#29983;&#22312;&#33145;&#33108;&#38236;&#22270;&#20687;&#19978;&#25237;&#23556;&#32959;&#30244;&#21644;&#23884;&#20837;&#22312;&#32925;&#33039;&#20869;&#37096;&#30340;&#34880;&#31649;&#65292;&#20197;&#24110;&#21161;&#23450;&#20301;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#26415;&#21069;&#20174;CT&#25110;MRI&#25968;&#25454;&#25552;&#21462;&#30340;3D&#27169;&#22411;&#34987;&#27880;&#20876;&#21040;&#26415;&#20013;&#30340;&#33145;&#33108;&#38236;&#22270;&#20687;&#20013;&#12290;&#20174;3D-2D&#34701;&#21512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#21033;&#29992;&#35299;&#21078;&#26631;&#24535;&#29289;&#26469;&#25351;&#23548;&#27880;&#20876;&#65292;&#36825;&#20123;&#26631;&#24535;&#29289;&#21253;&#25324;&#32925;&#33039;&#30340;&#19979;&#32447;&#12289;&#38203;&#38256;&#38887;&#24102;&#21644;&#38381;&#38145;&#36718;&#24275;&#12290;&#22312;&#33145;&#33108;&#38236;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#20013;&#25163;&#24037;&#26631;&#35760;&#36825;&#20123;&#26631;&#24535;&#29289;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#20351;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#20013;&#33021;&#22815;&#26377;&#25928;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#65288;MICCAI 2022&#65289;&#30340;"&#26415;&#21069;&#21040;&#26415;&#20013;&#33145;&#33108;&#38236;&#34701;&#21512;&#25361;&#25112;"&#65288;P2ILF&#65289;&#20013;&#25552;&#20986;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented reality for laparoscopic liver resection is a visualisation mode that allows a surgeon to localise tumours and vessels embedded within the liver by projecting them on top of a laparoscopic image. Preoperative 3D models extracted from CT or MRI data are registered to the intraoperative laparoscopic images during this process. In terms of 3D-2D fusion, most of the algorithms make use of anatomical landmarks to guide registration. These landmarks include the liver's inferior ridge, the falciform ligament, and the occluding contours. They are usually marked by hand in both the laparoscopic image and the 3D model, which is time-consuming and may contain errors if done by a non-experienced user. Therefore, there is a need to automate this process so that augmented reality can be used effectively in the operating room. We present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge (P2ILF), held during the Medical Imaging and Computer Assisted Interventions (MICCAI 2022)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2401.07836</link><description>&lt;p&gt;
&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#39118;&#38505;&#65306;&#20915;&#23450;&#24615;&#21644;&#32047;&#31215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Two Types of AI Existential Risk: Decisive and Accumulative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#23545;&#20154;&#24037;&#26234;&#33021;(AI)&#24341;&#36215;&#30340;&#23384;&#22312;&#39118;&#38505;(x-risks)&#30340;&#35752;&#35770;&#36890;&#24120;&#38598;&#20013;&#22312;&#30001;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#24341;&#36215;&#30340;&#31361;&#28982;&#12289;&#20005;&#37325;&#20107;&#20214;&#19978;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#21487;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#20107;&#20214;&#23558;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#35201;&#20040;&#23548;&#33268;&#20154;&#31867;&#28781;&#32477;&#65292;&#35201;&#20040;&#26080;&#27861;&#36870;&#36716;&#22320;&#20351;&#20154;&#31867;&#25991;&#26126;&#38519;&#20837;&#26080;&#27861;&#24674;&#22797;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35752;&#35770;&#32463;&#24120;&#24573;&#35270;AI x-risk&#36880;&#28176;&#36890;&#36807;&#19968;&#31995;&#21015;&#36739;&#23567;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#20013;&#26029;&#36880;&#28176;&#26174;&#29616;&#20986;&#26469;&#30340;&#20005;&#37325;&#21487;&#33021;&#24615;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#36328;&#36234;&#20851;&#38190;&#38408;&#20540;&#12290;&#35813;&#35770;&#25991;&#23558;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#36827;&#34892;&#23545;&#27604;&#12290;&#21069;&#32773;&#25551;&#32472;&#20102;&#19968;&#31181;&#26126;&#26174;&#30340;AI&#25509;&#31649;&#36335;&#24452;&#65292;&#20854;&#29305;&#24449;&#26159;&#26080;&#27861;&#25511;&#21046;&#30340;&#36229;&#32423;&#26234;&#33021;&#31561;&#24773;&#26223;&#65292;&#32780;&#21518;&#32773;&#21017;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#23548;&#33268;&#28781;&#32477;&#24615;&#28798;&#38590;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#36825;&#28041;&#21450;&#21040;&#30001;AI&#24341;&#36215;&#30340;&#20005;&#37325;&#23041;&#32961;&#30340;&#36880;&#28176;&#32047;&#31215;&#65292;&#20363;&#22914;&#20005;&#37325;&#30340;&#28431;&#27934;&#21644;&#31995;&#32479;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26469;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.12275</link><description>&lt;p&gt;
&#20174;&#22122;&#22768;&#33976;&#39311;&#20013;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Emergence of In-Context Reinforcement Learning from Noise Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26469;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#21464;&#24418;&#37329;&#21018;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#30446;&#21069;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#30001;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#25110;&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AD$^\varepsilon$&#65292;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22122;&#22768;&#35825;&#23548;&#30340;&#35838;&#31243;&#26469;&#23454;&#29616;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26500;&#24314;&#19968;&#20010;&#24110;&#21161;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#30340;&#21512;&#25104;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26080;&#38656;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#29983;&#25104;&#65292;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#33021;&#22815;&#20197;2&#20493;&#30340;&#36793;&#30028;&#20248;&#20110;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, extensive studies in Reinforcement Learning have been carried out on the ability of transformers to adapt in-context to various environments and tasks. Current in-context RL methods are limited by their strict requirements for data, which needs to be generated by RL agents or labeled with actions from an optimal policy. In order to address this prevalent problem, we propose AD$^\varepsilon$, a new data acquisition approach that enables in-context Reinforcement Learning from noise-induced curriculum. We show that it is viable to construct a synthetic noise injection curriculum which helps to obtain learning histories. Moreover, we experimentally demonstrate that it is possible to alleviate the need for generation using optimal policies, with in-context RL still able to outperform the best suboptimal policy in a learning dataset by a 2x margin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#23494;&#24230;&#65292;&#35782;&#21035;&#24322;&#24120;&#23454;&#20363;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#28165;&#27905;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2312.11549</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Label-Free Multivariate Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11549
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#23494;&#24230;&#65292;&#35782;&#21035;&#24322;&#24120;&#23454;&#20363;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#28165;&#27905;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#21333;&#31867;&#20998;&#31867;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#21333;&#31867;&#20998;&#31867;&#20013;&#65292;&#35757;&#32451;&#26679;&#26412;&#34987;&#20551;&#35774;&#20026;&#27491;&#24120;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24456;&#38590;&#20445;&#35777;&#12290;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#38477;&#20302;&#22522;&#20110;&#21333;&#31867;&#20998;&#31867;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#35757;&#32451;&#20998;&#24067;&#25311;&#21512;&#20026;&#27491;&#24577;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MTGFlow&#65292;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#22270;&#21644;&#23454;&#20307;&#24863;&#30693;&#30340;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;MTGFlow&#39318;&#20808;&#20272;&#35745;&#25972;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#23494;&#24230;&#65292;&#28982;&#21518;&#26681;&#25454;&#27979;&#35797;&#26679;&#26412;&#22312;&#25311;&#21512;&#20998;&#24067;&#20869;&#30340;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#23454;&#20363;&#12290;&#36825;&#20381;&#36182;&#20110;&#19968;&#20010;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#20551;&#35774;&#65292;&#21363;&#24322;&#24120;&#23454;&#20363;&#30340;&#23494;&#24230;&#27604;&#27491;&#24120;&#23454;&#20363;&#26356;&#31232;&#30095;&#65292;&#19981;&#20381;&#36182;&#20110;&#28165;&#27905;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#21644;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#65292;&#30452;&#25509;&#20272;&#35745;&#23494;&#24230;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in multivariate time series (MTS) has been widely studied in one-class classification (OCC) setting. The training samples in OCC are assumed to be normal, which is difficult to guarantee in practical situations. Such a case may degrade the performance of OCC-based anomaly detection methods which fit the training distribution as the normal distribution. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for MTS anomaly detection via dynamic Graph and entity-aware normalizing Flow. MTGFlow first estimates the density of the entire training samples and then identifies anomalous instances based on the density of the test samples within the fitted distribution. This relies on a widely accepted assumption that anomalous instances exhibit more sparse densities than normal ones, with no reliance on the clean training dataset. However, it is intractable to directly estimate the density due to complex dependencies among entities and their diverse inhe
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#20559;&#24046;&#30340;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#31216;&#20026;GreedyFed&#65292;&#23427;&#21033;&#29992;&#24555;&#36895;&#36924;&#36817;&#31639;&#27861;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#30340;Shapley&#20540;&#65292;&#24182;&#22312;&#27599;&#20010;&#36890;&#20449;&#24490;&#29615;&#20013;&#36138;&#23146;&#22320;&#36873;&#25321;&#26368;&#20855;&#36129;&#29486;&#30340;&#23458;&#25143;&#12290;&#19982;&#20854;&#20182;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#30456;&#27604;&#65292;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.09108</link><description>&lt;p&gt;
&#36138;&#24515;Shapley&#23458;&#25143;&#36873;&#25321;&#29992;&#20110;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Greedy Shapley Client Selection for Communication-Efficient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#20559;&#24046;&#30340;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#31216;&#20026;GreedyFed&#65292;&#23427;&#21033;&#29992;&#24555;&#36895;&#36924;&#36817;&#31639;&#27861;&#35745;&#31639;&#27599;&#20010;&#23458;&#25143;&#30340;Shapley&#20540;&#65292;&#24182;&#22312;&#27599;&#20010;&#36890;&#20449;&#24490;&#29615;&#20013;&#36138;&#23146;&#22320;&#36873;&#25321;&#26368;&#20855;&#36129;&#29486;&#30340;&#23458;&#25143;&#12290;&#19982;&#20854;&#20182;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#30456;&#27604;&#65292;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#20998;&#24067;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20043;&#38388;&#26174;&#30528;&#24322;&#36136;&#24615;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#22312;&#20256;&#32479;&#23458;&#25143;&#36873;&#25321;&#31639;&#27861;&#20013;&#24120;&#24120;&#23384;&#22312;&#20559;&#24046;&#65292;&#28041;&#21450;&#23545;&#23458;&#25143;&#30340;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#12290;&#36825;&#24050;&#34987;&#35777;&#26126;&#22312;&#23454;&#38469;&#35774;&#32622;&#19979;&#24555;&#36895;&#25910;&#25947;&#26159;&#27425;&#20248;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30001;&#20110;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#36890;&#20449;&#26426;&#20250;&#26377;&#38480;&#25152;&#23548;&#33268;&#30340;&#26102;&#38480;&#32422;&#26463;&#24212;&#29992;&#65292;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#23545;&#20110;&#22312;&#22266;&#23450;&#36890;&#20449;&#24490;&#29615;&#39044;&#31639;&#20869;&#23436;&#25104;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#20559;&#24046;&#30340;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#31216;&#20026;GreedyFed&#65292;&#23427;&#22312;&#27599;&#20010;&#36890;&#20449;&#24490;&#29615;&#20013;&#35782;&#21035;&#24182;&#36138;&#23146;&#22320;&#36873;&#25321;&#26368;&#20855;&#36129;&#29486;&#30340;&#23458;&#25143;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#24555;&#36895;&#36924;&#36817;&#31639;&#27861;&#26469;&#35745;&#31639;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#30340;Shapley Value&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#35768;&#22810;&#23458;&#25143;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#20197;&#36827;&#34892;&#21487;&#34892;&#30340;&#35745;&#31639;&#12290;&#36890;&#36807;&#19982;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;GreedyFed&#21487;&#20197;&#26356;&#22909;&#22320;&#36798;&#21040;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard client selection algorithms for Federated Learning (FL) are often unbiased and involve uniform random sampling of clients. This has been proven sub-optimal for fast convergence under practical settings characterized by significant heterogeneity in data distribution, computing, and communication resources across clients. For applications having timing constraints due to limited communication opportunities with the parameter server (PS), the client selection strategy is critical to complete model training within the fixed budget of communication rounds. To address this, we develop a biased client selection strategy, GreedyFed, that identifies and greedily selects the most contributing clients in each communication round. This method builds on a fast approximation algorithm for the Shapley Value at the PS, making the computation tractable for real-world applications with many clients. Compared to various client selection strategies on several real-world datasets, GreedyFed de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#20256;&#32479;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#26102;&#23384;&#22312;&#24179;&#20961;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#21407;&#21017;&#26469;&#36991;&#20813;&#27492;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#22240;&#24335;&#20998;&#35299;&#35299;&#37322;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05596</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#24335;&#20998;&#35299;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Factorized Explainer for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#20256;&#32479;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#26102;&#23384;&#22312;&#24179;&#20961;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#21407;&#21017;&#26469;&#36991;&#20813;&#27492;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#22240;&#24335;&#20998;&#35299;&#35299;&#37322;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#24320;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20107;&#21518;&#23454;&#20363;&#32423;&#35299;&#37322;&#26041;&#27861;&#26469;&#29702;&#35299;GNN&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21457;&#29616;&#35299;&#37322;&#24050;&#35757;&#32451;&#30340;GNN&#39044;&#27979;&#34892;&#20026;&#30340;&#23376;&#32467;&#26500;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#35299;&#37322;&#20219;&#21153;&#65292;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21407;&#21017;&#30340;&#20256;&#32479;&#26041;&#27861;&#23384;&#22312;&#19981;&#33021;&#19982;&#21487;&#35299;&#37322;&#24615;&#27010;&#24565;&#19968;&#33268;&#30340;&#24179;&#20961;&#35299;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;GIB&#21407;&#21017;&#26469;&#36991;&#20813;&#19978;&#36848;&#24179;&#20961;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#26032;&#22411;&#22240;&#24335;&#20998;&#35299;&#35299;&#37322;&#27169;&#22411;&#12290;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;GIB&#21407;&#21017;&#26469;&#20998;&#26512;&#25152;&#25552;&#20986;&#30340;&#22240;&#24335;&#20998;&#35299;&#35299;&#37322;&#22120;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world data
&lt;/p&gt;</description></item><item><title>DiSK&#26159;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2312.05253</link><description>&lt;p&gt;
DiSK: &#19968;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiSK: A Diffusion Model for Structured Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05253
&lt;/p&gt;
&lt;p&gt;
DiSK&#26159;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#65288;&#31867;&#20284;&#23383;&#20856;&#30340;&#65289;&#25968;&#25454;&#23545;&#20110;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22240;&#20026;&#26684;&#24335;&#21644;&#23646;&#24615;&#21576;&#29616;&#30340;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#32780;&#38590;&#20197;&#22788;&#29702;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;&#26631;&#31614;&#29983;&#25104;&#27169;&#22411;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#38480;&#21046;&#65292;&#27604;&#22914;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;Diffusion Models of Structured Knowledge&#65288;DiSK&#65289; - &#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;DiSK&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#22788;&#29702;&#25968;&#23383;&#26102;&#25552;&#39640;&#31934;&#30830;&#24230;&#12290;&#23427;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DiSK&#22312;&#36229;&#36807;15&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#21512;&#25104;&#21644;&#22635;&#20805;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;DiSK&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#25805;&#20316;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Structured (dictionary-like) data presents challenges for left-to-right language models, as they can struggle with structured entities for a wide variety of reasons such as formatting and sensitivity to the order in which attributes are presented. Tabular generative models suffer from a different set of limitations such as their lack of flexibility. We introduce Diffusion Models of Structured Knowledge (DiSK) - a new architecture and training approach specialized for structured data. DiSK handles text, categorical, and continuous numerical data using a Gaussian mixture model approach, which allows for improved precision when dealing with numbers. It employs diffusion training to model relationships between properties. Experiments demonstrate DiSK's state-of-the-art performance on tabular data modeling, synthesis, and imputation on over 15 datasets across diverse domains. DiSK provides an effective inductive bias for generative modeling and manipulation of structured data. The technique
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#26368;&#30701;&#36317;&#31163;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#29992;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20449;&#24687;&#21033;&#29992;&#19978;&#30340;&#22256;&#38590;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.01538</link><description>&lt;p&gt;
&#36882;&#24402;&#36317;&#31163;&#36807;&#28388;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recurrent Distance Filtering for Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#26368;&#30701;&#36317;&#31163;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#29992;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20449;&#24687;&#21033;&#29992;&#19978;&#30340;&#22256;&#38590;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#19968;&#36339;&#20449;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#25928;&#21033;&#29992;&#36828;&#36317;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#22270;&#21464;&#25442;&#22120;&#20801;&#35768;&#27599;&#20010;&#33410;&#28857;&#30452;&#25509;&#20851;&#27880;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#65292;&#20294;&#32570;&#20047;&#22270;&#30340;&#24402;&#32435;&#20559;&#24046;&#24182;&#19988;&#24517;&#39035;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#33258;&#20110;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;&#25552;&#20379;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#30446;&#26631;&#33410;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#30446;&#26631;&#33410;&#28857;&#21040;&#20854;&#20182;&#33410;&#28857;&#30340;&#26368;&#30701;&#36317;&#31163;&#26469;&#32858;&#21512;&#20854;&#20182;&#33410;&#28857;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;RNN&#23545;&#36339;&#20195;&#34920;&#30340;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#32447;&#24615;RNN&#20197;&#29305;&#23450;&#23545;&#35282;&#24418;&#24335;&#21442;&#25968;&#21270;&#65292;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#38271;&#36317;&#31163;&#20449;&#21495;&#20256;&#25773;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#32534;&#30721;&#37051;&#23621;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is highly competitive compared with 
&lt;/p&gt;</description></item><item><title>RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.00639</link><description>&lt;p&gt;
RefinedFields: &#23545;&#26080;&#32422;&#26463;&#22330;&#26223;&#30340;&#36752;&#23556;&#22330;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00639
&lt;/p&gt;
&lt;p&gt;
RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#32422;&#26463;&#30340;&#22270;&#20687;&#20013;&#24314;&#27169;&#22823;&#22330;&#26223;&#34987;&#35777;&#26126;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#26159;&#22312;&#23553;&#38381;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23545;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#33719;&#24471;&#30340;&#20808;&#39564;&#26465;&#20214;&#36827;&#34892;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RefinedFields&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25913;&#21892;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#20351;&#29992;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#26469;&#32454;&#21270;K-Planes&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#26053;&#28216;&#29031;&#29255;&#38598;&#19978;&#30340;&#20248;&#28857;&#12290;RefinedFields&#22686;&#24378;&#20102;&#28210;&#26579;&#22330;&#26223;&#30340;&#32454;&#33410;&#65292;&#20248;&#20110;&#20197;&#24448;&#22312;&#37326;&#22806;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#21487;&#20197;&#22312;https://refinedfields.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2311.14220</link><description>&lt;p&gt;
&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Assumption-lean and Data-adaptive Post-Prediction Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#26082;&#32791;&#36153;&#26102;&#38388;&#21448;&#36153;&#21147;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31185;&#23398;&#23478;&#20204;&#20381;&#36182;&#20110;ML&#31639;&#27861;&#20351;&#29992;&#26131;&#24471;&#30340;&#21327;&#21464;&#37327;&#26469;&#39044;&#27979;&#36825;&#20123;&#40644;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#24120;&#24120;&#30452;&#25509;&#29992;&#20110;&#21518;&#32493;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#65292;&#24573;&#30053;&#20102;&#39044;&#27979;&#36807;&#31243;&#24341;&#20837;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#27491;&#38754;&#32467;&#26524;&#21644;&#26080;&#25928;&#30340;&#31185;&#23398;&#32467;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#23427;&#20801;&#35768;&#22522;&#20110;ML&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#21644;&#26377;&#21147;&#30340;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#20551;&#35774;&#31616;&#21270;&#8221;&#23646;&#24615;&#20445;&#35777;&#22312;&#24191;&#27867;&#30340;&#32479;&#35745;&#37327;&#19978;&#19981;&#22522;&#20110;ML&#39044;&#27979;&#20570;&#20986;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#25968;&#25454;&#33258;&#36866;&#24212;&#8221;&#29305;&#24615;&#20445;&#35777;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. With the rapid development of machine learning (ML), scientists have relied on ML algorithms to predict these gold-standard outcomes with easily obtained covariates. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce an assumption-lean and data-adaptive Post-Prediction Inference (POP-Inf) procedure that allows valid and powerful inference based on ML-predicted outcomes. Its "assumption-lean" property guarantees reliable statistical inference without assumptions on the ML-prediction, for a wide range of statistical quantities. Its "data-adaptive'" feature guarantees an efficiency gain over existing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#24335;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#20998;&#37197;&#21333;&#35789;&#26631;&#31614;&#26469;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20027;&#39064;&#26356;&#21152;&#30456;&#20851;&#21644;&#20934;&#30830;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#21487;&#35757;&#32451;&#21644;&#21518;&#35757;&#32451;&#38598;&#25104;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.09438</link><description>&lt;p&gt;
&#26631;&#35760;&#20132;&#20114;&#24335;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Labeled Interactive Topic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#24335;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#20998;&#37197;&#21333;&#35789;&#26631;&#31614;&#26469;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20027;&#39064;&#26356;&#21152;&#30456;&#20851;&#21644;&#20934;&#30830;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#21487;&#35757;&#32451;&#21644;&#21518;&#35757;&#32451;&#38598;&#25104;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#37327;&#25991;&#26723;&#38598;&#21512;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#26159;&#23427;&#20204;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#20027;&#39064;&#12290;&#20256;&#32479;&#30340;&#27010;&#29575;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#20027;&#39064;&#27169;&#22411;&#25552;&#20379;&#20102;&#20801;&#35768;&#29992;&#25143;&#24341;&#23548;&#27169;&#22411;&#25351;&#21521;&#26356;&#30456;&#20851;&#20027;&#39064;&#30340;&#20132;&#20114;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#32570;&#20047;&#36825;&#31181;&#20132;&#20114;&#21151;&#33021;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20132;&#20114;&#26041;&#27861;&#12290;&#36825;&#31181;&#20132;&#20114;&#20801;&#35768;&#29992;&#25143;&#20026;&#19968;&#20010;&#20027;&#39064;&#20998;&#37197;&#19968;&#20010;&#21333;&#35789;&#26631;&#31614;&#65292;&#20174;&#32780;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#20027;&#39064;&#20013;&#30340;&#21333;&#35789;&#19982;&#32473;&#23450;&#30340;&#26631;&#31614;&#23494;&#20999;&#23545;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#21253;&#25324;&#20027;&#39064;&#23884;&#20837;&#21487;&#35757;&#32451;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28436;&#21464;&#30340;&#27169;&#22411;&#12290;&#31532;&#20108;&#31181;&#28041;&#21450;&#20027;&#39064;&#23884;&#20837;&#21518;&#35757;&#32451;&#38598;&#25104;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#20027;&#39064;&#32454;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#26041;&#20415;&#29992;&#25143;&#19982;&#36825;&#20123;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are valuable for understanding extensive document collections, but they don't always identify the most relevant topics. Classical probabilistic and anchor-based topic models offer interactive versions that allow users to guide the models towards more pertinent topics. However, such interactive features have been lacking in neural topic models. To correct this lacuna, we introduce a user-friendly interaction for neural topic models. This interaction permits users to assign a word label to a topic, leading to an update in the topic model where the words in the topic become closely aligned with the given label. Our approach encompasses two distinct kinds of neural topic models. The first includes models where topic embeddings are trainable and evolve during the training process. The second kind involves models where topic embeddings are integrated post-training, offering a different approach to topic refinement. To facilitate user interaction with these neural topic models, w
&lt;/p&gt;</description></item><item><title>PACOH-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#24341;&#20837;&#27491;&#21017;&#21270;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#30340;&#21160;&#24577;&#29615;&#22659;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2311.07558</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#20803;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Task Generalization via Probabilistic Model-based Meta Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07558
&lt;/p&gt;
&lt;p&gt;
PACOH-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#24341;&#20837;&#27491;&#21017;&#21270;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#30340;&#21160;&#24577;&#29615;&#22659;&#30340;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;PACOH-RL&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#25511;&#21046;&#31574;&#30053;&#35843;&#25972;&#21040;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#12290;PACOH-RL&#20803;&#23398;&#20064;&#20102;&#21160;&#24577;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#20197;&#26368;&#23567;&#30340;&#20132;&#20114;&#25968;&#25454;&#23545;&#26032;&#30340;&#21160;&#24577;&#29615;&#22659;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#12290;&#29616;&#26377;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#20803;&#23398;&#20064;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#31561;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#36739;&#39640;&#30340;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;PACOH-RL&#22312;&#20803;&#23398;&#20064;&#21644;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#37117;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#24403;&#38754;&#23545;&#26032;&#30340;&#21160;&#24577;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#26377;&#25928;&#22320;&#24341;&#23548;&#25506;&#32034;&#21644;&#25968;&#25454;&#25910;&#38598;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20351;&#24471;&#22312;&#20808;&#21069;&#20219;&#21153;&#25110;&#21160;&#24577;&#29615;&#22659;&#25968;&#25454;&#26497;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30456;&#27604;&#65292;PACOH-RL&#22312;&#36866;&#24212;&#26032;&#30340;&#21160;&#24577;&#26465;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning (Meta-RL) algorithm designed to efficiently adapt control policies to changing dynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift adaptation to new dynamics with minimal interaction data. Existing Meta-RL methods require abundant meta-learning data, limiting their applicability in settings such as robotics, where data is costly to obtain. To address this, PACOH-RL incorporates regularization and epistemic uncertainty quantification in both the meta-learning and task adaptation stages. When facing new dynamics, we use these uncertainty estimates to effectively guide exploration and data collection. Overall, this enables positive transfer, even when access to data from prior tasks or dynamic settings is severely limited. Our experiment results demonstrate that PACOH-RL outperforms model-based RL and model-based Meta-RL baselines in adapting to new dynamic conditions. Finally, on a real roboti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.06973</link><description>&lt;p&gt;
&#20998;&#26512;&#39564;&#35777;&#21516;&#27493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#23454;&#26102;&#19981;&#21487;&#35266;&#27979;&#20998;&#24067;&#31995;&#32479;&#30340;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20010;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#29366;&#24577;&#20272;&#35745;&#22120;&#22312;&#36755;&#20837;&#27979;&#37327;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#20998;&#26512;&#30028;&#38480;&#12290;&#24050;&#32463;&#26377;&#20154;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24615;&#33021;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#22320;&#35828;&#26126;&#35757;&#32451;&#22909;&#30340;DNN&#22788;&#29702;&#36755;&#20837;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25200;&#21160;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#38382;&#39064;&#20174;&#20998;&#26512;&#19978;&#39564;&#35777;&#20102;DNN&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#35299;&#20915;MILP&#20844;&#24335;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#21516;&#27493;&#26102;&#38388;&#30340;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#26469;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#20004;&#20010;&#31995;&#32479;&#37117;&#26159;&#36890;&#36807;&#24494;&#30456;&#20301;&#27979;&#37327;&#19981;&#23436;&#20840;&#35266;&#27979;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(RawHFL)&#65292;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#20197;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#36890;&#36807;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;CPU&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;RawHFL&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2311.06918</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(RawHFL)&#65292;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#35270;&#39057;&#32531;&#23384;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#20197;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#65292;&#36890;&#36807;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;CPU&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;RawHFL&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#32531;&#23384;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#29992;&#25143;&#39057;&#32321;&#35831;&#27714;&#30340;&#28909;&#38376;&#20869;&#23481;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22238;&#20256;&#27969;&#37327;&#25317;&#22622;&#38382;&#39064;&#12290;&#20026;&#20102;&#23398;&#20064;&#29992;&#25143;&#38656;&#27714;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#32780;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;(RawHFL)&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#12290;&#32771;&#34385;&#21040;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20840;&#23616;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#35813;&#33539;&#25968;&#21462;&#20915;&#20110;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;&#26080;&#32447;&#38142;&#36335;&#19978;&#32047;&#31215;&#26799;&#24230;&#30340;&#25104;&#21151;&#25509;&#25910;&#12290;&#22312;&#24310;&#36831;&#12289;&#33021;&#32791;&#21644;&#26080;&#32447;&#36164;&#28304;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#20248;&#21270;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#12289;&#26412;&#22320;&#35757;&#32451;&#36718;&#27425;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;(CPU)&#39057;&#29575;&#65292;&#20197;&#26368;&#23567;&#21270;&#19968;&#20010;&#21152;&#26435;&#25928;&#29992;&#20989;&#25968;&#65292;&#20174;&#32780;&#20419;&#36827;RawHFL&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video caching can significantly improve backhaul traffic congestion by locally storing the popular content that users frequently request. A privacy-preserving method is desirable to learn how users' demands change over time. As such, this paper proposes a novel resource-aware hierarchical federated learning (RawHFL) solution to predict users' future content requests under the realistic assumptions that content requests are sporadic and users' datasets can only be updated based on the requested content's information. Considering a partial client participation case, we first derive the upper bound of the global gradient norm that depends on the clients' local training rounds and the successful reception of their accumulated gradients over the wireless links. Under delay, energy and radio resource constraints, we then optimize client selection and their local rounds and central processing unit (CPU) frequencies to minimize a weighted utility function that facilitates RawHFL's convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#25903;&#32593;&#32476;&#30340;&#24555;&#36895;&#21551;&#21457;&#24335;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#25509;&#28304;&#21644;&#30446;&#26631;&#30340;&#30452;&#32447;&#27573;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#22312;&#20998;&#25903;&#32467;&#26500;&#23384;&#22312;&#26102;&#30340;&#19981;&#36866;&#29992;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.06650</link><description>&lt;p&gt;
&#20998;&#25903;&#32593;&#32476;&#20013;&#30340;&#21551;&#21457;&#24335;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Heuristic Optimal Transport in Branching Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#25903;&#32593;&#32476;&#30340;&#24555;&#36895;&#21551;&#21457;&#24335;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36830;&#25509;&#28304;&#21644;&#30446;&#26631;&#30340;&#30452;&#32447;&#27573;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#22312;&#20998;&#25903;&#32467;&#26500;&#23384;&#22312;&#26102;&#30340;&#19981;&#36866;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25104;&#26412;&#65288;&#36890;&#24120;&#23450;&#20041;&#20026;&#36317;&#31163;&#20989;&#25968;&#65289;&#26469;&#23398;&#20064;&#28304;&#21040;&#30446;&#26631;&#30340;&#26144;&#23556;&#12290;&#35813;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#29702;&#24819;&#36830;&#25509;&#28304;&#21644;&#30446;&#26631;&#30340;&#30452;&#32447;&#27573;&#32452;&#25104;&#65292;&#27809;&#26377;&#20998;&#25903;&#32467;&#26500;&#12290;&#36825;&#20123;&#26368;&#20248;&#35299;&#19982;&#33258;&#28982;&#21644;&#20154;&#36896;&#36816;&#36755;&#32593;&#32476;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#26222;&#36941;&#23384;&#22312;&#20998;&#25903;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#24555;&#36895;&#21551;&#21457;&#24335;&#20998;&#25903;&#26041;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#20013;&#30340;&#26368;&#20248;&#20256;&#36755;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#26679;&#20363;&#12289;&#31616;&#21270;&#24515;&#34880;&#31649;&#32593;&#32476;&#20197;&#21450;&#8220;&#22307;&#35806;&#32769;&#20154;&#8221;&#20998;&#37197;&#32593;&#32476;&#30340;&#22810;&#20010;&#25968;&#20540;&#24212;&#29992;&#65292;&#35813;&#20998;&#37197;&#32593;&#32476;&#21253;&#25324;&#20840;&#29699;141,182&#20010;&#24050;&#30693;&#20301;&#32622;&#21644;&#20154;&#21475;&#30340;&#22478;&#24066;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport aims to learn a mapping of sources to targets by minimizing the cost, which is typically defined as a function of distance. The solution to this problem consists of straight line segments optimally connecting sources to targets, and it does not exhibit branching. These optimal solutions are in stark contrast with both natural, and man-made transportation networks, where branching structures are prevalent. Here we discuss a fast heuristic branching method for optimal transport in networks. We also provide several numerical applications to synthetic examples, a simplified cardiovascular network, and the "Santa Claus" distribution network which includes 141,182 cities around the world, with known location and population.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#20010;&#20307;&#38388;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#19981;&#21516;&#20010;&#20307;&#30340;&#27963;&#21160;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26367;&#20195;&#20256;&#32479;&#25209;&#24402;&#19968;&#21270;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2310.18562</link><description>&lt;p&gt;
&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#36328;&#20154;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#20010;&#20307;&#38388;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#19981;&#21516;&#20010;&#20307;&#30340;&#27963;&#21160;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26367;&#20195;&#20256;&#32479;&#25209;&#24402;&#19968;&#21270;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27963;&#21160;&#27169;&#24335;&#22312;&#20010;&#20307;&#38388;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#65292;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#12290;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#21033;&#29992;&#27979;&#35797;&#25968;&#25454;&#27969;&#22312;&#23454;&#26102;&#25512;&#29702;&#20013;&#35843;&#25972;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#36825;&#22312;HAR&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TTA&#31639;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26080;&#27861;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#12290;OFTTA&#20197;&#26080;&#38656;&#20248;&#21270;&#30340;&#26041;&#24335;&#21516;&#26102;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#25209;&#24402;&#19968;&#21270;&#65288;CBN&#65289;&#23618;&#12290;EDTN&#36890;&#36807;&#32467;&#21512;CBN&#21644;&#27979;&#35797;&#26102;&#25209;&#24402;&#19968;&#21270;&#65288;TBN&#65289;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#20197;&#24212;&#23545;&#39046;&#22495;&#20559;&#31227;&#65292;&#20854;&#20013;TBN&#30340;&#24433;&#21709;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) models often suffer from performance degradation in real-world applications due to distribution shifts in activity patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning paradigm that aims to utilize the test stream to adjust predictions in real-time inference, which has not been explored in HAR before. However, the high computational cost of optimization-based TTA algorithms makes it intractable to run on resource-constrained edge devices. In this paper, we propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier simultaneously in an optimization-free manner. For the feature extractor, we propose Exponential DecayTest-time Normalization (EDTN) to replace the conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time batch Normalization (TBN) to extract reliable features against domain shifts with TBN's influence decrea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEAM&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;</title><link>https://arxiv.org/abs/2310.00841</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#36827;&#34892;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Drug Discovery with Dynamic Goal-aware Fragments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEAM&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29255;&#27573;&#30340;&#33647;&#29289;&#21457;&#29616;&#26159;&#22312;&#24222;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#20013;&#21457;&#29616;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#29255;&#27573;&#25552;&#21462;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#30446;&#26631;&#21270;&#23398;&#24615;&#36136;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#29255;&#27573;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#33021;&#20351;&#29992;&#29983;&#25104;&#36807;&#31243;&#20013;&#26032;&#21457;&#29616;&#30340;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#26469;&#26356;&#26032;&#29255;&#27573;&#35789;&#27719;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#25552;&#21462;&#12289;&#32452;&#35013;&#21644;&#20462;&#25913;&#65288;GEAM&#65289;&#12290;GEAM&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65292;&#20998;&#21035;&#36127;&#36131;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#25552;&#21462;&#12289;&#29255;&#27573;&#32452;&#35013;&#21644;&#29255;&#27573;&#20462;&#25913;&#12290;&#29255;&#27573;&#25552;&#21462;&#27169;&#22359;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#35782;&#21035;&#23545;&#25152;&#38656;&#30446;&#26631;&#29305;&#24615;&#26377;&#36129;&#29486;&#30340;&#37325;&#35201;&#29255;&#27573;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#30446;&#26631;&#24863;&#30693;&#29255;&#27573;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named Goal-aware fragment Extraction, Assembly, and Modification (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments contributing to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#22791;&#32447;&#24615;&#24490;&#29615;&#23618;&#21644;&#24102;&#26377;&#20056;&#27861;&#38376;&#25511;&#30340;&#21069;&#39304;&#36335;&#24452;&#30340;RNN&#21487;&#20197;&#31934;&#30830;&#22320;&#23454;&#29616;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;Transformer&#30340;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#12290;&#36870;&#21521;&#24037;&#31243;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#19979;&#38477;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20102;&#35813;&#26500;&#36896;&#65292;&#24182;&#20351;RNN&#20855;&#22791;&#19982;Transformer&#30456;&#21516;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.01775</link><description>&lt;p&gt;
&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gated recurrent neural networks discover attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#22791;&#32447;&#24615;&#24490;&#29615;&#23618;&#21644;&#24102;&#26377;&#20056;&#27861;&#38376;&#25511;&#30340;&#21069;&#39304;&#36335;&#24452;&#30340;RNN&#21487;&#20197;&#31934;&#30830;&#22320;&#23454;&#29616;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;Transformer&#30340;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#12290;&#36870;&#21521;&#24037;&#31243;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#19979;&#38477;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20102;&#35813;&#26500;&#36896;&#65292;&#24182;&#20351;RNN&#20855;&#22791;&#19982;Transformer&#30456;&#21516;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26550;&#26500;&#21457;&#23637;&#20351;&#24471;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#26576;&#20123;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#36798;&#21040;&#29978;&#33267;&#36229;&#36234;Transformer&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#29616;&#20195;RNN&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#35774;&#35745;&#27169;&#24335;&#65306;&#32447;&#24615;&#24490;&#29615;&#23618;&#36890;&#36807;&#24102;&#26377;&#20056;&#27861;&#38376;&#25511;&#30340;&#21069;&#39304;&#36335;&#24452;&#30456;&#20114;&#36830;&#25509;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#22791;&#36825;&#20004;&#20010;&#35774;&#35745;&#20803;&#32032;&#30340;RNN&#21487;&#20197;&#31934;&#30830;&#22320;&#23454;&#29616;&#65288;&#32447;&#24615;&#65289;&#33258;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;Transformer&#30340;&#20027;&#35201;&#26500;&#24314;&#27169;&#22359;&#12290;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#19968;&#32452;&#32463;&#36807;&#35757;&#32451;&#30340;RNN&#65292;&#25105;&#20204;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20102;&#25105;&#20204;&#30340;&#26500;&#36896;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#35757;&#32451;&#26377;&#32032;&#35299;&#20915;&#31616;&#21333;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;RNN&#65292;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#32473;&#25105;&#20204;&#30340;RNN&#27880;&#20837;&#20102;&#19982;Transformer&#20351;&#29992;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#30456;&#21516;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20056;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26263;&#31034;&#26576;&#20123;RNN&#21487;&#33021;&#24847;&#22806;&#22320;&#23454;&#29616;&#20102;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention und
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;c-&#20985;&#20989;&#25968;&#65292;&#25105;&#20204;&#22312;&#27969;&#24418;&#19978;&#23450;&#20041;&#20102;&#39640;&#32500;&#21464;&#37327;&#30340;&#26465;&#20214;&#21521;&#37327;&#20998;&#20301;&#25968;&#20989;&#25968;&#65288;M-CVQFs&#65289;&#65292;&#23454;&#29616;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#12289;&#22238;&#24402;&#21644;&#26465;&#20214;&#32622;&#20449;&#38598;&#21644;&#20284;&#28982;&#24230;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2307.01037</link><description>&lt;p&gt;
&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20998;&#20301;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Vector Quantile Regression on Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;c-&#20985;&#20989;&#25968;&#65292;&#25105;&#20204;&#22312;&#27969;&#24418;&#19978;&#23450;&#20041;&#20102;&#39640;&#32500;&#21464;&#37327;&#30340;&#26465;&#20214;&#21521;&#37327;&#20998;&#20301;&#25968;&#20989;&#25968;&#65288;M-CVQFs&#65289;&#65292;&#23454;&#29616;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#12289;&#22238;&#24402;&#21644;&#26465;&#20214;&#32622;&#20449;&#38598;&#21644;&#20284;&#28982;&#24230;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32473;&#23450;&#35299;&#37322;&#24615;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#20272;&#35745;&#30446;&#26631;&#21464;&#37327;&#26465;&#20214;&#20998;&#20301;&#25968;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290; QR&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#26159;&#19968;&#32500;&#30340;&#65292;&#24182;&#19988;&#22312;&#27431;&#20960;&#37324;&#24503;&#22495;&#19978;&#23450;&#20041;&#12290;&#23613;&#31649;&#20998;&#20301;&#25968;&#30340;&#27010;&#24565;&#26368;&#36817;&#25193;&#23637;&#21040;&#22810;&#21464;&#37327;&#20998;&#24067;&#65292;&#20294;&#26159;&#20851;&#20110;&#27969;&#24418;&#19978;&#22810;&#21464;&#37327;&#20998;&#24067;&#30340;QR&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#65292;&#23613;&#31649;&#35768;&#22810;&#37325;&#35201;&#30340;&#24212;&#29992;&#26412;&#36136;&#19978;&#28041;&#21450;&#20998;&#24067;&#22312;&#29699;&#38754;&#65288;&#27668;&#20505;&#21644;&#22320;&#36136;&#29616;&#35937;&#65289;&#21644;&#29615;&#38754;&#65288;&#34507;&#30333;&#36136;&#20013;&#30340;&#20108;&#38754;&#35282;&#65289;&#31561;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;c-&#20985;&#20989;&#25968;&#65292;&#25105;&#20204;&#26377;&#24847;&#20041;&#22320;&#23450;&#20041;&#20102;&#27969;&#24418;&#19978;&#39640;&#32500;&#21464;&#37327;&#30340;&#26465;&#20214;&#21521;&#37327;&#20998;&#20301;&#25968;&#20989;&#25968;&#65288;M-CVQFs&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20998;&#20301;&#25968;&#20272;&#35745;&#65292;&#22238;&#24402;&#65292;&#24182;&#35745;&#31639;&#26465;&#20214;&#32622;&#20449;&#38598;&#21644;&#20284;&#28982;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#20998;&#20301;&#25968;&#21547;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantile regression (QR) is a statistical tool for distribution-free estimation of conditional quantiles of a target variable given explanatory features. QR is limited by the assumption that the target distribution is univariate and defined on an Euclidean domain. Although the notion of quantiles was recently extended to multi-variate distributions, QR for multi-variate distributions on manifolds remains underexplored, even though many important applications inherently involve data distributed on, e.g., spheres (climate and geological phenomena), and tori (dihedral angles in proteins). By leveraging optimal transport theory and c-concave functions, we meaningfully define conditional vector quantile functions of high-dimensional variables on manifolds (M-CVQFs). Our approach allows for quantile estimation, regression, and computation of conditional confidence sets and likelihoods. We demonstrate the approach's efficacy and provide insights regarding the meaning of non-Euclidean quantile
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#23637;&#24320;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;SURF&#65289;&#26159;&#19968;&#31181;&#25193;&#23637;&#20102;&#31639;&#27861;&#23637;&#24320;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#38656;&#35201;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#21644;&#20445;&#25345;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#29305;&#24615;&#30340;&#21516;&#26102;&#65292;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.15371</link><description>&lt;p&gt;
&#38543;&#26426;&#23637;&#24320;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Unrolled Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15371
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23637;&#24320;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;SURF&#65289;&#26159;&#19968;&#31181;&#25193;&#23637;&#20102;&#31639;&#27861;&#23637;&#24320;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#38656;&#35201;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#21644;&#20445;&#25345;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#29305;&#24615;&#30340;&#21516;&#26102;&#65292;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#23637;&#24320;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20248;&#21270;&#33539;&#24335;&#65292;&#23427;&#23558;&#25130;&#26029;&#30340;&#36845;&#20195;&#31639;&#27861;&#23637;&#24320;&#20026;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#23637;&#24320;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;SURF&#65289;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#31639;&#27861;&#23637;&#24320;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#21152;&#24555;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#31181;&#25193;&#23637;&#38754;&#20020;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#38656;&#35201;&#23558;&#25972;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#32473;&#23637;&#24320;&#30340;&#20248;&#21270;&#22120;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32473;&#27599;&#20010;&#23637;&#24320;&#23618;&#25552;&#20379;&#38543;&#26426;&#23567;&#25209;&#37327;&#25968;&#25454;&#24182;&#26045;&#21152;&#19979;&#38477;&#32422;&#26463;&#26469;&#35299;&#20915;&#21069;&#19968;&#20010;&#25361;&#25112;&#65292;&#20197;&#20445;&#35777;&#20854;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#23637;&#24320;&#26550;&#26500;&#20013;&#23637;&#24320;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#21518;&#19968;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#20445;&#25345;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#35757;&#32451;&#20998;&#24067;&#24335;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#23637;&#24320;&#20248;&#21270;&#22120;&#25910;&#25947;&#20110;&#36817;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to federated learning in order to expedite its convergence. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to guarantee its convergence. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22810;&#26679;&#24615;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26412;&#36136;&#12290;&#23427;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#22312;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#26679;&#24615;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#29702;&#35770;&#23545;&#20110;&#25552;&#39640;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2301.03962</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#30340;&#32479;&#19968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Unified Theory of Diversity in Ensemble Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22810;&#26679;&#24615;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26412;&#36136;&#12290;&#23427;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#22312;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#26679;&#24615;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#29702;&#35770;&#23545;&#20110;&#25552;&#39640;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#22810;&#26679;&#24615;&#30340;&#26412;&#36136;&#12290;&#36825;&#20010;&#25361;&#25112;&#34987;&#31216;&#20026;&#38598;&#25104;&#23398;&#20064;&#30340;&#22307;&#26479;&#65292;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#24050;&#32463;&#26377;30&#22810;&#24180;&#20102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#23454;&#38469;&#19978;&#26159;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#19968;&#20010;&#38544;&#34255;&#32500;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#26063;&#31934;&#30830;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#22810;&#26679;&#24615;&#20998;&#35299;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#20363;&#22914;&#24179;&#26041;&#25439;&#22833;&#12289;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27850;&#26494;&#25439;&#22833;&#12290;&#23545;&#20110;&#27809;&#26377;&#21487;&#21152;&#24615;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;0/1&#25439;&#22833;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#37327;&#21270;&#22810;&#26679;&#24615;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#20381;&#36182;&#20110;&#26631;&#31614;&#20998;&#24067;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#22810;&#26679;&#24615;&#26159;&#27169;&#22411;&#25311;&#21512;&#24230;&#30340;&#24230;&#37327;&#65292;&#19982;&#20559;&#24046;&#21644;&#26041;&#24046;&#20855;&#26377;&#30456;&#21516;&#30340;&#24847;&#20041;&#65292;&#20294;&#32771;&#34385;&#20102;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#24212;&#35813;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#19988;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;(iPC)&#65292;&#36890;&#36807;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;iPC&#30456;&#27604;&#21407;&#22987;&#31639;&#27861;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2212.00720</link><description>&lt;p&gt;
&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#21644;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#19988;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;(iPC)&#65292;&#36890;&#36807;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;iPC&#30456;&#27604;&#21407;&#22987;&#31639;&#27861;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#26681;&#26893;&#20110;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38750;&#24120;&#20302;&#25928;&#19988;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#27604;&#21407;&#22987;&#31639;&#27861;&#26356;&#39640;&#25928;&#12289;&#26356;&#31283;&#23450;&#19988;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#34987;&#31216;&#20026;&#36882;&#22686;&#39044;&#27979;&#32534;&#30721;(iPC)&#65292;&#20063;&#27604;&#21407;&#22987;&#31639;&#27861;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;iPC&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#21644;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#65292;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#22823;&#37327;&#36229;&#21442;&#25968;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#32447;&#24615;&#25104;&#20687;&#36870;&#38382;&#39064;&#26102;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#21152;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2211.13692</link><description>&lt;p&gt;
&#26159;&#31283;&#23450;&#36824;&#26159;&#19981;&#31283;&#23450;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65306;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
To be or not to be stable, that is the question: understanding neural networks for inverse problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#32447;&#24615;&#25104;&#20687;&#36870;&#38382;&#39064;&#26102;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#21152;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#21495;&#21644;&#22270;&#20687;&#22788;&#29702;&#20013;&#20986;&#29616;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30149;&#24577;&#26465;&#20214;&#20250;&#22312;&#35299;&#20013;&#25918;&#22823;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23545;&#25968;&#25454;&#25200;&#21160;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#22312;&#38750;&#27424;&#23450;&#24773;&#20917;&#19979;&#65292;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#32447;&#24615;&#25104;&#20687;&#36870;&#38382;&#39064;&#26102;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32593;&#32476;&#35757;&#32451;&#26399;&#38388;&#20174;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#26041;&#26696;&#20013;&#32487;&#25215;&#30340;&#27491;&#21017;&#21270;&#23646;&#24615;&#21644;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#30340;&#39044;&#22788;&#29702;&#31283;&#23450;&#21270;&#31639;&#23376;&#26469;&#22686;&#21152;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#24182;&#20445;&#25345;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22270;&#20687;&#21435;&#27169;&#31946;&#30340;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of linear inverse problems arising, for example, in signal and image processing is a challenging problem since the ill-conditioning amplifies, in the solution, the noise present in the data. Recently introduced algorithms based on deep learning overwhelm the more traditional model-based approaches in performance, but they typically suffer from instability with respect to data perturbation. In this paper, we theoretically analyze the trade-off between stability and accuracy of neural networks, when used to solve linear imaging inverse problems for not under-determined cases. Moreover, we propose different supervised and unsupervised solutions to increase the network stability and maintain a good accuracy, by means of regularization properties inherited from a model-based iterative scheme during the network training and pre-processing stabilizing operator in the neural networks. Extensive numerical experiments on image deblurring confirm the theoretical results and the effec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#26469;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2205.14839</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Bandits against Arbitrary Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#26469;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#30340;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;S&#26159;&#38382;&#39064;&#38590;&#24230;&#30340;&#21442;&#25968;&#65292;&#35813;&#21442;&#25968;&#23545;&#20110;&#20195;&#29702;&#20154;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#65288;OMD&#65289;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;OMD&#30340;&#20027;&#25511;&#22522;&#30784;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$T^{2/3}$&#26469;&#33258;&#25439;&#22833;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#65292;&#24182;&#23454;&#29616;&#20102;$\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$\rho_T(h^\dagger)$&#26159;&#25439;&#22833;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the adversarial bandit problem against arbitrary strategies, in which $S$ is the parameter for the hardness of the problem and this parameter is not given to the agent. To handle this problem, we adopt the master-base framework using the online mirror descent method (OMD). We first provide a master-base algorithm with simple OMD, achieving $\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$, in which $T^{2/3}$ comes from the variance of loss estimators. To mitigate the impact of the variance, we propose using adaptive learning rates for OMD and achieve $\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$, where $\rho_T(h^\dagger)$ is a variance term for loss estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#21327;&#35758;&#65292;&#21487;&#22312;&#39640;&#36127;&#36733;&#32593;&#32476;&#20013;&#23454;&#29616;&#26368;&#23567;&#21518;&#24724;&#21644;&#39640;&#35889;&#25928;&#30340;&#35889;&#21327;&#20316;&#12290;&#35813;&#21327;&#35758;&#22522;&#20110;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20449;&#36947;&#20998;&#37197;&#21644;&#35775;&#38382;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21333;&#20449;&#36947;&#26426;&#20250;&#36733;&#27874;&#24863;&#30693;&#36827;&#34892;&#20302;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#25293;&#21334;&#12290;</title><link>https://arxiv.org/abs/2111.12581</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#21327;&#20316;&#35889;&#23398;&#20064;&#30340;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Medium Access Control protocol for Collaborative Spectrum Learning in Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#21327;&#35758;&#65292;&#21487;&#22312;&#39640;&#36127;&#36733;&#32593;&#32476;&#20013;&#23454;&#29616;&#26368;&#23567;&#21518;&#24724;&#21644;&#39640;&#35889;&#25928;&#30340;&#35889;&#21327;&#20316;&#12290;&#35813;&#21327;&#35758;&#22522;&#20110;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20449;&#36947;&#20998;&#37197;&#21644;&#35775;&#38382;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21333;&#20449;&#36947;&#26426;&#20250;&#36733;&#27874;&#24863;&#30693;&#36827;&#34892;&#20302;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#25293;&#21334;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#20379;&#29992;&#20110;&#35889;&#21327;&#20316;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#21162;&#21147;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#39640;&#36127;&#36733;&#32593;&#32476;&#20013;&#23454;&#29616;&#35889;&#21327;&#20316;&#65292;&#21516;&#26102;&#23454;&#29616;&#26368;&#23567;&#21518;&#24724;&#21644;&#39640;&#35889;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25317;&#22622;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#36827;&#34892;&#35889;&#21327;&#20316;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#35299;&#20915;&#20102;&#20449;&#36947;&#20998;&#37197;&#21644;&#35775;&#38382;&#35843;&#24230;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#26368;&#20339;&#30340;&#23545;&#25968;&#21518;&#24724;&#29575;&#12290;&#22522;&#20110;&#35813;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#36827;&#34892;&#20998;&#24067;&#24335;&#23454;&#29616;&#35813;&#31639;&#27861;&#12290;&#35813;&#21327;&#35758;&#21033;&#29992;&#21333;&#20449;&#36947;&#26426;&#20250;&#36733;&#27874;&#24863;&#30693;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#19978;&#36827;&#34892;&#20302;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#25293;&#21334;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#38480;&#23450;&#24103;&#22823;&#23567;&#21644;&#25910;&#25947;&#36895;&#24230;&#31561;&#23454;&#38469;&#23454;&#29616;&#38382;&#39064;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#65292;&#23558;&#35813;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#24335;&#20013;&#31561;&#35775;&#38382;&#25511;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there is a growing effort to provide learning algorithms for spectrum collaboration. In this paper we present a medium access control protocol which allows spectrum collaboration with minimal regret and high spectral efficiency in highly loaded networks. We present a fully-distributed algorithm for spectrum collaboration in congested ad-hoc networks. The algorithm jointly solves both the channel allocation and access scheduling problems. We prove that the algorithm has an optimal logarithmic regret. Based on the algorithm we provide a medium access control protocol which allows distributed implementation of the algorithm in ad-hoc networks. The protocol utilizes single-channel opportunistic carrier sensing to carry out a low-complexity distributed auction in time and frequency. We also discuss practical implementation issues such as bounded frame size and speed of convergence. Computer simulations comparing the algorithm to state-of-the-art distributed medium access con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39045;&#20869;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2105.00582</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21487;&#25512;&#24191;&#30340;&#39045;&#20869;&#20986;&#34880;&#26816;&#27979;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning for generalizable intracranial hemorrhage detection and segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.00582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39045;&#20869;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39045;&#20869;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#24182;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#22836;&#37096;CT&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#36825;&#39033;&#22238;&#39038;&#24615;&#30740;&#31350;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;2010&#24180;&#33267;2017&#24180;&#38388;&#25910;&#38598;&#30340;457&#20010;&#20687;&#32032;&#26631;&#35760;&#30340;&#22836;&#37096;CT&#25195;&#25551;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#8220;&#25945;&#24072;&#8221;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#29992;&#23427;&#22312;&#20174;RSNA&#21644;ASNR&#24471;&#21040;&#30340;&#21478;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#25105;&#20204;&#29992;&#36825;&#20010;&#20687;&#32032;&#21644;&#20266;&#26631;&#31614;&#32467;&#21512;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#31532;&#20108;&#20010;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#12290;&#22312;93&#20010;&#25195;&#25551;&#30340;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#23558;&#20998;&#31867;&#65288;n=481&#20010;&#25195;&#25551;&#65289;&#21644;&#20998;&#21106;&#65288;n=23&#20010;&#25195;&#25551;&#65292;&#25110;529&#20010;&#22270;&#20687;&#65289;&#30340;&#27979;&#35797;&#20998;&#21035;&#22312;CQ500&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22312;&#21360;&#24230;&#36827;&#34892;&#30340;481&#20010;&#25195;&#25551;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#19982;&#21482;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To develop and evaluate a semi-supervised learning model for intracranial hemorrhage detection and segmentation on an out-of-distribution head CT evaluation set.   Materials and Methods: This retrospective study used semi-supervised learning to bootstrap performance. An initial "teacher" deep learning model was trained on 457 pixel-labeled head CT scans collected from one US institution from 2010-2017 and used to generate pseudo-labels on a separate unlabeled corpus of 25000 examinations from the RSNA and ASNR. A second "student" model was trained on this combined pixel- and pseudo-labeled dataset. Hyperparameter tuning was performed on a validation set of 93 scans. Testing for both classification (n=481 examinations) and segmentation (n=23 examinations, or 529 images) was performed on CQ500, a dataset of 481 scans performed in India, to evaluate out-of-distribution generalizability. The semi-supervised model was compared with a baseline model trained on only labeled data usin
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#29616;&#26377;&#24320;&#25918;&#24335;CAN&#20837;&#20405;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20998;&#26512;&#20197;&#21450;&#21508;&#33258;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#24403;&#21069;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#30495;&#23454;&#21046;&#36896;&#21644;&#27169;&#25311;&#25915;&#20987;&#65292;&#32570;&#20047;&#20445;&#30495;&#24230;&#21644;&#23545;&#36710;&#36742;&#29289;&#29702;&#24433;&#21709;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2012.14600</link><description>&lt;p&gt;
&#12298;CAN IDS&#25968;&#25454;&#32508;&#21512;&#25351;&#21335;&#21450;ROAD&#25968;&#25454;&#38598;&#20171;&#32461;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Guide to CAN IDS Data &amp; Introduction of the ROAD Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.14600
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#29616;&#26377;&#24320;&#25918;&#24335;CAN&#20837;&#20405;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20998;&#26512;&#20197;&#21450;&#21508;&#33258;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#24403;&#21069;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#30495;&#23454;&#21046;&#36896;&#21644;&#27169;&#25311;&#25915;&#20987;&#65292;&#32570;&#20047;&#20445;&#30495;&#24230;&#21644;&#23545;&#36710;&#36742;&#29289;&#29702;&#24433;&#21709;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#20195;&#36710;&#36742;&#20013;&#26222;&#36941;&#23384;&#22312;&#30528;&#25511;&#21046;&#22120;&#23616;&#22495;&#32593;&#65288;CAN&#65289;&#65292;&#20294;CAN&#32570;&#20047;&#22522;&#26412;&#30340;&#23433;&#20840;&#29305;&#24615;&#65292;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;CAN&#23433;&#20840;&#30740;&#31350;&#39046;&#22495;&#36805;&#36895;&#21457;&#23637;&#65292;&#33268;&#21147;&#20110;&#26816;&#27979;CAN&#19978;&#30340;&#20837;&#20405;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29616;&#26377;&#24320;&#25918;&#24335;CAN&#20837;&#20405;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20998;&#26512;&#20197;&#21450;&#21508;&#33258;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#24403;&#21069;&#20844;&#24320;&#30340;CAN IDS&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#30495;&#23454;&#21046;&#36896;&#65288;&#31616;&#21333;&#20449;&#24687;&#25554;&#20837;&#65289;&#25915;&#20987;&#21644;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#25311;&#25915;&#20987;&#65292;&#32570;&#20047;&#20445;&#30495;&#24230;&#12290;&#24635;&#20307;&#19978;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#26410;&#39564;&#35777;&#25915;&#20987;&#23545;&#36710;&#36742;&#30340;&#29289;&#29702;&#24433;&#21709;&#12290;&#21482;&#26377;&#19968;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20449;&#21495;&#36716;&#25442;&#25968;&#25454;&#65292;&#20294;&#27809;&#26377;&#30456;&#24212;&#30340;&#21407;&#22987;&#20108;&#36827;&#21046;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although ubiquitous in modern vehicles, Controller Area Networks (CANs) lack basic security properties and are easily exploitable. A rapidly growing field of CAN security research has emerged that seeks to detect intrusions on CANs. Producing vehicular CAN data with a variety of intrusions is out of reach for most researchers as it requires expensive assets and expertise. To assist researchers, we present the first comprehensive guide to the existing open CAN intrusion datasets, including a quality analysis of each dataset and an enumeration of each's benefits, drawbacks, and suggested use case. Current public CAN IDS datasets are limited to real fabrication (simple message injection) attacks and simulated attacks often in synthetic data, which lack fidelity. In general, the physical effects of attacks on the vehicle are not verified in the available datasets. Only one dataset provides signal-translated data but not a corresponding raw binary version. Overall, the available data pigeon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2011.08388</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.08388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#38754;&#37096;&#21644;&#38750;&#38754;&#37096;&#29289;&#20307;&#20197;&#21450;&#38750;&#20154;&#31867;&#32452;&#20214;&#30340;&#36890;&#29992;&#22270;&#20687;&#20013;&#30340;&#24773;&#32490;&#12290;&#23427;&#35299;&#20915;&#20102;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65288;IER&#65289;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#20998;&#31867;&#20026;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#25552;&#20986;&#30340;FER&#31995;&#32479;&#36866;&#24212;&#20110;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#35782;&#21035;&#22270;&#20687;&#25152;&#20256;&#36798;&#30340;&#24773;&#32490;&#12290;&#23427;&#23558;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#20026;&#8220;&#24555;&#20048;&#8221;&#65292;&#8220;&#24754;&#20260;&#8221;&#65292;&#8220;&#20167;&#24680;&#8221;&#21644;&#8220;&#24868;&#24594;&#8221;&#31867;&#21035;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#30340;Shap&#65288;DnCShap&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#39640;&#24230;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial &amp; non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#36125;&#21494;&#26031;&#20808;&#39564;&#21644;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#32473;&#23450;&#24809;&#32602;&#39033;&#25152;&#23545;&#24212;&#30340;&#20808;&#39564;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2002.00178</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20808;&#39564;&#21644;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
An Equivalence between Bayesian Priors and Penalties in Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2002.00178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#36125;&#21494;&#26031;&#20808;&#39564;&#21644;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#32473;&#23450;&#24809;&#32602;&#39033;&#25152;&#23545;&#24212;&#30340;&#20808;&#39564;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#36890;&#36807;&#35843;&#33410;&#26576;&#20123;&#21442;&#25968;&#20540;&#30340;&#24809;&#32602;&#39033;&#26469;&#20248;&#21270;&#27010;&#29575;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#27491;&#21017;&#21270;&#39033;&#22312;&#21464;&#20998;&#25512;&#29702;&#20013;&#26159;&#33258;&#28982;&#22320;&#20986;&#29616;&#30340;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#26041;&#27861;&#65306;&#35201;&#20248;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20102;&#36817;&#20284;&#21518;&#39564;&#19982;&#36125;&#21494;&#26031;&#20808;&#39564;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#39033;&#12290;&#25105;&#20204;&#23436;&#20840;&#25551;&#36848;&#20102;&#36825;&#20010;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#19982;&#32473;&#23450;&#24809;&#32602;&#39033;&#30456;&#23545;&#24212;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#29305;&#24449;&#21270;&#21487;&#20197;&#29992;&#26469;&#21457;&#29616;&#23545;&#24809;&#32602;&#20989;&#25968;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#20445;&#25345;&#25972;&#20010;&#36807;&#31243;&#20855;&#26377;&#36125;&#21494;&#26031;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, it is common to optimize the parameters of a probabilistic model, modulated by an ad hoc regularization term that penalizes some values of the parameters. Regularization terms appear naturally in Variational Inference, a tractable way to approximate Bayesian posteriors: the loss to optimize contains a Kullback--Leibler divergence term between the approximate posterior and a Bayesian prior. We fully characterize the regularizers that can arise according to this procedure, and provide a systematic way to compute the prior corresponding to a given penalty. Such a characterization can be used to discover constraints over the penalty function, so that the overall procedure remains Bayesian.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.16803</link><description>&lt;p&gt;
PBSCSR&#65306;&#38050;&#29748;&#40657;&#24066;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset. (arXiv:2401.16803v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#30740;&#31350;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26082;&#20687;MNIST&#19968;&#26679;&#26131;&#20110;&#33719;&#21462;&#65292;&#21448;&#20687;ImageNet&#19968;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;IMSLP&#30340;&#38050;&#29748;&#20048;&#35889;&#22270;&#20687;&#20013;&#37319;&#26679;&#22266;&#23450;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#29255;&#27573;&#12290;&#25968;&#25454;&#38598;&#26412;&#36523;&#21253;&#21547;40,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;9&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;100,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;100&#20998;&#31867;&#20219;&#21153;&#65292;&#36824;&#26377;29,310&#20010;&#26080;&#26631;&#31614;&#30340;&#21487;&#21464;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#26631;&#35760;&#25968;&#25454;&#20197;&#19982;MNIST&#22270;&#20687;&#31867;&#20284;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#26497;&#20854;&#26041;&#20415;&#22320;&#21487;&#35270;&#21270;&#12289;&#25805;&#20316;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#65292;&#20197;&#20801;&#35768;&#35775;&#38382;IMSLP&#19978;&#30340;&#21407;&#22987;&#20048;&#35889;&#22270;&#20687;&#21644;&#20854;&#20182;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#20010;&#21487;&#20197;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is "as accessible as MNIST and as challenging as ImageNet." To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the data
&lt;/p&gt;</description></item><item><title>cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16356</link><description>&lt;p&gt;
cDVGAN: &#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25925;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16356
&lt;/p&gt;
&lt;p&gt;
cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30495;&#23454;&#30340;&#26102;&#38388;&#22495;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#35266;&#27979;&#21644;GW&#25506;&#27979;&#22120;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#25512;&#36827;GW&#25968;&#25454;&#20998;&#26512;&#12290;&#27169;&#25311;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#29992;&#20110;&#20449;&#21495;&#25628;&#32034;&#30340;&#25968;&#25454;&#38598;&#65292;&#24179;&#34913;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39564;&#35777;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cDVGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#30340;&#26032;&#22411;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#34920;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#30340;&#22810;&#31181;&#31867;&#21035;&#30340;&#26102;&#38388;&#22495;&#35266;&#27979;&#12290;cDVGAN&#36824;&#21487;&#20197;&#36890;&#36807;&#22312;&#26465;&#20214;&#31867;&#21035;&#21521;&#37327;&#20013;&#36827;&#34892;&#25554;&#20540;&#29983;&#25104;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24191;&#20041;&#28151;&#21512;&#26679;&#26412;&#12290;cDVGAN&#22312;&#20856;&#22411;&#30340;GANs&#30340;&#20108;&#20154;&#23545;&#25239;&#21338;&#24328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#21442;&#19982;&#32773;&#65292;&#20854;&#20013;&#19968;&#20010;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#25552;&#20379;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15879</link><description>&lt;p&gt;
&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#31639;&#27861;: lil'HDoC
&lt;/p&gt;
&lt;p&gt;
lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap. (arXiv:2401.15879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#33218;&#35782;&#21035;&#65288;GAI&#65289;&#26159;&#19968;&#20010;&#32431;&#25506;&#32034;&#24615;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#23398;&#20064;&#22120;&#20250;&#22312;&#30830;&#23450;&#19968;&#20010;&#33218;&#26159;&#22909;&#33218;&#26102;&#31435;&#21363;&#36755;&#20986;&#35813;&#33218;&#12290;&#22909;&#33218;&#34987;&#23450;&#20041;&#20026;&#26399;&#26395;&#22238;&#25253;&#22823;&#20110;&#31561;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#33218;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;GAI&#38382;&#39064;&#65292;&#35813;&#38388;&#38553;&#25351;&#30340;&#26159;&#33218;&#30340;&#26399;&#26395;&#22238;&#25253;&#19982;&#32473;&#23450;&#38408;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#26032;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;HDoC&#31639;&#27861;&#30340;&#24635;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#65292;lil'HDoC&#31639;&#27861;&#36755;&#20986;&#30340;&#31532;&#19968;&#20010;&#955;&#33218;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21407;&#22987;HDoC&#31639;&#27861;&#30456;&#27604;&#20165;&#26377;&#24494;&#23567;&#30340;&#24046;&#24322;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good arm identification (GAI) is a pure-exploration bandit problem in which a single learner outputs an arm as soon as it is identified as a good arm. A good arm is defined as an arm with an expected reward greater than or equal to a given threshold. This paper focuses on the GAI problem under a small threshold gap, which refers to the distance between the expected rewards of arms and the given threshold. We propose a new algorithm called lil'HDoC to significantly improve the total sample complexity of the HDoC algorithm. We demonstrate that the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded by the original HDoC algorithm, except for one negligible term, when the distance between the expected reward and threshold is small. Extensive experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.15604</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20998;&#25968;&#20272;&#35745;&#65306;&#20248;&#21270;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization. (arXiv:2401.15604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19982;GANs&#30456;&#23218;&#32654;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#20445;&#30495;&#24230;&#65292;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#26469;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#20197;&#21487;&#35777;&#23454;&#30340;&#20934;&#30830;&#24615;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#20316;&#20026;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#39318;&#35201;&#27493;&#39588;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20998;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#23398;&#20064;&#36807;&#31243;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#26469;&#23558;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#21046;&#23450;&#20026;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30456;&#27604;&#65292;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26080;&#30028;&#36755;&#20837;&#65292;&#21521;&#37327;&#20540;&#36755;&#20986;&#21644;&#39069;&#22806;&#30340;&#26102;&#38388;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03506</link><description>&lt;p&gt;
DiarizationLM: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#30446;&#26631;&#65292;&#22914;&#25913;&#21892;&#20998;&#31163;&#23545;&#35805;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#65292;&#25110;&#20943;&#23569;&#35789;&#32423;&#20998;&#31163;&#38169;&#35823;&#29575;&#65288;WDER&#65289;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#32039;&#20945;&#30340;&#25991;&#26412;&#26684;&#24335;&#65292;&#20854;&#21253;&#21547;&#22312;&#19968;&#20010;&#21487;&#36873;&#25321;&#35843;&#25972;&#30340;LLM&#30340;&#25552;&#31034;&#20013;&#12290;LLM&#30340;&#36755;&#20986;&#21487;&#20197;&#20316;&#20026;&#25152;&#38656;&#25913;&#36827;&#30340;&#31934;&#32454;&#21270;&#20998;&#31163;&#32467;&#26524;&#12290;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;ASR&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#22312;Fisher&#30005;&#35805;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23558;WDER&#38477;&#20302;55.5%&#65292;&#22312;Callhome&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#38477;&#20302;44.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
&lt;/p&gt;</description></item><item><title>&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07930</link><description>&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07930
&lt;/p&gt;
&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#27867;&#21270;&#20102;&#25152;&#26377;&#20043;&#21069;&#32479;&#35745;&#27700;&#21360;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23454;&#36341;&#20013;&#30340;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#23454;&#29616;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#30340;&#20551;&#35774;&#26816;&#39564;&#29615;&#22659;&#19979;&#34920;&#24449;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#20197;&#21450;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#29615;&#22659;&#20013;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#12290;&#22312;&#36755;&#20986;&#26159;$n$&#20010;&#20196;&#29260;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#38656;&#35201;&#20445;&#35777;&#23567;&#30340;&#31532;&#19968;&#31867;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#24314;&#31435;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;$ h ^ {-2} $&#36895;&#29575;&#30456;&#27604;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#27599;&#20010;&#20196;&#29260;&#30340;&#24179;&#22343;&#29109;$h$&#30340;&#36895;&#29575;&#20026;$ \Theta(h ^ {-1} \log (1/h)) $&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#26469;&#38477;&#20302;&#35745;&#31639;&#37327;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.20285</link><description>&lt;p&gt;
&#36890;&#36807;&#20197;&#35745;&#31639;&#20026;&#20195;&#20215;&#21152;&#36895;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Linear Models by Trading off Computation for Uncertainty. (arXiv:2310.20285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#26469;&#38477;&#20302;&#35745;&#31639;&#37327;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20998;&#31867;&#12289;&#26377;&#24207;&#21644;&#36830;&#32493;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;GLMs&#30340;&#31934;&#30830;&#25512;&#26029;&#20195;&#20215;&#22826;&#39640;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#36896;&#25104;&#30340;&#36817;&#20284;&#35823;&#24046;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#27809;&#26377;&#34987;&#32771;&#34385;&#22312;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#36845;&#20195;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#23545;&#36825;&#20010;&#35823;&#24046;&#24314;&#27169;&#12290;&#23427;&#20204;&#38750;&#24120;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#30828;&#20214;&#65292;&#26377;&#25928;&#22320;&#22238;&#25910;&#35745;&#31639;&#24182;&#21387;&#32553;&#20449;&#24687;&#65292;&#20197;&#20943;&#23569;GLMs&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#22823;&#22411;&#20998;&#31867;&#38382;&#39064;&#19978;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#20943;&#23569;&#35745;&#31639;&#19982;&#22686;&#21152;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#26435;&#34913;&#26469;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20172</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#29983;&#25104;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;CBS-GPT&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#29983;&#25104;&#65292;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#24341;&#21147;&#27874;&#25506;&#27979;&#26159;&#26410;&#26469;&#21313;&#24180;&#26368;&#21463;&#26399;&#24453;&#30340;&#24341;&#21147;&#27874;&#25506;&#27979;&#39033;&#30446;&#20043;&#19968;&#65292;&#23558;&#25506;&#27979;&#21040;&#20016;&#23500;&#30340;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31354;&#38388;&#24341;&#21147;&#27874;&#27874;&#24418;&#30340;&#31934;&#30830;&#39044;&#27979;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#20108;&#20195;&#26102;&#24310;&#24178;&#28041;&#65288;TDI 2.0&#65289;&#24341;&#36215;&#30340;&#27874;&#24418;&#22797;&#26434;&#24615;&#22686;&#21152;&#32780;&#24102;&#26469;&#30340;&#25968;&#25454;&#22788;&#29702;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBS-GPT&#65288;Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer&#65289;&#30340;&#21487;&#35299;&#37322;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#12290;&#23545;&#20110;&#32039;&#20945;&#20108;&#36827;&#21046;&#31995;&#32479;&#27874;&#24418;&#65292;&#35757;&#32451;&#20102;&#19977;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#36229;&#22823;&#36136;&#37327;&#40657;&#27934;&#20108;&#36827;&#21046;&#65288;MBHB&#65289;&#12289;&#26497;&#31471;&#36136;&#37327;&#27604;&#34701;&#21512;&#65288;EMRIs&#65289;&#21644;&#26143;&#31995;&#20108;&#36827;&#21046;&#65288;GB&#65289;&#30340;&#27874;&#24418;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;98%&#12289;91%&#21644;99%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;CBS-GPT&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#35299;&#37322;&#24615;&#65292;&#20854;&#38544;&#34255;&#21442;&#25968;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#27874;&#24418;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#19981;&#36830;&#32493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.20007</link><description>&lt;p&gt;
&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#27748;&#26222;&#26862;&#37319;&#26679;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#65292;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#25955;&#30340;&#20195;&#29702;&#29615;&#22659;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#19968;&#33268;&#24615;&#23545;&#20449;&#24687;&#27604;&#36827;&#34892;&#20102;&#31934;&#30830;&#20998;&#26512;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#20026;$\widetilde{O}(H\sqrt{d_{l_1}T})$&#65292;&#20854;&#20013;$H$&#20026;&#22238;&#21512;&#38271;&#24230;&#65292;$d_{l_1}$&#20026;&#29615;&#22659;&#31354;&#38388;&#30340;Kolmogorov $l_1$&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#25214;&#21040;&#20102;$d_{l_1}$&#30340;&#20855;&#20307;&#30028;&#38480;&#65292;&#27604;&#22914;&#34920;&#26684;&#12289;&#32447;&#24615;&#21644;&#26377;&#38480;&#28151;&#21512;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
&lt;/p&gt;</description></item><item><title>rTsfNet&#26159;&#19968;&#31181;&#26032;&#30340;DNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#23454;&#29616;&#20102;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19283</link><description>&lt;p&gt;
rTsfNet:&#19968;&#31181;&#20855;&#26377;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#30340;&#22522;&#20110;IMU&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;DNN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition. (arXiv:2310.19283v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19283
&lt;/p&gt;
&lt;p&gt;
rTsfNet&#26159;&#19968;&#31181;&#26032;&#30340;DNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#23454;&#29616;&#20102;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;rTsfNet&#65292;&#19968;&#31181;&#20855;&#26377;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#30340;DNN&#27169;&#22411;&#65292;&#20316;&#20026;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26032;&#22411;DNN&#27169;&#22411;&#12290;rTsfNet&#36890;&#36807;&#22312;DNN&#20869;&#37096;&#25512;&#23548;3D&#26059;&#36716;&#21442;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#24212;&#35813;&#20174;&#20013;&#27966;&#29983;&#29305;&#24449;&#30340;3D&#22522;&#20934;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;MLP&#25512;&#23548;&#26102;&#24207;&#29305;&#24449;&#65288;TSFs&#65289;&#24182;&#23454;&#29616;HAR&#12290;&#23613;&#31649;&#35813;&#27169;&#22411;&#19981;&#20351;&#29992;CNN&#65292;&#22312;&#33391;&#22909;&#31649;&#29702;&#30340;&#22522;&#20934;&#26465;&#20214;&#21644;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;UCI HAR, PAMAP2, Daphnet, &#21644;OPPORTUNITY&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#23567;&#28431;&#27934;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22240;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#30340;&#39278;&#29992;&#27700;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15830</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#28418;&#31227;&#35299;&#37322;&#26041;&#27861;&#23545;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#23567;&#28431;&#27934;&#36827;&#34892;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localization of Small Leakages in Water Distribution Networks using Concept Drift Explanation Methods. (arXiv:2310.15830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#23567;&#28431;&#27934;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22240;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#30340;&#39278;&#29992;&#27700;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#39278;&#29992;&#27700;&#30340;&#21487;&#29992;&#24615;&#23558;&#26469;&#20250;&#20943;&#23569;&#65292;&#20351;&#24471;&#39278;&#29992;&#27700;&#25104;&#20026;&#36234;&#26469;&#36234;&#31232;&#32570;&#30340;&#36164;&#28304;&#12290;&#22823;&#37327;&#30340;&#27700;&#36890;&#36807;&#27700;&#36816;&#36755;&#21644;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;&#27969;&#22833;&#12290;&#28431;&#27934;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#38656;&#27714;&#30340;&#21464;&#21270;&#12290;&#23588;&#20854;&#26159;&#23567;&#28431;&#27934;&#24456;&#38590;&#30830;&#23450;&#65292;&#20294;&#23427;&#20204;&#30340;&#23450;&#20301;&#23545;&#20110;&#36991;&#20813;&#38271;&#26102;&#38388;&#30340;&#27700;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28431;&#27934;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31995;&#32479;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#20363;&#22914;&#23454;&#26102;&#38656;&#27714;&#27979;&#37327;&#21644;&#31934;&#30830;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20165;&#20351;&#29992;&#21387;&#21147;&#27979;&#37327;&#26469;&#36827;&#34892;&#28431;&#27934;&#23450;&#20301;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#24314;&#31435;&#20102;&#27700;&#37197;&#27700;&#32593;&#32476;&#20013;&#30340;&#28431;&#27934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facing climate change the already limited availability of drinking water will decrease in the future rendering drinking water an increasingly scarce resource. Considerable amounts of it are lost through leakages in water transportation and distribution networks. Leakage detection and localization are challenging problems due to the complex interactions and changing demands in water distribution networks. Especially small leakages are hard to pinpoint yet their localization is vital to avoid water loss over long periods of time. While there exist different approaches to solving the tasks of leakage detection and localization, they are relying on various information about the system, e.g. real-time demand measurements and the precise network topology, which is an unrealistic assumption in many real-world scenarios. In contrast, this work attempts leakage localization using pressure measurements only. For this purpose, first, leakages in the water distribution network are modeled employin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08164</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#36807;RLHF&#35843;&#25972;&#30340;&#29256;&#26412;&#30340;&#28608;&#27963;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#21453;&#26144;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#26223;&#65292;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20196;&#29260;-&#22870;&#21169;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#37322;&#23398;&#20064;&#22870;&#21169;&#21644;&#24191;&#27867;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#65292;&#36825;&#20026;&#30830;&#20445;&#25351;&#23450;&#30446;&#26631;&#21644;&#27169;&#22411;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05401</link><description>&lt;p&gt;
Entropy-MCMC: &#36731;&#26494;&#20174;&#24179;&#22374;&#30406;&#22320;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#21518;&#39564;&#20998;&#24067;&#30340;&#36136;&#37327;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;&#22312;&#24615;&#36136;&#19978;&#26159;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#65292;&#23616;&#37096;&#27169;&#24335;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#65292;&#20174;&#21407;&#22987;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19968;&#20123;&#26679;&#26412;&#21487;&#33021;&#20250;&#38519;&#20837;&#8220;&#22351;&#8221;&#27169;&#24335;&#24182;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#20302;&#27867;&#21270;&#35823;&#24046;&#30340;&#8220;&#22909;&#8221;&#27169;&#24335;&#36890;&#24120;&#23384;&#22312;&#20110;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#22374;&#30406;&#22320;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20559;&#32622;&#37319;&#26679;&#26397;&#21521;&#36825;&#20123;&#24179;&#22374;&#21306;&#22495;&#30340;&#21518;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#24341;&#23548;&#21464;&#37327;&#65292;&#20854;&#31283;&#24577;&#20998;&#24067;&#31867;&#20284;&#20110;&#24179;&#28369;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#27809;&#26377;&#23574;&#38160;&#30340;&#27169;&#24577;&#65292;&#20197;&#24341;&#23548;MCMC&#37319;&#26679;&#22120;&#22312;&#24179;&#22374;&#30340;&#30406;&#22320;&#20013;&#37319;&#26679;&#12290;&#36890;&#36807;&#23558;&#27492;&#24341;&#23548;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#19979;&#23454;&#29616;&#39640;&#25928;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.03311</link><description>&lt;p&gt;
&#28145;&#24230;&#21464;&#20998;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;--&#19968;&#31181;&#21464;&#20998;&#25439;&#22833;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#20197;&#20854;&#39640;&#31934;&#24230;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#24456;&#22810;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30456;&#20114;&#26435;&#34913;&#12290;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22120;&#22270;&#65292;&#23427;&#25351;&#23450;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#26102;&#35201;&#20445;&#30041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#35299;&#30721;&#22120;&#22270;&#65292;&#23427;&#20026;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#25512;&#23548;&#20102;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(DVIB)&#12289;beta&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(beta-VAE)&#21644;&#28145;&#24230;&#21464;&#20998;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(DVCCA)&#12290;&#35813;&#26694;&#26550;&#33258;&#28982;&#22320;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01655</link><description>&lt;p&gt;
PolySketchFormer:&#22522;&#20110;&#33609;&#22270;&#30340;&#22810;&#39033;&#24335;&#26680;&#21464;&#25442;&#22120;&#21152;&#36895;Transformer
&lt;/p&gt;
&lt;p&gt;
PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#24555;&#36895;&#27880;&#24847;&#21147;&#26426;&#21046;PolySketchFormer&#65292;&#20197;&#31361;&#30772;Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38590;&#39064;&#65292;&#26080;&#38656;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#25193;&#23637;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#30340;&#29942;&#39048;&#12290;&#23454;&#38469;&#19978;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20551;&#35774;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36817;&#20284;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36755;&#20986;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#29992;&#22810;&#39033;&#24335;&#20989;&#25968;&#21644;&#22810;&#39033;&#24335;&#33609;&#22270;&#26367;&#20195;softmax&#26469;&#31361;&#30772;&#36825;&#20010;&#29702;&#35770;&#38556;&#30861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#22810;&#39033;&#24335;&#26680;&#30340;&#33609;&#22270;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#22810;&#39033;&#24335;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#24555;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#31232;&#30095;&#32467;&#26500;&#65292;&#36825;&#22312;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#24050;&#32463;&#23436;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22359;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22240;&#26524;&#25513;&#30721;&#24212;&#29992;&#20110;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#22320;&#35745;&#31639;$n \times n$&#27880;&#24847;&#21147;&#30697;&#38453;&#24182;&#35745;&#31639;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.16825</link><description>&lt;p&gt;
FENDA-FL&#65306;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#20811;&#26381;&#20020;&#24202;&#29615;&#22659;&#20013;&#25968;&#25454;&#23396;&#31435;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#22312;&#20020;&#24202;&#24212;&#29992;&#30340;FL&#30740;&#31350;&#20013;&#20570;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#23558;FENDA&#26041;&#27861;&#65288;Kim&#31561;&#20154;&#65292;2016&#65289;&#25193;&#23637;&#21040;FL&#30340;&#26041;&#27861;&#12290;&#22312;FLamby&#22522;&#20934;&#65288;du Terrail&#31561;&#20154;&#65292;2022a&#65289;&#21644;GEMINI&#25968;&#25454;&#38598;&#65288;Verma&#31561;&#20154;&#65292;2017&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;FL&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#21407;&#26377;&#30340;FLamby&#22522;&#20934;&#19978;&#34920;&#31034;&#20986;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#22522;&#20934;&#20197;&#21253;&#25324;&#20010;&#24615;&#21270;FL&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;FL&#26816;&#26597;&#28857;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#29615;&#22659;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13838</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#23545;&#32852;&#21512;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21487;&#20197;&#24418;&#25104;&#20856;&#22411;&#30340;&#20080;&#26041;&#24066;&#22330;&#65292;&#20854;&#20013;PS/&#20080;&#23478;&#25968;&#37327;&#36828;&#36828;&#23569;&#20110;&#23458;&#25143;&#31471;/&#21334;&#23478;&#25968;&#37327;&#12290;&#20026;&#20102;&#25913;&#21892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#20026;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#26381;&#21153;&#23450;&#20215;&#12290;&#20215;&#26684;&#24046;&#24322;&#21270;&#22522;&#20110;&#23545;&#32852;&#21512;&#23398;&#20064;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#35745;&#31639;&#36890;&#20449;&#33021;&#21147;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#26435;&#34913;&#12289;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#28608;&#21169;&#26426;&#21046;&#12290;&#30001;&#20110;PDG&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#21322;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08055</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#36827;&#34892;&#31616;&#21333;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#8212;&#8212;&#22312;&#20219;&#20309;&#26102;&#21051;&#65292;&#39044;&#35328;&#26426;&#37117;&#33021;&#32473;&#20986;&#19982;&#30446;&#21069;&#20026;&#27490;&#30475;&#21040;&#30340;&#25152;&#26377;&#31034;&#20363;&#19968;&#33268;&#30340;&#31867;&#20989;&#25968;&#12290;&#35813;&#27169;&#22411;&#26368;&#36817;&#30001;Assos&#31561;&#20154;&#65288;COLT'23&#65289;&#32771;&#34385;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#21160;&#26426;&#26159;&#26631;&#20934;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23376;&#31867;&#30340;Littlestone&#32500;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;Assos&#31561;&#20154;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32473;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#20110;Littlestone&#32500;&#24230;&#20026;d&#30340;&#31867;&#65292;&#26368;&#22810;&#20250;&#29359;C^d&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;C&#26159;&#19968;&#20010;&#26410;&#25351;&#23450;&#30340;&#32477;&#23545;&#24120;&#25968;&#19988;&#22823;&#20110;0&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26356;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#20102;Littlestone&#32500;&#24230;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#20197;&#21450;Assos&#31561;&#20154;&#30340;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C &gt; 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.01070</link><description>&lt;p&gt;
&#24403;&#35299;&#26512;&#24335;&#24494;&#31215;&#20998;&#30772;&#35299;AdaBoost&#23494;&#30721;&#26102;
&lt;/p&gt;
&lt;p&gt;
When Analytic Calculus Cracks AdaBoost Code. (arXiv:2308.01070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#23398;&#20064;&#20013;&#30340;&#22686;&#24378;&#21407;&#29702;&#28041;&#21450;&#23558;&#22810;&#20010;&#24369;&#20998;&#31867;&#22120;&#32452;&#21512;&#20197;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#30340;&#20998;&#31867;&#22120;&#12290;AdaBoost&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#23436;&#32654;&#20363;&#23376;&#12290;&#25105;&#20204;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;AdaBoost&#24182;&#19981;&#30495;&#27491;&#26159;&#19968;&#20010;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;AdaBoost&#21482;&#26159;&#19968;&#31181;&#21517;&#20041;&#19978;&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#30495;&#20540;&#34920;&#26126;&#30830;&#22320;&#35745;&#31639;&#24471;&#21040;&#24369;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#20004;&#31867;&#38382;&#39064;&#26469;&#36827;&#34892;&#65292;&#20197;&#19977;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#29305;&#27530;&#24773;&#20917;&#20026;&#20363;&#65292;&#24182;&#19982;Python&#24211;scikit-learn&#20013;AdaBoost&#31639;&#27861;&#30340;&#23454;&#29616;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16120</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#19982;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#29992;&#20110;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DuNets)&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;DuNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#65292;&#20294;&#38750;&#32447;&#24615;&#38382;&#39064;&#24448;&#24448;&#20250;&#24433;&#21709;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21463;&#20248;&#21270;&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;(RMA)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LSTM-RNN)&#26469;&#27169;&#25311;&#21160;&#37327;&#21152;&#36895;&#36807;&#31243;&#12290;RMA&#27169;&#22359;&#21033;&#29992;LSTM-RNN&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;RMA&#24212;&#29992;&#20110;&#20004;&#31181;&#27969;&#34892;&#30340;DuNets&#8212;&#8212;&#23398;&#20064;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;(LPGD)&#21644;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;(LPD)&#26041;&#27861;&#65292;&#20998;&#21035;&#24471;&#21040;LPGD-RMA&#21644;LPD-RMA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65306;&#38750;&#32447;&#24615;&#21435;&#21367;&#31215;&#38382;&#39064;&#12289;
&lt;/p&gt;
&lt;p&gt;
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04081</link><description>&lt;p&gt;
&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#19979;&#65292;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26657;&#20934;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#26041;&#27861;&#25913;&#36827;&#22522;&#20110;&#35780;&#20998;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#22120;&#20316;&#20026;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#33021;&#22815;&#36798;&#21040;&#39046;&#20808;&#30340;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;SGMs&#25193;&#23637;&#21040;&#22788;&#29702;&#31867;&#26465;&#20214;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#26080;&#26465;&#20214;&#30340;SGM&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#22312;&#35757;&#32451;&#25968;&#37327;&#36739;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26102;&#24182;&#19981;&#24635;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#25105;&#20204;&#35748;&#20026;&#38382;&#39064;&#26681;&#28304;&#22312;&#20110;&#20998;&#31867;&#22120;&#30340;&#19981;&#21487;&#38752;&#26799;&#24230;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35753;&#20998;&#31867;&#22120;&#33258;&#26657;&#20934;&#26469;&#25913;&#36827;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;SGMs&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#30340;&#21407;&#21017;&#23558;&#20998;&#31867;&#22120;&#36716;&#21270;&#20026;&#26080;&#26465;&#20214;SGM&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#37319;&#29992;&#29616;&#26377;&#30340;&#26080;&#26465;&#20214;SGM&#25439;&#22833;&#20989;&#25968;&#26469;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#26465;&#20214;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21512;&#25104;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#31665;&#65292;&#20197;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#19968;&#31181;&#20934;&#30830;&#19988;&#24191;&#27867;&#24212;&#29992;&#20110;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24037;&#20855;&#12290;&#20960;&#21313;&#24180;&#26469;&#65292;ECG&#20197;&#21360;&#21047;&#26684;&#24335;&#35760;&#24405;&#65292;&#24182;&#19988;&#23558;&#23427;&#20204;&#30340;&#25968;&#23383;&#21270;&#22312;&#31639;&#27861;&#24615;&#24515;&#30005;&#22270;&#35786;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29289;&#29702;&#24615;ECG&#23384;&#26723;&#38754;&#20020;&#36864;&#21270;&#39118;&#38505;&#65292;&#20165;&#25195;&#25551;&#21360;&#21047;ECG&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;ECG&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23558;&#32440;&#36136;ECG&#23384;&#26723;&#25968;&#23383;&#21270;&#21644;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;ECG&#23384;&#26723;&#31232;&#32570;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21033;&#29992;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#20266;&#24433;&#30340;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19979;&#30340;&#21512;&#25104;ECG&#22270;&#20687;&#12290;&#21253;&#25324;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#36716;&#25442;&#31561;&#30072;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;&#65292;&#26088;&#22312;&#35299;&#20915;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.12498</link><description>&lt;p&gt;
&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65306;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective and Improved Bounds. (arXiv:2306.12498v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#26377;Shuffled SGD&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21407;&#22987;-&#23545;&#20598;&#35270;&#35282;&#21644;&#25913;&#36827;&#30028;&#38480;&#65292;&#26088;&#22312;&#35299;&#20915;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#26222;&#36941;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#19982;&#27599;&#20010;&#26102;&#26399;&#20174;&#25968;&#25454;&#38598;&#20013;&#26080;&#26367;&#25442;&#38543;&#26426;&#25277;&#26679;&#21644;&#19982;&#65288;&#21487;&#33021;&#30340;&#65289;&#37325;&#25490;&#32451;&#30340;&#32463;&#39564;&#24815;&#20363;&#30456;&#21453;&#65292;SGD&#30340;&#29702;&#35770;&#23545;&#24212;&#36890;&#24120;&#20381;&#36182;&#20110;&#24102;&#26367;&#25442;&#30340;&#25277;&#26679;&#20551;&#35774;&#12290;&#20165;&#26368;&#36817;&#25165;&#20998;&#26512;&#20102;&#37319;&#29992;&#26080;&#26367;&#25442;&#25277;&#26679;&#30340;Shuffled SGD&#12290;&#23545;&#20110;&#20855;&#26377;$n$&#20010;&#32452;&#20214;&#21644;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#20989;&#25968;$L$-&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#20984;&#26377;&#38480;&#21644;&#38382;&#39064;&#65292;&#22312;&#36275;&#22815;&#23567;&#30340;&#27493;&#38271;&#65288;$\mathcal{O}(\frac{1}{nL})$&#65289;&#19979;&#65292;&#23384;&#22312;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30028;&#38480;&#20284;&#20046;&#36807;&#20110;&#24754;&#35266; - &#23454;&#38469;&#19978;&#65292;&#39044;&#27979;&#30340;&#24615;&#33021;&#36890;&#24120;&#19981;&#27604;&#20840;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909; - &#24182;&#19988;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#31526;&#12290;&#20026;&#20102;&#32553;&#23567;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#23558;&#28966;&#28857;&#20174;&#19968;&#33324;&#26377;&#38480;&#21644;&#38382;&#39064;&#38598;&#20013;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD with sampling without replacement -- shuffled SGD -- has been analyzed. For convex finite sum problems with $n$ components and under the $L$-smoothness assumption for each component function, there are matching upper and lower bounds, under sufficiently small -- $\mathcal{O}(\frac{1}{nL})$ -- step sizes. Yet those bounds appear too pessimistic -- in fact, the predicted performance is generally no better than for full gradient descent -- and do not agree with the empirical observations. In this work, to narrow the gap between the theory and practice of shuffled SGD, we sharpen the focus from general finite sum problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11903</link><description>&lt;p&gt;
Deep Fusion&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Deep Fusion: Efficient Network Training via Pre-trained Initializations. (arXiv:2306.11903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12289;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20247;&#22810;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Deep Fusion&#65292;&#19968;&#31181;&#21033;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#30340;&#39640;&#25928;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Deep Fusion&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#21644;T5&#27169;&#22411;&#22823;&#23567;&#19978;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#24182;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Deep Fusion&#26159;&#19968;&#31181;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32500;&#25345;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36234;&#20854;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has made remarkable progress in a wide range of domains, with a particularly notable impact on natural language processing tasks. One of the challenges associated with training deep neural networks is the need for large amounts of computational resources and time. In this paper, we present Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks. % We show that Deep Fusion accelerates the training process, reduces computational requirements, and leads to improved generalization performance on a variety of NLP tasks and T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical and effective approach to reduce the training time and resource consumption while maintaining, or even surpassing, the performance of traditional training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11305</link><description>&lt;p&gt;
&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#29992;&#20110;&#39034;&#24207;&#35270;&#39057;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;(NIR)&#22240;&#20854;&#23558;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#26144;&#23556;&#20989;&#25968;&#36731;&#26494;&#37325;&#26500;&#25968;&#25454;&#30340;&#38750;&#20961;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;NIR&#26041;&#27861;&#20551;&#23450;&#30446;&#26631;&#25968;&#25454;&#21644;&#34920;&#31034;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#32452;&#22797;&#26434;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#21463;&#25345;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#32534;&#30721;&#20250;&#35805;&#20013;&#32047;&#31215;&#21644;&#20256;&#36882;&#22810;&#20010;&#22797;&#26434;&#35270;&#39057;&#25968;&#25454;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;NIR&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;(PFNR)&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33258;&#36866;&#24212;&#21644;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#65292;&#20197;&#32534;&#30721;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32534;&#30721;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25345;&#26377;&#33258;&#30001;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#36845;&#20195;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#22810;&#20010;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.10947</link><description>&lt;p&gt;
&#20351;&#29992;&#36895;&#29575;&#20989;&#25968;&#29702;&#35299;&#25554;&#20540;&#21306;&#38388;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#26032;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#29992;&#23454;&#25968;&#20540;&#65288;&#22914;&#26435;&#37325;&#33539;&#25968;&#65289;&#26469;&#34920;&#24449;&#27169;&#22411;&#30340;&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#23454;&#20540;&#20989;&#25968;&#26469;&#25551;&#36848;&#24179;&#28369;&#24230;&#12290;&#22522;&#20110;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;$\ell_2$-&#35268;&#33539;&#21270;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#19981;&#21464;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#21270;&#65289;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#36807;&#31243;&#65292;&#36825;&#20123;&#36807;&#31243;&#20351;&#20248;&#21270;&#22120;&#20559;&#21521;&#20110;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#65292;&#32780;&#26681;&#25454;&#36825;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#26159;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#25554;&#20540;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.08754</link><description>&lt;p&gt;
ClimSim&#65306;&#29992;&#20110;&#22312;&#28151;&#21512;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27668;&#20505;&#39044;&#27979;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32570;&#20047;&#36275;&#22815;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#19968;&#20010;&#21518;&#26524;&#26159;&#23545;&#20851;&#38190;&#36807;&#31243;&#65288;&#22914;&#26292;&#39118;&#38632;&#65289;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#19981;&#31934;&#30830;&#12290;&#23558;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#24335;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#27668;&#20505;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#12289;&#30701;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#22996;&#25176;&#32473;ML&#20223;&#30495;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#25705;&#23572;&#23450;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28151;&#21512;&#30340;ML-&#29289;&#29702;&#20223;&#30495;&#26041;&#27861;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22521;&#35757;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#19968;&#30452;&#26080;&#27861;&#35775;&#38382;ML&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; ClimSim&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20026;&#28151;&#21512;ML-&#29289;&#29702;&#30740;&#31350;&#32780;&#35774;&#35745;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#27668;&#20505;&#31185;&#23398;&#23478;&#21644;ML&#30740;&#31350;&#20154;&#21592;&#32852;&#21512;&#24320;&#21457;&#30340;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#32452;&#25104;&#65292;&#21253;&#25324;57&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#38548;&#31163;&#20102;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23567;&#21488;&#38454;&#20449;&#21495;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.08191</link><description>&lt;p&gt;
&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Large-scale Spatial Problems with Convolutional Neural Networks. (arXiv:2306.08191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23567;&#21488;&#38454;&#20449;&#21495;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#24471;&#21040;&#20102;&#19981;&#26029;&#22686;&#24378;&#30340;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#20419;&#36827;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#25668;&#20837;&#25968;&#25454;&#37327;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#27491;&#22312;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#65292;&#22240;&#27492;&#65292;&#36716;&#21521;&#25552;&#39640;&#25928;&#29575;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21487;&#20197;&#22312;&#23567;&#21488;&#38454;&#20449;&#21495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#35780;&#20272;&#20219;&#24847;&#22823;&#23567;&#20449;&#21495;&#26102;&#20960;&#20046;&#19981;&#20250;&#21457;&#29983;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#24471;&#21040;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;CNN&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;&#36825;&#26159;&#36801;&#31227;&#23398;&#20064;&#20013;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#19968;&#20010;&#23646;&#24615;&#12290;&#22312;&#22522;&#20110;&#31227;&#21160;&#22522;&#30784;&#26550;&#26500;&#30340;&#38656;&#27714;&#65288;MID&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#32467;&#26524;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;MID&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#30334;&#20010;&#20195;&#29702;&#65292;&#36825;&#22312;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31614;&#21517;&#26465;&#30721;&#26469;&#31283;&#23450;&#21521;&#37327;&#21270;&#22810;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#65292;&#23558;&#22810;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#30340;&#20016;&#23500;&#20449;&#24687;&#21644;&#31283;&#23450;&#21521;&#37327;&#21270;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.03801</link><description>&lt;p&gt;
&#31614;&#21517;&#26465;&#30721;&#20316;&#20026;&#24230;&#37327;&#30340;&#22810;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#30340;&#31283;&#23450;&#21521;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures. (arXiv:2306.03801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31614;&#21517;&#26465;&#30721;&#26469;&#31283;&#23450;&#21521;&#37327;&#21270;&#22810;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#65292;&#23558;&#22810;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#30340;&#20016;&#23500;&#20449;&#24687;&#21644;&#31283;&#23450;&#21521;&#37327;&#21270;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#20037;&#21516;&#35843;&#65288;PH&#65289;&#25552;&#20379;&#20102;&#20960;&#20309;&#25968;&#25454;&#65288;&#20363;&#22914;&#21152;&#26435;&#22270;&#65289;&#30340;&#25299;&#25169;&#25551;&#36848;&#31526;&#65292;&#23427;&#20204;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#23545;&#25200;&#21160;&#31283;&#23450;&#65292;&#24182;&#20855;&#26377;&#35832;&#22914;&#37325;&#26631;&#35760;&#31561;&#19981;&#21464;&#24615;&#12290;&#22823;&#22810;&#25968;PH&#24212;&#29992;&#20851;&#27880;&#19968;&#21442;&#25968;&#24773;&#20917;&#8212;&#8212;&#25551;&#36848;&#31526;&#24635;&#32467;&#25968;&#25454;&#30340;&#25299;&#25169;&#38543;&#30528;&#21333;&#20010;&#24863;&#20852;&#36259;&#22240;&#32032;&#30340;&#28388;&#27874;&#32780;&#21457;&#29983;&#21464;&#21270;&#65307;&#29616;&#22312;&#65292;&#26377;&#21508;&#31181;&#26041;&#27861;&#20351;&#24471;&#19968;&#21442;&#25968;PH&#25551;&#36848;&#31526;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23558;&#36825;&#20123;&#25551;&#36848;&#31526;&#31283;&#23450;&#21521;&#37327;&#21270;&#20026;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#20803;&#32032;&#12290;&#34429;&#28982;&#30001;&#20960;&#20010;&#24863;&#20852;&#36259;&#22240;&#32032;&#36807;&#28388;&#30340;&#25968;&#25454;&#30340;&#22810;&#21442;&#25968;PH&#65288;MPH&#65289;&#32534;&#30721;&#27604;&#20854;&#19968;&#21442;&#25968;&#21516;&#22411;&#30340;&#20449;&#24687;&#26356;&#20016;&#23500;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;MPH&#25551;&#36848;&#31526;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#30340;&#31232;&#32570;&#24615;&#24050;&#32463;&#38480;&#21046;&#20102;MPH&#30340;&#31283;&#23450;&#21521;&#37327;&#21270;&#30340;&#21487;&#29992;&#36873;&#39033;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#35299;&#37322;&#31614;&#21517;&#26465;&#30721;&#26469;&#38598;&#32467;&#20004;&#26041;&#38754;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent famil
&lt;/p&gt;</description></item><item><title>PAGAR&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;IL&#20219;&#21153;&#21644;&#38646;-shot IL&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2306.01731</link><description>&lt;p&gt;
PAGAR: &#29992;&#20027;&#35282;-&#21453;&#27966;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#39535;&#26381;&#36870;&#24378;&#21270;&#23398;&#20064;&#22312;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01731
&lt;/p&gt;
&lt;p&gt;
PAGAR&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;IL&#20219;&#21153;&#21644;&#38646;-shot IL&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#27169;&#20223;&#23398;&#20064;(imitation learning, IL)&#31639;&#27861;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;(inverse reinforcement learning, IRL)&#26469;&#25512;&#26029;&#19987;&#23478;&#20197;&#38544;&#24335;&#26041;&#24335;&#20248;&#21270;&#30340;&#28508;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#22522;&#20110;&#20854;&#23637;&#31034;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#30340;&#22870;&#21169;&#19982;&#30495;&#23454;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#21487;&#33021;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20027;&#35282;-&#21453;&#27966;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;(PAGAR)&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#35774;&#35745;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;IRL-based IL&#20013;&#30340;&#22870;&#21169;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#28385;&#36275;&#30340;&#26465;&#20214;&#65292;PAGAR&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#19968;&#20010;&#22312;&#24213;&#23618;&#20219;&#21153;&#20013;&#25104;&#21151;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22312;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#26469;&#22312;IRL-based IL&#20013;&#23454;&#26045;PAGAR&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;IL&#20219;&#21153;&#21644;&#26377;&#38480;&#28436;&#31034;&#30340;&#36801;&#31227;&#29615;&#22659;&#30340;&#38646;-shot IL&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01237</link><description>&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#20013;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#31639;&#27861;&#24517;&#39035;&#20165;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#20248;&#21270;&#20915;&#31574;&#12290;&#31163;&#32447;&#36172;&#21338;&#20013;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#19988;&#36880;&#28176;&#27969;&#34892;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23454;&#29616;&#20302;&#36125;&#21494;&#26031;&#36951;&#25022;&#24182;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#21033;&#29992;&#39640;&#25928;&#30340;&#38181;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#33719;&#24471;&#20102;&#26356;&#20248;&#30340;&#31163;&#32447;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB&#65288;lower confidence bound&#65289;-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#21512;&#31163;&#32447;&#36172;&#21338;&#20013;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.19510</link><description>&lt;p&gt;
&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#20855;&#26377;&#26377;&#21033;&#30340;&#25439;&#22833;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;ReLU&#32593;&#32476;&#22312;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#35777;&#26126;&#20102;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#36755;&#20837;&#25968;&#25454;&#38598;&#19978;&#65292;&#20108;&#23618;&#30053;&#24494;&#36229;&#21442;&#25968;&#21270;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#20351;&#29992;&#20102;&#21442;&#25968;&#26144;&#23556;&#30340;Jacobian&#30697;&#38453;&#30340;&#31209;&#26469;&#20272;&#35745;&#23616;&#37096;&#21644;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#30340;&#32500;&#24230;&#12290;&#20351;&#29992;&#38543;&#26426;&#20108;&#36827;&#21046;&#30697;&#38453;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22823;&#22810;&#25968;&#28608;&#27963;&#27169;&#24335;&#23545;&#24212;&#30340;&#21442;&#25968;&#21306;&#22495;&#27809;&#26377;&#22351;&#30340;&#21487;&#24494;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#32500;&#36755;&#20837;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#30340;&#28608;&#27963;&#27169;&#24335;&#23454;&#29616;&#39640;&#32500;&#20840;&#23616;&#26497;&#23567;&#20540;&#38598;&#21512;&#32780;&#19981;&#20855;&#26377;&#22351;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#22823;&#22810;&#25968;&#21306;&#22495;&#20855;&#26377;&#23436;&#25972;&#30340;&#31209;&#25110;&#32570;&#20047;&#31209;&#65292;&#20197;&#23454;&#39564;&#30340;&#26041;&#24335;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15613</link><description>&lt;p&gt;
&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;
&lt;/p&gt;
&lt;p&gt;
Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;nD&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#28857;&#20113;&#20998;&#26512;&#20013;&#31561;&#21464;&#20110;&#27491;&#20132;&#36716;&#25442;&#65292;&#21033;&#29992;&#20102;&#36229;&#29699;&#20307;&#21644;&#24120;&#35268;n&#21333;&#24418;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36817;&#26399;&#21457;&#23637;&#30340;&#21487;&#25805;&#32437;3D&#29699;&#24418;&#31070;&#32463;&#20803;&#29702;&#35770;--&#22522;&#20110;&#29699;&#24418;&#20915;&#31574;&#38754;&#30340;SO&#65288;3&#65289;-&#31561;&#21464;&#28388;&#27874;&#22120;&#32452;&#65292;&#23558;&#35813;&#31070;&#32463;&#20803;&#25193;&#23637;&#21040;&#20102;nD&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22534;&#21472;&#22312;&#22810;&#23618;&#20013;&#12290;&#21033;&#29992;ModelNet40&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#28508;&#22312;&#23454;&#29992;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#26469;&#27867;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#24494;&#35843;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2303.17235</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65306;&#36830;&#32493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#26469;&#27867;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#24494;&#35843;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#24773;&#26223;&#20013;&#65292;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#27493;&#39588;&#20013;&#30340;&#21487;&#29992;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#27867;&#21270;&#33258;&#30417;&#30563;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17152</link><description>&lt;p&gt;
&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;MAE&#26500;&#26550;&#19979;&#36890;&#36807;&#21516;&#28304;&#35782;&#21035;&#31561;&#36741;&#21161;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#22686;&#24378;&#19979;&#30456;&#20114;&#20449;&#24687;&#22686;&#21152;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#36890;&#36807;&#38543;&#26426;&#36974;&#30422;&#22270;&#20687;&#34917;&#19969;&#21644;&#37325;&#24314;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MAE&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20173;&#28982;&#26159;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#19981;&#21516;&#20110;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;MAE&#30340;&#26222;&#36941;&#28151;&#21512;&#22686;&#24378;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26420;&#32032;&#28151;&#21512;&#23558;&#30001;&#20110;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#28304;&#35782;&#21035;&#26041;&#27861;&#65292;&#19968;&#31181;&#36741;&#21161;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#19981;&#20165;&#36890;&#36807;&#26126;&#30830;&#35201;&#27714;&#27599;&#20010;&#34917;&#19969;&#35782;&#21035;&#21516;&#28304;&#34917;&#19969;&#26469;&#32531;&#35299;&#30456;&#20114;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25191;&#34892;&#38754;&#21521;&#23545;&#35937;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#23494;&#38598;&#24863;&#30693;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;&#33258;&#32534;&#30721;&#22120;&#65288;MixedAE&#65289;&#22312;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22686;&#24378;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HCNN&#65292;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22312;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15919</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21452;&#26354;&#20960;&#20309;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks. (arXiv:2303.15919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HCNN&#65292;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22312;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25968;&#25454;&#21576;&#29616;&#20986;&#22266;&#26377;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#21487;&#20197;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26159;&#22312;&#36825;&#31181;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#27431;&#20960;&#37324;&#24471;&#39592;&#24178;&#65292;&#24182;&#19988;&#20165;&#22312;&#20219;&#21153;&#22836;&#20013;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#21452;&#26354;&#20960;&#20309;&#30340;&#22909;&#22788;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HCNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#22522;&#20110;&#27931;&#20262;&#20857;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#21367;&#31215;&#23618;&#12289;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#30340;&#26032;&#20844;&#24335;&#12290;&#22312;&#26631;&#20934;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#22312;&#28151;&#21512;&#21644;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#23558;&#26469;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10019</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#21450;&#20854;&#22312;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20998;&#20301;&#25968;&#21644;&#21327;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24179;&#28369;&#36807;&#31243;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#32500;&#25968;&#38477;&#20302;&#21644;&#32602;&#20989;&#25968;&#24179;&#28369;&#31561;&#20004;&#31181;&#24179;&#28369;&#26041;&#27861;&#26469;&#23558;&#26631;&#20934;CRPS&#23398;&#20064;&#26694;&#26550;&#25512;&#24191;&#21040;&#22810;&#20803;&#32500;&#24230;&#20013;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26085;&#21069;&#30005;&#20215;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#65292;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.08918</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26377;&#31526;&#21495;&#20256;&#25773;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24322;&#36136;&#22270;&#20013;&#30340;&#26377;&#31526;&#21495;&#20256;&#25773;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#22270;&#30340;&#26032;&#31574;&#30053;&#65292;&#24182;&#20811;&#26381;&#20102;&#20854;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#36136;&#22270;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#21364;&#24456;&#24046;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#26696;&#12290;&#29305;&#21035;&#22320;&#65292;&#32763;&#36716;&#36793;&#30340;&#31526;&#21495;&#26159;&#22522;&#20110;&#22362;&#23454;&#29702;&#35770;&#22522;&#30784;&#30340;&#24182;&#19988;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20998;&#26512;&#20551;&#23450;&#20102;&#20108;&#20803;&#20998;&#31867;&#22330;&#26223;&#65292;&#22240;&#27492;&#21463;&#21040;&#24212;&#29992;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#30340;&#29702;&#35299;&#25193;&#23637;&#21040;&#22810;&#31867;&#21035;&#24773;&#20917;&#65292;&#24182;&#25351;&#20986;&#20004;&#20010;&#32570;&#28857;&#65306;&#65288;1&#65289;&#22810;&#36339;&#37051;&#23621;&#30340;&#31526;&#21495;&#21462;&#20915;&#20110;&#28040;&#24687;&#20256;&#36882;&#36335;&#24452;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36825;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20363;&#22914;&#65292;&#20914;&#31361;&#35777;&#25454;&#65289;&#65292;&#21487;&#33021;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31867;&#21035;&#30340;&#22270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#32467;&#21512;&#20102;&#21407;&#26377;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20854;&#32570;&#28857;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36136;&#30097;&#20102;&#36882;&#24402;&#21010;&#20998;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.10805</link><description>&lt;p&gt;
&#20851;&#20110;&#36882;&#24402;&#21010;&#20998;&#30340;&#36880;&#28857;&#34892;&#20026;&#21450;&#20854;&#23545;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation. (arXiv:2211.10805v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36136;&#30097;&#20102;&#36882;&#24402;&#21010;&#20998;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#23398;&#20064;&#22312;&#36880;&#28857;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#12290;&#37325;&#35201;&#30340;&#24212;&#29992;&#21253;&#25324;&#24322;&#36136;&#22240;&#26524;&#27835;&#30103;&#25928;&#24212;&#21644;&#21160;&#24577;&#25919;&#31574;&#20915;&#31574;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#20301;&#25968;&#22238;&#24402;&#21644;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#26641;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#26159;&#22312;&#29305;&#23450;&#30340;&#21327;&#21464;&#37327;&#20540;&#19978;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#20915;&#31574;&#26641;&#65288;&#36890;&#36807;&#33258;&#36866;&#24212;&#36882;&#24402;&#21010;&#20998;&#35757;&#32451;&#65289;&#36827;&#34892;&#27492;&#31867;&#30446;&#30340;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#36890;&#36807;&#35777;&#26126;&#23427;&#20204;&#29978;&#33267;&#21487;&#20197;&#22312;&#20462;&#21098;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#33539;&#25968;&#30340;&#22810;&#39033;&#24335;&#25910;&#25947;&#36895;&#29575;&#12290;&#30456;&#21453;&#65292;&#25910;&#25947;&#36895;&#24230;&#21487;&#33021;&#26159;&#22810;&#39033;&#24335;&#23545;&#25968;&#32423;&#21035;&#30340;&#65292;&#25110;&#32773;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#35802;&#23454;&#22238;&#24402;&#26641;&#65292;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20302;&#24615;&#33021;&#30340;&#26641;&#36716;&#21270;&#20026;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#31243;&#65292;&#20294;&#20195;&#20215;&#26159;&#22833;&#21435;&#20102;&#35299;&#37322;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#12290;&#38543;&#26426;&#26862;&#26519;&#30340;&#20004;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#26159;&#23376;&#37319;&#26679;&#21644;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision tree learning is increasingly being used for pointwise inference. Important applications include causal heterogenous treatment effects and dynamic policy decisions, as well as conditional quantile regression and design of experiments, where tree estimation and inference is conducted at specific values of the covariates. In this paper, we call into question the use of decision trees (trained by adaptive recursive partitioning) for such purposes by demonstrating that they can fail to achieve polynomial rates of convergence in uniform norm, even with pruning. Instead, the convergence may be poly-logarithmic or, in some important special cases, such as honest regression trees, fail completely. We show that random forests can remedy the situation, turning poor performing trees into nearly optimal procedures, at the cost of losing interpretability and introducing two additional tuning parameters. The two hallmarks of random forests, subsampling and the random feature selection mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09869</link><description>&lt;p&gt;
RenderDiffusion: &#29992;&#20110;3D&#37325;&#24314;&#12289;&#20462;&#22797;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#25903;&#25345;&#29992;&#20110;3D&#29702;&#35299;&#25152;&#38656;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35282;&#19968;&#33268;&#30340;3D&#29983;&#25104;&#25110;&#21333;&#35270;&#35282;&#29289;&#20307;&#37325;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RenderDiffusion&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;3D&#29983;&#25104;&#21644;&#25512;&#26029;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#29983;&#25104;&#21644;&#28210;&#26579;&#22330;&#26223;&#30340;&#20013;&#38388;&#19977;&#32500;&#34920;&#31034;&#12290;&#36825;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24378;&#21046;&#23454;&#29616;&#20102;&#19968;&#20010;&#24378;&#30340;&#24402;&#32435;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;3D&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;2D&#30417;&#30563;&#12290;&#29983;&#25104;&#30340;3D&#34920;&#31034;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#28210;&#26579;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RenderDiffusion&#22312;FFHQ&#12289;AFHQ&#12289;ShapeNet&#21644;CLEVR&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20102;&#22312;&#29983;&#25104;3D&#22330;&#26223;&#21644;&#20174;2D&#22270;&#20687;&#25512;&#26029;3D&#22330;&#26223;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2210.01426</link><description>&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01426
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#32447;&#35268;&#21010;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#22312;&#32447;&#35268;&#21010;&#65292;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#26469;&#26435;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;MCTS&#22312;&#35768;&#22810;&#31163;&#25955;&#20915;&#31574;&#39046;&#22495;&#65288;&#22914;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#21644;&#23558;&#26827;&#65289;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#32780;&#38024;&#23545;&#36830;&#32493;&#39046;&#22495;&#30340;MCTS&#25193;&#23637;&#20063;&#24050;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#20998;&#25903;&#22240;&#23376;&#21644;&#23548;&#33268;&#25628;&#32034;&#26641;&#22823;&#23567;&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;MCTS&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#23558;&#30456;&#20284;&#29366;&#24577;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#21487;&#20197;&#24471;&#21040;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;CMCGS&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#38543;&#26426;&#21160;&#20316;&#36172;&#21338;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
&lt;/p&gt;</description></item></channel></rss>