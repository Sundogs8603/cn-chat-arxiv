<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#21516;&#20262;&#24310;&#32493;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQs&#65289;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#27169;&#22411;HomoODE&#65292;&#23427;&#32487;&#25215;&#20102;DEQs&#30340;&#39640;&#31934;&#24230;&#24615;&#33021;&#21644;Neural ODEs&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09583</link><description>&lt;p&gt;
&#20004;&#26522;&#30828;&#24065;&#30340;&#20004;&#38754;&#65306;&#36890;&#36807;&#21516;&#20262;&#24310;&#32493;&#36830;&#25509;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation. (arXiv:2310.09583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09583
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21516;&#20262;&#24310;&#32493;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQs&#65289;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#27169;&#22411;HomoODE&#65292;&#23427;&#32487;&#25215;&#20102;DEQs&#30340;&#39640;&#31934;&#24230;&#24615;&#33021;&#21644;Neural ODEs&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQs&#65289;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26159;&#20004;&#31181;&#38544;&#24335;&#27169;&#22411;&#30340;&#20998;&#25903;&#65292;&#20197;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20302;&#20869;&#23384;&#28040;&#32791;&#25104;&#23601;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#34429;&#28982;&#20004;&#32773;&#37117;&#26159;&#38544;&#24335;&#27169;&#22411;&#65292;&#20294;DEQs&#21644;Neural ODEs&#26159;&#20174;&#19981;&#21516;&#30340;&#25968;&#23398;&#24418;&#24335;&#23548;&#20986;&#30340;&#12290;&#21463;&#21516;&#20262;&#24310;&#32493;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20004;&#31181;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#23454;&#38469;&#19978;&#26159;&#21516;&#19968;&#20010;&#30828;&#24065;&#30340;&#20004;&#38754;&#12290;&#21516;&#20262;&#24310;&#32493;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#24212;ODE&#30340;&#35299;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#32473;&#23450;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#27169;&#22411;&#31216;&#20026;HomoODE&#65292;&#23427;&#32487;&#25215;&#20102;DEQs&#30340;&#39640;&#31934;&#24230;&#24615;&#36136;&#21644;Neural ODEs&#30340;&#31283;&#23450;&#24615;&#12290;&#19982;DEQs&#19981;&#21516;&#65292;HomoODE&#36890;&#36807;&#21516;&#20262;&#24310;&#32493;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#38544;&#24335;&#22320;&#35299;&#20915;&#24179;&#34913;&#28857;&#25214;&#23547;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations (Neural ODEs) are two branches of implicit models that have achieved remarkable success owing to their superior performance and low memory consumption. While both are implicit models, DEQs and Neural ODEs are derived from different mathematical formulations. Inspired by homotopy continuation, we establish a connection between these two models and illustrate that they are actually two sides of the same coin. Homotopy continuation is a classical method of solving nonlinear equations based on a corresponding ODE. Given this connection, we proposed a new implicit model called HomoODE that inherits the property of high accuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs, which explicitly solve an equilibrium-point-finding problem via Newton's methods in the forward pass, HomoODE solves the equilibrium-point-finding problem implicitly using a modified Neural ODE via homotopy continuation. Fur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03223</link><description>&lt;p&gt;
TacoGFN: &#38024;&#23545;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet
&lt;/p&gt;
&lt;p&gt;
TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#26159;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#20013;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#30340;&#20998;&#23376;&#20013;&#24456;&#38590;&#23454;&#29616;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#32467;&#21512;&#25913;&#21892;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23558;&#21475;&#34955;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#23450;&#20041;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;TacoGFN&#65292;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#26041;&#27861;&#26469;&#21152;&#24555;&#23545;&#25509;&#24471;&#20998;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;TacoGFN&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20960;&#36718;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25913;&#21892;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#25506;&#32034;&#26356;&#22810;&#30340;&#20998;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02679</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65306;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#20248;&#21270;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#36807;&#31243;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#23454;&#29616;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#21644;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#26469;&#25913;&#21892;&#23398;&#20064;&#20449;&#21495;&#30340;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#39640;&#32500;&#23494;&#24230;&#20989;&#25968;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#30340;&#38543;&#26426;&#36807;&#31243;&#26469;&#27169;&#25311;&#36825;&#20123;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#35757;&#32451;&#30446;&#26631;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#36712;&#36857;&#65292;&#23548;&#33268;&#30001;&#20110;&#20351;&#29992;&#23436;&#25972;&#36712;&#36857;&#21644;&#21482;&#22312;&#32456;&#31471;&#26102;&#38388;&#23384;&#22312;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#20351;&#29992;&#32780;&#20135;&#29983;&#32531;&#24930;&#30340;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#29983;&#25104;&#27969;&#37319;&#26679;&#22120;&#65288;DGFS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#23398;&#20064;&#36807;&#31243;&#21487;&#34892;&#22320;&#20998;&#35299;&#20026;&#30701;&#30340;&#37096;&#20998;&#36712;&#36857;&#27573;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27969;&#20989;&#25968;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20013;&#38388;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#24182;&#20174;&#38750;&#31574;&#30053;&#25506;&#32034;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12815</link><description>&lt;p&gt;
&#29992;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Game Agents with Data Augmentation in Imitation Learning. (arXiv:2309.12815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#35757;&#32451;&#28216;&#25103;&#26234;&#33021;&#20307;&#21644;&#39640;&#25928;&#28216;&#25103;&#29983;&#25104;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#33021;&#21147;&#8212;&#8212;&#22312;&#30456;&#20851;&#20294;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#30340;&#33021;&#21147;&#8212;&#8212;&#23545;&#20110;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#22411;&#21270;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31639;&#27861;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#37319;&#21462;&#26377;&#24847;&#20041;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#21463;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#25104;&#21151;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#30340;&#29366;&#24577;-&#34892;&#21160;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20110;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.07277</link><description>&lt;p&gt;
&#26080;&#20559;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#32780;&#24191;&#21463;&#27426;&#36814;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#29983;&#25104;&#21644;&#20462;&#25913;&#20154;&#33080;&#30340;&#33021;&#21147;&#24050;&#32463;&#25512;&#21160;&#20102;&#23545;&#20351;&#29992;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#24230;&#37327;&#21644;&#29992;&#25143;&#30740;&#31350;&#31561;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#19968;&#32452;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#33080;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data
&lt;/p&gt;</description></item><item><title>R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.03291</link><description>&lt;p&gt;
R2D2: &#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#36817;&#23454;&#26102;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy. (arXiv:2309.03291v1 [astro-ph.IM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03291
&lt;/p&gt;
&lt;p&gt;
R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#23556;&#30005;&#24178;&#28041;&#27979;&#37327;&#65288;RI&#65289;&#22312;&#22825;&#25991;&#23398;&#20013;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#21512;&#25104;&#25104;&#20687;&#12290;R2D2&#20195;&#34920;&#8220;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#21040;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#65292;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#28151;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#12290;&#23427;&#30340;&#37325;&#24314;&#26159;&#30001;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#27531;&#24046;&#22270;&#20687;&#34987;&#20272;&#35745;&#20026;DNN&#30340;&#36755;&#20986;&#65292;&#27599;&#20010;DNN&#37117;&#20197;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#27531;&#24046;&#33039;&#22270;&#29255;&#20316;&#20026;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#30340;&#23398;&#20064;&#29256;&#26412;&#65292;&#20854;&#20013;&#27169;&#22411;&#32452;&#20214;&#20174;&#27531;&#24046;&#33039;&#22270;&#29255;&#20013;&#36845;&#20195;&#22320;&#35782;&#21035;&#20986;&#26469;&#65292;CLEAN&#23601;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;R2D2&#27169;&#22411;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#20998;&#21035;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;DNN&#26550;&#26500;&#65306;&#26631;&#20934;&#30340;U-Net&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23637;&#24320;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;S&#27874;&#27573;&#23545;&#23556;&#30005;&#26143;&#31995;Cygnus~A&#30340;&#39640;&#28789;&#25935;&#24230;&#35266;&#27979;&#20013;&#29992;&#20110;&#21333;&#33394;&#24378;&#24230;&#25104;&#20687;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#27169;&#22411;&#20877;&#24179;&#34913;&#26469;&#20943;&#36731;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31181;&#26063;&#20844;&#24179;&#30340;&#35757;&#32451;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25913;&#36827;&#20102;&#36817;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.08638</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20877;&#24179;&#34913;&#23454;&#29616;&#20844;&#24179;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fair GANs through model rebalancing with synthetic data. (arXiv:2308.08638v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#27169;&#22411;&#20877;&#24179;&#34913;&#26469;&#20943;&#36731;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31181;&#26063;&#20844;&#24179;&#30340;&#35757;&#32451;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25913;&#36827;&#20102;&#36817;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#32780;&#25910;&#38598;&#20195;&#34920;&#36866;&#24403;&#30340;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26082;&#26114;&#36149;&#21448;&#22256;&#38590;&#65292;&#36825;&#23548;&#33268;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#36827;&#32780;&#22312;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#27169;&#22411;&#20998;&#24067;&#26469;&#20943;&#36731;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29616;&#26377;&#22833;&#34913;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#28508;&#31354;&#38388;&#25506;&#32034;&#29983;&#25104;&#22343;&#34913;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#22343;&#34913;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#20943;&#36731;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#22833;&#34913;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#20063;&#33021;&#26174;&#31034;&#20986;&#20844;&#24179;&#24230;&#25351;&#26631;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;FFHQ&#25968;&#25454;&#38598;&#36827;&#34892;&#31181;&#26063;&#20844;&#24179;&#35757;&#32451;&#30340;Stylegan2&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#24182;&#19988;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20844;&#24179;&#24230;&#25351;&#26631;&#19978;&#25552;&#21319;&#20102;&#36817;5&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models require large amounts of training data. This often poses a problem as the collection of datasets can be expensive and difficult, in particular datasets that are representative of the appropriate underlying distribution (e.g. demographic). This introduces biases in datasets which are further propagated in the models. We present an approach to mitigate biases in an existing generative adversarial network by rebalancing the model distribution. We do so by generating balanced data from an existing unbalanced deep generative model using latent space exploration and using this data to train a balanced generative model. Further, we propose a bias mitigation loss function that shows improvements in the fairness metric even when trained with unbalanced datasets. We show results for the Stylegan2 models while training on the FFHQ dataset for racial fairness and see that the proposed approach improves on the fairness metric by almost 5 times, whilst maintaining image qualit
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06668</link><description>&lt;p&gt;
&#26234;&#33021;&#20892;&#19994;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#22522;&#30784;&#30693;&#35782;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06668
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20892;&#19994;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#12289;&#38590;&#20197;&#33719;&#21462;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#32780;&#19988;&#22823;&#22810;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36328;&#36234;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#21508;&#31181;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#23578;&#26410;&#26377;&#22826;&#22810;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.06338</link><description>&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#22823;&#23567;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#21644;&#19968;&#27425;&#35299;&#20915;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#39318;&#27425;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#22823;&#23567;&#19979;&#30028;&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#20943;&#23567;&#32463;&#39564;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#24471;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#38656;&#35201;&#23558;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;n&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#12290;&#36825;&#21551;&#21457;&#20102;&#25105;&#20204;&#22312;&#35299;&#20915;&#23545;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#23545;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22266;&#23450;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#36825;&#31181;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#21487;&#20197;&#21333;&#35843;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20043;&#21576;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.01196</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#36879;&#26126;&#30340;&#25512;&#33616;&#31995;&#32479;: &#29992;&#20110;&#35299;&#37322;&#24615;&#30340;&#36125;&#21494;&#26031;&#22270;&#20687;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#24120;&#25351;&#23548;&#29992;&#25143;&#25214;&#21040;&#30456;&#20851;&#30340;&#20869;&#23481;&#25110;&#20135;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#21644;&#20844;&#27665;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65307;&#20010;&#24615;&#21270;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#25512;&#33616;&#25552;&#20379;&#29702;&#30001;&#12290;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#29992;&#25143;&#21019;&#24314;&#30340;&#35270;&#35273;&#20869;&#23481;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#36873;&#39033;&#65292;&#26377;&#28508;&#21147;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#35299;&#37322;&#25512;&#33616;&#26102;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#21487;&#25345;&#32493;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#30340;&#30899;&#25490;&#25918;&#37327;&#19982;&#23427;&#20204;&#34987;&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20351;&#29992;&#30340;&#26367;&#20195;&#23398;&#20064;&#30446;&#26631;&#19982;&#25490;&#21517;&#26368;&#26377;&#25928;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00788</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#20171;&#32461;&#65306;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#23618;&#20248;&#21270;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24314;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#23427;&#22312;&#26080;&#32447;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20123;&#28608;&#21160;&#20154;&#24515;&#30340;&#21457;&#23637;&#20013;&#21344;&#25454;&#20102;&#20013;&#24515;&#33310;&#21488;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;BLO&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20004;&#20010;&#23618;&#27425;&#65288;&#21363;&#19978;&#23618;&#21644;&#19979;&#23618;&#65289;&#65292;&#20854;&#20013;&#35299;&#20915;&#19978;&#23618;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#19979;&#23618;&#38382;&#39064;&#12290;BLO&#20043;&#25152;&#20197;&#21463;&#21040;&#27426;&#36814;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#22240;&#20026;&#23427;&#22312;&#24314;&#27169;&#28041;&#21450;&#20248;&#21270;&#23884;&#22871;&#30446;&#26631;&#20989;&#25968;&#30340;SP&#21644;ML&#31561;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#12290;BLO&#30340;&#26174;&#33879;&#24212;&#29992;&#33539;&#22260;&#20174;&#26080;&#32447;&#31995;&#32479;&#30340;&#36164;&#28304;&#20998;&#37197;&#21040;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#19968;&#31867;&#22312;SP&#21644;ML&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#21487;&#35299;BLO&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31867;BLO&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#27010;&#24565;&#30340;&#27010;&#36848;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#26368;&#20248;&#24615;&#26465;&#20214;&#12289;&#26631;&#20934;&#31639;&#27861;&#65288;&#21253;&#25324;&#23427;&#20204;&#30340;&#20248;&#21270;&#21407;&#29702;&#21644;&#23454;&#38469;&#23454;&#29616;&#26041;&#27861;&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#33021;&#22815;&#34987;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be levera
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.15043</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36890;&#29992;&#21644;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#24341;&#36215;&#21453;&#24863;&#30340;&#20869;&#23481;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#29983;&#25104;&#12290;&#23613;&#31649;&#22312;&#35268;&#36991;&#36825;&#20123;&#25514;&#26045;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#25152;&#35859;&#30340;&#23545;LLMs&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#20154;&#20026;&#30340;&#24039;&#24605;&#65292;&#23454;&#38469;&#19978;&#24182;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#21518;&#32512;&#65292;&#24403;&#38468;&#21152;&#21040;&#21508;&#31181;&#26597;&#35810;&#19978;&#65292;&#20379;LLM&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#26102;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#20135;&#29983;&#32943;&#23450;&#22238;&#31572;&#65288;&#32780;&#19981;&#26159;&#25298;&#32477;&#22238;&#31572;&#65289;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#19988;&#22312;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.06971</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#20316;&#20026;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#20013;&#36890;&#36807;&#30701;&#24067;&#23572;&#20844;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#12290;&#20316;&#20026;&#38271;&#24230;&#20026;k&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#38271;&#24230;&#20026;k&#30340;&#24067;&#23572;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#22312;&#35299;&#37322;&#30446;&#26631;&#23646;&#24615;&#26041;&#38754;&#30340;&#38169;&#35823;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#36825;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#20855;&#20307;&#30340;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#35813;&#35774;&#32622;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;Answer Set Programming&#20013;&#30340;&#32534;&#30721;&#35745;&#31639;&#19981;&#21516;&#38271;&#24230;&#30340;&#35299;&#37322;&#20844;&#24335;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#26368;&#20934;&#30830;&#30340;&#20844;&#24335;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#20844;&#24335;&#19981;&#19968;&#23450;&#26159;&#29702;&#24819;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#35299;&#37322;&#38271;&#24230;&#12290;&#36890;&#36807;&#38480;&#21046;&#20026;&#26356;&#30701;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#35299;&#37322;&#19981;&#20165;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#65292;&#32780;&#19988;&#20381;&#28982;&#30456;&#24403;&#20934;&#30830;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate explainability via short Boolean formulas in the data model based on unary relations. As an explanation of length k, we take a Boolean formula of length k that minimizes the error with respect to the target attribute to be explained. We first provide novel quantitative bounds for the expected error in this scenario. We then also demonstrate how the setting works in practice by studying three concrete data sets. In each case, we calculate explanation formulas of different lengths using an encoding in Answer Set Programming. The most accurate formulas we obtain achieve errors similar to other methods on the same data sets. However, due to overfitting, these formulas are not necessarily ideal explanations, so we use cross validation to identify a suitable length for explanations. By limiting to shorter formulas, we obtain explanations that avoid overfitting but are still reasonably accurate and also, importantly, human interpretable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03173</link><description>&lt;p&gt;
&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Linear Distance Metric Learning. (arXiv:2306.03173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23398;&#20064;&#20986;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26144;&#23556;&#65292;&#21363;&#20351;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20302;&#31209;&#27169;&#22411;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#20013;&#65292;&#32473;&#23450;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#36866;&#24403;&#30340;&#32447;&#24615;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#65292;&#23613;&#21487;&#33021;&#22320;&#28385;&#36275;&#19968;&#23450;&#30340;&#36317;&#31163;&#26465;&#20214;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#19968;&#31181;&#31616;&#21333;&#20248;&#32654;&#30340;&#26041;&#27861;&#65292;&#23427;&#31616;&#21270;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25968;&#25454;&#26377;&#22122;&#22768;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#30340;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#20197;&#20219;&#24847;&#31934;&#24230;&#23398;&#20064;&#30495;&#23454;&#30340;&#32447;&#24615;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#25130;&#26029;&#20026;&#20302;&#31209;&#27169;&#22411;&#65292;&#21487;&#20197;&#35777;&#26126;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20445;&#25345;&#31934;&#24230;&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20960;&#20010;&#23454;&#39564;&#35266;&#23519;&#25903;&#25345;&#21644;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01423</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20248;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#30452;&#25509;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#31181;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22810;&#31181;&#20248;&#21270;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#26799;&#24230;&#36235;&#21183;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;FAME&#65289;&#65292;&#23427;&#39318;&#27425;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#12290;&#23558;TEMA&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#19982;&#30446;&#21069;&#25152;&#26377;&#20027;&#35201;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;FAME&#20248;&#21270;&#22120;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#65292;PASCAL-VOC&#65292;MS-COCO&#21644;Cityscapes&#12290;
&lt;/p&gt;
&lt;p&gt;
Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#12290;</title><link>http://arxiv.org/abs/2305.16150</link><description>&lt;p&gt;
&#32479;&#19968;GAN&#21644;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#30340;&#31890;&#23376;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#19968;&#20010;&#25512;&#24191;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#26799;&#24230;&#27969;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#24778;&#20154;&#30340;&#24615;&#33021;&#32780;&#26368;&#36817;&#21463;&#21040;&#20851;&#27880;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#26469;&#31227;&#21160;&#31890;&#23376;&#20998;&#24067;&#30340;&#26041;&#27861;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#19982;&#20197;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30456;&#23545;&#31435;&#30340;&#65292;&#21518;&#32773;&#28041;&#21450;&#21040;&#35757;&#32451;&#19968;&#20010;&#21521;&#21069;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#31181;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#31890;&#23376;&#21644;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#22120;&#35757;&#32451;&#20316;&#20026;&#31890;&#23376;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#36825;&#34920;&#26126;&#65292;&#29983;&#25104;&#22120;&#26159;&#20219;&#20309;&#36825;&#26679;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#36873;&#38468;&#20214;&#12290;&#22240;&#27492;&#65292;&#23558;&#29983;&#25104;&#22120;&#38598;&#25104;&#21040;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#22312;&#27809;&#26377;&#29983;&#25104;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;GAN&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#36825;&#20123;&#21407;&#22987;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#25105;&#20204;&#26694;&#26550;&#21487;&#33021;&#24212;&#29992;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle-based deep generative models, such as gradient flows and score-based diffusion models, have recently gained traction thanks to their striking performance. Their principle of displacing particle distributions by differential equations is conventionally seen as opposed to the previously widespread generative adversarial networks (GANs), which involve training a pushforward generator network. In this paper, we challenge this interpretation and propose a novel framework that unifies particle and adversarial generative models by framing generator training as a generalization of particle models. This suggests that a generator is an optional addition to any such generative model. Consequently, integrating a generator into a score-based diffusion model and training a GAN without a generator naturally emerge from our framework. We empirically test the viability of these original models as proofs of concepts of potential applications of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15616</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#36870;&#21644;&#19981;&#21487;&#36870;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#32467;&#26500;&#20801;&#35768;&#35757;&#32451;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32780;&#19981;&#20250;&#36807;&#24230;&#20809;&#28369;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29289;&#29702;&#30340;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#65292;&#23613;&#31649;&#21487;&#36870;&#65288;&#20363;&#22914;&#21704;&#23494;&#39039;&#65289;&#21644;&#19981;&#21487;&#36870;&#65288;&#20363;&#22914;&#25193;&#25955;&#65289;&#29616;&#35937;&#30340;&#25104;&#21151;&#23454;&#20363;&#20135;&#29983;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#26426;&#21046;&#25130;&#28982;&#30456;&#21453;&#65292;&#24182;&#19988;&#30001;&#20110;&#32463;&#39564;&#19978;&#30340;&#31163;&#24320;&#25968;&#23398;&#29702;&#35770;&#32780;&#20986;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#37324;&#20351;&#29992;&#30340;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26694;&#26550;&#20801;&#35768;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#23558;&#24403;&#21069;&#26550;&#26500;&#20013;&#30340;&#31163;&#24320;&#29702;&#35770;&#20869;&#23481;&#25918;&#22312;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#26356;&#22909;&#22320;&#38416;&#26126;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.11650</link><description>&lt;p&gt;
&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#20026;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;EBMs &#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29992;&#20110;&#21487;&#25193;&#23637; EBM &#35757;&#32451;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#65288;DSM&#65289;&#26041;&#27861;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#21040;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65306;&#65288;&#20266;&#65289;Gibbs&#37319;&#26679;&#19982;&#21160;&#37327;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#32463;&#36807;DSM&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#22024;&#26434;&#8221;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22522;&#30784;&#8220;&#24178;&#20928;&#8221;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30456;&#20851;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.05807</link><description>&lt;p&gt;
&#21363;&#20351;&#24456;&#23567;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#21464;&#21270;&#20063;&#20250;&#23548;&#33268;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#21464;&#21270;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#24456;&#24120;&#35265;&#65292;&#20250;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#65306;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#65292;&#20351;&#29992;&#21516;&#26102;&#23384;&#22312;&#36825;&#20004;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#23427;&#20204;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#20102;&#36229;&#20986;&#25968;&#25454;&#38598;&#21644;&#19987;&#38376;&#30340;&#20559;&#24046;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#25581;&#31034;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;1&#65289;&#27169;&#22411;&#21363;&#20351;&#36827;&#34892;&#20102;&#20302;&#20559;&#24046;&#35757;&#32451;&#20063;&#20250;&#23398;&#20064;&#24182;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#32047;&#31215;&#21644;&#32467;&#21512;&#38590;&#20197;&#35299;&#37322;&#30340;&#24369;&#20559;&#24046;&#30340;&#39118;&#38505;&#65307;2&#65289;&#27169;&#22411;&#22312;&#39640;&#12289;&#20302;&#20559;&#24046;&#24773;&#20917;&#19979;&#21487;&#20197;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#65292;&#20294;&#26159;&#22914;&#26524;&#27979;&#35797;&#26679;&#26412;&#26377;&#38169;&#35823;&#30340;&#29305;&#24449;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10549</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20132;&#38598;&#20559;&#24207;&#36890;&#29992;&#38598;&#21512;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A note on the connectedness property of union-free generic sets of partial orders. (arXiv:2304.10549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30701;&#25991;&#25551;&#36848;&#24182;&#35777;&#26126;&#20102;&#22312;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#24341;&#20837;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290; &#36830;&#36890;&#24615;&#23646;&#24615;&#20026;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#25552;&#20379;&#20102;&#32467;&#26500;&#24615;&#30340;&#28145;&#20837;&#35748;&#35782;&#12290;&#36825;&#20123;&#38598;&#21512;&#26159;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#65292;&#23427;&#20204;&#20351;&#29992;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#29702;&#35770;&#20013;&#33258;&#28982;&#20986;&#29616;&#30340;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#19978;&#30340;&#38381;&#21253;&#36816;&#31639;&#36827;&#34892;&#23450;&#20041;&#12290;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#35821;&#35328;&#20013;&#65292;&#36830;&#36890;&#24615;&#30340;&#23646;&#24615;&#21487;&#20197;&#29983;&#21160;&#22320;&#34987;&#35777;&#26126;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#25105;&#20204;&#27809;&#26377;&#35752;&#35770;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;,&#22240;&#27492;&#25105;&#20204;&#25226;&#35777;&#26126;&#25918;&#21040;&#20102;&#36825;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short note describes and proves a connectedness property which was introduced in Blocher et al. [2023] in the context of data depth functions for partial orders. The connectedness property gives a structural insight into union-free generic sets. These sets, presented in Blocher et al. [2023], are defined by using a closure operator on the set of all partial orders which naturally appears within the theory of formal concept analysis. In the language of formal concept analysis, the property of connectedness can be vividly proven. However, since within Blocher et al. [2023] we did not discuss formal concept analysis, we outsourced the proof to this note.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.04273</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#33041;&#26426;&#25509;&#21475;&#22312;&#36710;&#20869;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#27979;&#37327;&#20013;&#30340;&#24212;&#29992;&#65306;&#25968;&#25454;&#38598;&#19982;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Multimodal Brain-Computer Interface for In-Vehicle Driver Cognitive Load Measurement: Dataset and Baselines. (arXiv:2304.04273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25968;&#25454;&#38598;CL-Drive&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#39550;&#39542;&#26102;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#12289;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#12289;&#30382;&#32932;&#30005;&#27963;&#21160;&#65288;EDA&#65289;&#20449;&#21495;&#20197;&#21450;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#12290;&#25968;&#25454;&#37319;&#38598;&#33258;21&#21517;&#34987;&#35797;&#22312;&#27785;&#28024;&#24335;&#36710;&#36742;&#27169;&#25311;&#22120;&#20013;&#39550;&#39542;&#65292;&#21253;&#25324;&#19981;&#21516;&#39550;&#39542;&#26465;&#20214;&#19979;&#30340;&#20219;&#21153;&#65292;&#20197;&#35825;&#21457;&#34987;&#35797;&#30340;&#19981;&#21516;&#35748;&#30693;&#36127;&#33655;&#27700;&#24179;&#65292;&#27599;&#20010;&#20219;&#21153;&#25345;&#32493;3&#20998;&#38047;&#65292;&#20849;9&#20010;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;&#27599;&#20010;&#39550;&#39542;&#21592;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#27599;10&#31186;&#25253;&#21578;&#19968;&#27425;&#20027;&#35266;&#35748;&#30693;&#36127;&#33655;&#12290;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20027;&#35266;&#35748;&#30693;&#36127;&#33655;&#35760;&#24405;&#20316;&#20026;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20108;&#20803;&#21644;&#19977;&#20803;&#26631;&#31614;&#20998;&#24067;&#30340;&#22522;&#20934;&#20998;&#31867;&#32467;&#26524;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#35780;&#20272;&#26631;&#20934;&#65292;&#21363;10&#20493;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#19968;&#27861;&#12290;&#25105;&#20204;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#21644;&#21407;&#22987;&#20449;&#21495;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#19982;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#30456;&#27604;&#20165;&#20351;&#29992;EEG&#25968;&#25454;&#20855;&#26377;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#39550;&#39542;&#21592;&#35748;&#30693;&#36127;&#33655;&#35780;&#20272;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through this paper, we introduce a novel driver cognitive load assessment dataset, CL-Drive, which contains Electroencephalogram (EEG) signals along with other physiological signals such as Electrocardiography (ECG) and Electrodermal Activity (EDA) as well as eye tracking data. The data was collected from 21 subjects while driving in an immersive vehicle simulator, in various driving conditions, to induce different levels of cognitive load in the subjects. The tasks consisted of 9 complexity levels for 3 minutes each. Each driver reported their subjective cognitive load every 10 seconds throughout the experiment. The dataset contains the subjective cognitive load recorded as ground truth. In this paper, we also provide benchmark classification results for different machine learning and deep learning models for both binary and ternary label distributions. We followed 2 evaluation criteria namely 10-fold and leave-one-subject-out (LOSO). We have trained our models on both hand-crafted fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03907</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#32500;&#35889;&#21160;&#24577;&#23884;&#20837;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;Ren&#31561;&#20154;&#24341;&#20837;&#20102;&#35889;&#21160;&#24577;&#23884;&#20837;&#26469;&#24320;&#21457;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#26080;&#31351;&#32500;&#29305;&#24449;&#26469;&#32447;&#24615;&#34920;&#31034;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#32500;&#30340;&#25130;&#26029;&#36924;&#36817;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24050;&#30693;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#20013;&#30340;&#26377;&#38480;&#32500;&#36924;&#36817;&#24615;&#36136;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#65288;SDEC&#65289;&#65292;&#24182;&#36827;&#34892;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#30001;&#26377;&#38480;&#32500;&#25130;&#26029;&#24341;&#36215;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#30001;&#26377;&#38480;&#26679;&#26412;&#36924;&#36817;&#24341;&#36215;&#30340;&#32479;&#35745;&#35823;&#24046;&#65292;&#21516;&#26102;&#36827;&#34892;&#25919;&#31574;&#35780;&#20272;&#21644;&#25919;&#31574;&#20248;&#21270;&#30340;&#23454;&#39564;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control is notoriously difficult for stochastic nonlinear systems. Ren et al. introduced Spectral Dynamics Embedding for developing reinforcement learning methods for controlling an unknown system. It uses an infinite-dimensional feature to linearly represent the state-value function and exploits finite-dimensional truncation approximation for practical implementation. However, the finite-dimensional approximation properties in control have not been investigated even when the model is known. In this paper, we provide a tractable stochastic nonlinear control algorithm that exploits the nonlinear dynamics upon the finite-dimensional feature approximation, Spectral Dynamics Embedding Control (SDEC), with an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. We also empirically test the algorithm and compare th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.14496</link><description>&lt;p&gt;
&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#20551;&#35774;&#23384;&#22312;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#26377;&#20851;&#20110;&#27169;&#22411;&#24212;&#22914;&#20309;&#36816;&#34892;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20174;&#35299;&#37322;&#32422;&#26463;&#20013;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;EPAC&#27169;&#22411;&#65288;&#22312;&#26032;&#25968;&#25454;&#26399;&#26395;&#20013;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#27169;&#22411;&#65289;&#26469;&#22238;&#31572;&#21738;&#20123;&#27169;&#22411;&#20250;&#21463;&#30410;&#20110;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23398;&#20064;&#29702;&#35770;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#20110;&#30001;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20449;&#24687;&#32473;&#20986;&#30340;&#35268;&#33539;&#35299;&#37322;&#30340;&#38480;&#21046;&#65288;&#20197;&#20854;Rademacher&#22797;&#26434;&#24230;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21464;&#20998;&#36817;&#20284;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
&lt;/p&gt;</description></item><item><title>AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10512</link><description>&lt;p&gt;
&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10512
&lt;/p&gt;
&lt;p&gt;
AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#24403;&#23384;&#22312;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#20197;&#20197;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#35757;&#32451;&#21152;&#26435;&#30340;&#22686;&#37327;&#26356;&#26032;&#65292;&#20363;&#22914;&#20302;&#31209;&#22686;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#22343;&#21248;&#20998;&#37197;&#21040;&#25152;&#26377;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#26435;&#37325;&#21442;&#25968;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65292;&#24494;&#35843;&#30340;&#34920;&#29616;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaLoRA&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#33258;&#36866;&#24212;&#20998;&#37197;&#26435;&#37325;&#30697;&#38453;&#30340;&#21442;&#25968;&#39044;&#31639;&#12290;&#29305;&#21035;&#22320;&#65292;AdaLoRA&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#21442;&#25968;&#21270;&#20026;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21098;&#26525;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unim
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2302.03616</link><description>&lt;p&gt;
&#28216;&#25103;&#21270;&#33021;&#21542;&#20943;&#36731;mHealth&#24212;&#29992;&#20013;&#33258;&#25105;&#25253;&#21578;&#30340;&#36127;&#25285;&#65311;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#27835;&#30103;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#36890;&#36807;&#35201;&#27714;&#24739;&#32773;&#36890;&#36807;&#24212;&#29992;&#31243;&#24207;&#33258;&#25105;&#25253;&#21578;&#20854;&#29366;&#24577;&#26469;&#34913;&#37327;&#65292;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#20196;&#20154;&#19981;&#30693;&#25152;&#25514;&#24182;&#23548;&#33268;&#22833;&#21435;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#25506;&#35752;&#28216;&#25103;&#21270;&#23545;&#33258;&#25105;&#25253;&#21578;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#26512;&#20809;-&#34880;&#23481;&#31215;&#21464;&#21270;&#20449;&#21495;&#26469;&#35780;&#20272;&#35748;&#30693;&#36127;&#33655;&#65288;CL&#65289;&#12290;&#21033;&#29992;11&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;CL&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#35843;&#26597;&#38382;&#21367;&#65306;&#19968;&#20010;&#26159;&#28216;&#25103;&#21270;&#29256;&#26412;&#65292;&#19968;&#20010;&#26159;&#20256;&#32479;&#29256;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;13&#21517;&#65289;&#22312;&#23436;&#25104;&#35843;&#26597;&#38382;&#21367;&#26102;&#32463;&#21382;&#30340;CL&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#20808;&#22312;&#24212;&#28608;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#22686;&#24378;CL&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;13&#21517;&#21442;&#19982;&#32773;&#20013;&#30340;10&#21517;&#65292;&#20010;&#24615;&#21270;CL&#26816;&#27979;&#22120;&#21487;&#20197;&#23454;&#29616;&#39640;&#20110;0.7&#30340;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;CL&#26041;&#38754;&#65292;&#28216;&#25103;&#21270;&#21644;&#38750;&#28216;&#25103;&#21270;&#30340;&#35843;&#26597;&#38382;&#21367;&#27809;&#26377;&#21306;&#21035;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of digital treatments can be measured by requiring patients to self-report their state through applications, however, it can be overwhelming and causes disengagement. We conduct a study to explore the impact of gamification on self-reporting. Our approach involves the creation of a system to assess cognitive load (CL) through the analysis of photoplethysmography (PPG) signals. The data from 11 participants is utilized to train a machine learning model to detect CL. Subsequently, we create two versions of surveys: a gamified and a traditional one. We estimate the CL experienced by other participants (13) while completing surveys. We find that CL detector performance can be enhanced via pre-training on stress detection tasks. For 10 out of 13 participants, a personalized CL detector can achieve an F1 score above 0.7. We find no difference between the gamified and non-gamified surveys in terms of CL but participants prefer the gamified version.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.11442</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#39640;&#25928;&#27807;&#36890;&#21512;&#20316;&#21518;&#24724;&#26368;&#23567;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Collaborative Regret Minimization in Multi-Armed Bandits. (arXiv:2301.11442v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#21518;&#24724;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the collaborative learning model, which concerns the tradeoff between parallelism and communication overhead in multi-agent multi-armed bandits. For regret minimization in multi-armed bandits, we present the first set of tradeoffs between the number of rounds of communication among the agents and the regret of the collaborative learning process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#20570;&#20915;&#31574;&#21046;&#23450;&#26102;&#65292;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#26080;&#27450;&#35784;&#24178;&#39044;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.14236</link><description>&lt;p&gt;
&#38754;&#26495;&#25968;&#25454;&#20013;&#26080;&#27450;&#35784;&#20915;&#31574;&#21046;&#23450;&#30340;&#30740;&#31350;&#21450;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Strategyproof Decision-Making in Panel Data Settings and Beyond. (arXiv:2211.14236v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#20570;&#20915;&#31574;&#21046;&#23450;&#26102;&#65292;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#26080;&#27450;&#35784;&#24178;&#39044;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38754;&#26495;&#25968;&#25454;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#24471;&#21040;&#20102;&#22810;&#20010;&#21333;&#20301;&#65288;&#25110;&#20195;&#29702;&#20154;&#65289;&#30340;&#26377;&#22122;&#22768;&#12289;&#37325;&#22797;&#30340;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#20854;&#20013;&#23384;&#22312;&#19968;&#20010;&#24178;&#39044;&#21069;&#26399;&#65292;&#24403;&#20915;&#31574;&#32773;&#35266;&#23519;&#27599;&#20010;&#21333;&#20301;&#30340;&#32467;&#26524;&#21518;&#65292;&#20250;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20026;&#27599;&#20010;&#21333;&#20301;&#20998;&#37197;&#19968;&#20010;&#22788;&#29702;&#12290;&#19982;&#20256;&#32479;&#30340;&#35774;&#32622;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20801;&#35768;&#29983;&#25104;&#38754;&#26495;&#25968;&#25454;&#30340;&#21333;&#20301;&#37319;&#21462;&#31574;&#30053;&#65292;&#21363;&#21333;&#20301;&#21487;&#33021;&#20250;&#20462;&#25913;&#20854;&#24178;&#39044;&#21069;&#30340;&#32467;&#26524;&#20197;&#33719;&#24471;&#26356;&#29702;&#24819;&#30340;&#24178;&#39044;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#26080;&#27450;&#35784;&#30340;&#24178;&#39044;&#31574;&#30053;&#65292;&#20063;&#23601;&#26159;&#19968;&#20010;&#33021;&#22815;&#23545;&#21333;&#20301;&#36827;&#34892;&#27491;&#30830;&#24178;&#39044;&#30340;&#31574;&#30053;&#65292;&#26080;&#35770;&#21333;&#20301;&#26159;&#21542;&#36827;&#34892;&#20102;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#26465;&#20214;&#26469;&#21028;&#26029;&#26080;&#27450;&#35784;&#30340;&#24178;&#39044;&#31574;&#30053;&#26159;&#21542;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;&#38381;&#21512;&#24418;&#24335;&#30340;&#26080;&#27450;&#35784;&#26426;&#21046;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#24212;&#29992;&#20110;&#21171;&#21160;&#21147;&#24066;&#22330;&#20449;&#21495;&#30340;&#25112;&#30053;&#24615;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the classical problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their correct interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multicl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.07864</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#23436;&#25972;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#31867;&#20284; CLIP &#30340;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#32852;&#37030;&#25552;&#31034;&#35843;&#20248;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20026;&#20854;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#23427;&#21253;&#25324;&#20803;&#25552;&#31034;&#65292;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#19968;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#26381;&#21153;&#22120;&#38543;&#26426;&#29983;&#25104;&#19968;&#32452;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25152;&#26377;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2206.14203</link><description>&lt;p&gt;
&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Latent Combinational Game Design. (arXiv:2206.14203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25511;&#21046;&#27599;&#20010;&#28216;&#25103;&#22312;&#28151;&#21512;&#28216;&#25103;&#20013;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28508;&#22312;&#32452;&#21512;&#28216;&#25103;&#35774;&#35745;&#8221;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#23558;&#32473;&#23450;&#30340;&#19968;&#32452;&#28216;&#25103;&#28151;&#21512;&#21040;&#25152;&#38656;&#32452;&#21512;&#20013;&#20197;&#29983;&#25104;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (GMVAEs) &#23545; VAE &#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#35757;&#32451;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#28216;&#25103;&#30340;&#27700;&#24179;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#28151;&#21512;&#28216;&#25103;&#23450;&#20041;&#20026;&#36825;&#20123;&#32452;&#20214;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#26032;&#28216;&#25103;&#65292;&#24182;&#25511;&#21046;&#28151;&#21512;&#20013;&#27599;&#20010;&#28216;&#25103;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26377;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25193;&#23637;&#20197;&#21069;&#30340;&#28151;&#21512;&#24037;&#20316;&#65292;&#24182;&#19982; GMVAE &#36827;&#34892;&#27604;&#36739;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#28151;&#21512;&#26465;&#20214; GMVAE (CGMVAE) &#32467;&#26500;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#25972;&#20010;&#28151;&#21512;&#27700;&#24179;&#21644;&#24067;&#23616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#36848;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#25353;&#25351;&#23450;&#32452;&#21512;&#28151;&#21512;&#30340;&#21487;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;&#24179;&#21488;&#28216;&#25103;&#21644;&#22320;&#19979;&#22478;&#31867;&#28216;&#25103;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present latent combinational game design -- an approach for generating playable games that blend a given set of games in a desired combination using deep generative latent variable models. We use Gaussian Mixture Variational Autoencoders (GMVAEs) which model the VAE latent space via a mixture of Gaussian components. Through supervised training, each component encodes levels from one game and lets us define blended games as linear combinations of these components. This enables generating new games that blend the input games and controlling the relative proportions of each game in the blend. We also extend prior blending work using conditional VAEs and compare against the GMVAE and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture which lets us generate whole blended levels and layouts. Results show that the above approaches can generate playable games that blend the input games in specified combinations. We use both platformers and dungeon-based games to demonst
&lt;/p&gt;</description></item></channel></rss>